{
  "index": 30719,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " a",
            " switch",
            " in",
            " settings",
            ".\"",
            "According",
            " to",
            " published",
            " reports",
            " by",
            " Bloomberg",
            " News",
            " on",
            " March",
            " ",
            "30",
            ",",
            " ",
            "202",
            "2",
            ",",
            " Apple",
            " turned",
            " over",
            " data",
            " such",
            " as",
            " phone",
            " numbers",
            ",",
            " physical",
            " addresses",
            ",",
            " and",
            " IP",
            " addresses",
            " to",
            " hackers",
            " posing",
            " as",
            " law",
            " enforcement",
            " officials",
            " using",
            " forged",
            " documents",
            ".",
            " The",
            " law",
            " enforcement",
            " requests",
            " sometimes",
            " included",
            " forged",
            " signatures",
            " of",
            " real",
            " or",
            " fictional",
            " officials",
            ".",
            " When"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " a",
            " switch",
            " in",
            " settings",
            ".\"",
            "According",
            " to",
            " published",
            " reports",
            " by",
            " Bloomberg",
            " News",
            " on",
            " March",
            " ",
            "30",
            ",",
            " ",
            "202",
            "2",
            ",",
            " Apple",
            " turned",
            " over",
            " data",
            " such",
            " as",
            " phone",
            " numbers",
            ",",
            " physical",
            " addresses",
            ",",
            " and",
            " IP",
            " addresses",
            " to",
            " hackers",
            " posing",
            " as",
            " law",
            " enforcement",
            " officials",
            " using",
            " forged",
            " documents",
            ".",
            " The",
            " law",
            " enforcement",
            " requests",
            " sometimes",
            " included",
            " forged",
            " signatures",
            " of",
            " real",
            " or",
            " fictional",
            " officials",
            ".",
            " When"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " FM",
            " (",
            "Frequency",
            " Mod",
            "ulation",
            ")",
            " transmission",
            ",",
            " IR",
            " (",
            "Inf",
            "ra",
            " Red",
            ")",
            " transmission",
            ",",
            " IL",
            " (",
            "Ind",
            "uction",
            " Loop",
            ")",
            " transmission",
            ",",
            " or",
            " other",
            " transmission",
            " methods",
            ".",
            " The",
            " person",
            " who",
            " is",
            " listening",
            " may",
            " use",
            " an",
            " FM",
            "/",
            "IR",
            "/",
            "IL",
            " Receiver",
            " to",
            " tune",
            " into",
            " the",
            " signal",
            " and",
            " listen",
            " at",
            " his",
            "/her",
            " preferred",
            " volume",
            ".",
            "Am",
            "pl",
            "ified",
            " telephone",
            " equipment",
            " ",
            "This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " York",
            ":",
            " St",
            ".",
            " Martin",
            "'s",
            " Press",
            ",",
            " ",
            "199",
            "4",
            ".",
            " Tr",
            "ig",
            "ilio",
            ",",
            " Tony",
            ".",
            " Allen",
            " Gins",
            "berg",
            "'s",
            " Buddhist",
            " Po",
            "etics",
            ".",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " University",
            " Press",
            ",",
            " ",
            "200",
            "7",
            ".",
            " ",
            " Tr",
            "ig",
            "ilio",
            ",",
            " Tony",
            ".",
            " \"",
            "Strange",
            " Prop",
            "hec",
            "ies",
            " A",
            "new",
            "\":",
            " R",
            "ere",
            "ading",
            " Apocalypse",
            " in",
            " Blake",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " York",
            ":",
            " St",
            ".",
            " Martin",
            "'s",
            " Press",
            ",",
            " ",
            "199",
            "4",
            ".",
            " Tr",
            "ig",
            "ilio",
            ",",
            " Tony",
            ".",
            " Allen",
            " Gins",
            "berg",
            "'s",
            " Buddhist",
            " Po",
            "etics",
            ".",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " University",
            " Press",
            ",",
            " ",
            "200",
            "7",
            ".",
            " ",
            " Tr",
            "ig",
            "ilio",
            ",",
            " Tony",
            ".",
            " \"",
            "Strange",
            " Prop",
            "hec",
            "ies",
            " A",
            "new",
            "\":",
            " R",
            "ere",
            "ading",
            " Apocalypse",
            " in",
            " Blake",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "R",
            " v",
            " O",
            "'",
            "Connell",
            " (",
            "184",
            "4",
            ")",
            " ",
            "7",
            " IL",
            "R",
            " ",
            "261",
            ").",
            "There",
            " were",
            " four",
            " kinds",
            " of",
            " alleg",
            "iances",
            " (",
            "R",
            "itt",
            "son",
            " v",
            " St",
            "ord",
            "y",
            " (",
            "185",
            "5",
            ")",
            " ",
            "3",
            " Sm",
            " &",
            " G",
            " ",
            "230",
            ";",
            " De",
            " Ge",
            "er",
            " v",
            " Stone",
            " (",
            "188",
            "2",
            ")",
            " ",
            "22",
            " Ch",
            " D",
            " ",
            "243",
            ";",
            " Isaac",
            "son",
            " v"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Post",
            "card",
            " Book",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " (",
            "200",
            "2",
            ").",
            " ",
            " H",
            "re",
            "ben",
            "i",
            "ak",
            ",",
            " Michael",
            ".",
            " Action",
            " Writing",
            ":",
            " Jack",
            " Ker",
            "ou",
            "ac",
            "'s",
            " Wild",
            " Form",
            ",",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " UP",
            ",",
            " ",
            "200",
            "6",
            ".",
            " Kash",
            "ner",
            ",",
            " Sam",
            ",",
            " When",
            " I",
            " Was",
            " Cool",
            ",",
            " My",
            " Life",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ir",
            "DA",
            ",",
            " though",
            " the",
            " Sharp",
            " AS",
            "K",
            " protocol",
            " was",
            " kept",
            " in",
            " for",
            " compatibility",
            " reasons",
            ".",
            " Unlike",
            " the",
            " Palm",
            " Pilot",
            ",",
            " all",
            " Newton",
            " devices",
            " are",
            " equipped",
            " with",
            " a",
            " standard",
            " PC",
            " Card",
            " expansion",
            " slot",
            " (",
            "two",
            " on",
            " the",
            " ",
            "200",
            "0",
            "/",
            "210",
            "0",
            ").",
            " This",
            " allows",
            " native",
            " modem",
            " and",
            " even",
            " Ethernet",
            " connectivity",
            ";",
            " Newton",
            " users",
            " have",
            " also",
            " written",
            " drivers",
            " for",
            " ",
            "802",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ir",
            "vin",
            " McD",
            "owell",
            " on",
            " the",
            " Confederate",
            " forces",
            " led",
            " by",
            " Gen",
            ".",
            " P",
            ".",
            " G",
            ".",
            " T",
            ".",
            " Bea",
            "ure",
            "gard",
            " near",
            " Washington",
            " was",
            " rep",
            "uls",
            "ed",
            " at",
            " the",
            " First",
            " Battle",
            " of",
            " Bull",
            " Run",
            " (",
            "also",
            " known",
            " as",
            " First",
            " Man",
            "ass",
            "as",
            ").",
            "The",
            " Union",
            " had",
            " the",
            " upper",
            " hand",
            " at",
            " first",
            ",",
            " nearly",
            " pushing",
            " conf",
            "eder",
            "ate",
            " forces",
            " holding",
            " a",
            " defensive",
            " position",
            " into"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ir",
            "vin",
            " McD",
            "owell",
            " on",
            " the",
            " Confederate",
            " forces",
            " led",
            " by",
            " Gen",
            ".",
            " P",
            ".",
            " G",
            ".",
            " T",
            ".",
            " Bea",
            "ure",
            "gard",
            " near",
            " Washington",
            " was",
            " rep",
            "uls",
            "ed",
            " at",
            " the",
            " First",
            " Battle",
            " of",
            " Bull",
            " Run",
            " (",
            "also",
            " known",
            " as",
            " First",
            " Man",
            "ass",
            "as",
            ").",
            "The",
            " Union",
            " had",
            " the",
            " upper",
            " hand",
            " at",
            " first",
            ",",
            " nearly",
            " pushing",
            " conf",
            "eder",
            "ate",
            " forces",
            " holding",
            " a",
            " defensive",
            " position",
            " into"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "202",
            "1",
            ".",
            " pp",
            ".",
            " ",
            "57",
            "–",
            "84",
            ".",
            " https",
            "://",
            "doi",
            "-org",
            ".w",
            "ikip",
            "ed",
            "ial",
            "ibrary",
            ".id",
            "m",
            ".oc",
            "lc",
            ".org",
            "/",
            "10",
            ".",
            "151",
            "5",
            "/",
            "978",
            "311",
            "074",
            "878",
            "9",
            "-",
            "008",
            "Green",
            "berg",
            ",",
            " Joseph",
            " H",
            ".",
            " ",
            "199",
            "7",
            ".",
            " \"",
            "Does",
            " Alta",
            "ic",
            " exist",
            "?",
            "\".",
            " In",
            ":",
            " Ir",
            "Ã©n",
            " Heg",
            "ed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            -0.0,
            0.326,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "|",
            "Dual",
            "-mode",
            " IR",
            ";",
            " Ir",
            "DA",
            " &",
            " SH",
            "ARP",
            " AS",
            "K",
            ",",
            " Local",
            "Talk",
            ",",
            " Audio",
            " I",
            "/O",
            ",",
            " Aut",
            "od",
            "ock",
            "|",
            " colspan",
            "=\"",
            "3",
            "\"",
            " |",
            "Dual",
            "-mode",
            " IR",
            ";",
            "Ir",
            "DA",
            " &",
            " SH",
            "ARP",
            " AS",
            "K",
            ",",
            " Local",
            "Talk",
            ",",
            " Audio",
            " I",
            "/O",
            ",",
            " Aut",
            "od",
            "ock",
            ",",
            " Phone",
            " I",
            "/O",
            "|",
            "?",
            "|",
            "Dual",
            "-mode"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " film",
            "ography",
            ".",
            "Vert",
            "igo",
            " contains",
            " a",
            " camera",
            " technique",
            " developed",
            " by",
            " Ir",
            "min",
            " Roberts",
            ",",
            " commonly",
            " referred",
            " to",
            " as",
            " a",
            " d",
            "olly",
            " zoom",
            ",",
            " which",
            " has",
            " been",
            " copied",
            " by",
            " many",
            " filmmakers",
            ".",
            " The",
            " film",
            " premiered",
            " at",
            " the",
            " San",
            " Sebast",
            "i",
            "Ã¡n",
            " International",
            " Film",
            " Festival",
            ",",
            " and",
            " Hitch",
            "cock",
            " won",
            " the",
            " Silver",
            " Se",
            "ash",
            "ell",
            " prize",
            ".",
            " Vert",
            "igo",
            " is",
            " considered",
            " a",
            " classic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " trip",
            " to",
            " Italy",
            ").",
            " Y",
            "alom",
            ",",
            " Ir",
            "vin",
            " D",
            ".:",
            " The",
            " Sch",
            "openh",
            "auer",
            " Cure",
            ".",
            " Harper",
            "Coll",
            "ins",
            ",",
            " New",
            " York",
            " City",
            " ",
            "200",
            "5",
            ",",
            " ",
            " (",
            "The",
            " novel",
            " switches",
            " between",
            " the",
            " current",
            " events",
            " happening",
            " around",
            " a",
            " therapy",
            " group",
            " and",
            " the",
            " psych",
            "obi",
            "ography",
            " of",
            " Arthur",
            " Sch",
            "openh",
            "auer",
            ").",
            " K",
            "ort",
            "mann",
            ",",
            " Christian",
            ":",
            " Happy",
            " Hour",
            " Sch"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " divorced",
            " his",
            " wife",
            ",",
            " Ir",
            "ina",
            ",",
            " in",
            " June",
            " ",
            "197",
            "0",
            ".",
            " In",
            " the",
            " same",
            " year",
            ",",
            " he",
            " married",
            " Lar",
            "isa",
            " K",
            "iz",
            "il",
            "ova",
            " (",
            "n",
            "Ã©e",
            " Eg",
            "ork",
            "ina",
            "),",
            " who",
            " had",
            " been",
            " a",
            " production",
            " assistant",
            " for",
            " the",
            " film",
            " Andre",
            "i",
            " Rub",
            "lev",
            " (",
            "they",
            " had",
            " been",
            " living",
            " together",
            " since",
            " ",
            "196",
            "5",
            ").",
            " Their",
            " son",
            ",",
            " Andre",
            "i",
            " And"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            "16",
            " children",
            " with",
            " A",
            "DEM",
            ",",
            " ",
            "10",
            " recovered",
            " completely",
            " after",
            " high",
            "-d",
            "ose",
            " methyl",
            "pred",
            "nis",
            "ol",
            "one",
            ",",
            " one",
            " severe",
            " case",
            " that",
            " failed",
            " to",
            " respond",
            " to",
            " steroids",
            " recovered",
            " completely",
            " after",
            " IV",
            " Ig",
            ";",
            " the",
            " five",
            " most",
            " severe",
            " cases",
            " –",
            " with",
            " AD",
            "AM",
            " and",
            " severe",
            " peripheral",
            " neurop",
            "athy",
            " –",
            " were",
            " treated",
            " with",
            " combined",
            " high",
            "-d",
            "ose",
            " methyl",
            "pred",
            "nis",
            "ol",
            "one",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " Rak",
            "uten",
            " and",
            " Ok",
            "amoto",
            " Ip",
            "pei",
            " used",
            " film",
            " elements",
            " in",
            " their",
            " strips",
            ".",
            "P",
            "ione",
            "ers",
            " ",
            "Animation",
            " in",
            " Japan",
            " began",
            " in",
            " the",
            " early",
            " ",
            "20",
            "th",
            " century",
            ",",
            " when",
            " filmmakers",
            " started",
            " to",
            " experiment",
            " with",
            " techniques",
            " pioneered",
            " in",
            " France",
            ",",
            " Germany",
            ",",
            " the",
            " United",
            " States",
            ",",
            " and",
            " Russia",
            ".",
            " A",
            " claim",
            " for",
            " the",
            " earliest",
            " Japanese",
            " animation",
            " is",
            " K",
            "ats",
            "ud",
            "Åį",
            " Sh"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " Rak",
            "uten",
            " and",
            " Ok",
            "amoto",
            " Ip",
            "pei",
            " used",
            " film",
            " elements",
            " in",
            " their",
            " strips",
            ".",
            "P",
            "ione",
            "ers",
            " ",
            "Animation",
            " in",
            " Japan",
            " began",
            " in",
            " the",
            " early",
            " ",
            "20",
            "th",
            " century",
            ",",
            " when",
            " filmmakers",
            " started",
            " to",
            " experiment",
            " with",
            " techniques",
            " pioneered",
            " in",
            " France",
            ",",
            " Germany",
            ",",
            " the",
            " United",
            " States",
            ",",
            " and",
            " Russia",
            ".",
            " A",
            " claim",
            " for",
            " the",
            " earliest",
            " Japanese",
            " animation",
            " is",
            " K",
            "ats",
            "ud",
            "Åį",
            " Sh"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " medieval",
            " rulers",
            ".",
            " One",
            " of",
            " the",
            " most",
            " credible",
            " claims",
            " has",
            " been",
            " that",
            " of",
            " the",
            " Nom",
            "inal",
            "ia",
            " of",
            " the",
            " Bulgarian",
            " kh",
            "ans",
            " for",
            " myth",
            "ological",
            " Av",
            "it",
            "oh",
            "ol",
            " and",
            " Ir",
            "nik",
            " from",
            " the",
            " D",
            "ulo",
            " clan",
            " of",
            " the",
            " Bulg",
            "ars",
            ".",
            " The",
            " Hungarian",
            " Ãģ",
            "rp",
            "Ã¡d",
            " dynasty",
            " also",
            " claimed",
            " to",
            " be",
            " a",
            " direct",
            " descendant",
            " of",
            " At",
            "til",
            "a",
            ".",
            " Medieval",
            " Hungarian",
            " chron"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " W",
            ".",
            " (",
            "201",
            "1",
            ").",
            " The",
            " Union",
            " War",
            ".",
            " Cambridge",
            ",",
            " Massachusetts",
            ":",
            " Harvard",
            " University",
            " Press",
            ".",
            " .",
            " G",
            "ara",
            ",",
            " Larry",
            " (",
            "196",
            "4",
            ").",
            " \"",
            "The",
            " F",
            "ug",
            "itive",
            " Slave",
            " Law",
            ":",
            " A",
            " Double",
            " Par",
            "adox",
            ",\"",
            " in",
            " U",
            "nger",
            ",",
            " Ir",
            "win",
            ",",
            " Essays",
            " on",
            " the",
            " Civil",
            " War",
            " and",
            " Reconstruction",
            ",",
            " New",
            " York",
            ":",
            " Holt",
            ",",
            " R",
            "ine",
            "hart"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " W",
            ".",
            " (",
            "201",
            "1",
            ").",
            " The",
            " Union",
            " War",
            ".",
            " Cambridge",
            ",",
            " Massachusetts",
            ":",
            " Harvard",
            " University",
            " Press",
            ".",
            " .",
            " G",
            "ara",
            ",",
            " Larry",
            " (",
            "196",
            "4",
            ").",
            " \"",
            "The",
            " F",
            "ug",
            "itive",
            " Slave",
            " Law",
            ":",
            " A",
            " Double",
            " Par",
            "adox",
            ",\"",
            " in",
            " U",
            "nger",
            ",",
            " Ir",
            "win",
            ",",
            " Essays",
            " on",
            " the",
            " Civil",
            " War",
            " and",
            " Reconstruction",
            ",",
            " New",
            " York",
            ":",
            " Holt",
            ",",
            " R",
            "ine",
            "hart"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "935",
            " and",
            " IC",
            " ",
            "180",
            "1",
            ".",
            "NGC",
            " ",
            "821",
            " is",
            " an",
            " E",
            "6",
            " ellipt",
            "ical",
            " galaxy",
            ".",
            " It",
            " is",
            " unusual",
            " because",
            " it",
            " has",
            " hints",
            " of",
            " an",
            " early",
            " spiral",
            " structure",
            ",",
            " which",
            " is",
            " normally",
            " only",
            " found",
            " in",
            " l",
            "entic",
            "ular",
            " and",
            " spiral",
            " galaxies",
            ".",
            " NGC",
            " ",
            "821",
            " is",
            " ",
            "2",
            ".",
            "6",
            " by",
            " ",
            "2",
            ".",
            "0",
            " arc",
            "minutes",
            " and",
            " has",
            " a",
            " visual"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " Early",
            " C",
            "ret",
            "aceous",
            " (",
            "133",
            "–",
            "130",
            "Âł",
            "Ma",
            ")",
            " intensive",
            " mag",
            "mat",
            "ism",
            " of",
            " the",
            " Paran",
            "Ã¡",
            "–",
            "Et",
            "end",
            "eka",
            " Large",
            " Ig",
            "ne",
            "ous",
            " Province",
            " produced",
            " by",
            " the",
            " Tristan",
            " hotspot",
            " resulted",
            " in",
            " an",
            " estimated",
            " volume",
            " of",
            " .",
            " It",
            " covered",
            " an",
            " area",
            " of",
            " ",
            " in",
            " Brazil",
            ",",
            " Par",
            "aguay",
            ",",
            " and",
            " Uruguay",
            " and",
            " ",
            " in",
            " Africa",
            ".",
            " Dy",
            "ke",
            " sw",
            "arms"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.081,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "z",
            "ade",
            ",",
            " T",
            "ogr",
            "ul",
            " Nar",
            "iman",
            "bek",
            "ov",
            ",",
            " T",
            "ahir",
            " Salah",
            "ov",
            ",",
            " Al",
            "ak",
            "bar",
            " Re",
            "zag",
            "ul",
            "iy",
            "ev",
            ",",
            " Mir",
            "za",
            " Gad",
            "im",
            " Ir",
            "av",
            "ani",
            ",",
            " Mik",
            "ay",
            "il",
            " Abd",
            "ull",
            "ay",
            "ev",
            " and",
            " Boy",
            "uk",
            "ag",
            "ha",
            " Mir",
            "z",
            "az",
            "ade",
            ".",
            "C",
            "inema",
            "The",
            " film",
            " industry",
            " in",
            " Azerbaijan",
            " dates",
            " back",
            " to",
            " ",
            "189"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Iv",
            "orian",
            " filmmaker",
            " (",
            "d",
            ".",
            " ",
            "200",
            "9",
            ")",
            "194",
            "0",
            " –",
            " Jeffrey",
            " Archer",
            ",",
            " English",
            " author",
            ",",
            " playwright",
            ",",
            " and",
            " politician",
            " ",
            " ",
            "194",
            "0",
            "  ",
            " –",
            " Pen",
            "elope",
            " Co",
            "elen",
            ",",
            " South",
            " African",
            " actress",
            ",",
            " model",
            ",",
            " beauty",
            " queen",
            " and",
            " ",
            "195",
            "8",
            " Miss",
            " World",
            " ",
            " ",
            "194",
            "0",
            "  ",
            " –",
            " Willie",
            " Davis",
            ",",
            " American",
            " baseball",
            " player",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " products",
            " such",
            " as",
            " iP",
            "ads",
            " and",
            " personal",
            " computers",
            " for",
            " about",
            " half",
            " the",
            " French",
            " retail",
            " market",
            ".",
            " According",
            " to",
            " the",
            " French",
            " regulators",
            ",",
            " the",
            " abuses",
            " occurred",
            " between",
            " ",
            "200",
            "5",
            " and",
            " ",
            "201",
            "7",
            " but",
            " were",
            " first",
            " discovered",
            " after",
            " a",
            " complaint",
            " by",
            " an",
            " independent",
            " res",
            "eller",
            ",",
            " e",
            "Biz",
            "c",
            "uss",
            ",",
            " in",
            " ",
            "201",
            "2",
            ".",
            "On",
            " August",
            " ",
            "13",
            ",",
            " ",
            "202"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Ple",
            "i",
            "ades",
            " Advanced",
            " Top",
            "ographic",
            " Laser",
            " Al",
            "tim",
            "eter",
            " System",
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " space",
            "-based",
            " lid",
            "ar",
            " instrument",
            " on",
            " IC",
            "ES",
            "at",
            "-",
            "2",
            " Aster",
            "oid",
            " Ter",
            "restrial",
            "-",
            "impact",
            " Last",
            " Alert",
            " System",
            " (",
            "AT",
            "LAS",
            ")",
            "Math",
            "ematics",
            " Atlas",
            " (",
            "top",
            "ology",
            "),",
            " a",
            " set",
            " of",
            " charts",
            " A",
            " set",
            " of",
            " charts",
            " which",
            " covers",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " E",
            "vert",
            " in",
            " ",
            "197",
            "2",
            ".",
            " There",
            " she",
            " lost",
            " to",
            " eventual",
            " champion",
            " Mart",
            "ina",
            " H",
            "ing",
            "is",
            ".",
            "At",
            " the",
            " US",
            " Open",
            ",",
            " she",
            " lost",
            " in",
            " the",
            " second",
            " round",
            " to",
            " the",
            " ele",
            "venth",
            " seed",
            " Ir",
            "ina",
            " Sp",
            "Ã®",
            "r",
            "lea",
            ".",
            " Partner",
            "ing",
            " with",
            " L",
            "ikh",
            "ov",
            "t",
            "se",
            "va",
            ",",
            " she",
            " reached",
            " the",
            " third",
            " round",
            " of",
            " the",
            " women",
            "'s",
            " doubles",
            " event"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " purchases",
            " of",
            " iP",
            "ads",
            " and",
            " computers",
            " due",
            " to",
            " increased",
            " pricing",
            ".",
            " However",
            ",",
            " iPhone",
            " sales",
            " held",
            " up",
            " with",
            " a",
            " year",
            "-on",
            "-year",
            " increase",
            " of",
            " ",
            "1",
            ".",
            "5",
            "%.",
            " According",
            " to",
            " Apple",
            ",",
            " demands",
            " for",
            " such",
            " devices",
            " were",
            " strong",
            ",",
            " particularly",
            " in",
            " Latin",
            " America",
            " and",
            " South",
            " Asia",
            ".",
            "T",
            "axes",
            " ",
            "Apple",
            " has",
            " created",
            " subsidiaries",
            " in",
            " low",
            "-tax",
            " places",
            " such",
            " as",
            " Ireland",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Il",
            " T",
            "ardo",
            "ant",
            "ico",
            " alle",
            " sog",
            "lie",
            " del",
            " Du",
            "em",
            "ila",
            ",",
            " E",
            "TS",
            ",",
            " ",
            "200",
            "0",
            ",",
            " p",
            ".",
            "Âł",
            "73",
            "–",
            "98",
            ".",
            " Alberto",
            " Cam",
            "pl",
            "ani",
            " and",
            " Marco",
            " Z",
            "amb",
            "on",
            ",",
            " \"",
            " Il",
            " sacrific",
            "io",
            " come",
            " problema",
            " in",
            " alc",
            "une",
            " cor",
            "rent",
            "i",
            " fil",
            "os",
            "of",
            "ice",
            " di",
            " et",
            "Ãł",
            " imper",
            "iale",
            " \",",
            " Ann",
            "ali",
            " di",
            " storia"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " educational",
            " activities",
            ",",
            " but",
            " on",
            " a",
            " more",
            " local",
            " scale",
            ".",
            "The",
            " State",
            " Library",
            " (",
            "Stats",
            "bib",
            "li",
            "ot",
            "eket",
            ")",
            " at",
            " the",
            " university",
            " campus",
            " has",
            " status",
            " of",
            " a",
            " national",
            " library",
            ".",
            " The",
            " city",
            " is",
            " a",
            " member",
            " of",
            " the",
            " IC",
            "ORN",
            " organisation",
            " (",
            "International",
            " Cities",
            " of",
            " Refuge",
            " Network",
            ")",
            " in",
            " an",
            " effort",
            " to",
            " provide",
            " a",
            " safe",
            " haven",
            " to",
            " authors",
            " and",
            " writers",
            " persecuted",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " It",
            " proved",
            " to",
            " be",
            " the",
            " largest",
            " surrender",
            " of",
            " Confederate",
            " forces",
            ".",
            " On",
            " May",
            " ",
            "4",
            ",",
            " all",
            " remaining",
            " Confederate",
            " forces",
            " in",
            " Alabama",
            ",",
            " Louisiana",
            " east",
            " of",
            " the",
            " Mississippi",
            " River",
            ",",
            " and",
            " Mississippi",
            " under",
            " Lieutenant",
            " General",
            " Richard",
            " Taylor",
            " surrendered",
            ".",
            "The",
            " Confederate",
            " president",
            ",",
            " Jefferson",
            " Davis",
            ",",
            " was",
            " captured",
            " at",
            " Ir",
            "win",
            "ville",
            ",",
            " Georgia",
            " on",
            " May",
            " ",
            "10",
            ",",
            " ",
            "186",
            "5",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " purchases",
            " of",
            " iP",
            "ads",
            " and",
            " computers",
            " due",
            " to",
            " increased",
            " pricing",
            ".",
            " However",
            ",",
            " iPhone",
            " sales",
            " held",
            " up",
            " with",
            " a",
            " year",
            "-on",
            "-year",
            " increase",
            " of",
            " ",
            "1",
            ".",
            "5",
            "%.",
            " According",
            " to",
            " Apple",
            ",",
            " demands",
            " for",
            " such",
            " devices",
            " were",
            " strong",
            ",",
            " particularly",
            " in",
            " Latin",
            " America",
            " and",
            " South",
            " Asia",
            ".",
            "T",
            "axes",
            " ",
            "Apple",
            " has",
            " created",
            " subsidiaries",
            " in",
            " low",
            "-tax",
            " places",
            " such",
            " as",
            " Ireland",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " French",
            " model",
            " and",
            " actress",
            " ",
            " ",
            "197",
            "6",
            "  ",
            " –",
            " Jessica",
            " Cap",
            "shaw",
            ",",
            " American",
            " actress",
            "197",
            "7",
            " –",
            " Jason",
            " Fr",
            "as",
            "or",
            ",",
            " American",
            " baseball",
            " player",
            " ",
            " ",
            "197",
            "7",
            "  ",
            " –",
            " Cham",
            "ique",
            " Holds",
            "cl",
            "aw",
            ",",
            " American",
            " basketball",
            " player",
            " ",
            " ",
            "197",
            "7",
            "  ",
            " –",
            " Rav",
            "shan",
            " Ir",
            "mat",
            "ov",
            ",",
            " Uzbek",
            " football",
            " referee"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            "387",
            " BC",
            ")",
            " A",
            "iol",
            "os",
            "icon",
            " (",
            "Îĳ",
            "á¼",
            "°",
            "Î¿",
            "Î»Î¿",
            "Ïĥ",
            "Î¯Îº",
            "ÏīÎ½",
            ",",
            " second",
            " version",
            ",",
            " ",
            "386",
            " BC",
            ")",
            "Und",
            "ated",
            " non",
            "-sur",
            "v",
            "iving",
            " (",
            "lost",
            ")",
            " plays",
            " A",
            "iol",
            "os",
            "icon",
            " (",
            "first",
            " version",
            ")",
            " An",
            "ag",
            "yr",
            "us",
            " (",
            "á¼",
            "Ī",
            "Î½Î¬",
            "Î³",
            "Ïħ",
            "ÏģÎ¿ÏĤ",
            ")",
            " F",
            "rying",
            "-P",
            "an",
            " Men",
            " (",
            "Î¤Î±",
            "Î³"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " is",
            " only",
            " ",
            "269",
            "Âł",
            "years",
            ".",
            " As",
            " a",
            " result",
            ",",
            " the",
            " underground",
            " Ar",
            ",",
            " shield",
            "ed",
            " by",
            " rock",
            " and",
            " water",
            ",",
            " has",
            " much",
            " less",
            " ",
            " contamination",
            ".",
            " Dark",
            "-m",
            "atter",
            " detectors",
            " currently",
            " operating",
            " with",
            " liquid",
            " arg",
            "on",
            " include",
            " Dark",
            "Side",
            ",",
            " W",
            "Ar",
            "P",
            ",",
            " Ar",
            "DM",
            ",",
            " micro",
            "CLE",
            "AN",
            " and",
            " DE",
            "AP",
            ".",
            " Ne",
            "utr",
            "ino",
            " experiments",
            " include",
            " IC"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " is",
            " only",
            " ",
            "269",
            "Âł",
            "years",
            ".",
            " As",
            " a",
            " result",
            ",",
            " the",
            " underground",
            " Ar",
            ",",
            " shield",
            "ed",
            " by",
            " rock",
            " and",
            " water",
            ",",
            " has",
            " much",
            " less",
            " ",
            " contamination",
            ".",
            " Dark",
            "-m",
            "atter",
            " detectors",
            " currently",
            " operating",
            " with",
            " liquid",
            " arg",
            "on",
            " include",
            " Dark",
            "Side",
            ",",
            " W",
            "Ar",
            "P",
            ",",
            " Ar",
            "DM",
            ",",
            " micro",
            "CLE",
            "AN",
            " and",
            " DE",
            "AP",
            ".",
            " Ne",
            "utr",
            "ino",
            " experiments",
            " include",
            " IC"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Mediterranean",
            " Sea",
            " extending",
            " in",
            " the",
            " south",
            " to",
            " the",
            " mouth",
            " of",
            " the",
            " V",
            "jos",
            "Ã«",
            ".",
            " The",
            " first",
            " account",
            " of",
            " the",
            " Il",
            "ly",
            "rian",
            " groups",
            " comes",
            " from",
            " Per",
            "i",
            "plus",
            " of",
            " the",
            " E",
            "ux",
            "ine",
            " Sea",
            ",",
            " a",
            " Greek",
            " text",
            " written",
            " in",
            " the",
            " ",
            "4",
            "th",
            " century",
            " BC",
            ".",
            " The",
            " Bry",
            "ges",
            " were",
            " also",
            " present",
            " in",
            " ",
            " central",
            " Albania",
            ",",
            " while",
            " the",
            " south"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " the",
            " world",
            ".",
            " A",
            "FI",
            " educ",
            "ates",
            " audiences",
            " and",
            " recognizes",
            " artistic",
            " excellence",
            " through",
            " its",
            " awards",
            " programs",
            " and",
            " ",
            "10",
            " Top",
            " ",
            "10",
            " Lists",
            ".",
            "In",
            " ",
            "201",
            "7",
            ",",
            " then",
            "-as",
            "piring",
            " filmmaker",
            " Il",
            "ana",
            " Bar",
            "-D",
            "in",
            " Gi",
            "ann",
            "ini",
            " claimed",
            " that",
            " the",
            " A",
            "FI",
            " expelled",
            " her",
            " after",
            " she",
            " accused",
            " Dez",
            "so",
            " Mag",
            "yar",
            " of",
            " sexually",
            " harassing",
            " her",
            " in",
            " the",
            " early"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ig",
            "uation",
            ")",
            "<|begin_of_text|>",
            "Animation",
            " is",
            " the",
            " method",
            " that",
            " encompasses",
            " myriad",
            " filmm",
            "aking",
            " techniques",
            ",",
            " by",
            " which",
            " still",
            " images",
            " are",
            " manipulated",
            " to",
            " create",
            " moving",
            " images",
            ".",
            " In",
            " traditional",
            " animation",
            ",",
            " images",
            " are",
            " drawn",
            " or",
            " painted",
            " by",
            " hand",
            " on",
            " transparent",
            " cellul",
            "oid",
            " sheets",
            " (",
            "c",
            "els",
            ")",
            " to",
            " be",
            " photographed",
            " and",
            " exhibited",
            " on",
            " film",
            ".",
            " Animation",
            " has",
            " been",
            " recognized",
            " as",
            " an",
            " artistic",
            " medium",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.373,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " more",
            " than",
            " ",
            "400",
            " languages",
            " spoken",
            " in",
            " India",
            ",",
            " and",
            " more",
            " than",
            " ",
            "100",
            " are",
            " spoken",
            " in",
            " the",
            " Philippines",
            ".",
            " China",
            " has",
            " many",
            " languages",
            " and",
            " dialect",
            "s",
            " in",
            " different",
            " provinces",
            ".",
            "Rel",
            "ig",
            "ions",
            " ",
            "Many",
            " of",
            " the",
            " world",
            "'s",
            " major",
            " religions",
            " have",
            " their",
            " origins",
            " in",
            " Asia",
            ",",
            " including",
            " the",
            " five",
            " most",
            " practiced",
            " in",
            " the",
            " world",
            " (",
            "excluding",
            " ir",
            "rel",
            "igion",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " when",
            " the",
            " impact",
            "or",
            " was",
            " vapor",
            "ized",
            " and",
            " settled",
            " across",
            " the",
            " Earth",
            "'s",
            " surface",
            " among",
            " other",
            " material",
            " thrown",
            " up",
            " by",
            " the",
            " impact",
            ",",
            " producing",
            " the",
            " layer",
            " of",
            " ir",
            "id",
            "ium",
            "-en",
            "rich",
            "ed",
            " clay",
            ".",
            " At",
            " the",
            " time",
            ",",
            " consensus",
            " was",
            " not",
            " settled",
            " on",
            " what",
            " caused",
            " the",
            " C",
            "ret",
            "aceous",
            "–",
            "Pale",
            "ogene",
            " extinction",
            " and",
            " the",
            " boundary",
            " layer",
            ",",
            " with",
            " theories",
            " including",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "Autom",
            "obile",
            " (",
            "F",
            "IA",
            ")",
            " Grand",
            " American",
            " Road",
            " Racing",
            " Association",
            " International",
            " Conference",
            " of",
            " Sports",
            " Car",
            " Clubs",
            " (",
            "IC",
            "SC",
            "C",
            ")",
            " International",
            " Hot",
            " Rod",
            " Association",
            " (",
            "IH",
            "RA",
            ")",
            " International",
            " Motor",
            " Sports",
            " Association",
            " (",
            "IM",
            "SA",
            ")",
            " National",
            " Auto",
            " Sport",
            " Association",
            " National",
            " Association",
            " for",
            " Stock",
            " Car",
            " Auto",
            " Racing",
            " (",
            "N",
            "ASC",
            "AR",
            ")",
            " National",
            " Hot",
            " Rod",
            " Association",
            " (",
            "NH",
            "RA"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " vegetables",
            " such",
            " as",
            " garlic",
            ",",
            " onions",
            ",",
            " peppers",
            ",",
            " potatoes",
            ",",
            " tomatoes",
            ",",
            " as",
            " well",
            " as",
            " leg",
            "umes",
            " of",
            " all",
            " types",
            ".",
            "With",
            " a",
            " coastline",
            " along",
            " the",
            " Adri",
            "atic",
            " and",
            " Ion",
            "ian",
            " in",
            " the",
            " Mediterranean",
            " Sea",
            ",",
            " fish",
            ",",
            " crust",
            "ace",
            "ans",
            ",",
            " and",
            " seafood",
            " are",
            " a",
            " popular",
            " and",
            " an",
            " integral",
            " part",
            " of",
            " the",
            " Alban",
            "ian",
            " diet",
            ".",
            " Otherwise",
            ",",
            " lamb",
            " is",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.279,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " head",
            ",",
            " neither",
            " of",
            " these",
            " features",
            " being",
            " found",
            " elsewhere",
            " in",
            " the",
            " animal",
            " kingdom",
            ".",
            "At",
            " the",
            " end",
            " of",
            " the",
            " Devon",
            "ian",
            " period",
            " (",
            "360",
            " million",
            " years",
            " ago",
            "),",
            " the",
            " seas",
            ",",
            " rivers",
            " and",
            " lakes",
            " were",
            " te",
            "eming",
            " with",
            " life",
            " while",
            " the",
            " land",
            " was",
            " the",
            " realm",
            " of",
            " early",
            " plants",
            " and",
            " devoid",
            " of",
            " verte",
            "brates",
            ",",
            " though",
            " some",
            ",",
            " such",
            " as",
            " Ich",
            "thy",
            "ost"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.228,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "ole",
            "pid",
            "iform",
            "es",
            " Order",
            " âĢł",
            "Cross",
            "ogn",
            "ath",
            "iform",
            "es",
            " T",
            "aver",
            "ne",
            " ",
            "198",
            "9",
            " Order",
            " âĢł",
            "Ich",
            "thy",
            "od",
            "ect",
            "iform",
            "es",
            " Bar",
            "deck",
            " &",
            " Spr",
            "inkle",
            " ",
            "196",
            "9",
            " Tele",
            "o",
            "ceph",
            "ala",
            " de",
            " Pin",
            "na",
            " ",
            "199",
            "6",
            " s",
            ".s",
            ".",
            "Meg",
            "ac",
            "oh",
            "ort",
            " Elo",
            "po",
            "ceph",
            "al",
            "ai",
            " Patterson",
            " ",
            "197",
            "7",
            " sens"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.209,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " literary",
            " essay",
            " The",
            " Myth",
            " of",
            " S",
            "isy",
            "ph",
            "us",
            ",",
            " (",
            "Le",
            " My",
            "the",
            " de",
            " S",
            "is",
            "yp",
            "he",
            "),",
            " his",
            " major",
            " work",
            " on",
            " the",
            " subject",
            ".",
            " In",
            " ",
            "194",
            "2",
            ",",
            " he",
            " published",
            " the",
            " story",
            " of",
            " a",
            " man",
            " living",
            " an",
            " absurd",
            " life",
            " in",
            " The",
            " Stranger",
            ".",
            " He",
            " also",
            " wrote",
            " a",
            " play",
            " about",
            " the",
            " Roman",
            " emperor",
            " Cal",
            "ig",
            "ula",
            ",",
            " pursuing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.151,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " Caribbean",
            ".",
            " ",
            "68",
            ".",
            "47",
            "%",
            " of",
            " the",
            " population",
            " was",
            " born",
            " in",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            "Languages",
            "The",
            " language",
            " most",
            " commonly",
            " used",
            " in",
            " business",
            " is",
            " English",
            ".",
            " There",
            " is",
            " a",
            " noticeable",
            " distinction",
            " between",
            " the",
            " Ant",
            "ig",
            "uan",
            " accent",
            " and",
            " the",
            " Barb",
            "ud",
            "an",
            " one",
            ".",
            "When",
            " compared",
            " to",
            " Ant",
            "ig",
            "uan",
            " Cre",
            "ole",
            ",",
            " Standard",
            " English",
            " was",
            " the",
            " language"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            "ÏĤ",
            " ",
            " sign",
            "ifying",
            " \"",
            "firm",
            ",\"",
            " \"",
            "stable",
            ",\"",
            " \"",
            "secure",
            ",\"",
            " and",
            " the",
            " corresponding",
            " verb",
            " á¼",
            "Ħ",
            "ÏĥÏĨ",
            "Î±Î»",
            "Î¯",
            "Î¾",
            "Ïī",
            ",",
            " Î¯",
            "ÏĥÏī",
            " meaning",
            " \"",
            "to",
            " make",
            " firm",
            " or",
            " stable",
            ",\"",
            " \"",
            "to",
            " secure",
            "\".",
            "The",
            " word",
            " \"",
            "as",
            "phalt",
            "\"",
            " is",
            " derived",
            " from",
            " the",
            " late",
            " Middle",
            " English",
            ",",
            " in",
            " turn",
            " from",
            " French",
            " as",
            "ph",
            "alte",
            ",",
            " based",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.14,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " times",
            " the",
            " size",
            " of",
            " Central",
            " Park",
            ".",
            " The",
            " Am",
            "st",
            "elp",
            "ark",
            " in",
            " the",
            " Z",
            "uid",
            " borough",
            " houses",
            " the",
            " R",
            "ie",
            "ker",
            " wind",
            "mill",
            ",",
            " which",
            " dates",
            " to",
            " ",
            "163",
            "6",
            ".",
            " Other",
            " parks",
            " include",
            " the",
            " Sar",
            "ph",
            "at",
            "ip",
            "ark",
            " in",
            " the",
            " De",
            " P",
            "ij",
            "p",
            " neighbourhood",
            ",",
            " the",
            " O",
            "oster",
            "park",
            " in",
            " the",
            " O",
            "ost",
            " borough",
            " and",
            " the",
            " West",
            "erp",
            "ark"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " weary",
            " of",
            " preserving",
            " them",
            ".",
            "He",
            " is",
            " the",
            " Sub",
            "lime",
            ",",
            " the",
            " Trem",
            "end",
            "ous",
            ".",
            "In",
            " Islamic",
            " tradition",
            ",",
            " there",
            " are",
            " ",
            "99",
            " Names",
            " of",
            " God",
            " (",
            " lit",
            ".",
            " meaning",
            ":",
            " '",
            "the",
            " best",
            " names",
            "'",
            " or",
            " '",
            "the",
            " most",
            " beautiful",
            " names",
            "'),",
            " each",
            " of",
            " which",
            " evoke",
            " a",
            " distinct",
            " characteristic",
            " of",
            " Allah",
            ".",
            " All",
            " these",
            " names",
            " refer",
            " to",
            " Allah",
            ",",
            " the",
            " supreme"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " Deniz",
            " gas",
            " field",
            ".",
            " Shah",
            " Deniz",
            " is",
            " expected",
            " to",
            " produce",
            " up",
            " to",
            " ",
            "296",
            "Âłb",
            "illion",
            " cubic",
            " meters",
            " of",
            " natural",
            " gas",
            " per",
            " year",
            ".",
            " Azerbaijan",
            " also",
            " plays",
            " a",
            " major",
            " role",
            " in",
            " the",
            " EU",
            "-sponsored",
            " Silk",
            " Road",
            " Project",
            ".",
            "In",
            " ",
            "200",
            "2",
            ",",
            " the",
            " Azerbai",
            "j",
            "ani",
            " government",
            " established",
            " the",
            " Ministry",
            " of",
            " Transport",
            " with",
            " a",
            " broad",
            " range",
            " of",
            " policy",
            " and",
            " regulatory",
            " functions",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " among",
            " the",
            " biggest",
            " markets",
            " for",
            " the",
            " Hindi",
            " film",
            " industry",
            ".",
            " The",
            " stereotypes",
            " of",
            " Af",
            "gh",
            "ans",
            " in",
            " India",
            " (",
            "K",
            "ab",
            "uli",
            "w",
            "ala",
            " or",
            " Path",
            "ani",
            ")",
            " have",
            " also",
            " been",
            " represented",
            " in",
            " some",
            " Bollywood",
            " films",
            " by",
            " actors",
            ".",
            " Many",
            " Bollywood",
            " film",
            " stars",
            " have",
            " roots",
            " in",
            " Afghanistan",
            ",",
            " including",
            " Salman",
            " Khan",
            ",",
            " Sa",
            "if",
            " Ali",
            " Khan",
            ",",
            " A",
            "am",
            "ir",
            " Khan",
            ",",
            " F"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " came",
            " to",
            " doubt",
            " the",
            " Thomson",
            " model",
            " after",
            " they",
            " encountered",
            " difficulties",
            " when",
            " they",
            " tried",
            " to",
            " build",
            " an",
            " instrument",
            " to",
            " measure",
            " the",
            " charge",
            "-to",
            "-m",
            "ass",
            " ratio",
            " of",
            " alpha",
            " particles",
            " (",
            "these",
            " are",
            " positively",
            "-ch",
            "arged",
            " particles",
            " emitted",
            " by",
            " certain",
            " radioactive",
            " substances",
            " such",
            " as",
            " rad",
            "ium",
            ").",
            " The",
            " alpha",
            " particles",
            " were",
            " being",
            " scattered",
            " by",
            " the",
            " air",
            " in",
            " the",
            " detection",
            " chamber",
            ",",
            " which",
            " made",
            " the",
            " measurements"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " correct",
            " within",
            " its",
            " domain",
            " of",
            " validity",
            ",",
            " that",
            " of",
            " objects",
            " in",
            " the",
            " Earth",
            "'s",
            " gravitational",
            " field",
            " immersed",
            " in",
            " a",
            " fluid",
            " such",
            " as",
            " air",
            ".",
            " In",
            " this",
            " system",
            ",",
            " heavy",
            " bodies",
            " in",
            " steady",
            " fall",
            " indeed",
            " travel",
            " faster",
            " than",
            " light",
            " ones",
            " (",
            "whether",
            " friction",
            " is",
            " ignored",
            ",",
            " or",
            " not",
            "),",
            " and",
            " they",
            " do",
            " fall",
            " more",
            " slowly",
            " in",
            " a",
            " dens",
            "er",
            " medium",
            ".",
            "Newton",
            "'s",
            " \""
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "à¸²à¸ģà¸¥",
    "rech",
    "kich",
    "å±ķ",
    "auss"
  ],
  "bottom_logits": [
    "ãĥ³ãĤº",
    "enne",
    " Ð»ÑĮ",
    " burst",
    "áº©u"
  ],
  "act_min": -0.0,
  "act_max": 0.676
}