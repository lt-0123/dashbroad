{
  "index": 79960,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "ea",
            " Empire",
            " of",
            " Tre",
            "biz",
            "ond",
            " Gord",
            "ium",
            " Ly",
            "ca",
            "onia",
            " Mid",
            "as",
            " M",
            "ilet",
            "us",
            " My",
            "ra",
            " Pent",
            "archy",
            " Pont",
            "ic",
            " Greeks",
            " R",
            "umi",
            " Saint",
            " Anat",
            "olia",
            " Saint",
            " John",
            " Saint",
            " Nicholas",
            " Saint",
            " Paul",
            " Sele",
            "ucid",
            " Empire",
            " Seven",
            " churches",
            " of",
            " Asia",
            " Seven",
            " Sleep",
            "ers",
            " T",
            "ars"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "12",
            ".",
            "4",
            "%),",
            " Pent",
            "ec",
            "ost",
            "al",
            "ism",
            " (",
            "12",
            ".",
            "2",
            "%),",
            " Mor",
            "avian",
            " Church",
            " (",
            "8",
            ".",
            "3",
            "%),",
            " Roman",
            " Catholics",
            "(",
            "8",
            ".",
            "2",
            "%),",
            " Methodist",
            " Church",
            " (",
            "5",
            ".",
            "6",
            "%),",
            " Wesley",
            "an",
            " Hol",
            "iness",
            " Church",
            " (",
            "4",
            ".",
            "5",
            "%),",
            " Church",
            " of",
            " God",
            " (",
            "4",
            ".",
            "1",
            "%),",
            " Bapt",
            "ists",
            " (",
            "3",
            ".",
            "6",
            "%),",
            " Mormon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "12",
            ".",
            "4",
            "%),",
            " Pent",
            "ec",
            "ost",
            "al",
            "ism",
            " (",
            "12",
            ".",
            "2",
            "%),",
            " Mor",
            "avian",
            " Church",
            " (",
            "8",
            ".",
            "3",
            "%),",
            " Roman",
            " Catholics",
            "(",
            "8",
            ".",
            "2",
            "%),",
            " Methodist",
            " Church",
            " (",
            "5",
            ".",
            "6",
            "%),",
            " Wesley",
            "an",
            " Hol",
            "iness",
            " Church",
            " (",
            "4",
            ".",
            "5",
            "%),",
            " Church",
            " of",
            " God",
            " (",
            "4",
            ".",
            "1",
            "%),",
            " Bapt",
            "ists",
            " (",
            "3",
            ".",
            "6",
            "%),",
            " Mormon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Register",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "3",
            ":",
            " Index",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Rob",
            "be",
            "ets",
            ",",
            " Mart",
            "ine",
            ".",
            " ",
            "200",
            "4",
            ".",
            " \"",
            "Sw",
            "adesh",
            " ",
            "100",
            " on",
            " Japanese",
            ",",
            " Korean",
            " and",
            " Alta",
            "ic",
            ".\"",
            " Tokyo"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "1",
            ":",
            " Phon",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "195",
            "7",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " II",
            ".",
            " Form"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " time",
            " of",
            " St",
            ".",
            " Is",
            "id",
            "ore",
            " of",
            " Se",
            "ville",
            " (",
            "d",
            ".",
            " ",
            "636",
            ")",
            " who",
            " lived",
            " in",
            " what",
            " is",
            " today",
            " Spain",
            ",",
            " the",
            " Monday",
            " after",
            " Pent",
            "ec",
            "ost",
            " was",
            " designated",
            " to",
            " remember",
            " the",
            " deceased",
            ".",
            " At",
            " the",
            " beginning",
            " of",
            " the",
            " ninth",
            " century",
            ",",
            " Ab",
            "bot",
            " Eig",
            "il",
            " of",
            " Ful",
            "da",
            " set",
            " ",
            "17",
            " December",
            " as",
            " commem",
            "oration",
            " of",
            " all",
            " deceased"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            "leh",
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "2",
            ":",
            " Morph",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "196",
            "6",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " III"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " Japon",
            "ic",
            " languages",
            " No",
            "str",
            "atic",
            " languages",
            " Pan",
            "-T",
            "uran",
            "ism",
            " Tur",
            "co",
            "-M",
            "ong",
            "ol",
            " ",
            " U",
            "ral",
            "o",
            "-S",
            "iber",
            "ian",
            " languages",
            " X",
            "ion",
            "gnu",
            " Comparison",
            " of",
            " Japanese",
            " and",
            " Korean",
            "References",
            "Notes",
            "C",
            "itations",
            "Sources",
            "A",
            "alto",
            ",",
            " Pent",
            "ti",
            ".",
            " ",
            "195",
            "5",
            ".",
            " \"",
            "On",
            " the",
            " Alta",
            "ic",
            " initial"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " Paris",
            " Pal",
            "ais",
            " Bourbon",
            " (",
            "183",
            "3",
            "–",
            "184",
            "7",
            "),",
            " one",
            " of",
            " the",
            " seats",
            " of",
            " the",
            " French",
            " Parliament",
            ".",
            " ",
            " created",
            " a",
            " statue",
            " group",
            " Achilles",
            " and",
            " Pent",
            "hes",
            "ile",
            "a",
            " (",
            "189",
            "5",
            ";",
            " Vienna",
            ").",
            " Ach",
            "ille",
            "us",
            " (",
            "190",
            "8",
            ")",
            " is",
            " a",
            " lith",
            "ography",
            " by",
            " Max",
            " S",
            "lev",
            "og",
            "t",
            ".",
            "Music",
            " ",
            "A",
            "ch",
            "illes",
            " has",
            " been",
            " frequently"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " sprink",
            "ling",
            " of",
            " Kim",
            "b",
            "angu",
            "ism",
            " can",
            " be",
            " found",
            ",",
            " spreading",
            " from",
            " the",
            " Congo",
            "/Z",
            "a",
            "Ã¯",
            "re",
            ".",
            " Since",
            " independence",
            ",",
            " hundreds",
            " of",
            " Pent",
            "ec",
            "ost",
            "al",
            " and",
            " similar",
            " communities",
            " have",
            " sprung",
            " up",
            " in",
            " the",
            " cities",
            ",",
            " whereby",
            " now",
            " about",
            " ",
            "50",
            "%",
            " of",
            " the",
            " population",
            " is",
            " living",
            ";",
            " several",
            " of",
            " these",
            " communities",
            "/ch",
            "urch",
            "es",
            " are",
            " of",
            " Brazilian",
            " origin"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            "omer",
            "ica",
            ",",
            " composed",
            " by",
            " Quint",
            "us",
            " of",
            " Smy",
            "rna",
            " in",
            " the",
            " fourth",
            " century",
            " CE",
            ",",
            " relate",
            " further",
            " events",
            " from",
            " the",
            " Trojan",
            " War",
            ".",
            " When",
            " Pent",
            "hes",
            "ile",
            "a",
            ",",
            " queen",
            " of",
            " the",
            " Am",
            "az",
            "ons",
            " and",
            " daughter",
            " of",
            " A",
            "res",
            ",",
            " arrives",
            " in",
            " Troy",
            ",",
            " Pri",
            "am",
            " hopes",
            " that",
            " she",
            " will",
            " defeat",
            " Achilles",
            ".",
            " After",
            " his",
            " temporary",
            " tr",
            "uce",
            " with",
            " Pri",
            "am"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            "omer",
            "ica",
            ",",
            " composed",
            " by",
            " Quint",
            "us",
            " of",
            " Smy",
            "rna",
            " in",
            " the",
            " fourth",
            " century",
            " CE",
            ",",
            " relate",
            " further",
            " events",
            " from",
            " the",
            " Trojan",
            " War",
            ".",
            " When",
            " Pent",
            "hes",
            "ile",
            "a",
            ",",
            " queen",
            " of",
            " the",
            " Am",
            "az",
            "ons",
            " and",
            " daughter",
            " of",
            " A",
            "res",
            ",",
            " arrives",
            " in",
            " Troy",
            ",",
            " Pri",
            "am",
            " hopes",
            " that",
            " she",
            " will",
            " defeat",
            " Achilles",
            ".",
            " After",
            " his",
            " temporary",
            " tr",
            "uce",
            " with",
            " Pri",
            "am"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "in",
            " the",
            " tradition",
            " of",
            " Ni",
            "obe",
            "'s",
            " offspring",
            ").",
            " The",
            " poem",
            " ends",
            " with",
            " a",
            " description",
            " of",
            " Hector",
            "'s",
            " funeral",
            ",",
            " with",
            " the",
            " doom",
            " of",
            " Troy",
            " and",
            " Achilles",
            " himself",
            " still",
            " to",
            " come",
            ".",
            "Later",
            " epic",
            " accounts",
            ":",
            " fighting",
            " Pent",
            "hes",
            "ile",
            "a",
            " and",
            " Mem",
            "non",
            " ",
            "The",
            " A",
            "eth",
            "i",
            "opis",
            " (",
            "7",
            "th",
            " century",
            " BC",
            ")",
            " and",
            " a",
            " work",
            " named",
            " Post",
            "h"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Official",
            " statistics",
            " do",
            " not",
            " exist",
            ",",
            " however",
            " it",
            " is",
            " estimated",
            " that",
            " over",
            " ",
            "80",
            "%",
            " belong",
            " to",
            " a",
            " Christian",
            " church",
            " or",
            " community",
            ".",
            " More",
            " than",
            " half",
            " are",
            " Catholic",
            ",",
            " the",
            " remaining",
            " ones",
            " comprising",
            " members",
            " of",
            " traditional",
            " Protestant",
            " churches",
            " as",
            " well",
            " as",
            " of",
            " Pent",
            "ec",
            "ost",
            "al",
            " communities",
            ".",
            " Only",
            " ",
            "0",
            ".",
            "1",
            "%",
            " are",
            " Muslims",
            " ",
            " -",
            " generally",
            " immigrants",
            " from",
            " other",
            " African"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " hundred",
            " genera",
            ").",
            " Other",
            " Ast",
            "era",
            "les",
            " are",
            " Rousse",
            "aceae",
            " (",
            "four",
            " genera",
            "),",
            " Cam",
            "pan",
            "ul",
            "aceae",
            " (",
            "eight",
            "y",
            "-four",
            " genera",
            ")",
            " and",
            " Pent",
            "aphrag",
            "m",
            "ata",
            "ceae",
            " (",
            "one",
            " genus",
            ").",
            "All",
            " Ast",
            "era",
            "les",
            " families",
            " are",
            " represented",
            " in",
            " the",
            " Southern",
            " Hemisphere",
            ";",
            " however",
            ",",
            " Aster",
            "aceae",
            " and",
            " Cam",
            "pan",
            "ul",
            "aceae",
            " are",
            " cosm",
            "opolitan",
            " and",
            " Men",
            "yan",
            "th",
            "aceae"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Official",
            " statistics",
            " do",
            " not",
            " exist",
            ",",
            " however",
            " it",
            " is",
            " estimated",
            " that",
            " over",
            " ",
            "80",
            "%",
            " belong",
            " to",
            " a",
            " Christian",
            " church",
            " or",
            " community",
            ".",
            " More",
            " than",
            " half",
            " are",
            " Catholic",
            ",",
            " the",
            " remaining",
            " ones",
            " comprising",
            " members",
            " of",
            " traditional",
            " Protestant",
            " churches",
            " as",
            " well",
            " as",
            " of",
            " Pent",
            "ec",
            "ost",
            "al",
            " communities",
            ".",
            " Only",
            " ",
            "0",
            ".",
            "1",
            "%",
            " are",
            " Muslims",
            " ",
            " -",
            " generally",
            " immigrants",
            " from",
            " other",
            " African"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " hundred",
            " genera",
            ").",
            " Other",
            " Ast",
            "era",
            "les",
            " are",
            " Rousse",
            "aceae",
            " (",
            "four",
            " genera",
            "),",
            " Cam",
            "pan",
            "ul",
            "aceae",
            " (",
            "eight",
            "y",
            "-four",
            " genera",
            ")",
            " and",
            " Pent",
            "aphrag",
            "m",
            "ata",
            "ceae",
            " (",
            "one",
            " genus",
            ").",
            "All",
            " Ast",
            "era",
            "les",
            " families",
            " are",
            " represented",
            " in",
            " the",
            " Southern",
            " Hemisphere",
            ";",
            " however",
            ",",
            " Aster",
            "aceae",
            " and",
            " Cam",
            "pan",
            "ul",
            "aceae",
            " are",
            " cosm",
            "opolitan",
            " and",
            " Men",
            "yan",
            "th",
            "aceae"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " Rough",
            "ly",
            " ",
            "26",
            "%",
            " are",
            " followers",
            " of",
            " traditional",
            " forms",
            " of",
            " Protestant",
            "ism",
            " (",
            "Cong",
            "regation",
            "als",
            ",",
            " Method",
            "ists",
            ",",
            " Bapt",
            "ista",
            ",",
            " Luther",
            "ans",
            ",",
            " Re",
            "formed",
            "),",
            " but",
            " over",
            " the",
            " last",
            " decades",
            " there",
            " has",
            " in",
            " addition",
            " been",
            " a",
            " growth",
            " of",
            " Pent",
            "ec",
            "ost",
            "al",
            " communities",
            " and",
            " African",
            " Init",
            "iated",
            " Churches",
            ".",
            " In",
            " ",
            "200",
            "6",
            ",",
            " one",
            " out",
            " of",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            "ummies",
            " of",
            " two",
            " ",
            "20",
            "th",
            " dynasty",
            " individuals",
            ",",
            " Ram",
            "esses",
            " III",
            " and",
            " \"",
            "Unknown",
            " Man",
            " E",
            "\"",
            " believed",
            " to",
            " be",
            " Ram",
            "esses",
            " III",
            "'s",
            " son",
            " Pent",
            "aw",
            "er",
            ",",
            " were",
            " analyzed",
            " by",
            " Albert",
            " Z",
            "ink",
            ",",
            " Y",
            "eh",
            "ia",
            " Z",
            " Gad",
            " and",
            " a",
            " team",
            " of",
            " researchers",
            " under",
            " Z",
            "ahi",
            " Haw",
            "ass",
            ".",
            " Genetic",
            " kin",
            "ship",
            " analyses",
            " revealed",
            " identical",
            " hap",
            "lot",
            "ypes",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " my",
            "a",
            ".",
            "Some",
            " more",
            " recent",
            " studies",
            " have",
            " used",
            " the",
            " word",
            " anth",
            "oph",
            "yte",
            " to",
            " describe",
            " a",
            " hypothetical",
            " group",
            " which",
            " includes",
            " the",
            " ang",
            "ios",
            "perms",
            " and",
            " a",
            " variety",
            " of",
            " extinct",
            " seed",
            " plant",
            " groups",
            " (",
            "with",
            " various",
            " suggestions",
            " including",
            " at",
            " least",
            " some",
            " of",
            " the",
            " following",
            " groups",
            ":",
            " g",
            "los",
            "so",
            "pter",
            "ids",
            ",",
            " cor",
            "yst",
            "os",
            "perms",
            ",",
            " Pet",
            "ri",
            "ell",
            "ales",
            " Pent",
            "ox"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " my",
            "a",
            ".",
            "Some",
            " more",
            " recent",
            " studies",
            " have",
            " used",
            " the",
            " word",
            " anth",
            "oph",
            "yte",
            " to",
            " describe",
            " a",
            " hypothetical",
            " group",
            " which",
            " includes",
            " the",
            " ang",
            "ios",
            "perms",
            " and",
            " a",
            " variety",
            " of",
            " extinct",
            " seed",
            " plant",
            " groups",
            " (",
            "with",
            " various",
            " suggestions",
            " including",
            " at",
            " least",
            " some",
            " of",
            " the",
            " following",
            " groups",
            ":",
            " g",
            "los",
            "so",
            "pter",
            "ids",
            ",",
            " cor",
            "yst",
            "os",
            "perms",
            ",",
            " Pet",
            "ri",
            "ell",
            "ales",
            " Pent",
            "ox"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " faint",
            " or",
            " intermittent",
            " comet",
            "-like",
            " tail",
            " does",
            " not",
            " necessarily",
            " result",
            " in",
            " a",
            " classification",
            " as",
            " a",
            " near",
            "-E",
            "arth",
            " comet",
            ",",
            " making",
            " the",
            " boundaries",
            " somewhat",
            " fuzzy",
            ".",
            " The",
            " rest",
            " of",
            " the",
            " near",
            "-E",
            "arth",
            " asteroids",
            " are",
            " driven",
            " out",
            " of",
            " the",
            " asteroid",
            " belt",
            " by",
            " gravitational",
            " interactions",
            " with",
            " Jupiter",
            ".",
            "Many",
            " asteroids",
            " have",
            " natural",
            " satellites",
            " (",
            "minor",
            "-",
            "planet",
            " moons",
            ").",
            " ,",
            " there",
            " were",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            "Golden",
            " Age",
            "\"",
            " of",
            " detective",
            " fiction",
            ".",
            " Author",
            " Dil",
            "ys",
            " Winn",
            " called",
            " Christie",
            " \"",
            "the",
            " do",
            "y",
            "enne",
            " of",
            " Co",
            "z",
            "iness",
            "\",",
            " a",
            " sub",
            "-gen",
            "re",
            " which",
            " \"",
            "featured",
            " a",
            " small",
            " village",
            " setting",
            ",",
            " a",
            " hero",
            " with",
            " faint",
            "ly",
            " arist",
            "ocratic",
            " family",
            " connections",
            ",",
            " a",
            " plethora",
            " of",
            " red",
            " herr",
            "ings",
            " and",
            " a",
            " tendency",
            " to",
            " commit",
            " homicide",
            " with",
            " sterling",
            " silver",
            " letter",
            " open",
            "ers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " working",
            " model",
            " of",
            " his",
            " telephone",
            ".",
            " On",
            " August",
            " ",
            "3",
            ",",
            " ",
            "187",
            "6",
            ",",
            " from",
            " the",
            " tele",
            "graph",
            " office",
            " in",
            " Br",
            "ant",
            "ford",
            ",",
            " Ontario",
            ",",
            " Bell",
            " sent",
            " a",
            " tentative",
            " telegram",
            " to",
            " the",
            " village",
            " of",
            " Mount",
            " Pleasant",
            " ",
            " distant",
            ",",
            " indicating",
            " that",
            " he",
            " was",
            " ready",
            ".",
            " He",
            " made",
            " a",
            " telephone",
            " call",
            " via",
            " tele",
            "graph",
            " wires",
            " and",
            " faint",
            " voices",
            " were",
            " heard",
            " rep",
            "lying"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " working",
            " model",
            " of",
            " his",
            " telephone",
            ".",
            " On",
            " August",
            " ",
            "3",
            ",",
            " ",
            "187",
            "6",
            ",",
            " from",
            " the",
            " tele",
            "graph",
            " office",
            " in",
            " Br",
            "ant",
            "ford",
            ",",
            " Ontario",
            ",",
            " Bell",
            " sent",
            " a",
            " tentative",
            " telegram",
            " to",
            " the",
            " village",
            " of",
            " Mount",
            " Pleasant",
            " ",
            " distant",
            ",",
            " indicating",
            " that",
            " he",
            " was",
            " ready",
            ".",
            " He",
            " made",
            " a",
            " telephone",
            " call",
            " via",
            " tele",
            "graph",
            " wires",
            " and",
            " faint",
            " voices",
            " were",
            " heard",
            " rep",
            "lying"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Article",
            " on",
            " Bad",
            "-B",
            "re",
            "ath",
            " Prevention",
            " Products",
            " –",
            " from",
            " MSNBC",
            " Mayo",
            " Clinic",
            " Q",
            "&A",
            " on",
            " Magic",
            " Mouth",
            "wash",
            " for",
            " chemotherapy",
            " so",
            "res",
            " American",
            " Dental",
            " Association",
            " article",
            " on",
            " mouth",
            "wash",
            "D",
            "ent",
            "if",
            "rices",
            "Or",
            "al",
            " hygiene",
            "Drug",
            " delivery",
            " devices",
            "Dos",
            "age",
            " forms",
            "<|begin_of_text|>",
            "Alexander",
            " III",
            " of",
            " Maced",
            "on",
            " (;",
            " ",
            "20",
            "/",
            "21",
            " July",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Article",
            " on",
            " Bad",
            "-B",
            "re",
            "ath",
            " Prevention",
            " Products",
            " –",
            " from",
            " MSNBC",
            " Mayo",
            " Clinic",
            " Q",
            "&A",
            " on",
            " Magic",
            " Mouth",
            "wash",
            " for",
            " chemotherapy",
            " so",
            "res",
            " American",
            " Dental",
            " Association",
            " article",
            " on",
            " mouth",
            "wash",
            "D",
            "ent",
            "if",
            "rices",
            "Or",
            "al",
            " hygiene",
            "Drug",
            " delivery",
            " devices",
            "Dos",
            "age",
            " forms",
            "<|begin_of_text|>",
            "Alexander",
            " III",
            " of",
            " Maced",
            "on",
            " (;",
            " ",
            "20",
            "/",
            "21",
            " July",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "200",
            "3",
            " and",
            " ",
            "200",
            "6",
            " respectively",
            ".",
            " The",
            " number",
            " of",
            " sewage",
            " treatment",
            " plants",
            " is",
            " planned",
            " to",
            " be",
            " reduced",
            " from",
            " ",
            "17",
            " to",
            " ",
            "2",
            " by",
            " ",
            "202",
            "5",
            ",",
            " as",
            " the",
            " treatment",
            " plants",
            " in",
            " Mars",
            "el",
            "is",
            "borg",
            " and",
            " Eg",
            "Ã¥",
            " are",
            " scheduled",
            " for",
            " expansion",
            " to",
            " take",
            " over",
            " all",
            " waste",
            " water",
            " treatment",
            ".",
            " They",
            " have",
            " already",
            " been",
            " ref",
            "itted",
            " for",
            " bi"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "izziness",
            ",",
            " trembling",
            " or",
            " shaking",
            ",",
            " feeling",
            " faint",
            ",",
            " nausea",
            ",",
            " fear",
            " that",
            " you",
            " are",
            " losing",
            " control",
            " or",
            " are",
            " about",
            " to",
            " die",
            ".",
            " Even",
            " though",
            " they",
            " have",
            " these",
            " symptoms",
            " during",
            " an",
            " attack",
            ",",
            " the",
            " main",
            " symptom",
            " is",
            " the",
            " persistent",
            " fear",
            " of",
            " having",
            " future",
            " panic",
            " attacks",
            ".",
            "An",
            "xiety",
            " disorders",
            " ",
            "An",
            "xiety",
            " disorders",
            " are",
            " a",
            " group",
            " of",
            " mental",
            " disorders",
            " characterized",
            " by",
            " exaggerated",
            " feelings"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "izziness",
            ",",
            " trembling",
            " or",
            " shaking",
            ",",
            " feeling",
            " faint",
            ",",
            " nausea",
            ",",
            " fear",
            " that",
            " you",
            " are",
            " losing",
            " control",
            " or",
            " are",
            " about",
            " to",
            " die",
            ".",
            " Even",
            " though",
            " they",
            " have",
            " these",
            " symptoms",
            " during",
            " an",
            " attack",
            ",",
            " the",
            " main",
            " symptom",
            " is",
            " the",
            " persistent",
            " fear",
            " of",
            " having",
            " future",
            " panic",
            " attacks",
            ".",
            "An",
            "xiety",
            " disorders",
            " ",
            "An",
            "xiety",
            " disorders",
            " are",
            " a",
            " group",
            " of",
            " mental",
            " disorders",
            " characterized",
            " by",
            " exaggerated",
            " feelings"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "og",
            "as",
            " production",
            " to",
            " become",
            " net",
            " producers",
            " of",
            " electricity",
            " and",
            " heat",
            ".",
            " To",
            " aid",
            " the",
            " new",
            " treatment",
            " plants",
            ",",
            " and",
            " avoid",
            " flood",
            "ings",
            ",",
            " sewage",
            " and",
            " storm",
            "water",
            " throughout",
            " the",
            " municipality",
            " is",
            " planned",
            " to",
            " be",
            " separated",
            " into",
            " two",
            " different",
            " drainage",
            " systems",
            ".",
            " Construction",
            " began",
            " in",
            " ",
            "201",
            "7",
            " in",
            " several",
            " areas",
            ",",
            " but",
            " it",
            " is",
            " a",
            " long",
            " process",
            " that",
            " is",
            " scheduled",
            " to",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " U",
            ".S",
            ".",
            " prisoners",
            " earlier",
            " in",
            " the",
            " battle",
            ".",
            "Within",
            " a",
            " few",
            " minutes",
            ",",
            " Johnston",
            " was",
            " observed",
            " by",
            " his",
            " staff",
            " to",
            " be",
            " nearly",
            " faint",
            "ing",
            ".",
            " Among",
            " his",
            " staff",
            " was",
            " Ish",
            "am",
            " G",
            ".",
            " Harris",
            ",",
            " the",
            " Governor",
            " of",
            " Tennessee",
            ",",
            " who",
            " had",
            " ceased",
            " to",
            " make",
            " any",
            " real",
            " effort",
            " to",
            " function",
            " as",
            " governor",
            " after",
            " learning",
            " that",
            " Abraham",
            " Lincoln",
            " had",
            " appointed",
            " Andrew",
            " Johnson"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " rather",
            " small",
            " and",
            " faint",
            " objects",
            ",",
            " the",
            " data",
            " that",
            " can",
            " be",
            " obtained",
            " from",
            " ground",
            "-based",
            " observations",
            " (",
            "G",
            "BO",
            ")",
            " are",
            " limited",
            ".",
            " By",
            " means",
            " of",
            " ground",
            "-based",
            " optical",
            " telesc",
            "opes",
            " the",
            " visual",
            " magnitude",
            " can",
            " be",
            " obtained",
            ";",
            " when",
            " converted",
            " into",
            " the",
            " absolute",
            " magnitude",
            " it",
            " gives",
            " a",
            " rough",
            " estimate",
            " of",
            " the",
            " asteroid",
            "'s",
            " size",
            ".",
            " Light",
            "-c",
            "urve",
            " measurements",
            " can",
            " also",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " rather",
            " small",
            " and",
            " faint",
            " objects",
            ",",
            " the",
            " data",
            " that",
            " can",
            " be",
            " obtained",
            " from",
            " ground",
            "-based",
            " observations",
            " (",
            "G",
            "BO",
            ")",
            " are",
            " limited",
            ".",
            " By",
            " means",
            " of",
            " ground",
            "-based",
            " optical",
            " telesc",
            "opes",
            " the",
            " visual",
            " magnitude",
            " can",
            " be",
            " obtained",
            ";",
            " when",
            " converted",
            " into",
            " the",
            " absolute",
            " magnitude",
            " it",
            " gives",
            " a",
            " rough",
            " estimate",
            " of",
            " the",
            " asteroid",
            "'s",
            " size",
            ".",
            " Light",
            "-c",
            "urve",
            " measurements",
            " can",
            " also",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " NGC",
            " ",
            "673",
            ",",
            " a",
            " face",
            "-on",
            " class",
            " S",
            "AB",
            "(s",
            ")c",
            " galaxy",
            ".",
            " It",
            " is",
            " a",
            " weak",
            "ly",
            " barred",
            " spiral",
            " galaxy",
            " with",
            " loosely",
            " wound",
            " arms",
            ".",
            " It",
            " has",
            " no",
            " ring",
            " and",
            " a",
            " faint",
            " bul",
            "ge",
            " and",
            " is",
            " ",
            "2",
            ".",
            "5",
            " by",
            " ",
            "1",
            ".",
            "9",
            " arc",
            "minutes",
            ".",
            " It",
            " has",
            " two",
            " primary",
            " arms",
            " with",
            " fragments",
            " located",
            " farther",
            " from",
            " the",
            " core"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.167,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " non",
            "comm",
            "ut",
            "ative",
            " geometry",
            ".",
            " He",
            " is",
            " a",
            " professor",
            " at",
            " the",
            " ,",
            " ,",
            " Ohio",
            " State",
            " University",
            " and",
            " Vanderbilt",
            " University",
            ".",
            " He",
            " was",
            " awarded",
            " the",
            " Fields",
            " Medal",
            " in",
            " ",
            "198",
            "2",
            ".",
            "Career",
            "Al",
            "ain",
            " Con",
            "nes",
            " attended",
            " high",
            " school",
            " at",
            " ",
            " in",
            " Marseille",
            ",",
            " and",
            " was",
            " then",
            " a",
            " student",
            " of",
            " the",
            " classes",
            " prÃ©",
            "par",
            "ato",
            "ires",
            " in",
            " .",
            " Between",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            " he",
            " holds",
            " the",
            " L",
            "Ã©",
            "on",
            " Mot",
            "ch",
            "ane",
            " Chair",
            " at",
            " IH",
            "ES",
            ".",
            " From",
            " ",
            "198",
            "4",
            " until",
            " his",
            " retirement",
            " in",
            " ",
            "201",
            "7",
            " he",
            " held",
            " the",
            " chair",
            " of",
            " Analysis",
            " and",
            " Geometry",
            " at",
            " Coll",
            "Ã¨ge",
            " de",
            " France",
            ".",
            "In",
            " parallel",
            ",",
            " he",
            " was",
            " awarded",
            " a",
            " distinguished",
            " professor",
            "ship",
            " at",
            " Vanderbilt",
            " University",
            " between",
            " ",
            "200",
            "3",
            " and",
            " ",
            "201",
            "2",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            " he",
            " holds",
            " the",
            " L",
            "Ã©",
            "on",
            " Mot",
            "ch",
            "ane",
            " Chair",
            " at",
            " IH",
            "ES",
            ".",
            " From",
            " ",
            "198",
            "4",
            " until",
            " his",
            " retirement",
            " in",
            " ",
            "201",
            "7",
            " he",
            " held",
            " the",
            " chair",
            " of",
            " Analysis",
            " and",
            " Geometry",
            " at",
            " Coll",
            "Ã¨ge",
            " de",
            " France",
            ".",
            "In",
            " parallel",
            ",",
            " he",
            " was",
            " awarded",
            " a",
            " distinguished",
            " professor",
            "ship",
            " at",
            " Vanderbilt",
            " University",
            " between",
            " ",
            "200",
            "3",
            " and",
            " ",
            "201",
            "2",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " wrote",
            " throughout",
            " his",
            " four",
            " years",
            " at",
            " the",
            " University",
            " of",
            " Wisconsin",
            ",",
            " where",
            " he",
            " received",
            " a",
            " B",
            ".A",
            ".",
            " in",
            " ",
            "193",
            "0",
            ".",
            " During",
            " this",
            " time",
            " he",
            " also",
            " served",
            " briefly",
            " as",
            " associate",
            " editor",
            " of",
            " Minneapolis",
            "-based",
            " F",
            "aw",
            "c",
            "ett",
            " Publications",
            " Mystic",
            " Magazine",
            ".",
            "Returning",
            " to",
            " Sau",
            "k",
            " City",
            " in",
            " the",
            " summer",
            " of",
            " ",
            "193",
            "1",
            ",",
            " Der",
            "le",
            "th",
            " worked",
            " in",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            ").",
            "In",
            " ",
            "200",
            "6",
            ",",
            " Dr",
            ".",
            " Muhammad",
            " Yun",
            "us",
            " of",
            " Bangladesh",
            " was",
            " awarded",
            " the",
            " Nobel",
            " Peace",
            " Prize",
            " for",
            " the",
            " establishment",
            " of",
            " G",
            "rame",
            "en",
            " Bank",
            ",",
            " a",
            " community",
            " development",
            " bank",
            " that",
            " lends",
            " money",
            " to",
            " poor",
            " people",
            ",",
            " especially",
            " women",
            " in",
            " Bangladesh",
            ".",
            " Dr",
            ".",
            " Yun",
            "us",
            " received",
            " his",
            " PhD",
            " in",
            " economics",
            " from",
            " Vanderbilt",
            " University",
            ",",
            " United",
            " States",
            ".",
            " He",
            " is",
            " internationally"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "Music",
            " ",
            "War",
            "hol",
            " strongly",
            " influenced",
            " the",
            " new",
            " wave",
            "/p",
            "unk",
            " rock",
            " band",
            " De",
            "vo",
            ",",
            " as",
            " well",
            " as",
            " David",
            " Bowie",
            ".",
            " Bowie",
            " recorded",
            " a",
            " song",
            " called",
            " \"",
            "Andy",
            " War",
            "hol",
            "\"",
            " for",
            " his",
            " ",
            "197",
            "1",
            " album",
            " H",
            "unky",
            " D",
            "ory",
            ".",
            " Lou",
            " Reed",
            " wrote",
            " the",
            " song",
            " \"",
            "Andy",
            "'s",
            " Chest",
            "\",",
            " about",
            " Valerie",
            " Sol",
            "anas",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "41",
            " Ari",
            "et",
            "is",
            " (",
            "also",
            " fourth",
            " magnitude",
            ").",
            " The",
            " few",
            " deep",
            "-s",
            "ky",
            " objects",
            " within",
            " the",
            " constellation",
            " are",
            " quite",
            " faint",
            " and",
            " include",
            " several",
            " pairs",
            " of",
            " interacting",
            " galaxies",
            ".",
            " Several",
            " meteor",
            " showers",
            " appear",
            " to",
            " radi",
            "ate",
            " from",
            " A",
            "ries",
            ",",
            " including",
            " the",
            " Day",
            "time",
            " Ari",
            "et",
            "ids",
            " and",
            " the",
            " E",
            "psilon",
            " Ari",
            "et",
            "ids",
            ".",
            "History",
            " and",
            " mythology",
            " ",
            "A",
            "ries",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "196",
            "0",
            "s",
            ",",
            " the",
            " state",
            "'s",
            " economy",
            " shifted",
            " away",
            " from",
            " its",
            " traditional",
            " lumber",
            ",",
            " steel",
            ",",
            " and",
            " textile",
            " industries",
            " because",
            " of",
            " increased",
            " foreign",
            " competition",
            ".",
            " Steel",
            " jobs",
            ",",
            " for",
            " instance",
            ",",
            " declined",
            " from",
            " ",
            "46",
            ",",
            "314",
            " in",
            " ",
            "195",
            "0",
            " to",
            " ",
            "14",
            ",",
            "185",
            " in",
            " ",
            "201",
            "1",
            ".",
            " However",
            ",",
            " the",
            " state",
            ",",
            " particularly",
            " Hunts",
            "ville",
            ",",
            " benefited",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.247,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "ati",
            "ens",
            ",",
            " mir",
            "abil",
            "is",
            ",",
            " wax",
            " beg",
            "onia",
            ",",
            " snap",
            "dragon",
            ",",
            " pel",
            "argon",
            "ium",
            ",",
            " cole",
            "us",
            " and",
            " pet",
            "un",
            "ia",
            ".",
            " Examples",
            " of",
            " true",
            " annual",
            "s",
            " include",
            " corn",
            ",",
            " wheat",
            ",",
            " rice",
            ",",
            " lettuce",
            ",",
            " peas",
            ",",
            " water",
            "melon",
            ",",
            " beans",
            ",",
            " z",
            "inn",
            "ia",
            " and",
            " mar",
            "ig",
            "old",
            ".",
            "Summer",
            "Summer",
            " annual",
            "s",
            " spr",
            "out",
            ",",
            " flower"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " made",
            " it",
            " the",
            " biggest",
            " contributor",
            " to",
            " parent",
            " Volkswagen",
            " Group",
            "'s",
            " nine",
            "-month",
            " operating",
            " profit",
            " of",
            " âĤ¬",
            "1",
            ".",
            "5",
            "Âłb",
            "illion",
            ",",
            " while",
            " the",
            " other",
            " mar",
            "ques",
            " in",
            " Group",
            " such",
            " as",
            " Bentley",
            " and",
            " SE",
            "AT",
            " had",
            " suffered",
            " considerable",
            " losses",
            ".",
            " May",
            " ",
            "201",
            "1",
            " saw",
            " record",
            " sales",
            " for",
            " Audi",
            " of",
            " America",
            " with",
            " the",
            " new",
            " Audi",
            " A",
            "7",
            " and",
            " Audi",
            " A",
            "3",
            " T"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.175,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " P",
            "aine",
            " was",
            " then",
            " asked",
            ",",
            " and",
            " agreed",
            ",",
            " to",
            " host",
            " protesters",
            " as",
            " spectators",
            " at",
            " the",
            " launch",
            ",",
            " and",
            " Aber",
            "n",
            "athy",
            ",",
            " aw",
            "estr",
            "uck",
            " by",
            " the",
            " spectacle",
            ",",
            " prayed",
            " for",
            " the",
            " astronauts",
            ".",
            " R",
            "acial",
            " and",
            " financial",
            " inequalities",
            " frustrated",
            " citizens",
            " who",
            " wondered",
            " why",
            " money",
            " spent",
            " on",
            " the",
            " Apollo",
            " program",
            " was",
            " not",
            " spent",
            " taking",
            " care",
            " of",
            " humans",
            " on",
            " Earth",
            ".",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.172,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Ches",
            "apeake",
            " Bay",
            " on",
            " August",
            " ",
            "24",
            ".",
            " He",
            " now",
            " compounded",
            " failure",
            " to",
            " support",
            " Burg",
            "oy",
            "ne",
            " by",
            " missing",
            " repeated",
            " opportunities",
            " to",
            " destroy",
            " his",
            " opponent",
            ",",
            " defeating",
            " Washington",
            " at",
            " the",
            " Battle",
            " of",
            " Br",
            "andy",
            "wine",
            " on",
            " September",
            " ",
            "11",
            ",",
            " then",
            " allowing",
            " him",
            " to",
            " withdraw",
            " in",
            " good",
            " order",
            ".",
            " After",
            " dispers",
            "ing",
            " an",
            " American",
            " detachment",
            " at",
            " Pa",
            "oli",
            " on",
            " September",
            " ",
            "20"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.159,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " shaped",
            " trench",
            " with",
            " their",
            " fore",
            "fe",
            "et",
            " and",
            " then",
            " sniff",
            " it",
            " prof",
            "us",
            "ely",
            " as",
            " a",
            " means",
            " to",
            " explore",
            " their",
            " location",
            ".",
            " When",
            " a",
            " concentration",
            " of",
            " ants",
            " or",
            " ter",
            "mites",
            " is",
            " detected",
            ",",
            " the",
            " a",
            "ard",
            "v",
            "ark",
            " digs",
            " into",
            " it",
            " with",
            " its",
            " powerful",
            " front",
            " legs",
            ",",
            " keeping",
            " its",
            " long",
            " ears",
            " upright",
            " to",
            " listen",
            " for",
            " predators",
            ",",
            " and",
            " takes",
            " up",
            " an",
            " astonishing",
            " number"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.155,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " North",
            " America",
            " in",
            " ",
            "191",
            "6",
            " (",
            "first",
            " to",
            " Canada",
            ",",
            " then",
            " to",
            " the",
            " United",
            " States",
            ")",
            " to",
            " coordinate",
            " the",
            " shipment",
            " of",
            " artillery",
            " to",
            " Russia",
            ".",
            " He",
            " also",
            " lect",
            "ured",
            " to",
            " Polish",
            "-American",
            " audiences",
            " about",
            " the",
            " conflict",
            ",",
            " promoting",
            " the",
            " sale",
            " of",
            " war",
            " bonds",
            ".",
            " After",
            " the",
            " war",
            " he",
            " decided",
            " to",
            " remain",
            " in",
            " the",
            " United",
            " States",
            ",",
            " becoming",
            " a",
            " natural",
            "ized",
            " citizen",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " Zeus",
            " then",
            " stole",
            " him",
            " from",
            " E",
            "os",
            " and",
            " employed",
            " him",
            " as",
            " cup",
            "-b",
            "earer",
            ".",
            " Yet",
            " another",
            " figure",
            " associated",
            " with",
            " the",
            " water",
            " bearer",
            " is",
            " Cec",
            "rops",
            " I",
            ",",
            " a",
            " king",
            " of",
            " Athens",
            " who",
            " sacrificed",
            " water",
            " instead",
            " of",
            " wine",
            " to",
            " the",
            " gods",
            ".",
            "Dep",
            "ictions",
            "In",
            " the",
            " first",
            " century",
            ",",
            " P",
            "to",
            "le",
            "my",
            "'s",
            " Al",
            "mage",
            "st",
            " established",
            " the",
            " common",
            " Western"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "202",
            "1",
            ",",
            " Audi",
            " announced",
            " that",
            " it",
            " is",
            " planning",
            " to",
            " sell",
            " ",
            "1",
            " million",
            " vehicles",
            " in",
            " China",
            " in",
            " ",
            "202",
            "3",
            ",",
            " comparing",
            " to",
            " ",
            "726",
            ",",
            "000",
            " vehicles",
            " in",
            " ",
            "202",
            "0",
            ".",
            "Technology",
            "A",
            "udi",
            " AI",
            " ",
            "A",
            "udi",
            " AI",
            " is",
            " a",
            " driver",
            " assist",
            " feature",
            " offered",
            " by",
            " Audi",
            ".",
            " The",
            " company",
            "'s",
            " stated",
            " intent",
            " is",
            " to",
            " offer",
            " fully",
            " autonomous"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " potential",
            " donors",
            " hoping",
            " to",
            " induce",
            " recipro",
            "city",
            ".",
            " Another",
            " method",
            " is",
            " to",
            " announce",
            " publicly",
            " that",
            " someone",
            " has",
            " given",
            " a",
            " large",
            " donation",
            ".",
            " The",
            " tendency",
            " to",
            " recip",
            "roc",
            "ate",
            " can",
            " even",
            " generalize",
            ",",
            " so",
            " people",
            " become",
            " more",
            " helpful",
            " toward",
            " others",
            " after",
            " being",
            " helped",
            ".",
            " On",
            " the",
            " other",
            " hand",
            ",",
            " people",
            " will",
            " avoid",
            " or",
            " even",
            " retal",
            "iate",
            " against",
            " those",
            " perceived",
            " not",
            " to",
            " be",
            " cooperating",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " modern",
            " understanding",
            " of",
            " Bach",
            "'s",
            " music",
            ".",
            " He",
            " became",
            " a",
            " welcome",
            " guest",
            " at",
            " the",
            " W",
            "agn",
            "ers",
            "'",
            " home",
            ",",
            " W",
            "ahn",
            "fried",
            ".",
            " He",
            " also",
            " correspond",
            "ed",
            " with",
            " composer",
            " Clara",
            " F",
            "ais",
            "st",
            ",",
            " who",
            " became",
            " a",
            " good",
            " friend",
            ".",
            "His",
            " pamph",
            "let",
            " \"",
            "The",
            " Art",
            " of",
            " Organ",
            " Building",
            " and",
            " Organ",
            " Playing",
            " in",
            " Germany",
            " and",
            " France",
            "\"",
            " (",
            "190",
            "6",
            ",",
            " rep"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ly",
            " collaborated",
            " in",
            " one",
            " of",
            " his",
            " myths",
            ".",
            "S",
            "peer",
            " also",
            " sought",
            " to",
            " portray",
            " himself",
            " as",
            " an",
            " opponent",
            " of",
            " Hitler",
            "'s",
            " leadership",
            ".",
            " Despite",
            " his",
            " opposition",
            " to",
            " the",
            " ",
            "20",
            " July",
            " plot",
            ",",
            " he",
            " falsely",
            " claimed",
            " in",
            " his",
            " memoir",
            "s",
            " to",
            " have",
            " been",
            " sympathetic",
            " to",
            " the",
            " pl",
            "ott",
            "ers",
            ".",
            " He",
            " maintained",
            " Hitler",
            " was",
            " cool",
            " towards",
            " him",
            " for",
            " the",
            " remainder",
            " of",
            " his",
            " life"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "rah",
            "edral",
            " [",
            "Li",
            "(H",
            "2",
            "O",
            ")",
            "4",
            "]+",
            ":",
            " while",
            " sol",
            "vation",
            " numbers",
            " of",
            " ",
            "3",
            " to",
            " ",
            "6",
            " have",
            " been",
            " found",
            " for",
            " lithium",
            " aqu",
            "a",
            " ions",
            ",",
            " sol",
            "vation",
            " numbers",
            " less",
            " than",
            " ",
            "4",
            " may",
            " be",
            " the",
            " result",
            " of",
            " the",
            " formation",
            " of",
            " contact",
            " ion",
            " pairs",
            ",",
            " and",
            " the",
            " higher",
            " sol",
            "vation",
            " numbers",
            " may",
            " be",
            " interpreted",
            " in",
            " terms",
            " of",
            " water",
            " molecules"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "oma",
    "undy",
    "chten",
    "_Lean",
    "onde"
  ],
  "bottom_logits": [
    " -",
    "akes",
    " b",
    "umann",
    "CA"
  ],
  "act_min": -0.0,
  "act_max": 0.59
}