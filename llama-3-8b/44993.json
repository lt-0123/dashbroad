{
  "index": 44993,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " they",
            " concentrated",
            " on",
            " looking",
            " for",
            " the",
            " philosophers",
            "'",
            " stone",
            ".",
            " Bernard",
            " Tre",
            "vis",
            "an",
            " and",
            " George",
            " Rip",
            "ley",
            " made",
            " similar",
            " contributions",
            ".",
            " Their",
            " crypt",
            "ic",
            " all",
            "usions",
            " and",
            " symbolism",
            " led",
            " to",
            " wide",
            " variations",
            " in",
            " interpretation",
            " of",
            " the",
            " art",
            ".",
            "A",
            " common",
            " idea",
            " in",
            " European",
            " al",
            "chemy",
            " in",
            " the",
            " medieval",
            " era",
            " was",
            " a",
            " metaph",
            "ysical",
            " \"",
            "Hom",
            "eric",
            " chain",
            " of",
            " wise",
            " men",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "191",
            "9",
            "–",
            "193",
            "9",
            "F",
            "amous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            "While",
            " still",
            " at",
            " Hen",
            "ley",
            "'s",
            ",",
            " he",
            " read",
            " in",
            " a",
            " trade",
            " paper",
            " that",
            " Famous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            ",",
            " the",
            " production",
            " arm",
            " of",
            " Paramount",
            " Pictures",
            ",",
            " was",
            " opening",
            " a",
            " studio",
            " in",
            " London",
            ".",
            " They",
            " were",
            " planning",
            " to",
            " film",
            " The",
            " Sor",
            "rows",
            " of",
            " Satan",
            " by",
            " Marie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "32",
            " UTC",
            ",",
            " and",
            " it",
            " was",
            " the",
            " fifth",
            " crew",
            "ed",
            " mission",
            " of",
            " NASA",
            "'s",
            " Apollo",
            " program",
            ".",
            " The",
            " Apollo",
            " spacecraft",
            " had",
            " three",
            " parts",
            ":",
            " a",
            " command",
            " module",
            " (",
            "CM",
            ")",
            " with",
            " a",
            " cabin",
            " for",
            " the",
            " three",
            " astronauts",
            ",",
            " the",
            " only",
            " part",
            " that",
            " returned",
            " to",
            " Earth",
            ";",
            " a",
            " service",
            " module",
            " (",
            "SM",
            "),",
            " which",
            " supported",
            " the",
            " command",
            " module",
            " with",
            " propulsion",
            ",",
            " electrical",
            " power"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " DÃ¼nya",
            ".",
            "D",
            "us",
            "in",
            "ber",
            "re",
            ",",
            " El",
            "sp",
            "eth",
            " R",
            ".",
            " M",
            ".",
            " ",
            "201",
            "3",
            ".",
            " Empire",
            ",",
            " Authority",
            ",",
            " and",
            " Aut",
            "onomy",
            " In",
            " A",
            "cha",
            "emen",
            "id",
            " Anat",
            "olia",
            ".",
            " Cambridge",
            ":",
            " Cambridge",
            " University",
            " Press",
            ".",
            "G",
            "ates",
            ",",
            " Charles",
            ",",
            " Jacques",
            " Mor",
            "in",
            ",",
            " and",
            " Thomas",
            " Zimmer",
            "mann",
            ".",
            " ",
            "200",
            "9",
            ".",
            " Sacred",
            " Land",
            "sc",
            "apes"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " \"",
            "If",
            " you",
            " see",
            " something",
            " horrible",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ",",
            " and",
            " if",
            " you",
            " see",
            " something",
            " beautiful",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ".\"",
            "After",
            " returning",
            " to",
            " the",
            " United",
            " States",
            ",",
            " a",
            " chance",
            " encounter",
            " on",
            " a",
            " New",
            " York",
            " City",
            " street",
            " with",
            " Ch",
            "Ã¶",
            "gy",
            "am",
            " Trung",
            "pa",
            " Rin",
            "po",
            "che",
            " (",
            "they",
            " both",
            " tried",
            " to",
            " catch",
            " the",
            " same",
            " cab",
            "),"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.045,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "00",
            " UTC",
            " on",
            " July",
            " ",
            "28",
            ".",
            " Columbia",
            " was",
            " taken",
            " to",
            " Ford",
            " Island",
            " for",
            " de",
            "activation",
            ",",
            " and",
            " its",
            " py",
            "rote",
            "chn",
            "ics",
            " made",
            " safe",
            ".",
            " It",
            " was",
            " then",
            " taken",
            " to",
            " Hick",
            "ham",
            " Air",
            " Force",
            " Base",
            ",",
            " from",
            " whence",
            " it",
            " was",
            " flown",
            " to",
            " Houston",
            " in",
            " a",
            " Douglas",
            " C",
            "-",
            "133",
            " C",
            "arg",
            "om",
            "aster",
            ",",
            " reaching",
            " the",
            " Lunar",
            " Re",
            "ceiving",
            " Laboratory",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Become",
            " Human",
            " also",
            " explores",
            " how",
            " android",
            "s",
            " are",
            " treated",
            " as",
            " second",
            " class",
            " citizens",
            " in",
            " a",
            " near",
            " future",
            " society",
            ".",
            "Female",
            " android",
            "s",
            ",",
            " or",
            " \"",
            "g",
            "yn",
            "oids",
            "\",",
            " are",
            " often",
            " seen",
            " in",
            " science",
            " fiction",
            ",",
            " and",
            " can",
            " be",
            " viewed",
            " as",
            " a",
            " continuation",
            " of",
            " the",
            " long",
            " tradition",
            " of",
            " men",
            " attempting",
            " to",
            " create",
            " the",
            " stere",
            "otypical",
            " \"",
            "perfect",
            " woman",
            "\".",
            " Examples",
            " include",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.227,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " S",
            "ins",
            " of",
            " Norman",
            " Os",
            "born",
            " #",
            "1",
            ",",
            " FC",
            "BD",
            " ",
            "202",
            "0",
            ":",
            " Spider",
            "-Man",
            "/V",
            "en",
            "om",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "11",
            ":",
            " Last",
            " Rem",
            "ains",
            " [#",
            "50",
            "–",
            "55",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            ":",
            " Last",
            " Rem",
            "ains",
            " Companion",
            " [#",
            "50",
            ".",
            "1",
            "–",
            "54",
            ".",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "12"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.227,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.141,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            0.322
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            " Birth",
            " of",
            " the",
            " Newton",
            ":",
            " ",
            " The",
            " Newton",
            " Hall",
            " of",
            " Fame",
            ":",
            " People",
            " behind",
            " the",
            " Newton",
            ":",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Why",
            " did",
            " Apple",
            " kill",
            " the",
            " Newton",
            "?:",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Newton",
            " Notes",
            " column",
            " archive",
            ":",
            " ",
            " A",
            ".I",
            ".",
            " Magazine",
            " article",
            " by",
            " Ya",
            "eger",
            " on",
            " Newton",
            " H",
            "WR",
            " design",
            ",",
            " algorithms",
            ",",
            " &",
            " quality",
            ":",
            " ",
            " Associated",
            " slides",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " (",
            "1",
            ")",
            " Korean",
            " did",
            " not",
            " belong",
            " with",
            " the",
            " other",
            " three",
            " gene",
            "alog",
            "ically",
            ",",
            " but",
            " had",
            " been",
            " influenced",
            " by",
            " an",
            " Alta",
            "ic",
            " substr",
            "atum",
            ";",
            " (",
            "2",
            ")",
            " Korean",
            " was",
            " related",
            " to",
            " the",
            " other",
            " three",
            " at",
            " the",
            " same",
            " level",
            " they",
            " were",
            " related",
            " to",
            " each",
            " other",
            ";",
            " (",
            "3",
            ")",
            " Korean",
            " had",
            " split",
            " off",
            " from",
            " the",
            " other",
            " three",
            " before",
            " they",
            " underwent",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " character",
            " who",
            " first",
            " questions",
            " whether",
            " it",
            " is",
            " moral",
            " to",
            " turn",
            " a",
            " violent",
            " person",
            " into",
            " a",
            " behavioural",
            " autom",
            "aton",
            " who",
            " can",
            " make",
            " no",
            " choice",
            " in",
            " such",
            " matters",
            ".",
            " This",
            " is",
            " the",
            " only",
            " character",
            " who",
            " is",
            " truly",
            " concerned",
            " about",
            " Alex",
            "'s",
            " welfare",
            ";",
            " he",
            " is",
            " not",
            " taken",
            " seriously",
            " by",
            " Alex",
            ",",
            " though",
            ".",
            " He",
            " is",
            " nicknamed",
            " by",
            " Alex",
            " \"",
            "pr",
            "ison",
            " char",
            "lie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "2",
            " languages",
            ",",
            " the",
            " Mon",
            " language",
            " of",
            " Burma",
            " and",
            " the",
            " Ny",
            "ah",
            "kur",
            " language",
            " of",
            " Thailand",
            ".",
            "Sid",
            "well",
            " (",
            "200",
            "9",
            "–",
            "201",
            "5",
            ")",
            " ",
            "Paul",
            " Sid",
            "well",
            " (",
            "200",
            "9",
            "),",
            " in",
            " a",
            " le",
            "xic",
            "ostat",
            "istical",
            " comparison",
            " of",
            " ",
            "36",
            " languages",
            " which",
            " are",
            " well",
            " known",
            " enough",
            " to",
            " exclude",
            " loan",
            "words",
            ",",
            " finds",
            " little",
            " evidence",
            " for",
            " internal",
            " branching"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "21",
            " languages",
            " of",
            " Burma",
            ",",
            " southern",
            " China",
            ",",
            " and",
            " Thailand",
            " Nuclear",
            " Mon",
            "–",
            "Kh",
            "mer",
            " languages",
            " Kh",
            "mer",
            "o",
            "-V",
            "iet",
            "ic",
            " languages",
            " (",
            "Eastern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " Viet",
            "o",
            "-K",
            "atu",
            "ic",
            " languages",
            " ?",
            " Viet",
            "ic",
            ":",
            " ",
            "10",
            " languages",
            " of",
            " Vietnam",
            " and",
            " Laos",
            ",",
            " including",
            " Mu",
            "ong",
            " and",
            " Vietnamese",
            ",",
            " which",
            " has",
            " the",
            " most",
            " speakers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " potassium",
            "-",
            "40",
            ",",
            " van",
            "adium",
            "-",
            "50",
            ",",
            " lan",
            "than",
            "um",
            "-",
            "138",
            ",",
            " and",
            " lut",
            "et",
            "ium",
            "-",
            "176",
            ".",
            " Most",
            " odd",
            "-",
            "odd",
            " nuclei",
            " are",
            " highly",
            " unstable",
            " with",
            " respect",
            " to",
            " beta",
            " decay",
            ",",
            " because",
            " the",
            " decay",
            " products",
            " are",
            " even",
            "-even",
            ",",
            " and",
            " are",
            " therefore",
            " more",
            " strongly",
            " bound",
            ",",
            " due",
            " to",
            " nuclear",
            " pairing",
            " effects",
            ".",
            "Mass",
            " ",
            "The",
            " large",
            " majority"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " '",
            "To",
            " a",
            " gas",
            " chamber",
            "—",
            "go",
            "!.",
            "Rand",
            "'s",
            " non",
            "fiction",
            " received",
            " far",
            " fewer",
            " reviews",
            " than",
            " her",
            " novels",
            ".",
            " The",
            " ten",
            "or",
            " of",
            " the",
            " criticism",
            " for",
            " her",
            " first",
            " non",
            "fiction",
            " book",
            ",",
            " For",
            " the",
            " New",
            " Intellectual",
            ",",
            " was",
            " similar",
            " to",
            " that",
            " for",
            " Atlas",
            " Shr",
            "ugged",
            ".",
            " Phil",
            "osopher",
            " Sidney",
            " Hook",
            " liken",
            "ed",
            " her",
            " certainty",
            " to",
            " \"",
            "the",
            " way",
            " philosophy",
            " is",
            " written"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " On",
            " This",
            " Day",
            " ",
            " Historical",
            " Events",
            " on",
            " April",
            " ",
            "12",
            "Days",
            " of",
            " the",
            " year",
            "April",
            "<|begin_of_text|>",
            "Events",
            "Pre",
            "-",
            "160",
            "0",
            " ",
            "769",
            " –",
            " The",
            " Later",
            "an",
            " Council",
            " ends",
            " by",
            " condemning",
            " the",
            " Council",
            " of",
            " Hier",
            "ia",
            " and",
            " an",
            "ath",
            "emat",
            "izing",
            " its",
            " icon",
            "oc",
            "lastic",
            " rulings",
            ".",
            "107",
            "1",
            " –",
            " B",
            "ari",
            ",",
            " the",
            " last",
            " Byz",
            "antine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.199,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Mol",
            "oday",
            "a",
            " G",
            "vard",
            "ia",
            ".",
            " (",
            "also",
            " a",
            " ",
            "199",
            "2",
            " Simon",
            " &",
            " Sch",
            "uster",
            " edition",
            ")",
            "References",
            "Further",
            " reading",
            " Fine",
            ",",
            " Rue",
            "ben",
            " (",
            "198",
            "3",
            ").",
            " The",
            " World",
            "'s",
            " Great",
            " Chess",
            " Games",
            ".",
            " Dover",
            ".",
            " .",
            " Hur",
            "st",
            ",",
            " Sarah",
            " (",
            "200",
            "2",
            ").",
            " Curse",
            " of",
            " K",
            "irs",
            "an",
            ":",
            " Adventures",
            " in",
            " the",
            " Chess",
            " Under",
            "world"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Po",
            "et",
            " in",
            " the",
            " Cinema",
            " (",
            "198",
            "4",
            "):",
            " directed",
            " by",
            " Don",
            "at",
            "ella",
            " Bag",
            "l",
            "ivo",
            ".",
            " Moscow",
            " E",
            "leg",
            "y",
            " (",
            "198",
            "7",
            "),",
            " a",
            " documentary",
            "/h",
            "om",
            "age",
            " to",
            " T",
            "ark",
            "ovsky",
            " by",
            " Aleks",
            "andr",
            " Sok",
            "uro",
            "v",
            ".",
            " Auf",
            " der",
            " Suche",
            " nach",
            " der",
            " ver",
            "lo",
            "ren",
            "en",
            " Zeit",
            " (",
            "198",
            "8",
            "):",
            " Andre",
            "j",
            " T",
            "ark",
            "owski"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " two",
            " lengths",
            " must",
            " not",
            " be",
            " prime",
            " to",
            " one",
            " another",
            ".",
            " Eu",
            "clid",
            " stip",
            "ulated",
            " this",
            " so",
            " that",
            " he",
            " could",
            " construct",
            " a",
            " re",
            "duct",
            "io",
            " ad",
            " absurd",
            "um",
            " proof",
            " that",
            " the",
            " two",
            " numbers",
            "'",
            " common",
            " measure",
            " is",
            " in",
            " fact",
            " the",
            " greatest",
            ".",
            " While",
            " Nic",
            "om",
            "ach",
            "us",
            "'",
            " algorithm",
            " is",
            " the",
            " same",
            " as",
            " Eu",
            "clid",
            "'s",
            ",",
            " when",
            " the",
            " numbers",
            " are",
            " prime"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.172,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.162,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Post",
            "card",
            " Book",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " (",
            "200",
            "2",
            ").",
            " ",
            " H",
            "re",
            "ben",
            "i",
            "ak",
            ",",
            " Michael",
            ".",
            " Action",
            " Writing",
            ":",
            " Jack",
            " Ker",
            "ou",
            "ac",
            "'s",
            " Wild",
            " Form",
            ",",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " UP",
            ",",
            " ",
            "200",
            "6",
            ".",
            " Kash",
            "ner",
            ",",
            " Sam",
            ",",
            " When",
            " I",
            " Was",
            " Cool",
            ",",
            " My",
            " Life",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.206,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.098,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "57",
            "Âł",
            "minutes",
            ".",
            " Wimbledon",
            ":",
            " The",
            " Record",
            " Break",
            "ers",
            " (",
            "200",
            "5",
            ")",
            " St",
            "arring",
            ":",
            " Andre",
            " Ag",
            "assi",
            ",",
            " Boris",
            " Becker",
            ";",
            " Standing",
            " Room",
            " Only",
            ",",
            " DVD",
            " Release",
            " Date",
            ":",
            " August",
            " ",
            "16",
            ",",
            " ",
            "200",
            "5",
            ",",
            " Run",
            " Time",
            ":",
            " ",
            "52",
            "Âł",
            "minutes",
            ",",
            " .",
            "Video",
            " games",
            " Andre",
            " Ag",
            "assi",
            " Tennis",
            " for",
            " the",
            " Super",
            " Nintendo",
            " Entertainment"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " UTC",
            ",",
            " GPS",
            ",",
            " L",
            "OR",
            "AN",
            " and",
            " T",
            "AI",
            "Time",
            " scales",
            "<|begin_of_text|>",
            "Al",
            "tru",
            "ism",
            " is",
            " the",
            " principle",
            " and",
            " practice",
            " of",
            " concern",
            " for",
            " the",
            " well",
            "-being",
            " and",
            "/or",
            " happiness",
            " of",
            " other",
            " humans",
            " or",
            " animals",
            ".",
            " While",
            " objects",
            " of",
            " altru",
            "istic",
            " concern",
            " vary",
            ",",
            " it",
            " is",
            " an",
            " important",
            " moral",
            " value",
            " in",
            " many",
            " cultures",
            " and",
            " religions",
            ".",
            " It",
            " may",
            " be",
            " considered",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.324,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Sh",
            "attered",
            " Web",
            " [#",
            "56",
            "–",
            "60",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "13",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " [#",
            "61",
            "–",
            "65",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " #",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "14",
            ":",
            " Ch",
            "ameleon",
            " Conspiracy",
            " [#",
            "66",
            "–",
            "69",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Ch",
            "ameleon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "34",
            ")",
            " or",
            ",",
            " \"",
            "have",
            " taken",
            " place",
            "\"",
            " (",
            "Luke",
            " ",
            "21",
            ":",
            "32",
            ").",
            " Similarly",
            ",",
            " in",
            " ",
            "1",
            "st",
            " Peter",
            " ",
            "1",
            ":",
            "20",
            ",",
            " \"",
            "Christ",
            ",",
            " who",
            " ver",
            "ily",
            " was",
            " fore",
            "ord",
            "ained",
            " before",
            " the",
            " foundation",
            " of",
            " the",
            " world",
            " but",
            " was",
            " manifest",
            " in",
            " these",
            " last",
            " times",
            " for",
            " you",
            "\",",
            " as",
            " well",
            " as",
            " \"",
            "But",
            " the",
            " end",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "Although",
            " I",
            " have",
            " been",
            " prevented",
            " by",
            " outward",
            " circumstances",
            " from",
            " observing",
            " a",
            " strictly",
            " vegetarian",
            " diet",
            ",",
            " I",
            " have",
            " long",
            " been",
            " an",
            " adher",
            "ent",
            " to",
            " the",
            " cause",
            " in",
            " principle",
            ".",
            " Besides",
            " agreeing",
            " with",
            " the",
            " aims",
            " of",
            " vegetarian",
            "ism",
            " for",
            " aesthetic",
            " and",
            " moral",
            " reasons",
            ",",
            " it",
            " is",
            " my",
            " view",
            " that",
            " a",
            " vegetarian",
            " manner",
            " of",
            " living",
            " by",
            " its",
            " purely",
            " physical",
            " effect",
            " on",
            " the",
            " human",
            " temperament",
            " would"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Debug",
            "ging",
            " the",
            " Gender",
            " Gap",
            " List",
            " of",
            " pioneers",
            " in",
            " computer",
            " science",
            " Timeline",
            " of",
            " women",
            " in",
            " science",
            " Women",
            " in",
            " computing",
            " Women",
            " in",
            " STEM",
            " fields",
            "Ex",
            "plan",
            "atory",
            " notes",
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            " .",
            " .",
            " .",
            " .",
            " .",
            " .",
            "  ",
            " .",
            " ",
            " With",
            " notes",
            " upon",
            " the",
            " memoir",
            " by",
            " the",
            " translator",
            ".",
            " Miller"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " glob",
            "ular",
            " clusters",
            " Mess",
            "ier",
            "Âł",
            "2",
            ",",
            " Mess",
            "ier",
            "Âł",
            "72",
            ",",
            " and",
            " the",
            " aster",
            "ism",
            " Mess",
            "ier",
            "Âł",
            "73",
            ".",
            " While",
            " M",
            "73",
            " was",
            " originally",
            " catalog",
            "ued",
            " as",
            " a",
            " sp",
            "ars",
            "ely",
            " populated",
            " open",
            " cluster",
            ",",
            " modern",
            " analysis",
            " indicates",
            " the",
            " ",
            "6",
            " main",
            " stars",
            " are",
            " not",
            " close",
            " enough",
            " together",
            " to",
            " fit",
            " this",
            " definition",
            ",",
            " re",
            "class",
            "ifying",
            " M",
            "73"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.33,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Civil",
            " War",
            " [#",
            "532",
            "–",
            "538",
            "]",
            " ()",
            "Vol",
            ".",
            " ",
            "12",
            ":",
            " Back",
            " in",
            " Black",
            " [#",
            "539",
            "–",
            "543",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "17",
            "–",
            "23",
            ",",
            " Annual",
            " #",
            "1",
            "]",
            " ()",
            "Spider",
            "-Man",
            ":",
            " One",
            " More",
            " Day",
            " [#",
            "544",
            "–",
            "545",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "24",
            ";",
            " The",
            " Sens",
            "ational",
            " Spider",
            "-Man",
            " #",
            "41",
            ";",
            " Marvel"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " annual",
            " production",
            " of",
            " aluminium",
            " exceeded",
            " ",
            "50",
            ",",
            "000",
            ",",
            "000",
            " metric",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            "The",
            " real",
            " price",
            " for",
            " aluminium",
            " declined",
            " from",
            " $",
            "14",
            ",",
            "000",
            " per",
            " metric",
            " ton",
            " in",
            " ",
            "190",
            "0",
            " to",
            " $",
            "2",
            ",",
            "340",
            " in",
            " ",
            "194",
            "8",
            " (",
            "in",
            " ",
            "199",
            "8",
            " United",
            " States",
            " dollars",
            ").",
            " Extraction",
            " and",
            " processing",
            " costs",
            " were",
            " lowered",
            " over"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.199,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Mol",
            "oday",
            "a",
            " G",
            "vard",
            "ia",
            ".",
            " (",
            "also",
            " a",
            " ",
            "199",
            "2",
            " Simon",
            " &",
            " Sch",
            "uster",
            " edition",
            ")",
            "References",
            "Further",
            " reading",
            " Fine",
            ",",
            " Rue",
            "ben",
            " (",
            "198",
            "3",
            ").",
            " The",
            " World",
            "'s",
            " Great",
            " Chess",
            " Games",
            ".",
            " Dover",
            ".",
            " .",
            " Hur",
            "st",
            ",",
            " Sarah",
            " (",
            "200",
            "2",
            ").",
            " Curse",
            " of",
            " K",
            "irs",
            "an",
            ":",
            " Adventures",
            " in",
            " the",
            " Chess",
            " Under",
            "world"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.527,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " \"",
            "If",
            " you",
            " see",
            " something",
            " horrible",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ",",
            " and",
            " if",
            " you",
            " see",
            " something",
            " beautiful",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ".\"",
            "After",
            " returning",
            " to",
            " the",
            " United",
            " States",
            ",",
            " a",
            " chance",
            " encounter",
            " on",
            " a",
            " New",
            " York",
            " City",
            " street",
            " with",
            " Ch",
            "Ã¶",
            "gy",
            "am",
            " Trung",
            "pa",
            " Rin",
            "po",
            "che",
            " (",
            "they",
            " both",
            " tried",
            " to",
            " catch",
            " the",
            " same",
            " cab",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            ":",
            " Serbia",
            " declares",
            " war",
            " on",
            " Germany",
            ";",
            " Austria",
            " declares",
            " war",
            " on",
            " Russia",
            ".",
            "191",
            "5",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " S",
            "ari",
            " Bair",
            ":",
            " The",
            " Allies",
            " mount",
            " a",
            " diversion",
            "ary",
            " attack",
            " timed",
            " to",
            " coincide",
            " with",
            " a",
            " major",
            " Allied",
            " landing",
            " of",
            " reinforcements",
            " at",
            " S",
            "uv",
            "la",
            " Bay",
            ".",
            "191",
            "7",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " M",
            "Äĥr",
            "Äĥ",
            "ÈĻ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.111,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " lunar",
            " descent",
            ".",
            " At",
            " ",
            "17",
            ":",
            "44",
            ":",
            "00",
            " Eagle",
            " separated",
            " from",
            " Columbia",
            ".",
            " Collins",
            ",",
            " alone",
            " aboard",
            " Columbia",
            ",",
            " inspected",
            " Eagle",
            " as",
            " it",
            " pir",
            "ou",
            "et",
            "ted",
            " before",
            " him",
            " to",
            " ensure",
            " the",
            " craft",
            " was",
            " not",
            " damaged",
            ",",
            " and",
            " that",
            " the",
            " landing",
            " gear",
            " was",
            " correctly",
            " deployed",
            ".",
            " Armstrong",
            " exclaimed",
            ":",
            " \"",
            "The",
            " Eagle",
            " has",
            " wings",
            "!\"",
            "As",
            " the",
            " descent",
            " began",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.279,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " displays",
            " ",
            "16",
            ":",
            "10",
            " =",
            " ",
            "1",
            ".",
            "6",
            ":",
            " commonly",
            " used",
            " wides",
            "creen",
            " computer",
            " displays",
            " (",
            "WX",
            "GA",
            ")",
            " Î¦",
            ":",
            "1",
            " =",
            " ",
            "1",
            ".",
            "618",
            "...",
            ":",
            " golden",
            " ratio",
            ",",
            " close",
            " to",
            " ",
            "16",
            ":",
            "10",
            " ",
            "5",
            ":",
            "3",
            " =",
            " ",
            "1",
            ".:",
            " super",
            " ",
            "16",
            " mm",
            ",",
            " ",
            " a",
            " standard",
            " film",
            " gauge",
            " in",
            " many",
            " European"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " years",
            " have",
            " passed",
            " since",
            " then",
            ",",
            " implying",
            " that",
            " she",
            " is",
            " nearly",
            " ",
            "75",
            " years",
            " old",
            ";",
            " but",
            " in",
            " ",
            "4",
            ":",
            "50",
            " from",
            " Padding",
            "ton",
            ",",
            " published",
            " almost",
            " a",
            " decade",
            " earlier",
            " in",
            " ",
            "195",
            "7",
            ",",
            " she",
            " says",
            " she",
            " will",
            " be",
            " \"",
            "90",
            " next",
            " year",
            ".\"",
            "Ex",
            "cluding",
            " Sleeping",
            " Murder",
            ",",
            " ",
            "41",
            " years",
            " passed",
            " between",
            " the",
            " first",
            " and",
            " last",
            "-written",
            " novels",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " an",
            " empty",
            " Hass",
            "el",
            "bl",
            "ad",
            " camera",
            ",",
            " and",
            " other",
            " equipment",
            ".",
            " The",
            " hatch",
            " was",
            " closed",
            " again",
            " at",
            " ",
            "05",
            ":",
            "11",
            ":",
            "13",
            ".",
            " They",
            " then",
            " press",
            "ur",
            "ized",
            " the",
            " LM",
            " and",
            " settled",
            " down",
            " to",
            " sleep",
            ".",
            "Pres",
            "idential",
            " speech",
            " writer",
            " William",
            " Saf",
            "ire",
            " had",
            " prepared",
            " an",
            " In",
            " Event",
            " of",
            " Moon",
            " Disaster",
            " announcement",
            " for",
            " Nixon",
            " to",
            " read",
            " in",
            " the",
            " event",
            " the",
            " Apollo"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " larger",
            " outputs",
            " expected",
            " to",
            " be",
            " obtained",
            " if",
            " more",
            " time",
            "-taking",
            " production",
            " processes",
            " are",
            " undertaken",
            ".",
            "He",
            " included",
            " two",
            " additional",
            " ten",
            "ets",
            " held",
            " by",
            " the",
            " M",
            "ises",
            " branch",
            " of",
            " Austrian",
            " economics",
            ":",
            " Consumer",
            " sovereignty",
            ":",
            " the",
            " influence",
            " consumers",
            " have",
            " on",
            " the",
            " effective",
            " demand",
            " for",
            " goods",
            " and",
            " services",
            " and",
            " through",
            " the",
            " prices",
            " which",
            " result",
            " in",
            " free",
            " competitive",
            " markets",
            ",",
            " on",
            " the",
            " production",
            " plans",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.336,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " reservation",
            ".",
            "191",
            "4",
            " –",
            " World",
            " War",
            " I",
            ":",
            " U",
            "-",
            "boat",
            " campaign",
            ":",
            " Two",
            " days",
            " after",
            " the",
            " United",
            " Kingdom",
            " had",
            " declared",
            " war",
            " on",
            " Germany",
            " over",
            " the",
            " German",
            " invasion",
            " of",
            " Belgium",
            ",",
            " ten",
            " German",
            " U",
            "-bo",
            "ats",
            " leave",
            " their",
            " base",
            " in",
            " Hel",
            "ig",
            "oland",
            " to",
            " attack",
            " Royal",
            " Navy",
            " war",
            "ships",
            " in",
            " the",
            " North",
            " Sea",
            ".",
            " ",
            " ",
            "191",
            "4",
            "  ",
            " –",
            " World",
            " War"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.336,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "Algorithm",
            "The",
            " calculations",
            " of",
            " AN",
            "O",
            "VA",
            " can",
            " be",
            " characterized",
            " as",
            " computing",
            " a",
            " number",
            " of",
            " means",
            " and",
            " var",
            "iances",
            ",",
            " dividing",
            " two",
            " var",
            "iances",
            " and",
            " comparing",
            " the",
            " ratio",
            " to",
            " a",
            " handbook",
            " value",
            " to",
            " determine",
            " statistical",
            " significance",
            ".",
            " Calcul",
            "ating",
            " a",
            " treatment",
            " effect",
            " is",
            " then",
            " trivial",
            ":",
            " \"",
            "the",
            " effect",
            " of",
            " any",
            " treatment",
            " is",
            " estimated",
            " by",
            " taking",
            " the",
            " difference",
            " between",
            " the",
            " mean"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " Languages",
            ":",
            " Studies",
            " in",
            " Honour",
            " of",
            " Roy",
            " Andrew",
            " Miller",
            " on",
            " His",
            " ",
            "75",
            "th",
            " Birthday",
            ",",
            " edited",
            " by",
            " Karl",
            " H",
            ".",
            " Meng",
            "es",
            " and",
            " N",
            "elly",
            " Na",
            "umann",
            ",",
            " ",
            "1",
            "–",
            "13",
            ".",
            " W",
            "ies",
            "bad",
            "en",
            ":",
            " Otto",
            " Harr",
            "ass",
            "owitz",
            ".",
            " (",
            "Also",
            ":",
            " HTML",
            " version",
            ".)",
            "J",
            "oh",
            "anson",
            ",",
            " Lars",
            ".",
            " ",
            "199",
            "9",
            ".",
            " \"",
            "At",
            "tract"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " were",
            " slaves",
            ",",
            " while",
            " in",
            " B",
            "ona",
            "ire",
            ",",
            " it",
            " was",
            " ",
            "27",
            " percent",
            ".",
            "A",
            " Population",
            " Report",
            " from",
            " ",
            "182",
            "0",
            " indicates",
            " that",
            " Ar",
            "uba",
            " had",
            " ",
            "331",
            " slaves",
            ":",
            " ",
            "157",
            " were",
            " indigenous",
            " people",
            ",",
            " and",
            " ",
            "174",
            " were",
            " of",
            " African",
            " descent",
            ".",
            " In",
            " ",
            "182",
            "0",
            ",",
            " there",
            " were",
            " also",
            " ",
            "15",
            " free",
            "-born",
            " black",
            " individuals",
            " and",
            " ",
            "19",
            " foreign"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " calculate",
            " an",
            " unbiased",
            " estimate",
            " of",
            " the",
            " population",
            " variance",
            " from",
            " a",
            " finite",
            " sample",
            " of",
            " n",
            " observations",
            ",",
            " the",
            " formula",
            " is",
            ":",
            "Therefore",
            ",",
            " a",
            " naÃ¯",
            "ve",
            " algorithm",
            " to",
            " calculate",
            " the",
            " estimated",
            " variance",
            " is",
            " given",
            " by",
            " the",
            " following",
            ":",
            " Let",
            " ",
            " For",
            " each",
            " datum",
            " :",
            "  ",
            " ",
            " ",
            "This",
            " algorithm",
            " can",
            " easily",
            " be",
            " adapted",
            " to",
            " compute",
            " the",
            " variance",
            " of",
            " a",
            " finite",
            " population",
            ":",
            " simply",
            " divide"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " chatter",
            "boxes",
            " who",
            " pretend",
            " to",
            " speak",
            " in",
            " the",
            " name",
            " of",
            " the",
            " people",
            ".\"",
            " After",
            " France",
            "'s",
            " liberation",
            ",",
            " Cam",
            "us",
            " remarked",
            ",",
            " \"",
            "This",
            " country",
            " does",
            " not",
            " need",
            " a",
            " Tal",
            "ley",
            "rand",
            ",",
            " but",
            " a",
            " Saint",
            "-",
            "Just",
            ".\"",
            " The",
            " reality",
            " of",
            " the",
            " post",
            "war",
            " trib",
            "un",
            "als",
            " soon",
            " changed",
            " his",
            " mind",
            ":",
            " Cam",
            "us",
            " publicly",
            " reversed",
            " himself",
            " and",
            " became",
            " a",
            " lifelong",
            " opponent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.275,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " the",
            " C",
            "optic",
            " Book",
            " of",
            " Adam",
            " and",
            " Eve",
            " (",
            "at",
            " ",
            "2",
            ":",
            "1",
            "–",
            "15",
            "),",
            " and",
            " the",
            " Sy",
            "ri",
            "ac",
            " Cave",
            " of",
            " Tre",
            "asures",
            ",",
            " Abel",
            "'s",
            " body",
            ",",
            " after",
            " many",
            " days",
            " of",
            " mourning",
            ",",
            " was",
            " placed",
            " in",
            " the",
            " Cave",
            " of",
            " Tre",
            "asures",
            ",",
            " before",
            " which",
            " Adam",
            " and",
            " Eve",
            ",",
            " and",
            " descendants",
            ",",
            " offered",
            " their",
            " prayers",
            ".",
            " In",
            " addition",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " performed",
            " by",
            " Classic",
            " Stage",
            " Company",
            ",",
            " New",
            " York",
            ",",
            " US",
            ".",
            " ",
            "202",
            "3",
            ",",
            " May",
            ":",
            " The",
            " Was",
            "ps",
            ",",
            " adapted",
            " by",
            " the",
            " NSW",
            " Arts",
            " Unit",
            " Drama",
            " Company",
            ",",
            " directed",
            " by",
            " Gene",
            "v",
            "ieve",
            " de",
            " Sou",
            "za",
            ",",
            " N",
            "IDA",
            "Liter",
            "ature",
            " The",
            " romantic",
            " poet",
            ",",
            " Percy",
            " Shelley",
            ",",
            " wrote",
            " a",
            " comic",
            ",",
            " lyr",
            "ical",
            " drama",
            " (",
            "S",
            "well",
            "foot"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.198,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "London",
            ",",
            " Vintage",
            ",",
            " ",
            "199",
            "4",
            ")",
            "  ",
            " ",
            " James",
            ",",
            " James",
            ",",
            " \"",
            "Andy",
            " War",
            "hol",
            ":",
            " The",
            " Producer",
            " as",
            " Author",
            "\",",
            " in",
            " Alleg",
            "ories",
            " of",
            " Cinema",
            ":",
            " American",
            " Film",
            " in",
            " the",
            " ",
            "196",
            "0",
            "s",
            " (",
            "198",
            "9",
            "),",
            " pp",
            ".",
            "Âł",
            "58",
            "–",
            "84",
            ".",
            " Princeton",
            ":",
            " Princeton",
            " University",
            " Press",
            ".",
            " ",
            " Kra",
            "uss",
            ",",
            " Ros",
            "al",
            "ind",
            " E"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Kor",
            "zy",
            "bs",
            "ki",
            " Memorial",
            " Lecture",
            " Concept",
            " and",
            " object",
            " E",
            "-",
            "Prime",
            " Institute",
            " of",
            " General",
            " Sem",
            "antics",
            " Robert",
            " P",
            "ula",
            " Structural",
            " differential",
            " Neuro",
            " Lingu",
            "istic",
            " Programming",
            "References",
            "Further",
            " reading",
            " Kod",
            "ish",
            ",",
            " Bruce",
            ".",
            " ",
            "201",
            "1",
            ".",
            " Kor",
            "zy",
            "bs",
            "ki",
            ":",
            " A",
            " Biography",
            ".",
            " Pasadena",
            ",",
            " CA",
            ":",
            " Extension",
            "al",
            " Publishing",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.201,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " (",
            "NB",
            ":",
            " A",
            " number",
            " of",
            " sup",
            "ernational",
            " economies",
            " are",
            " larger",
            ",",
            " such",
            " as",
            " the",
            " European",
            " Union",
            " (",
            "EU",
            "),",
            " the",
            " North",
            " American",
            " Free",
            " Trade",
            " Agreement",
            " (",
            "NA",
            "FTA",
            ")",
            " or",
            " A",
            "PEC",
            ").",
            " This",
            " ended",
            " in",
            " ",
            "201",
            "0",
            " when",
            " China",
            " over",
            "took",
            " Japan",
            " to",
            " become",
            " the",
            " world",
            "'s",
            " second",
            " largest",
            " economy",
            ".",
            " It",
            " is",
            " forecast",
            "ed",
            " that",
            " India",
            " will",
            " over"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.2,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " states",
            ",",
            " the",
            " question",
            " of",
            " the",
            " territorial",
            " expansion",
            " of",
            " slavery",
            " west",
            "ward",
            " again",
            " became",
            " explosive",
            ".",
            " Both",
            " the",
            " South",
            " and",
            " the",
            " North",
            " drew",
            " the",
            " same",
            " conclusion",
            ":",
            " \"",
            "The",
            " power",
            " to",
            " decide",
            " the",
            " question",
            " of",
            " slavery",
            " for",
            " the",
            " territories",
            " was",
            " the",
            " power",
            " to",
            " determine",
            " the",
            " future",
            " of",
            " slavery",
            " itself",
            ".\"",
            " Soon",
            " after",
            " the",
            " Utah",
            " Territory",
            " legalized",
            " slavery",
            " in",
            " ",
            "185",
            "2",
            ",",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.191,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " Bass",
            "—who",
            " would",
            " produce",
            " posters",
            " that",
            " accurately",
            " represented",
            " his",
            " films",
            ".",
            "Legacy",
            "A",
            "wards",
            " and",
            " hon",
            "ours",
            "H",
            "itch",
            "cock",
            " was",
            " in",
            "ducted",
            " into",
            " the",
            " Hollywood",
            " Walk",
            " of",
            " Fame",
            " on",
            " ",
            "8",
            " February",
            " ",
            "196",
            "0",
            " with",
            " two",
            " stars",
            ":",
            " one",
            " for",
            " television",
            " and",
            " a",
            " second",
            " for",
            " motion",
            " pictures",
            ".",
            " In",
            " ",
            "197",
            "8",
            ",",
            " John",
            " Russell",
            " Taylor",
            " described",
            " him",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " ly",
            "re",
            " and",
            " threw",
            " it",
            " away",
            ".",
            " The",
            " ly",
            "re",
            " was",
            " later",
            " discovered",
            " by",
            " the",
            " M",
            "uses",
            " and",
            " Apollo",
            "'s",
            " sons",
            " Lin",
            "us",
            " and",
            " Or",
            "phe",
            "us",
            ".",
            " The",
            " M",
            "uses",
            " fixed",
            " the",
            " middle",
            " string",
            ",",
            " Lin",
            "us",
            " the",
            " string",
            " struck",
            " with",
            " the",
            " fore",
            "finger",
            ",",
            " and",
            " Or",
            "phe",
            "us",
            " the",
            " lowest",
            " string",
            " and",
            " the",
            " one",
            " next",
            " to",
            " it",
            ".",
            " They",
            " took"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " exported",
            " to",
            " other",
            " countries",
            ".",
            "G",
            "reece",
            " ",
            "The",
            " earliest",
            " archaeological",
            " evidence",
            " for",
            " the",
            " use",
            " of",
            " the",
            " Greek",
            " ab",
            "acus",
            " dates",
            " to",
            " the",
            " ",
            "5",
            "th",
            " century",
            " BC",
            ".",
            " Dem",
            "ost",
            "hen",
            "es",
            " (",
            "384",
            " BC",
            "–",
            "322",
            " BC",
            ")",
            " complained",
            " that",
            " the",
            " need",
            " to",
            " use",
            " pe",
            "bb",
            "les",
            " for",
            " calculations",
            " was",
            " too",
            " difficult",
            ".",
            " A",
            " play",
            " by",
            " Alexis",
            " from",
            " the",
            " ",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Amsterdam",
            ".",
            " The",
            " oldest",
            " stone",
            " building",
            " of",
            " the",
            " Netherlands",
            ",",
            " The",
            " Mor",
            "ia",
            "an",
            " is",
            " built",
            " in",
            " '",
            "s",
            "-H",
            "ert",
            "ogen",
            "bos",
            "ch",
            ".",
            "In",
            " the",
            " ",
            "16",
            "th",
            " century",
            ",",
            " wooden",
            " buildings",
            " were",
            " raz",
            "ed",
            " and",
            " replaced",
            " with",
            " brick",
            " ones",
            ".",
            " During",
            " this",
            " period",
            ",",
            " many",
            " buildings",
            " were",
            " constructed",
            " in",
            " the",
            " architectural",
            " style",
            " of",
            " the",
            " Renaissance",
            ".",
            " Buildings",
            " of",
            " this",
            " period"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " purchases",
            " of",
            " iP",
            "ads",
            " and",
            " computers",
            " due",
            " to",
            " increased",
            " pricing",
            ".",
            " However",
            ",",
            " iPhone",
            " sales",
            " held",
            " up",
            " with",
            " a",
            " year",
            "-on",
            "-year",
            " increase",
            " of",
            " ",
            "1",
            ".",
            "5",
            "%.",
            " According",
            " to",
            " Apple",
            ",",
            " demands",
            " for",
            " such",
            " devices",
            " were",
            " strong",
            ",",
            " particularly",
            " in",
            " Latin",
            " America",
            " and",
            " South",
            " Asia",
            ".",
            "T",
            "axes",
            " ",
            "Apple",
            " has",
            " created",
            " subsidiaries",
            " in",
            " low",
            "-tax",
            " places",
            " such",
            " as",
            " Ireland",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "culo",
    "inati",
    ".ribbon",
    "isoft",
    "Ð¾ÐºÐ¸"
  ],
  "bottom_logits": [
    "n",
    " Face",
    "ows",
    "Î½Î¿Ïį",
    " Till"
  ],
  "act_min": -0.0,
  "act_max": 0.527
}