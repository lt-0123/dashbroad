{
  "index": 83169,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            1.258,
            0.312,
            0.124,
            0.038,
            0.124,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "16",
            " children",
            " with",
            " A",
            "DEM",
            ",",
            " ",
            "10",
            " recovered",
            " completely",
            " after",
            " high",
            "-d",
            "ose",
            " methyl",
            "pred",
            "nis",
            "ol",
            "one",
            ",",
            " one",
            " severe",
            " case",
            " that",
            " failed",
            " to",
            " respond",
            " to",
            " steroids",
            " recovered",
            " completely",
            " after",
            " IV",
            " Ig",
            ";",
            " the",
            " five",
            " most",
            " severe",
            " cases",
            " –",
            " with",
            " AD",
            "AM",
            " and",
            " severe",
            " peripheral",
            " neurop",
            "athy",
            " –",
            " were",
            " treated",
            " with",
            " combined",
            " high",
            "-d",
            "ose",
            " methyl",
            "pred",
            "nis",
            "ol",
            "one",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.258,
            0.414,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "16",
            "th",
            " most",
            " prevalent",
            " element",
            " in",
            " the",
            " earth",
            "'s",
            " crust",
            ";",
            " however",
            ",",
            " it",
            " is",
            " quite",
            " rare",
            ".",
            " Some",
            " minerals",
            " found",
            " in",
            " North",
            " America",
            ",",
            " South",
            " Africa",
            ",",
            " Russia",
            ",",
            " and",
            " Canada",
            " contain",
            " rub",
            "id",
            "ium",
            ".",
            " Some",
            " potassium",
            " minerals",
            " (",
            "le",
            "pid",
            "ol",
            "ites",
            ",",
            " bi",
            "ot",
            "ites",
            ",",
            " f",
            "eld",
            "spar",
            ",",
            " carn",
            "all",
            "ite",
            ")",
            " contain",
            " it",
            ",",
            " together",
            " with"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.258,
            0.527,
            0.291,
            0.447,
            0.121,
            0.04,
            0.143,
            0.051,
            -0.0,
            0.131,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.123,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "16",
            ",",
            " ",
            "186",
            "2",
            ",",
            " Brig",
            ".",
            " Gen",
            ".",
            " Simon",
            " Buck",
            "ner",
            ",",
            " having",
            " been",
            " abandoned",
            " by",
            " Floyd",
            " and",
            " Pillow",
            ",",
            " surrendered",
            " Fort",
            " Don",
            "elson",
            ".",
            " Colonel",
            " Nathan",
            " Bedford",
            " Forrest",
            " escaped",
            " with",
            " his",
            " cavalry",
            " force",
            " of",
            " about",
            " ",
            "700",
            " men",
            " before",
            " the",
            " surrender",
            ".",
            " The",
            " Confeder",
            "ates",
            " suffered",
            " about",
            " ",
            "1",
            ",",
            "500",
            " casualties",
            ",",
            " with",
            " an",
            " estimated",
            " ",
            "12",
            ",",
            "000"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.48,
            0.254,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.076,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " AA",
            " or",
            " Ni",
            "Cd",
            " recharge",
            "able",
            " or",
            " external",
            " power",
            " supply",
            " ",
            "|",
            "4",
            " AAA",
            " or",
            " Ni",
            "Cd",
            " recharge",
            "able",
            " or",
            " external",
            " power",
            " supply",
            "|",
            "4",
            " AA",
            " or",
            " Ni",
            "Cd",
            " recharge",
            "able",
            " or",
            " external",
            " power",
            " supply",
            "|",
            " colspan",
            "=\"",
            "2",
            "\"",
            " |",
            "Ni",
            "Cd",
            " battery",
            " pack",
            " or",
            " external",
            " power",
            " supply",
            "|",
            "4",
            " AA",
            " or",
            " Ni",
            "Cd",
            " recharge",
            "able",
            " or",
            " external"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.668,
            0.394,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ")",
            " How",
            "l",
            " An",
            "notated",
            " (",
            "199",
            "5",
            ")",
            " Illum",
            "inated",
            " Po",
            "ems",
            " (",
            "199",
            "6",
            ")",
            " Selected",
            " Po",
            "ems",
            ":",
            " ",
            "194",
            "7",
            "–",
            "199",
            "5",
            " (",
            "199",
            "6",
            ")",
            " Death",
            " and",
            " Fame",
            ":",
            " Po",
            "ems",
            " ",
            "199",
            "3",
            "–",
            "199",
            "7",
            " (",
            "199",
            "9",
            ")",
            " Del",
            "iber",
            "ate",
            " Pro",
            "se",
            " ",
            "195",
            "2",
            "–",
            "199",
            "5",
            " (",
            "200",
            "0",
            ")"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            0.582,
            0.359,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.054,
            -0.0,
            -0.0,
            0.046,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "  ",
            " –",
            " Mahar",
            "ana",
            " Sang",
            "ram",
            " Singh",
            ",",
            " R",
            "ana",
            " of",
            " M",
            "ew",
            "ar",
            " (",
            "d",
            ".",
            " ",
            "152",
            "7",
            ")",
            "150",
            "0",
            " –",
            " Jo",
            "ach",
            "im",
            " Cam",
            "er",
            "arius",
            ",",
            " German",
            " scholar",
            " and",
            " translator",
            " (",
            "d",
            ".",
            " ",
            "157",
            "4",
            ")",
            "152",
            "6",
            " –",
            " M",
            "ure",
            "t",
            "us",
            ",",
            " French",
            " philosopher",
            " and",
            " author",
            " (",
            "d",
            ".",
            " ",
            "158",
            "5",
            ")",
            "155"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.617,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ".",
            " Mel",
            "v",
            "yn",
            " Br",
            "agg",
            " with",
            " guests",
            " Ruth",
            " Richardson",
            ",",
            " Andrew",
            " Cunningham",
            " and",
            " Harold",
            " Ellis",
            ".",
            " \"",
            "An",
            "atomy",
            " of",
            " the",
            " Human",
            " Body",
            "\".",
            " ",
            "20",
            "th",
            " edition",
            ".",
            " ",
            "191",
            "8",
            ".",
            " Henry",
            " Gray",
            " ",
            " An",
            "atom",
            "ia",
            " Collection",
            ":",
            " anatom",
            "ical",
            " plates",
            " ",
            "152",
            "2",
            " to",
            " ",
            "186",
            "7",
            " (",
            "digit",
            "ized",
            " books",
            " and",
            " images",
            ")",
            "Ly",
            "man",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.617,
            0.369,
            0.135,
            0.012,
            0.028,
            -0.0,
            0.166,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.213,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.071,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ".",
            "15",
            " days",
            " only",
            " ",
            "0",
            ".",
            "052",
            "Âł",
            "AU",
            " from",
            " its",
            " sun",
            ",",
            " yellow",
            " dwarf",
            " (",
            "G",
            "9",
            "V",
            ")",
            " WAS",
            "P",
            "-",
            "47",
            ".",
            " WAS",
            "P",
            "-",
            "47",
            " is",
            " close",
            " in",
            " size",
            " to",
            " the",
            " Sun",
            ",",
            " having",
            " a",
            " radius",
            " of",
            " ",
            "1",
            ".",
            "15",
            " solar",
            " radi",
            "i",
            " and",
            " a",
            " mass",
            " even",
            " closer",
            " at",
            " ",
            "1",
            ".",
            "08",
            " solar",
            " masses",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.582,
            0.359,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.1,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            0.069,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "  ",
            " –",
            " Dor",
            "ival",
            " Cay",
            "mm",
            "i",
            ",",
            " Brazilian",
            " singer",
            "-song",
            "writer",
            ",",
            " actor",
            ",",
            " and",
            " painter",
            " (",
            "d",
            ".",
            " ",
            "200",
            "8",
            ")",
            "191",
            "6",
            " –",
            " Paul",
            " Ku",
            "us",
            "berg",
            ",",
            " Eston",
            "ian",
            " journalist",
            " and",
            " author",
            " (",
            "d",
            ".",
            " ",
            "200",
            "3",
            ")",
            " ",
            " ",
            "191",
            "6",
            "  ",
            " –",
            " Claude",
            " Shannon",
            ",",
            " American",
            " mathematic",
            "ian",
            " and",
            " engineer",
            " (",
            "d",
            ".",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.268,
            0.107,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.046,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.046,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " Children",
            "'s",
            " Day",
            " (",
            "Hong",
            " Kong",
            ",",
            " Taiwan",
            ")",
            " Independence",
            " Day",
            " (",
            "Sen",
            "egal",
            ")",
            " International",
            " Day",
            " for",
            " Mine",
            " Awareness",
            " and",
            " Assistance",
            " in",
            " Mine",
            " Action",
            " Peace",
            " Day",
            " (",
            "Ang",
            "ola",
            ")",
            " April",
            " ",
            "5",
            " Children",
            "'s",
            " Day",
            " (",
            "Pale",
            "stinian",
            " territories",
            ")",
            " National",
            " C",
            "aramel",
            " Day",
            " (",
            "United",
            " States",
            ")",
            " Sik",
            "m",
            "og",
            "il",
            " (",
            "South",
            " Korea",
            ")",
            " April"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            0.227,
            0.186,
            -0.0,
            0.116,
            0.05,
            0.029,
            0.044,
            0.028,
            0.136,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " Phantom",
            ";",
            " and",
            " cargo",
            " planes",
            " such",
            " as",
            " the",
            " Trans",
            "all",
            " C",
            "-",
            "160",
            ".)",
            " Also",
            " a",
            " Hungarian",
            " Mi",
            "G",
            "-",
            "21",
            ",",
            " a",
            " Pakistani",
            " Mi",
            "G",
            "-",
            "19",
            ",",
            " and",
            " a",
            " Bulgarian",
            " Mi",
            "G",
            "-",
            "17",
            " are",
            " on",
            " display",
            " at",
            " the",
            " museum",
            ".",
            "MET",
            "U",
            " Science",
            " and",
            " Technology",
            " Museum",
            "The",
            " MET",
            "U",
            " Science",
            " and",
            " Technology",
            " Museum",
            " (",
            "OD",
            "T",
            "Ãľ",
            " Bilim"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.539,
            0.367,
            0.153,
            0.196,
            0.082,
            0.024,
            -0.0,
            0.071,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ",",
            " the",
            " FAA",
            " do",
            " not",
            " have",
            " a",
            " specialized",
            " par",
            "atro",
            "oper",
            " unit",
            ".",
            " However",
            ",",
            " elements",
            " of",
            " the",
            " command",
            "os",
            ",",
            " special",
            " operations",
            " and",
            " mar",
            "ines",
            " are",
            " parachute",
            " qualified",
            ".",
            "Terr",
            "itor",
            "ial",
            " troops",
            " ",
            "The",
            " Directorate",
            " of",
            " People",
            "'s",
            " Defense",
            " and",
            " Terr",
            "itor",
            "ial",
            " Tro",
            "ops",
            " of",
            " the",
            " Defence",
            " Ministry",
            " or",
            " O",
            "DP",
            " was",
            " established",
            " in",
            " late",
            " ",
            "197",
            "5",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.408,
            0.144,
            0.075,
            -0.0,
            -0.0,
            0.123,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " years",
            " ",
            "female",
            ":",
            " ",
            "16",
            ".",
            "3",
            " years",
            " (",
            "201",
            "8",
            " est",
            ".)",
            "Population",
            " growth",
            " rate",
            "3",
            ".",
            "36",
            "%",
            " (",
            "202",
            "2",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "6",
            "th",
            "3",
            ".",
            "49",
            "%",
            " (",
            "201",
            "8",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "2",
            "nd",
            "The",
            " population",
            " is",
            " growing",
            " by",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.475,
            0.34,
            0.098,
            0.041,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ")",
            " sold",
            " for",
            " $",
            "195",
            " million",
            ",",
            " which",
            " is",
            " the",
            " most",
            " expensive",
            " work",
            " of",
            " art",
            " sold",
            " at",
            " auction",
            " by",
            " an",
            " American",
            " artist",
            ".",
            "Bi",
            "ography",
            "Early",
            " life",
            " and",
            " beginnings",
            " (",
            "192",
            "8",
            "–",
            "194",
            "9",
            ")",
            "War",
            "hol",
            " was",
            " born",
            " on",
            " August",
            " ",
            "6",
            ",",
            " ",
            "192",
            "8",
            ",",
            " in",
            " Pittsburgh",
            ",",
            " Pennsylvania",
            ".",
            " He",
            " was",
            " the",
            " fourth",
            " child",
            " of",
            " O"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.336,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            0.07,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "e",
            " Gym",
            "nas",
            "ium",
            " and",
            " the",
            " Cy",
            "gn",
            "us",
            " Gym",
            "nas",
            "ium",
            " where",
            " a",
            " classical",
            " curriculum",
            " including",
            " Latin",
            " and",
            " classical",
            " Greek",
            " is",
            " taught",
            ".",
            " Though",
            " believed",
            " until",
            " recently",
            " by",
            " many",
            " to",
            " be",
            " an",
            " an",
            "ach",
            "ron",
            "istic",
            " and",
            " elit",
            "ist",
            " concept",
            " that",
            " would",
            " soon",
            " die",
            " out",
            ",",
            " the",
            " gymn",
            "asia",
            " have",
            " recently",
            " experienced",
            " a",
            " revival",
            ",",
            " leading",
            " to",
            " the",
            " formation",
            " of",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            0.203,
            0.141,
            0.164,
            0.233,
            -0.0,
            0.239,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " comment",
            " at",
            " a",
            " lecture",
            ",",
            " Rand",
            " said",
            " that",
            " D",
            "annes",
            "kj",
            "Ã¶",
            "ld",
            "'s",
            " name",
            " was",
            " a",
            " tribute",
            " to",
            " Victor",
            " Hugo",
            "'s",
            " novel",
            ",",
            " ,",
            " wherein",
            " the",
            " hero",
            " becomes",
            " the",
            " first",
            " of",
            " the",
            " Counts",
            " of",
            " D",
            "annes",
            "kj",
            "Ã¶",
            "ld",
            ".",
            " In",
            " the",
            " published",
            " book",
            ",",
            " D",
            "annes",
            "kj",
            "Ã¶",
            "ld",
            " is",
            " always",
            " seen",
            " through",
            " the",
            " eyes",
            " of",
            " others",
            " (",
            "D",
            "agn"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.598,
            0.342,
            0.291,
            0.344,
            0.144,
            0.104,
            0.025,
            0.228,
            -0.0,
            0.12,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "%",
            " and",
            " as",
            " low",
            " as",
            " ",
            "0",
            ".",
            "2",
            "%.",
            "A",
            "gricult",
            "ure",
            " and",
            " forestry",
            " ",
            "A",
            "gricult",
            "ure",
            " has",
            " a",
            " significant",
            " position",
            " in",
            " the",
            " province",
            "'s",
            " economy",
            ".",
            " The",
            " province",
            " has",
            " over",
            " three",
            " million",
            " head",
            " of",
            " cattle",
            ",",
            " and",
            " Alberta",
            " beef",
            " has",
            " a",
            " healthy",
            " worldwide",
            " market",
            ".",
            " Nearly",
            " one",
            " half",
            " of",
            " all",
            " Canadian",
            " beef",
            " is",
            " produced",
            " in",
            " Alberta",
            ".",
            " Alberta",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.469,
            0.35,
            0.12,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            0.104,
            0.074,
            0.045,
            0.033,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.045,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " was",
            " about",
            " $",
            "100",
            ",",
            "000",
            ",",
            " which",
            " is",
            " ",
            "23",
            "%",
            " higher",
            " than",
            " the",
            " Canadian",
            " national",
            " average",
            ".",
            "Based",
            " on",
            " Stat",
            "istic",
            " Canada",
            " reports",
            ",",
            " low",
            "-income",
            " Albert",
            "ans",
            ",",
            " who",
            " earn",
            " less",
            " than",
            " $",
            "25",
            ",",
            "000",
            " and",
            " those",
            " in",
            " the",
            " high",
            "-income",
            " bracket",
            " earning",
            " $",
            "150",
            ",",
            "000",
            " or",
            " more",
            ",",
            " are",
            " the",
            " lowest",
            "-t",
            "axed",
            " people",
            " in",
            " Canada"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.539,
            0.301,
            0.332,
            0.23,
            0.082,
            0.232,
            0.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.2,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ",",
            " ",
            "10",
            ")",
            " (",
            "with",
            " Mend",
            "els",
            "so",
            "hn",
            ",",
            " Sonata",
            " in",
            " D",
            " minor",
            " op",
            " ",
            "65",
            ".",
            "6",
            ").",
            " Ch",
            "or",
            "ale",
            "-P",
            "rel",
            "udes",
            ":",
            " ",
            " (",
            "1",
            "st",
            " and",
            " ",
            "2",
            "nd",
            " versions",
            ",",
            " Peters",
            " Vol",
            " ",
            "5",
            ",",
            " ",
            "45",
            ");",
            " ",
            " (",
            "vol",
            " ",
            "7",
            ",",
            " ",
            "58",
            " (",
            "Le",
            "ipzig",
            " ",
            "18",
            "));",
            " ",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.122,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " at",
            " Emmanuel",
            " Church",
            ",",
            " Cl",
            "ifton",
            ",",
            " Bristol",
            ",",
            " close",
            " to",
            " the",
            " home",
            " of",
            " his",
            " mother",
            " and",
            " step",
            "father",
            ",",
            " when",
            " Archie",
            " was",
            " on",
            " home",
            " leave",
            ".",
            " Rising",
            " through",
            " the",
            " ranks",
            ",",
            " he",
            " was",
            " posted",
            " back",
            " to",
            " Britain",
            " in",
            " September",
            " ",
            "191",
            "8",
            " as",
            " a",
            " colon",
            "el",
            " in",
            " the",
            " Air",
            " Ministry",
            ".",
            " Christie",
            " involved",
            " herself",
            " in",
            " the",
            " war",
            " effort",
            " as",
            " a",
            " member"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            0.582,
            0.211,
            0.268,
            0.086,
            0.05,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.098
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ":",
            " ",
            " (",
            "M",
            "ons",
            "ieur",
            " Berger",
            "et",
            " in",
            " Paris",
            ")",
            " (",
            "190",
            "1",
            ")",
            " C",
            "lio",
            " (",
            "190",
            "0",
            ")",
            " ",
            " (",
            "A",
            " M",
            "ummer",
            "'s",
            " Tale",
            ")",
            " (",
            "190",
            "3",
            ")",
            " ",
            " (",
            "The",
            " White",
            " Stone",
            ")",
            " (",
            "190",
            "5",
            ")",
            " ",
            " (",
            "190",
            "1",
            ")",
            " ",
            " (",
            "P",
            "enguin",
            " Island",
            ")",
            " (",
            "190",
            "8",
            ")",
            " ",
            " (",
            "The",
            " Mer"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.475,
            0.324,
            0.019,
            0.223,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ")",
            " to",
            " financier",
            " Steven",
            " A",
            ".",
            " Cohen",
            " for",
            " $",
            "80",
            " million",
            ".",
            " In",
            " May",
            " ",
            "200",
            "7",
            ",",
            " Green",
            " Car",
            " Crash",
            " (",
            "196",
            "3",
            ")",
            " sold",
            " for",
            " $",
            "71",
            ".",
            "1",
            " million",
            " and",
            " Lemon",
            " Marilyn",
            " (",
            "196",
            "2",
            ")",
            " sold",
            " for",
            " $",
            "28",
            " million",
            " at",
            " Christie",
            "'s",
            " post",
            "-war",
            " and",
            " contemporary",
            " art",
            " auction",
            ".",
            " In",
            " ",
            "200",
            "7",
            ",",
            " Large",
            " Campbell",
            "'s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.555,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " –",
            " Deng",
            " Xia",
            "oping",
            ",",
            " Chinese",
            " soldier",
            " and",
            " politician",
            ",",
            " ",
            "1",
            "st",
            " Vice",
            " Premier",
            " of",
            " the",
            " People",
            "'s",
            " Republic",
            " of",
            " China",
            " (",
            "d",
            ".",
            " ",
            "199",
            "7",
            ")",
            "190",
            "8",
            " –",
            " Henri",
            " Cart",
            "ier",
            "-B",
            "ress",
            "on",
            ",",
            " French",
            " photographer",
            " and",
            " painter",
            " (",
            "d",
            ".",
            " ",
            "200",
            "4",
            ")",
            " ",
            " ",
            "190",
            "8",
            "  ",
            " –",
            " Er",
            "win",
            " Th",
            "ies",
            "ies",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.475,
            0.359,
            0.205,
            0.146,
            0.132,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ")",
            " point",
            " to",
            " the",
            " financial",
            " weakness",
            " as",
            " well",
            " as",
            " the",
            " norms",
            ",",
            " rules",
            " and",
            " institutional",
            " structures",
            " of",
            " the",
            " Congress",
            ",",
            " and",
            " the",
            " propensity",
            " to",
            " divide",
            " along",
            " sectional",
            " lines",
            ".",
            "R",
            "ak",
            "ove",
            " identifies",
            " several",
            " factors",
            " that",
            " explain",
            " the",
            " collapse",
            " of",
            " the",
            " Confeder",
            "ation",
            ".",
            " The",
            " lack",
            " of",
            " compulsory",
            " direct",
            " taxation",
            " power",
            " was",
            " objection",
            "able",
            " to",
            " those",
            " wanting",
            " a",
            " strong",
            " centralized",
            " state",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.539,
            0.394,
            0.252,
            0.089,
            0.049,
            0.085,
            -0.0,
            0.05,
            0.054,
            -0.0,
            0.195,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ",",
            " she",
            " participated",
            " in",
            " three",
            " events",
            " organized",
            " by",
            " El",
            "ton",
            " John",
            " and",
            " by",
            " fellow",
            " tennis",
            " players",
            " Serena",
            " Williams",
            " and",
            " Andy",
            " Rod",
            "d",
            "ick",
            ".",
            " In",
            " January",
            " ",
            "200",
            "5",
            ",",
            " she",
            " played",
            " in",
            " a",
            " doubles",
            " charity",
            " event",
            " for",
            " the",
            " Indian",
            " Ocean",
            " tsunami",
            " with",
            " John",
            " Mc",
            "En",
            "roe",
            ",",
            " Andy",
            " Rod",
            "d",
            "ick",
            ",",
            " and",
            " Chris",
            " E",
            "vert",
            ".",
            " In",
            " November",
            " ",
            "200"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            0.555,
            0.237,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " –",
            " Thomas",
            " Caj",
            "et",
            "an",
            ",",
            " Italian",
            " cardinal",
            " and",
            " philosopher",
            " (",
            "b",
            ".",
            " ",
            "147",
            "0",
            ")",
            "158",
            "0",
            " –",
            " Metro",
            "phan",
            "es",
            " III",
            " of",
            " Constantin",
            "ople",
            " (",
            "b",
            ".",
            " ",
            "152",
            "0",
            ")",
            "160",
            "1",
            "–",
            "190",
            "0",
            "160",
            "1",
            " –",
            " Michael",
            " the",
            " Brave",
            ",",
            " Romanian",
            " prince",
            " (",
            "b",
            ".",
            " ",
            "155",
            "8",
            ")",
            "163",
            "4",
            " –",
            " William",
            " N",
            "oy"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.162,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            0.212,
            0.262,
            -0.0,
            -0.0,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "Br",
            "2",
            "O",
            ",",
            " are",
            " also",
            " known",
            ".",
            " The",
            " poly",
            "hal",
            "ides",
            " are",
            " rather",
            " unstable",
            ",",
            " although",
            " those",
            " of",
            " rub",
            "id",
            "ium",
            " and",
            " ca",
            "esium",
            " are",
            " greatly",
            " stabil",
            "ised",
            " by",
            " the",
            " fee",
            "ble",
            " polar",
            "ising",
            " power",
            " of",
            " these",
            " extremely",
            " large",
            " c",
            "ations",
            ".",
            "Coord",
            "ination",
            " complexes",
            " ",
            "Al",
            "k",
            "ali",
            " metal",
            " c",
            "ations",
            " do",
            " not",
            " usually",
            " form",
            " coordination",
            " complexes",
            " with",
            " simple",
            " Lewis"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.582,
            0.359,
            0.186,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            0.097,
            0.186,
            0.128,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            0.016,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            0.068,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "  ",
            " –",
            " Sam",
            " Elliott",
            ",",
            " American",
            " actor",
            " and",
            " producer",
            " ",
            " ",
            "194",
            "4",
            "  ",
            " –",
            " Patricia",
            " McK",
            "iss",
            "ack",
            ",",
            " American",
            " soldier",
            ",",
            " engineer",
            ",",
            " and",
            " author",
            " (",
            "d",
            ".",
            " ",
            "201",
            "7",
            ")",
            "194",
            "5",
            " –",
            " Barbara",
            " Del",
            "insky",
            ",",
            " American",
            " author",
            " ",
            " ",
            "194",
            "5",
            "  ",
            " –",
            " Aleks",
            "andr",
            " Gore",
            "lik",
            ",",
            " Russian",
            " figure",
            " sk",
            "ater",
            " and",
            " sport"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.582,
            0.359,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            0.136,
            -0.0,
            -0.0,
            0.214,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            "  ",
            " –",
            " Henry",
            " I",
            "ba",
            ",",
            " American",
            " basketball",
            " player",
            " and",
            " coach",
            " (",
            "d",
            ".",
            " ",
            "199",
            "3",
            ")",
            "190",
            "6",
            " –",
            " Vic",
            " Dick",
            "enson",
            ",",
            " American",
            " trom",
            "bon",
            "ist",
            " (",
            "d",
            ".",
            " ",
            "198",
            "4",
            ")",
            "190",
            "8",
            " –",
            " Maria",
            " Lud",
            "w",
            "ika",
            " Bern",
            "hard",
            ",",
            " ",
            " Polish",
            " classical",
            " archae",
            "ologist",
            " and",
            " a",
            " ",
            " member",
            " of",
            " WWII",
            " Polish",
            " resistance",
            " (",
            "d",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            0.559,
            0.357,
            0.22,
            0.236,
            0.041,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            0.011,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.289,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " but",
            " was",
            " down",
            " to",
            " ",
            "628",
            ",",
            "000",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            " Recovery",
            " plans",
            " for",
            " cod",
            ",",
            " sole",
            ",",
            " and",
            " pla",
            "ice",
            " have",
            " reduced",
            " mortality",
            " in",
            " these",
            " species",
            ".",
            " Arctic",
            " cod",
            " reached",
            " its",
            " lowest",
            " levels",
            " in",
            " the",
            " ",
            "196",
            "0",
            "s",
            "–",
            "198",
            "0",
            "s",
            " but",
            " is",
            " now",
            " recovered",
            ".",
            " Arctic",
            " sa",
            "ithe",
            " and",
            " h",
            "addock",
            " are",
            " considered",
            " fully",
            " f"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.195,
            0.156,
            0.268,
            0.167,
            0.137,
            0.038,
            0.02,
            -0.0,
            0.001,
            0.252,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "21",
            " chapters",
            " was",
            " an",
            " intentional",
            " nod",
            " to",
            " the",
            " age",
            " of",
            " ",
            "21",
            " being",
            " recognised",
            " as",
            " a",
            " milestone",
            " in",
            " human",
            " mat",
            "uration",
            ".",
            " The",
            " ",
            "21",
            "st",
            " chapter",
            " was",
            " omitted",
            " from",
            " the",
            " editions",
            " published",
            " in",
            " the",
            " United",
            " States",
            " prior",
            " to",
            " ",
            "198",
            "6",
            ".",
            " In",
            " the",
            " introduction",
            " to",
            " the",
            " updated",
            " American",
            " text",
            " (",
            "these",
            " newer",
            " editions",
            " include",
            " the",
            " missing",
            " ",
            "21",
            "st",
            " chapter",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.188,
            0.547,
            0.213,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "5",
            " –",
            " Albert",
            " Einstein",
            " completes",
            " his",
            " doctoral",
            " thesis",
            " at",
            " the",
            " University",
            " of",
            " Zurich",
            ".",
            "192",
            "5",
            " –",
            " Autom",
            "aker",
            " Dodge",
            " Brothers",
            ",",
            " Inc",
            " is",
            " sold",
            " to",
            " Dillon",
            ",",
            " Read",
            " &",
            " Co",
            ".",
            " for",
            " US",
            "$",
            "146",
            "Âł",
            "million",
            " plus",
            " $",
            "50",
            "Âł",
            "million",
            " for",
            " charity",
            ".",
            "192",
            "7",
            " –",
            " The",
            " Federal",
            " Industrial",
            " Institute",
            " for",
            " Women",
            " opens",
            " in",
            " Ald",
            "erson",
            ",",
            " West",
            " Virginia",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.172,
            0.574,
            -0.0,
            0.314,
            0.09,
            0.23,
            0.062,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            0.046,
            -0.0,
            -0.0,
            0.061,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " an",
            "oxic",
            " environments",
            ".",
            " Ar",
            "sen",
            "ite",
            " is",
            " more",
            " soluble",
            " and",
            " mobile",
            " than",
            " arsen",
            "ate",
            ".",
            " Many",
            " species",
            " of",
            " bacteria",
            " can",
            " transform",
            " arsen",
            "ite",
            " to",
            " arsen",
            "ate",
            " in",
            " an",
            "oxic",
            " conditions",
            " by",
            " using",
            " arsen",
            "ite",
            " as",
            " an",
            " electron",
            " donor",
            ".",
            " This",
            " is",
            " a",
            " useful",
            " method",
            " in",
            " ground",
            " water",
            " remed",
            "iation",
            ".",
            " Another",
            " bi",
            "ore",
            "medi",
            "ation",
            " strategy",
            " is",
            " to",
            " use",
            " plants",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.164,
            0.15,
            0.078,
            0.249,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " consec",
            "rated",
            " the",
            " superiority",
            " of",
            " Hub",
            "al",
            " (",
            "the",
            " supreme",
            " deity",
            " of",
            " Qur",
            "ay",
            "sh",
            ")",
            " over",
            " the",
            " other",
            " gods",
            ".",
            " However",
            ",",
            " there",
            " is",
            " also",
            " evidence",
            " that",
            " Allah",
            " and",
            " Hub",
            "al",
            " were",
            " two",
            " distinct",
            " de",
            "ities",
            ".",
            " According",
            " to",
            " that",
            " hypothesis",
            ",",
            " the",
            " Ka",
            "aba",
            " was",
            " first",
            " consec",
            "rated",
            " to",
            " a",
            " supreme",
            " deity",
            " named",
            " Allah",
            " and",
            " then",
            " hosted",
            " the",
            " pan",
            "theon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.156,
            0.445,
            0.562,
            0.273,
            0.15,
            -0.0,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " ",
            "194",
            "3",
            " with",
            " Shadow",
            " of",
            " Night",
            ",",
            " a",
            " Scri",
            "b",
            "ners",
            "'",
            " novel",
            " of",
            " which",
            " The",
            " Chicago",
            " Sun",
            " wrote",
            ":",
            " \"",
            "Struct",
            "urally",
            " it",
            " has",
            " the",
            " perfection",
            " of",
            " a",
            " carved",
            " jewel",
            "...",
            "A",
            " psychological",
            " novel",
            " of",
            " the",
            " first",
            " order",
            ",",
            " and",
            " an",
            " adventure",
            " tale",
            " that",
            " is",
            " unique",
            " and",
            " inspir",
            "iting",
            ".\"",
            "In",
            " November",
            " ",
            "194",
            "5",
            ",",
            " however",
            ",",
            " Der",
            "le"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            1.062,
            0.356,
            0.053,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.175,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Rose",
            " Louise",
            " Volk",
            ",",
            " Der",
            "le",
            "th",
            " grew",
            " up",
            " in",
            " Sau",
            "k",
            " City",
            ",",
            " Wisconsin",
            ".",
            " He",
            " was",
            " educated",
            " in",
            " local",
            " par",
            "och",
            "ial",
            " and",
            " public",
            " high",
            " school",
            ".",
            " Der",
            "le",
            "th",
            " wrote",
            " his",
            " first",
            " fiction",
            " at",
            " age",
            " ",
            "13",
            ".",
            " He",
            " was",
            " interested",
            " most",
            " in",
            " reading",
            ",",
            " and",
            " he",
            " made",
            " three",
            " trips",
            " to",
            " the",
            " library",
            " a",
            " week",
            ".",
            " He",
            " would",
            " save"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.062,
            0.516,
            0.406,
            0.293,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " ",
            "1",
            ",",
            "511",
            " Flight",
            " In",
            "structors",
            ".",
            "Other",
            " transport",
            "Another",
            " Al",
            "askan",
            " transportation",
            " method",
            " is",
            " the",
            " dogs",
            "led",
            ".",
            " In",
            " modern",
            " times",
            " (",
            "that",
            " is",
            ",",
            " any",
            " time",
            " after",
            " the",
            " mid",
            "-l",
            "ate",
            " ",
            "192",
            "0",
            "s",
            "),",
            " dog",
            " mush",
            "ing",
            " is",
            " more",
            " of",
            " a",
            " sport",
            " than",
            " a",
            " true",
            " means",
            " of",
            " transportation",
            ".",
            " Various",
            " races",
            " are",
            " held",
            " around",
            " the",
            " state"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.871,
            0.463,
            0.264,
            0.046,
            0.11,
            -0.0,
            0.06,
            0.028,
            0.153,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            0.05,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " point",
            " to",
            " any",
            " other",
            " point",
            ".",
            " It",
            " is",
            " possible",
            " to",
            " extend",
            " a",
            " line",
            " segment",
            " continuously",
            " in",
            " both",
            " directions",
            ".",
            " It",
            " is",
            " possible",
            " to",
            " describe",
            " a",
            " circle",
            " with",
            " any",
            " center",
            " and",
            " any",
            " radius",
            ".",
            " It",
            " is",
            " true",
            " that",
            " all",
            " right",
            " angles",
            " are",
            " equal",
            " to",
            " one",
            " another",
            ".",
            " (\"",
            "Parallel",
            " post",
            "ulate",
            "\")",
            " It",
            " is",
            " true",
            " that",
            ",",
            " if",
            " a",
            " straight",
            " line",
            " falling",
            " on",
            " two"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.867,
            0.43,
            0.309,
            -0.0,
            0.052,
            0.092,
            -0.0,
            -0.0,
            0.354,
            -0.0,
            -0.0,
            0.165,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.129,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " where",
            " it",
            " acts",
            " as",
            " a",
            " scaffold",
            " for",
            " the",
            " recruitment",
            " of",
            " the",
            " XR",
            "CC",
            "4",
            "-D",
            "NA",
            " lig",
            "ase",
            " protein",
            " complex",
            " that",
            " then",
            " acts",
            " to",
            " repair",
            " double",
            "-str",
            "and",
            " breaks",
            ".",
            " About",
            " ",
            "95",
            "%",
            " of",
            " ALS",
            " patients",
            " have",
            " abnormalities",
            " in",
            " the",
            " nucleus",
            "-c",
            "y",
            "topl",
            "asm",
            "ic",
            " localization",
            " in",
            " spinal",
            " motor",
            " neurons",
            " of",
            " T",
            "DP",
            "43",
            ".",
            " In",
            " T",
            "DP",
            "-",
            "43",
            " depleted"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.859,
            -0.0,
            0.019,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " National",
            " Museum",
            " of",
            " Afghanistan",
            " in",
            " Kabul",
            " hosts",
            " a",
            " large",
            " number",
            " of",
            " Buddhist",
            ",",
            " B",
            "act",
            "rian",
            " Greek",
            " and",
            " early",
            " Islamic",
            " antiqu",
            "ities",
            ";",
            " the",
            " museum",
            " suffered",
            " greatly",
            " by",
            " civil",
            " war",
            " but",
            " has",
            " been",
            " slowly",
            " restoring",
            " since",
            " the",
            " early",
            " ",
            "200",
            "0",
            "s",
            ".",
            "Communication",
            " ",
            "Tele",
            "communication",
            " services",
            " in",
            " Afghanistan",
            " are",
            " provided",
            " by",
            " Afghan",
            " Telecom",
            ",",
            " Afghan",
            " Wireless",
            ",",
            " Et",
            "is",
            "alat",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            0.832,
            0.32,
            0.383,
            0.05,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ranging",
            " from",
            " indie",
            " rock",
            " to",
            " hip",
            " hop",
            ",",
            " R",
            "&B",
            ",",
            " and",
            " other",
            " popular",
            " genres",
            ".",
            " Other",
            " more",
            " sub",
            "c",
            "ultural",
            " music",
            " venues",
            " are",
            " OCC",
            "II",
            ",",
            " OT",
            "301",
            ",",
            " De",
            " N",
            "ieu",
            "we",
            " Anita",
            ",",
            " Winston",
            " Kingdom",
            ",",
            " and",
            " Z",
            "aal",
            " ",
            "100",
            ".",
            " Jazz",
            " has",
            " a",
            " strong",
            " following",
            " in",
            " Amsterdam",
            ",",
            " with",
            " the",
            " B",
            "im",
            "h",
            "uis",
            " being",
            " the",
            " premier"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.805,
            0.404,
            0.359,
            0.111,
            -0.0,
            0.05,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "us",
            "'s",
            " main",
            " role",
            " within",
            " the",
            " PCA",
            " was",
            " to",
            " organise",
            " the",
            " Th",
            "Ã©",
            "Ã¢",
            "tre",
            " du",
            " Trav",
            "ail",
            " (\"",
            "Workers",
            "'",
            " Theatre",
            "\").",
            " Cam",
            "us",
            " was",
            " also",
            " close",
            " to",
            " the",
            " Parti",
            " du",
            " Pe",
            "uple",
            " Alg",
            "Ã©ri",
            "en",
            " (",
            "Al",
            "ger",
            "ian",
            " People",
            "'s",
            " Party",
            " (",
            "PP",
            "A",
            ")),",
            " which",
            " was",
            " a",
            " moderate",
            " anti",
            "-col",
            "on",
            "ial",
            "ist",
            "/n",
            "ational",
            "ist",
            " party",
            ".",
            " As"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.805,
            0.451,
            0.266,
            0.049,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            0.095,
            -0.0,
            -0.0,
            0.057,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iner",
            ",",
            " Larry",
            ".",
            " The",
            " In",
            "vention",
            " of",
            " Art",
            ":",
            " A",
            " Cultural",
            " History",
            ".",
            " Chicago",
            ":",
            " University",
            " of",
            " Chicago",
            " Press",
            ",",
            " ",
            "200",
            "3",
            ".",
            " ",
            " Arthur",
            " D",
            "anto",
            ",",
            " The",
            " Abuse",
            " of",
            " Beauty",
            ":",
            " A",
            "est",
            "hetics",
            " and",
            " the",
            " Concept",
            " of",
            " Art",
            ".",
            " ",
            "200",
            "3",
            " Dana",
            " Arnold",
            " and",
            " Margaret",
            " I",
            "vers",
            "en",
            ",",
            " eds",
            ".",
            " Art",
            " and",
            " Thought",
            ".",
            " London"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.793,
            0.508,
            0.122,
            0.029,
            0.01,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " A",
            " temple",
            " of",
            " Apollo",
            " was",
            " built",
            " probably",
            " in",
            " the",
            " ",
            "4",
            "th",
            "-",
            "3",
            "rd",
            " century",
            " BC",
            ".",
            " Parts",
            " of",
            " a",
            " terr",
            "ac",
            "otta",
            " capital",
            ",",
            " and",
            " a",
            " terr",
            "ac",
            "otta",
            " base",
            " have",
            " been",
            " found",
            ".",
            " It",
            " seems",
            " that",
            " the",
            " E",
            "tr",
            "us",
            "can",
            " columns",
            " were",
            " derived",
            " from",
            " the",
            " arch",
            "aic",
            " Dor",
            "ic",
            ".",
            " A",
            " cult",
            " of",
            " Apollo",
            " Sor",
            "anus",
            " is",
            " att",
            "ested"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.77,
            0.26,
            0.24,
            0.281,
            0.018,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " depicted",
            " in",
            " confined",
            " or",
            " cramped",
            " quarters",
            ",",
            " in",
            " this",
            " case",
            " Stewart",
            "'s",
            " studio",
            " apartment",
            ".",
            " Hitch",
            "cock",
            " uses",
            " close",
            "-ups",
            " of",
            " Stewart",
            "'s",
            " face",
            " to",
            " show",
            " his",
            " character",
            "'s",
            " reactions",
            ",",
            " \"",
            "from",
            " the",
            " comic",
            " voyeur",
            "ism",
            " directed",
            " at",
            " his",
            " neighbours",
            " to",
            " his",
            " helpless",
            " terror",
            " watching",
            " Kelly",
            " and",
            " Burr",
            " in",
            " the",
            " villain",
            "'s",
            " apartment",
            "\".",
            "Al",
            "fred",
            " Hitch",
            "cock",
            " Presents",
            "From"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            0.77,
            -0.0,
            0.183,
            0.02,
            0.04,
            0.05,
            0.238,
            -0.0,
            0.172,
            -0.0,
            0.02,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " van",
            " Vog",
            "t",
            " maintained",
            " his",
            " association",
            " with",
            " the",
            " organization",
            " and",
            " was",
            " still",
            " president",
            " of",
            " the",
            " Californ",
            "ian",
            " Association",
            " of",
            " D",
            "ian",
            "etic",
            " Aud",
            "itors",
            " into",
            " the",
            " ",
            "198",
            "0",
            "s",
            ".",
            "Return",
            " to",
            " writing",
            " and",
            " later",
            " career",
            " (",
            "196",
            "2",
            "–",
            "198",
            "6",
            ")",
            "Though",
            " the",
            " constant",
            " re",
            "-pack",
            "aging",
            " of",
            " his",
            " older",
            " work",
            " meant",
            " that",
            " he",
            " had",
            " never",
            " really",
            " been",
            " away",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.75,
            0.202,
            0.246,
            0.064,
            0.049,
            0.123,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.166,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.041,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ind",
            "istinguish",
            "able",
            " particles",
            ".",
            " Einstein",
            " noted",
            " that",
            " Bose",
            "'s",
            " statistics",
            " applied",
            " to",
            " some",
            " atoms",
            " as",
            " well",
            " as",
            " to",
            " the",
            " proposed",
            " light",
            " particles",
            ",",
            " and",
            " submitted",
            " his",
            " translation",
            " of",
            " Bose",
            "'s",
            " paper",
            " to",
            " the",
            " Ze",
            "its",
            "chrift",
            " fÃ¼r",
            " Phys",
            "ik",
            ".",
            " Einstein",
            " also",
            " published",
            " his",
            " own",
            " articles",
            " describing",
            " the",
            " model",
            " and",
            " its",
            " implications",
            ",",
            " among",
            " them",
            " the",
            " Bose",
            "–",
            "E",
            "instein",
            " cond",
            "ens"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.719,
            0.357,
            0.195,
            0.05,
            0.115,
            0.107,
            0.056,
            0.176,
            0.07,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " did",
            " not",
            " realize",
            " that",
            " this",
            " was",
            " an",
            " unknown",
            " continent",
            ".",
            " Character",
            "istic",
            " of",
            " his",
            " Christian",
            ",",
            " medieval",
            " attitude",
            ",",
            " Columbus",
            " solved",
            " the",
            " puzzle",
            " by",
            " assuming",
            " that",
            " he",
            " had",
            " discovered",
            " the",
            " earthly",
            " paradise",
            ".",
            " The",
            " earthly",
            " paradise",
            " was",
            " inaccessible",
            " to",
            " humans",
            " without",
            " God",
            "'s",
            " permission",
            ".",
            " Columbus",
            " experienced",
            " the",
            " geographical",
            " discovery",
            " of",
            " the",
            " New",
            " World",
            " in",
            " Christian",
            " terms",
            " and",
            " assigned",
            " himself",
            " a",
            " special",
            " role"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.664,
            0.371,
            0.225,
            -0.0,
            0.11,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            0.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Newton",
            " &",
            " Message",
            "Pad",
            " reviews",
            "Apple",
            " Newton",
            "Products",
            " introduced",
            " in",
            " ",
            "199",
            "3",
            "Apple",
            " Inc",
            ".",
            " personal",
            " digital",
            " assistants",
            "<|begin_of_text|>",
            "Al",
            "fred",
            " El",
            "ton",
            " van",
            " Vog",
            "t",
            " ",
            " (",
            " ;",
            " April",
            " ",
            "26",
            ",",
            " ",
            "191",
            "2",
            "Âł",
            "–",
            " January",
            " ",
            "26",
            ",",
            " ",
            "200",
            "0",
            ")",
            " was",
            " a",
            " Canadian",
            "-born",
            " American",
            " science",
            " fiction",
            " author",
            ".",
            " His",
            " fragmented",
            ",",
            " bizarre"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.641,
            0.377,
            0.197,
            0.177,
            0.163,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "deck",
            ",",
            " a",
            " number",
            " of",
            " initial",
            " concepts",
            " were",
            " built",
            " as",
            " experimental",
            " models",
            ",",
            " including",
            " the",
            " Dh",
            "onn",
            "as",
            " Be",
            "ag",
            " (",
            "Scott",
            "ish",
            " Gael",
            "ic",
            " for",
            " '",
            "little",
            " devil",
            "'),",
            " the",
            " first",
            " self",
            "-prop",
            "elled",
            " Bell",
            "-B",
            "ald",
            "win",
            " hydro",
            "foil",
            ".",
            " The",
            " experimental",
            " boats",
            " were",
            " essentially",
            " proof",
            "-of",
            "-con",
            "cept",
            " prototypes",
            " that",
            " cul",
            "minated",
            " in",
            " the",
            " more",
            " substantial",
            " HD",
            "-",
            "4",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.637,
            0.295,
            0.342,
            0.14,
            0.064,
            0.108,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            -0.0,
            0.127,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " April",
            " ",
            "194",
            "8",
            " to",
            " rave",
            " reviews",
            " and",
            " was",
            " chosen",
            " by",
            " the",
            " prestigious",
            " Kin",
            "ema",
            " Jun",
            "po",
            " critics",
            " poll",
            " as",
            " the",
            " best",
            " film",
            " of",
            " its",
            " year",
            ",",
            " the",
            " first",
            " of",
            " three",
            " K",
            "uros",
            "awa",
            " movies",
            " to",
            " be",
            " so",
            " honored",
            ".",
            "K",
            "uros",
            "awa",
            ",",
            " with",
            " producer",
            " S",
            "Åį",
            "j",
            "ir",
            "Åį",
            " Mot",
            "oki",
            " and",
            " fellow",
            " directors",
            " and",
            " friends",
            " K",
            "aj",
            "iro",
            " Yam",
            "amoto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.633,
            0.436,
            0.221,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Europe",
            "'s",
            " largest",
            " fishing",
            " fleet",
            ".",
            " Fish",
            " captured",
            " include",
            " s",
            "ard",
            "ines",
            ",",
            " m",
            "acker",
            "el",
            ",",
            " grou",
            "per",
            ",",
            " grey",
            " mul",
            "lets",
            ",",
            " sea",
            " bass",
            ",",
            " and",
            " seab",
            "ream",
            ".",
            " There",
            " is",
            " a",
            " considerable",
            " difference",
            " between",
            " fish",
            " catches",
            " between",
            " the",
            " pel",
            "agic",
            " and",
            " dem",
            "ers",
            "al",
            " zones",
            ";",
            " with",
            " respect",
            " to",
            " pel",
            "agic",
            " fisheries",
            ",",
            " the",
            " catches",
            " from",
            " the",
            " northern",
            ",",
            " central"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.594,
            0.379,
            0.157,
            0.23,
            0.149,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " system",
            " of",
            " urban",
            " and",
            " rural",
            " municipalities",
            " or",
            " comm",
            "unes",
            " ()",
            " in",
            " ",
            "201",
            "5",
            ".",
            " For",
            " smaller",
            " issues",
            " of",
            " local",
            " government",
            ",",
            " the",
            " municipalities",
            " are",
            " organized",
            " into",
            " ",
            "373",
            " administrative",
            " units",
            " (/",
            ").",
            " There",
            " are",
            " also",
            " ",
            "298",
            "0",
            " villages",
            " (),",
            " neighborhoods",
            " or",
            " wards",
            " (),",
            " and",
            " local",
            "ities",
            " ()",
            " previously",
            " used",
            " as",
            " administrative",
            " units",
            ".",
            "E",
            "conomy",
            " ",
            "The",
            " transition",
            " from",
            " a",
            " socialist"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.41,
            0.566,
            0.293,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " regardless",
            " of",
            " its",
            " semantic",
            " number",
            " when",
            " the",
            " subject",
            " of",
            " the",
            " verb",
            " is",
            " explicitly",
            " mentioned",
            " as",
            " a",
            " noun",
            ".",
            " Numer",
            "als",
            " between",
            " three",
            " and",
            " ten",
            " show",
            " \"",
            "ch",
            "ias",
            "mic",
            "\"",
            " agreement",
            ",",
            " in",
            " that",
            " gramm",
            "atically",
            " masculine",
            " numer",
            "als",
            " have",
            " feminine",
            " marking",
            " and",
            " vice",
            " versa",
            ".",
            "Ver",
            "bs",
            "Ver",
            "bs",
            " in",
            " Literary",
            " Arabic",
            " are",
            " marked",
            " for",
            " person",
            " (",
            "first",
            ",",
            " second",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.551,
            0.214,
            0.193,
            0.119,
            0.19,
            -0.0,
            0.056,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " tourist",
            " area",
            " to",
            " the",
            " north",
            ".",
            " Although",
            " the",
            " timing",
            " of",
            " the",
            " bus",
            "'",
            " departure",
            " is",
            " frequently",
            " up",
            " to",
            " the",
            " discretion",
            " of",
            " the",
            " driver",
            ",",
            " most",
            " buses",
            " operate",
            " according",
            " to",
            " a",
            " predetermined",
            " schedule",
            ".",
            " The",
            " routes",
            " that",
            " most",
            " buses",
            " take",
            " are",
            " typically",
            " displayed",
            " in",
            " the",
            " front",
            " windows",
            " of",
            " the",
            " vehicles",
            ",",
            " which",
            " are",
            " typically",
            " private",
            " min",
            "iv",
            "ans",
            " with",
            " seating",
            " for",
            " approximately",
            " ",
            "15"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "ylum",
    "orean",
    "fax",
    "rnek",
    ".pnl"
  ],
  "bottom_logits": [
    "serter",
    "abcdefghijklmnop",
    "andom",
    " sac",
    "æĬ¬"
  ],
  "act_min": -0.0,
  "act_max": 1.258
}