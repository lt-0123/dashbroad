{
  "index": 38538,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            "up",
            "ers",
            "'",
            " Press",
            ",",
            " UK",
            ",",
            " )",
            "See",
            " also",
            "Ex",
            "plan",
            "atory",
            " notes",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " references",
            "External",
            " links",
            " ",
            " ",
            " Sev",
            "agram",
            ",",
            " the",
            " A",
            ".E",
            ".",
            " van",
            " Vog",
            "t",
            " information",
            " site",
            " O",
            "bit",
            "uary",
            " at",
            " L",
            "ocus",
            " \"",
            "W",
            "riters",
            ":",
            " A",
            ".",
            " E",
            ".",
            " van",
            " Vog",
            "t",
            " (",
            "191",
            "2",
            "–"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " improvis",
            "ation",
            ".",
            " We",
            " are",
            " given",
            " an",
            " idea",
            " and",
            " then",
            " we",
            " are",
            " turned",
            " loose",
            " to",
            " develop",
            " in",
            " any",
            " way",
            " we",
            " want",
            " to",
            ".'",
            " I",
            " said",
            ",",
            " '",
            "That",
            "'s",
            " not",
            " acting",
            ".",
            " That",
            "'s",
            " writing",
            ".'\"",
            "Rec",
            "alling",
            " their",
            " experiences",
            " on",
            " Life",
            "boat",
            " for",
            " Charles",
            " Chandler",
            ",",
            " author",
            " of",
            " It",
            "'s",
            " Only",
            " a",
            " Movie",
            ":",
            " Alfred",
            " Hitch",
            "cock",
            " A",
            " Personal",
            " Biography",
            ",",
            " Walter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.609,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Real",
            "ency",
            "c",
            "lop",
            "Ã¤",
            "die",
            " der",
            " klass",
            "ischen",
            " Alter",
            "t",
            "um",
            "sw",
            "issenschaft",
            ":",
            " II",
            ",",
            " \"",
            "Ap",
            "oll",
            "on",
            "\".",
            " The",
            " best",
            " rep",
            "ert",
            "ory",
            " of",
            " cult",
            " sites",
            " (",
            "Bur",
            "k",
            "ert",
            ").",
            " Pe",
            "ck",
            ",",
            " Harry",
            " Thur",
            "ston",
            ",",
            " Har",
            "pers",
            " Dictionary",
            " of",
            " Classical",
            " Ant",
            "iqu",
            "ities",
            ",",
            " New",
            " York",
            ".",
            " Harper",
            " and",
            " Brothers",
            ".",
            " ",
            "189",
            "8",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " Since",
            " the",
            " ",
            "200",
            "0",
            "s",
            ",",
            " an",
            " increased",
            " number",
            " of",
            " anime",
            " works",
            " have",
            " been",
            " adaptations",
            " of",
            " light",
            " novels",
            " and",
            " visual",
            " novels",
            ";",
            " successful",
            " examples",
            " include",
            " The",
            " Mel",
            "anch",
            "oly",
            " of",
            " Har",
            "u",
            "hi",
            " Suz",
            "umi",
            "ya",
            " and",
            " Fate",
            "/st",
            "ay",
            " night",
            " (",
            "both",
            " ",
            "200",
            "6",
            ").",
            " Demon",
            " Slayer",
            ":",
            " Kim",
            "ets",
            "u",
            " no",
            " Ya",
            "iba",
            " the",
            " Movie",
            ":",
            " Mug",
            "en",
            " Train"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " Since",
            " the",
            " ",
            "200",
            "0",
            "s",
            ",",
            " an",
            " increased",
            " number",
            " of",
            " anime",
            " works",
            " have",
            " been",
            " adaptations",
            " of",
            " light",
            " novels",
            " and",
            " visual",
            " novels",
            ";",
            " successful",
            " examples",
            " include",
            " The",
            " Mel",
            "anch",
            "oly",
            " of",
            " Har",
            "u",
            "hi",
            " Suz",
            "umi",
            "ya",
            " and",
            " Fate",
            "/st",
            "ay",
            " night",
            " (",
            "both",
            " ",
            "200",
            "6",
            ").",
            " Demon",
            " Slayer",
            ":",
            " Kim",
            "ets",
            "u",
            " no",
            " Ya",
            "iba",
            " the",
            " Movie",
            ":",
            " Mug",
            "en",
            " Train"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " Del",
            "ph",
            "ic",
            " Myth",
            " and",
            " Its",
            " Origins",
            ",",
            " University",
            " of",
            " California",
            " Press",
            ",",
            " ",
            "195",
            "9",
            ".",
            " .",
            " G",
            "antz",
            ",",
            " Timothy",
            ",",
            " Early",
            " Greek",
            " Myth",
            ":",
            " A",
            " Guide",
            " to",
            " Literary",
            " and",
            " Art",
            "istic",
            " Sources",
            ",",
            " Johns",
            " Hopkins",
            " University",
            " Press",
            ",",
            " ",
            "199",
            "6",
            ",",
            " Two",
            " volumes",
            ":",
            " ",
            " (",
            "Vol",
            ".",
            " ",
            "1",
            "),",
            " ",
            " (",
            "Vol",
            ".",
            " ",
            "2",
            ").",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "iba",
            " –",
            " The",
            " Movie",
            ":",
            " Mug",
            "en",
            " Train",
            " in",
            " ",
            "202",
            "0",
            ".",
            " It",
            " was",
            " also",
            " the",
            " highest",
            "-g",
            "ross",
            "ing",
            " anime",
            " film",
            " worldwide",
            " until",
            " it",
            " was",
            " overt",
            "aken",
            " by",
            " Mak",
            "oto",
            " Sh",
            "ink",
            "ai",
            "'s",
            " ",
            "201",
            "6",
            " film",
            " Your",
            " Name",
            ".",
            " Anime",
            " films",
            " represent",
            " a",
            " large",
            " part",
            " of",
            " the",
            " highest",
            "-g",
            "ross",
            "ing",
            " Japanese",
            " films",
            " yearly",
            " in",
            " Japan",
            ",",
            " with",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " Modern",
            "ism",
            " and",
            " Music",
            ":",
            " An",
            " Anth",
            "ology",
            " of",
            " Sources",
            ".",
            " University",
            " of",
            " Chicago",
            " Press",
            ".",
            " ",
            " Be",
            "iser",
            ",",
            " Frederick",
            " C",
            ".,",
            " Wel",
            "ts",
            "ch",
            "mer",
            "z",
            ":",
            " P",
            "essim",
            "ism",
            " in",
            " German",
            " Philosophy",
            ",",
            " ",
            "186",
            "0",
            "-",
            "190",
            "0",
            " (",
            "O",
            "xford",
            ":",
            " Oxford",
            " University",
            " Press",
            ",",
            " ",
            "201",
            "6",
            ").",
            "  ",
            " Hann",
            "an",
            ",",
            " Barbara",
            ",",
            " The",
            " R"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "s",
            " (",
            "192",
            "0",
            ",",
            " Min",
            "erva",
            " Films",
            ")",
            " The",
            " Great",
            " Bro",
            "x",
            "opp",
            " (",
            "192",
            "1",
            ")",
            " The",
            " Dover",
            " Road",
            " (",
            "192",
            "1",
            ")",
            " The",
            " Lucky",
            " One",
            " (",
            "192",
            "2",
            ")",
            " The",
            " Truth",
            " About",
            " Bl",
            "ay",
            "ds",
            " (",
            "192",
            "2",
            ")",
            " The",
            " Artist",
            ":",
            " A",
            " Du",
            "ologue",
            " (",
            "192",
            "3",
            ")",
            " Give",
            " Me",
            " Yesterday",
            " (",
            "192",
            "3",
            ")",
            " (",
            "a",
            ".k",
            ".a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " ",
            "23",
            ").",
            " U",
            "MI",
            " Research",
            " Press",
            ".",
            " .",
            "  ",
            "  ",
            "  ",
            " Leonard",
            ",",
            " Kend",
            "ra",
            " Preston",
            " (",
            "200",
            "9",
            ").",
            " Shakespeare",
            ",",
            " Madness",
            ",",
            " and",
            " Music",
            ":",
            " Sc",
            "oring",
            " Ins",
            "anity",
            " in",
            " Cin",
            "ematic",
            " Adapt",
            "ations",
            ".",
            " Plymouth",
            ":",
            " The",
            " Sc",
            "are",
            "crow",
            " Press",
            ".",
            " .",
            "  ",
            "  ",
            "  ",
            " S",
            "ore",
            "ns",
            "en",
            ",",
            " Lars",
            "-M",
            "artin",
            " (",
            "200",
            "9",
            ")."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            "istics",
            " and",
            " related",
            " resources",
            " .",
            " E",
            ".",
            " K",
            ".",
            " Brown",
            " (",
            "ed",
            ".)",
            " Encyclopedia",
            " of",
            " Languages",
            " and",
            " Lingu",
            "istics",
            ".",
            " Oxford",
            ":",
            " Else",
            "vier",
            " Press",
            ".",
            " Gregory",
            " D",
            ".",
            " S",
            ".",
            " Anderson",
            " and",
            " Norman",
            " H",
            ".",
            " Z",
            "ide",
            ".",
            " ",
            "200",
            "2",
            ".",
            " Issues",
            " in",
            " Proto",
            "-M",
            "unda",
            " and",
            " Proto",
            "-A",
            "ust",
            "ro",
            "asi",
            "atic",
            " Nom",
            "inal",
            " Der",
            "ivation",
            ":",
            " The",
            " B",
            "im"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " central",
            " characters",
            " in",
            " Jordan",
            " Strat",
            "ford",
            "'s",
            " ste",
            "amp",
            "unk",
            " series",
            ",",
            " The",
            " W",
            "oll",
            "stone",
            "craft",
            " Detective",
            " Agency",
            ".",
            "L",
            "ovel",
            "ace",
            " features",
            " in",
            " John",
            " Crowley",
            "'s",
            " ",
            "200",
            "5",
            " novel",
            ",",
            " Lord",
            " Byron",
            "'s",
            " Novel",
            ":",
            " The",
            " Evening",
            " Land",
            ",",
            " as",
            " an",
            " unseen",
            " character",
            " whose",
            " personality",
            " is",
            " forcefully",
            " depicted",
            " in",
            " her",
            " annotations",
            " and",
            " anti",
            "-hero",
            "ic",
            " efforts",
            " to",
            " archive",
            " her",
            " father"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.159,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            "omp",
            "ertz",
            ".",
            " What",
            " Are",
            " You",
            " Looking",
            " At",
            "?:",
            " ",
            "150",
            " Years",
            " of",
            " Modern",
            " Art",
            " in",
            " the",
            " Blink",
            " of",
            " an",
            " Eye",
            ".",
            " New",
            " York",
            ":",
            " Viking",
            ",",
            " ",
            "201",
            "2",
            ".",
            " ",
            " W",
            "ÅĤad",
            "ys",
            "ÅĤaw",
            " Tat",
            "ark",
            "iew",
            "icz",
            ",",
            " A",
            " History",
            " of",
            " Six",
            " Ideas",
            ":",
            " an",
            " Essay",
            " in",
            " A",
            "est",
            "hetics",
            ",",
            " translated",
            " from",
            " the",
            " Polish",
            " by",
            " Christopher",
            " Kas",
            "pare",
            "k"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.383,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "199",
            "9",
            ".",
            " ATP",
            " Player",
            " of",
            " the",
            " Year",
            ":",
            " ",
            "199",
            "9",
            ".",
            " ATP",
            " Most",
            " Improved",
            " Player",
            ":",
            " ",
            "198",
            "8",
            ",",
            " ",
            "199",
            "8",
            "Recognition",
            " In",
            " ",
            "199",
            "2",
            ",",
            " Ag",
            "assi",
            " was",
            " named",
            " the",
            " BBC",
            " Overse",
            "as",
            " Sports",
            " Personality",
            " of",
            " the",
            " Year",
            ".",
            " In",
            " ",
            "201",
            "0",
            ",",
            " Sports",
            " Illustrated",
            " named",
            " Ag",
            "assi",
            " the",
            " ",
            "7",
            "th"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " text",
            "'s",
            ",",
            " and",
            " that",
            " a",
            " moral",
            "-pol",
            "itical",
            " argument",
            " is",
            " being",
            " carried",
            " out",
            " by",
            " means",
            " of",
            " parody",
            "\".",
            "While",
            " Swift",
            "'s",
            " proposal",
            " is",
            " obviously",
            " not",
            " a",
            " serious",
            " economic",
            " proposal",
            ",",
            " George",
            " Witt",
            "k",
            "ows",
            "ky",
            ",",
            " author",
            " of",
            " \"",
            "Swift",
            "'s",
            " Mod",
            "est",
            " Proposal",
            ":",
            " The",
            " Biography",
            " of",
            " an",
            " Early",
            " Georgian",
            " Pam",
            "ph",
            "let",
            "\",",
            " argues",
            " that",
            " to",
            " understand",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " Man",
            "hood",
            " of",
            " Humanity",
            ",",
            " fore",
            "word",
            " by",
            " Edward",
            " Kas",
            "ner",
            ",",
            " notes",
            " by",
            " M",
            ".",
            " Kend",
            "ig",
            ",",
            " Institute",
            " of",
            " General",
            " Sem",
            "antics",
            ",",
            " ",
            "195",
            "0",
            ",",
            " hard",
            "cover",
            ",",
            " ",
            "2",
            "nd",
            " edition",
            ",",
            " ",
            "391",
            " pages",
            ",",
            " .",
            " (",
            "Copy",
            " of",
            " the",
            " first",
            " edition",
            ".)",
            " Science",
            " and",
            " Sanity",
            ":",
            " An",
            " Introduction",
            " to",
            " Non",
            "-A",
            "rist",
            "otel",
            "ian",
            " Systems",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " Miranda",
            " J",
            ".",
            " Green",
            ",",
            " ",
            "199",
            "7",
            ".",
            " Dictionary",
            " of",
            " Celtic",
            " Myth",
            " and",
            " Legend",
            ",",
            " Thames",
            " and",
            " Hudson",
            ".",
            " Gr",
            "imal",
            ",",
            " Pierre",
            ",",
            " The",
            " Dictionary",
            " of",
            " Classical",
            " Myth",
            "ology",
            ",",
            " Wiley",
            "-",
            "Black",
            "well",
            ",",
            " ",
            "199",
            "6",
            ".",
            " .",
            " Hard",
            ",",
            " Robin",
            ",",
            " The",
            " Rout",
            "ledge",
            " Handbook",
            " of",
            " Greek",
            " Myth",
            "ology",
            ":",
            " Based",
            " on",
            " H",
            ".J",
            ".",
            " Rose",
            "'s",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.081,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.451,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "three",
            " different",
            " awards",
            "):",
            " since",
            " ",
            "193",
            "1",
            " Gordon",
            " E",
            ".",
            " Sawyer",
            " Award",
            ":",
            " since",
            " ",
            "198",
            "1",
            " Jean",
            " H",
            "ersh",
            "olt",
            " Human",
            "itarian",
            " Award",
            ":",
            " since",
            " ",
            "195",
            "7",
            " ",
            " Irving",
            " G",
            ".",
            " Th",
            "al",
            "berg",
            " Memorial",
            " Award",
            ":",
            " since",
            " ",
            "193",
            "8",
            " ",
            " Academy",
            " Special",
            " Achievement",
            " Award",
            ":",
            " from",
            " ",
            "197",
            "2",
            " to",
            " ",
            "199",
            "5",
            ",",
            " and",
            " again"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " ste",
            "le",
            " was",
            " erected",
            " in",
            " ",
            "196",
            "1",
            " in",
            " honor",
            " of",
            " Albert",
            " Cam",
            "us",
            " with",
            " this",
            " phrase",
            " in",
            " French",
            " extracted",
            " from",
            " his",
            " work",
            " N",
            "oc",
            "es",
            " Ãł",
            " Tip",
            "asa",
            ":",
            " \"",
            "I",
            " understand",
            " here",
            " what",
            " is",
            " called",
            " glory",
            ":",
            " the",
            " right",
            " to",
            " love",
            " beyond",
            " measure",
            " \"",
            " (Â«",
            " Je",
            " comprend",
            "s",
            " ici",
            " ce",
            " qu",
            "'on",
            " app",
            "elle",
            " glo",
            "ire",
            " :",
            " le",
            " droit",
            " d"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spotlight",
            ":",
            " Spider",
            "-Man",
            " –",
            " One",
            " More",
            " Day",
            "/",
            "Brand",
            " New",
            " Day",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " ",
            "1",
            " [#",
            "546",
            "–",
            "551",
            ";",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Swing",
            " Shift",
            " (",
            "Director",
            "'s",
            " Cut",
            ");",
            " Venom",
            " Super",
            "-S",
            "pecial",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " ",
            "2",
            " [#",
            "552",
            "–",
            "558",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ")",
            " How",
            "l",
            " An",
            "notated",
            " (",
            "199",
            "5",
            ")",
            " Illum",
            "inated",
            " Po",
            "ems",
            " (",
            "199",
            "6",
            ")",
            " Selected",
            " Po",
            "ems",
            ":",
            " ",
            "194",
            "7",
            "–",
            "199",
            "5",
            " (",
            "199",
            "6",
            ")",
            " Death",
            " and",
            " Fame",
            ":",
            " Po",
            "ems",
            " ",
            "199",
            "3",
            "–",
            "199",
            "7",
            " (",
            "199",
            "9",
            ")",
            " Del",
            "iber",
            "ate",
            " Pro",
            "se",
            " ",
            "195",
            "2",
            "–",
            "199",
            "5",
            " (",
            "200",
            "0",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.081,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.451,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "three",
            " different",
            " awards",
            "):",
            " since",
            " ",
            "193",
            "1",
            " Gordon",
            " E",
            ".",
            " Sawyer",
            " Award",
            ":",
            " since",
            " ",
            "198",
            "1",
            " Jean",
            " H",
            "ersh",
            "olt",
            " Human",
            "itarian",
            " Award",
            ":",
            " since",
            " ",
            "195",
            "7",
            " ",
            " Irving",
            " G",
            ".",
            " Th",
            "al",
            "berg",
            " Memorial",
            " Award",
            ":",
            " since",
            " ",
            "193",
            "8",
            " ",
            " Academy",
            " Special",
            " Achievement",
            " Award",
            ":",
            " from",
            " ",
            "197",
            "2",
            " to",
            " ",
            "199",
            "5",
            ",",
            " and",
            " again"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "2",
            ").",
            " Line",
            " by",
            " Line",
            " Commentary",
            " on",
            " Aristotle",
            "'s",
            " De",
            " An",
            "ima",
            " ,",
            " Volume",
            " ",
            "1",
            ":",
            " Books",
            " I",
            " &",
            " II",
            ";",
            " Volume",
            " ",
            "2",
            ":",
            " Book",
            " III",
            ".",
            " The",
            " F",
            "ocusing",
            " Institute",
            ".",
            " Gill",
            ",",
            " Mary",
            " Louise",
            " (",
            "198",
            "9",
            ").",
            " Aristotle",
            " on",
            " Substance",
            ":",
            " The",
            " Par",
            "adox",
            " of",
            " Unity",
            ".",
            " Princeton",
            " University",
            " Press",
            ".",
            "  ",
            "  ",
            " ",
            " J",
            "ori",
            ",",
            " Alberto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "2",
            ").",
            " Line",
            " by",
            " Line",
            " Commentary",
            " on",
            " Aristotle",
            "'s",
            " De",
            " An",
            "ima",
            " ,",
            " Volume",
            " ",
            "1",
            ":",
            " Books",
            " I",
            " &",
            " II",
            ";",
            " Volume",
            " ",
            "2",
            ":",
            " Book",
            " III",
            ".",
            " The",
            " F",
            "ocusing",
            " Institute",
            ".",
            " Gill",
            ",",
            " Mary",
            " Louise",
            " (",
            "198",
            "9",
            ").",
            " Aristotle",
            " on",
            " Substance",
            ":",
            " The",
            " Par",
            "adox",
            " of",
            " Unity",
            ".",
            " Princeton",
            " University",
            " Press",
            ".",
            "  ",
            "  ",
            " ",
            " J",
            "ori",
            ",",
            " Alberto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "192",
            "1",
            " Man",
            "hood",
            " of",
            " Humanity",
            ":",
            " The",
            " Science",
            " and",
            " Art",
            " of",
            " Human",
            " Engineering",
            ",",
            " E",
            ".P",
            ".",
            " D",
            "utton",
            ",",
            " New",
            " York",
            ",",
            " USA",
            " ",
            "193",
            "3",
            " Science",
            " and",
            " Sanity",
            ":",
            " An",
            " Introduction",
            " to",
            " Non",
            "-A",
            "rist",
            "otel",
            "ian",
            " Systems",
            " and",
            " General",
            " Sem",
            "antics",
            ",",
            " Science",
            " Press",
            " Printing",
            " Co",
            ".,",
            " Lancaster",
            ",",
            " Pa",
            ".,",
            " USA",
            "See",
            " also",
            " Alfred"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "Memory",
            ":",
            " Allen",
            " Gins",
            "berg",
            ")",
            " Cherry",
            " Valley",
            " Edition",
            "s",
            ",",
            " ",
            "198",
            "2",
            "  ",
            " ",
            " Morgan",
            ",",
            " Bill",
            " (",
            "ed",
            ".),",
            " I",
            " G",
            "reet",
            " You",
            " at",
            " the",
            " Beginning",
            " of",
            " a",
            " Great",
            " Career",
            ":",
            " The",
            " Selected",
            " Correspond",
            "ence",
            " of",
            " Lawrence",
            " Fer",
            "ling",
            "h",
            "etti",
            " and",
            " Allen",
            " Gins",
            "berg",
            ",",
            " ",
            "195",
            "5",
            "–",
            "199",
            "7",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " Publishers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " OH",
            ":",
            " Bowling",
            " Green",
            " State",
            " University",
            " Popular",
            " Press",
            ".",
            " .",
            " Per",
            "k",
            "owitz",
            ",",
            " Sidney",
            " (",
            "200",
            "4",
            ").",
            " Digital",
            " People",
            ":",
            " From",
            " B",
            "ionic",
            " Humans",
            " to",
            " Android",
            "s",
            ".",
            " Joseph",
            " Henry",
            " Press",
            ".",
            " .",
            " Shel",
            "de",
            ",",
            " Per",
            " (",
            "199",
            "3",
            ").",
            " Android",
            "s",
            ",",
            " Human",
            "oids",
            ",",
            " and",
            " Other",
            " Science",
            " Fiction",
            " Monsters",
            ":",
            " Science",
            " and",
            " Soul",
            " in",
            " Science",
            " Fiction",
            " Films",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "201",
            "6",
            "),",
            " Themes",
            " of",
            " the",
            " Trojan",
            " Cycle",
            ":",
            " Contribution",
            " to",
            " the",
            " study",
            " of",
            " the",
            " g",
            "reek",
            " myth",
            "ological",
            " tradition",
            " (",
            "Co",
            "im",
            "bra",
            ").",
            "External",
            " links",
            " ",
            " Trojan",
            " War",
            " Resources",
            " Gallery",
            " of",
            " the",
            " Ancient",
            " Art",
            ":",
            " Achilles",
            " ",
            " Po",
            "em",
            " by",
            " Florence",
            " Ear",
            "le",
            " Co",
            "ates",
            "Greek",
            " myth",
            "ological",
            " heroes",
            "K",
            "ings",
            " of",
            " the",
            " My",
            "rm",
            "id",
            "ons"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "201",
            "6",
            "),",
            " Themes",
            " of",
            " the",
            " Trojan",
            " Cycle",
            ":",
            " Contribution",
            " to",
            " the",
            " study",
            " of",
            " the",
            " g",
            "reek",
            " myth",
            "ological",
            " tradition",
            " (",
            "Co",
            "im",
            "bra",
            ").",
            "External",
            " links",
            " ",
            " Trojan",
            " War",
            " Resources",
            " Gallery",
            " of",
            " the",
            " Ancient",
            " Art",
            ":",
            " Achilles",
            " ",
            " Po",
            "em",
            " by",
            " Florence",
            " Ear",
            "le",
            " Co",
            "ates",
            "Greek",
            " myth",
            "ological",
            " heroes",
            "K",
            "ings",
            " of",
            " the",
            " My",
            "rm",
            "id",
            "ons"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Mod",
            "est",
            " Proposal",
            ":",
            " the",
            " voice",
            " of",
            " Swift",
            " and",
            " the",
            " voice",
            " of",
            " the",
            " Prop",
            "oser",
            ".",
            " Ph",
            "idd",
            "ian",
            " stresses",
            " that",
            " a",
            " reader",
            " of",
            " the",
            " pamph",
            "let",
            " must",
            " learn",
            " to",
            " distinguish",
            " between",
            " the",
            " sat",
            "irical",
            " voice",
            " of",
            " Jonathan",
            " Swift",
            " and",
            " the",
            " apparent",
            " economic",
            " projections",
            " of",
            " the",
            " Prop",
            "oser",
            ".",
            " He",
            " reminds",
            " readers",
            " that",
            " \"",
            "there",
            " is",
            " a",
            " gap",
            " between",
            " the",
            " narrator",
            "'s",
            " meaning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Essays",
            " and",
            " Aph",
            "or",
            "isms",
            ",",
            " being",
            " excerpts",
            " from",
            " Volume",
            " ",
            "2",
            " of",
            " Par",
            "erg",
            "a",
            " und",
            " Par",
            "al",
            "ip",
            "omen",
            "a",
            ",",
            " selected",
            " and",
            " translated",
            " by",
            " R",
            ".",
            " J",
            ".",
            " Holl",
            "ing",
            "dale",
            ",",
            " with",
            " Introduction",
            " by",
            " R",
            " J",
            " Holl",
            "ing",
            "dale",
            ",",
            " Penguin",
            " Classics",
            ",",
            " ",
            "197",
            "0",
            ",",
            " Paperback",
            " ",
            "197",
            "3",
            ":",
            " ",
            " An",
            " En",
            "quiry",
            " concerning",
            " Ghost"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            "200",
            "9",
            ")",
            " I",
            " G",
            "reet",
            " You",
            " at",
            " the",
            " Beginning",
            " of",
            " a",
            " Great",
            " Career",
            ":",
            " The",
            " Selected",
            " Correspond",
            "ence",
            " of",
            " Lawrence",
            " Fer",
            "ling",
            "h",
            "etti",
            " and",
            " Allen",
            " Gins",
            "berg",
            ",",
            " ",
            "195",
            "5",
            "–",
            "199",
            "7",
            " (",
            "City",
            " Lights",
            ",",
            " ",
            "201",
            "5",
            ")",
            " The",
            " Best",
            " Minds",
            " of",
            " My",
            " Generation",
            ":",
            " A",
            " Literary",
            " History",
            " of",
            " the",
            " Beats",
            " (",
            "G",
            "rove",
            " Press",
            ",",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "1",
            ":",
            " Phon",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "195",
            "7",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " II",
            ".",
            " Form"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " subjects",
            " by",
            " reason",
            " of",
            " parent",
            "age",
            ",",
            " may",
            ",",
            " by",
            " declarations",
            " of",
            " alien",
            "age",
            ",",
            " get",
            " rid",
            " of",
            " British",
            " nationality",
            ".",
            " Em",
            "igration",
            " to",
            " an",
            " unc",
            "ivil",
            "ized",
            " country",
            " left",
            " British",
            " nationality",
            " unaffected",
            ":",
            " indeed",
            " the",
            " right",
            " claimed",
            " by",
            " all",
            " states",
            " to",
            " follow",
            " with",
            " their",
            " authority",
            " their",
            " subjects",
            " so",
            " em",
            "igr",
            "ating",
            " was",
            " one",
            " of",
            " the",
            " usual",
            " and",
            " recognized",
            " means",
            " of",
            " colonial",
            " expansion"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " stress",
            " is",
            " often",
            " re",
            "tracted",
            ":",
            " ",
            " '",
            "two",
            " houses",
            "',",
            " ",
            " '",
            "their",
            " table",
            "',",
            " ",
            " '",
            "des",
            "ks",
            "',",
            " ",
            " '",
            "sometimes",
            "',",
            " ",
            " '",
            "their",
            " school",
            "'.",
            " In",
            " this",
            " dialect",
            ",",
            " only",
            " syll",
            "ables",
            " with",
            " long",
            " vowels",
            " or",
            " d",
            "iph",
            "th",
            "ongs",
            " are",
            " considered",
            " heavy",
            ";",
            " in",
            " a",
            " two",
            "-s",
            "yll",
            "able",
            " word",
            ",",
            " the",
            " final",
            " syll",
            "able",
            " can",
            " be"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " alien",
            "ation",
            " caused",
            " by",
            " capitalism",
            " and",
            " it",
            " prevents",
            " humans",
            " from",
            " living",
            " a",
            " joyful",
            " life",
            ".",
            "Other",
            " anarchists",
            " advocated",
            " for",
            " or",
            " used",
            " art",
            " as",
            " a",
            " means",
            " to",
            " achieve",
            " anarchist",
            " ends",
            ".",
            " In",
            " his",
            " book",
            " Breaking",
            " the",
            " Spell",
            ":",
            " A",
            " History",
            " of",
            " An",
            "arch",
            "ist",
            " Fil",
            "mm",
            "akers",
            ",",
            " Vide",
            "ot",
            "ape",
            " Guerr",
            "illas",
            ",",
            " and",
            " Digital",
            " Nin",
            "jas",
            ",",
            " Chris",
            " Rob",
            "Ã©",
            " claims",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " wear",
            " black",
            " (",
            "the",
            " traditional",
            " funeral",
            " color",
            ")",
            " while",
            " attending",
            " his",
            " service",
            ",",
            " during",
            " which",
            " solo",
            "ist",
            " Jean",
            " MacDonald",
            " sang",
            " a",
            " verse",
            " of",
            " Robert",
            " Louis",
            " Stevenson",
            "'s",
            " \"",
            "Re",
            "qu",
            "iem",
            "\":",
            "Upon",
            " the",
            " conclusion",
            " of",
            " Bell",
            "'s",
            " funeral",
            ",",
            " for",
            " one",
            " minute",
            " at",
            " ",
            "6",
            ":",
            "25",
            "Âłp",
            ".m",
            ".",
            " Eastern",
            " Time",
            ",",
            " \"",
            "every",
            " phone",
            " on",
            " the",
            " continent",
            " of",
            " North",
            " America"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "male",
            " ",
            "466",
            ",",
            "085",
            " /",
            "female",
            " ",
            "540",
            ",",
            "452",
            ")",
            "65",
            " years",
            " and",
            " over",
            ":",
            " ",
            "2",
            ".",
            "32",
            "%",
            " (",
            "male",
            " ",
            "296",
            ",",
            "411",
            " /",
            "female",
            " ",
            "408",
            ",",
            "648",
            ")",
            " (",
            "201",
            "8",
            " est",
            ".)",
            "Total",
            " fertility",
            " rate",
            "5",
            ".",
            "83",
            " children",
            " born",
            "/w",
            "oman",
            " (",
            "202",
            "2",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "28",
            "th",
            " book",
            " of",
            " al",
            "-Z",
            "ahr",
            "Äģ",
            "w",
            "Ä«",
            "'s",
            " (",
            "Latin",
            ":",
            " Ab",
            "ul",
            "c",
            "asis",
            ",",
            " ",
            "936",
            "–",
            "101",
            "3",
            ")",
            " Kit",
            "Äģ",
            "b",
            " al",
            "-Ta",
            "á¹",
            "£",
            "r",
            "Ä«",
            "f",
            " (",
            "later",
            " translated",
            " into",
            " Latin",
            " as",
            " Liber",
            " serv",
            "ator",
            "is",
            ").",
            " In",
            " the",
            " tw",
            "elfth",
            " century",
            ",",
            " recipes",
            " for",
            " the",
            " production",
            " of",
            " aqu",
            "a",
            " ar",
            "dens",
            " (\""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ã§a",
            ".",
            " \"",
            "Cit",
            "izens",
            "hip",
            ",",
            " National",
            " Identity",
            ",",
            " and",
            " Nation",
            "-B",
            "uild",
            "ing",
            " in",
            " Azerbaijan",
            ":",
            " Between",
            " the",
            " Legacy",
            " of",
            " the",
            " Past",
            " and",
            " the",
            " Spirit",
            " of",
            " Independence",
            ".\"",
            " National",
            "ities",
            " Papers",
            " (",
            "202",
            "1",
            "):",
            " ",
            "1",
            "–",
            "18",
            ".",
            " online",
            " G",
            "olt",
            "z",
            ",",
            " Thomas",
            ".",
            " Azerbaijan",
            " Diary",
            " :",
            " A",
            " Rogue",
            " Reporter",
            "'s",
            " Adventures",
            " in",
            " an",
            " Oil",
            "-R",
            "ich"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            "in",
            "organic",
            " acids",
            ")",
            " Hydro",
            "gen",
            " hal",
            "ides",
            " and",
            " their",
            " solutions",
            ":",
            " hydro",
            "flu",
            "oric",
            " acid",
            " (",
            "HF",
            "),",
            " hydro",
            "chlor",
            "ic",
            " acid",
            " (",
            "H",
            "Cl",
            "),",
            " hydro",
            "b",
            "rom",
            "ic",
            " acid",
            " (",
            "H",
            "Br",
            "),",
            " hydro",
            "iod",
            "ic",
            " acid",
            " (",
            "HI",
            ")",
            " Hal",
            "ogen",
            " ox",
            "o",
            "ac",
            "ids",
            ":",
            " hyp",
            "och",
            "lor",
            "ous",
            " acid",
            " (",
            "H",
            "Cl",
            "O",
            "),",
            " chlor",
            "ous",
            " acid"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " att",
            "ested",
            " in",
            " Ber",
            "ber",
            ",",
            " Ch",
            "adic",
            ",",
            " Cush",
            "itic",
            ",",
            " and",
            " Sem",
            "itic",
            ":",
            " it",
            " usually",
            " affects",
            " features",
            " such",
            " as",
            " ph",
            "ary",
            "nge",
            "al",
            "ization",
            ",",
            " pal",
            "atal",
            "ization",
            ",",
            " and",
            " lab",
            "ial",
            "ization",
            ".",
            " Several",
            " Om",
            "otic",
            " languages",
            " have",
            " \"",
            "s",
            "ibil",
            "ant",
            " harmony",
            "\",",
            " meaning",
            " that",
            " all",
            " s",
            "ibil",
            "ants",
            " (",
            "s",
            ",",
            " sh",
            ",",
            " z",
            ",",
            " ts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.4,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Thank",
            " You",
            " AB",
            "BA",
            ".",
            " Willow",
            " Wil",
            " Studios",
            "/A",
            "2",
            "C",
            " Video",
            ",",
            " ",
            "199",
            "3",
            " Barry",
            " Barnes",
            ":",
            " AB",
            "BA",
            " âĪĴ",
            " The",
            " History",
            ".",
            " Polar",
            " Music",
            " International",
            " AB",
            ",",
            " ",
            "199",
            "9",
            " Chris",
            " Hunt",
            ":",
            " The",
            " Winner",
            " Takes",
            " it",
            " All",
            " âĪĴ",
            " The",
            " AB",
            "BA",
            " Story",
            ".",
            " Little",
            "star",
            " Services",
            "/l",
            "amb",
            "ic",
            " Productions",
            ",",
            " ",
            "199",
            "9",
            " Steve",
            " Cole"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " frontal",
            " prem",
            "otor",
            " and",
            " motor",
            " cortex",
            ".",
            "See",
            " also",
            " ",
            " At",
            "ax",
            "ic",
            " cerebral",
            " p",
            "alsy",
            " L",
            "ocom",
            "otor",
            " at",
            "ax",
            "ia",
            " Br",
            "uns",
            " apr",
            "ax",
            "ia",
            " National",
            " At",
            "ax",
            "ia",
            " Foundation",
            "References",
            "Further",
            " reading",
            " ",
            "  ",
            " ",
            "Sym",
            "ptoms",
            " and",
            " signs",
            ":",
            " N",
            "erv",
            "ous",
            " system",
            "Stroke",
            "<|begin_of_text|>",
            "August",
            "a",
            " Ada",
            " King",
            ",",
            " Count",
            "ess"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " Berlin",
            ",",
            " ",
            "195",
            "7",
            "Bob",
            " Kingston",
            " The",
            " Desert",
            "ed",
            " Village",
            " at",
            " Sl",
            "iev",
            "em",
            "ore",
            ",",
            " Castle",
            "bar",
            ",",
            " ",
            "199",
            "0",
            "Th",
            "eresa",
            " McDonald",
            ":",
            " Ach",
            "ill",
            ":",
            " ",
            "500",
            "0",
            " B",
            ".C",
            ".",
            " to",
            " ",
            "190",
            "0",
            " A",
            ".D",
            ".:",
            " Ar",
            "che",
            "ology",
            " History",
            " Folk",
            "lore",
            ",",
            " I",
            ".A",
            ".S",
            ".",
            " Publications",
            " [",
            "199",
            "2",
            "]",
            "R",
            "osa"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " spoken",
            " varieties",
            ":",
            " from",
            " nearly",
            " pure",
            " Modern",
            " Standard",
            " Arabic",
            " (",
            "MS",
            "A",
            "),",
            " to",
            " a",
            " form",
            " that",
            " still",
            " uses",
            " M",
            "SA",
            " grammar",
            " and",
            " vocabulary",
            " but",
            " with",
            " significant",
            " collo",
            "qu",
            "ial",
            " influence",
            ",",
            " to",
            " a",
            " form",
            " of",
            " the",
            " collo",
            "qu",
            "ial",
            " language",
            " that",
            " imports",
            " a",
            " number",
            " of",
            " words",
            " and",
            " gramm",
            "atical",
            " constructions",
            " in",
            " M",
            "SA",
            ",",
            " to",
            " a",
            " form",
            " that",
            " is",
            " close",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " which",
            " has",
            " different",
            " boundaries",
            " and",
            " was",
            " abandoned",
            " as",
            " an",
            " administrative",
            " area",
            " in",
            " ",
            "197",
            "5",
            " under",
            " the",
            " Local",
            " Government",
            " (",
            "Scotland",
            ")",
            " Act",
            " ",
            "197",
            "3",
            ".",
            " It",
            " was",
            " replaced",
            " by",
            " Gr",
            "amp",
            "ian",
            " Regional",
            " Council",
            " and",
            " five",
            " district",
            " councils",
            ":",
            " Ban",
            "ff",
            " and",
            " Buch",
            "an",
            ",",
            " Gordon",
            ",",
            " K",
            "inc",
            "ard",
            "ine",
            " and",
            " De",
            "es",
            "ide",
            ",",
            " Mor",
            "ay",
            " and",
            " the",
            " City"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.249,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " such",
            " poems",
            " as",
            " Jub",
            "ilate",
            " Ag",
            "no",
            ".",
            " Gins",
            "berg",
            " also",
            " claimed",
            " other",
            " more",
            " traditional",
            " influences",
            ",",
            " such",
            " as",
            ":",
            " Franz",
            " Kafka",
            ",",
            " Herman",
            " Mel",
            "ville",
            ",",
            " F",
            "y",
            "odor",
            " Dost",
            "oe",
            "v",
            "sky",
            ",",
            " Edgar",
            " Allan",
            " Poe",
            ",",
            " and",
            " Emily",
            " Dickinson",
            ".",
            "G",
            "ins",
            "berg",
            " also",
            " made",
            " an",
            " intense",
            " study",
            " of",
            " ha",
            "iku",
            " and",
            " the",
            " paintings",
            " of",
            " Paul",
            " C",
            "Ã©",
            "z",
            "anne"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.217,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " say",
            ":",
            " '",
            "You",
            "'re",
            " just",
            " ob",
            "noxious",
            ",",
            " I",
            " can",
            "'t",
            " bear",
            " you",
            ".'\"",
            "Billy",
            " Name",
            " also",
            " denied",
            " that",
            " War",
            "hol",
            " was",
            " only",
            " a",
            " voyeur",
            ",",
            " saying",
            ":",
            " \"",
            "He",
            " was",
            " the",
            " essence",
            " of",
            " sexuality",
            ".",
            " It",
            " perme",
            "ated",
            " everything",
            ".",
            " Andy",
            " ex",
            "uded",
            " it",
            ",",
            " along",
            " with",
            " his",
            " great",
            " artistic",
            " creativity",
            "....",
            "It",
            " brought",
            " a",
            " joy",
            " to",
            " the",
            " whole",
            " art",
            " world"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " accum",
            "ulations",
            " of",
            " pus",
            " in",
            " a",
            " pre",
            "existing",
            ",",
            " rather",
            " than",
            " a",
            " newly",
            " formed",
            ",",
            " anatom",
            "ical",
            " cavity",
            ".",
            "Other",
            " conditions",
            " that",
            " can",
            " cause",
            " similar",
            " symptoms",
            " include",
            ":",
            " cellul",
            "itis",
            ",",
            " a",
            " seb",
            "aceous",
            " cyst",
            ",",
            " and",
            " nec",
            "rot",
            "ising",
            " fasc",
            "i",
            "itis",
            ".",
            " Cell",
            "ul",
            "itis",
            " typically",
            " also",
            " has",
            " an",
            " ery",
            "th",
            "emat",
            "ous",
            " reaction",
            ",",
            " but",
            " does",
            " not",
            " confer",
            " any",
            " pur"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " View",
            " from",
            " the",
            " Top",
            " of",
            " the",
            " World",
            ",",
            " ",
            "202",
            "1",
            "Video",
            " games",
            " ",
            " Alien",
            " (",
            "198",
            "4",
            " video",
            " game",
            "),",
            " based",
            " on",
            " the",
            " film",
            " Alien",
            " (",
            "At",
            "ari",
            " ",
            "260",
            "0",
            "),",
            " a",
            " ",
            "198",
            "2",
            " maze",
            " game",
            " based",
            " on",
            " the",
            " ",
            "197",
            "9",
            " film",
            " Alien",
            ":",
            " Is",
            "olation",
            ",",
            " a",
            " ",
            "201",
            "4",
            " video",
            " game",
            " based",
            " on",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " There",
            " were",
            " extensive",
            " gold",
            " mines",
            " in",
            " N",
            "ub",
            "ia",
            ",",
            " and",
            " one",
            " of",
            " the",
            " first",
            " maps",
            " known",
            " is",
            " of",
            " a",
            " gold",
            " mine",
            " in",
            " this",
            " region",
            ".",
            " The",
            " W",
            "adi",
            " Hamm",
            "amat",
            " was",
            " a",
            " notable",
            " source",
            " of",
            " granite",
            ",",
            " grey",
            "w",
            "ack",
            "e",
            ",",
            " and",
            " gold",
            ".",
            " Flint",
            " was",
            " the",
            " first",
            " mineral",
            " collected",
            " and",
            " used",
            " to",
            " make",
            " tools",
            ",",
            " and",
            " fl",
            "int",
            " hand"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " broadcast",
            " in",
            " both",
            " of",
            " Afghanistan",
            "'s",
            " official",
            " languages",
            " on",
            " radio",
            ".",
            " Press",
            " restrictions",
            " have",
            " been",
            " gradually",
            " relaxed",
            " and",
            " private",
            " media",
            " diversified",
            " since",
            " ",
            "200",
            "2",
            ",",
            " after",
            " more",
            " than",
            " two",
            " decades",
            " of",
            " tight",
            " controls",
            ".",
            "Af",
            "gh",
            "ans",
            " have",
            " long",
            " been",
            " accustomed",
            " to",
            " watching",
            " Indian",
            " Bollywood",
            " films",
            " and",
            " listening",
            " to",
            " its",
            " film",
            "i",
            " songs",
            ".",
            " It",
            " has",
            " been",
            " claimed",
            " that",
            " Afghanistan",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " topic",
            " of",
            " Paul",
            "ine",
            " thought",
            ",",
            " which",
            " has",
            " been",
            " the",
            " most",
            " popular",
            " argument",
            " set",
            " forward",
            " by",
            " Martin",
            " Luther",
            ",",
            " Schwe",
            "itzer",
            " argues",
            " that",
            " Paul",
            "'s",
            " emphasis",
            " was",
            " on",
            " the",
            " mystical",
            " union",
            " with",
            " God",
            " by",
            " \"",
            "being",
            " in",
            " Christ",
            "\".",
            " Jar",
            "oslav",
            " Pel",
            "ikan",
            ",",
            " in",
            " his",
            " fore",
            "word",
            " to",
            " The",
            " Myst",
            "icism",
            " of",
            " Paul",
            " the",
            " Apostle",
            ",",
            " points",
            " out",
            " that",
            ":",
            "Paul",
            "'s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "L",
            "'",
            "Hom",
            "me",
            " rÃ©",
            "volt",
            "Ã©",
            ")",
            " (",
            "195",
            "1",
            ")",
            " Alger",
            "ian",
            " Chronicles",
            " (",
            "Chron",
            "iques",
            " alg",
            "Ã©ri",
            "ennes",
            ")",
            " (",
            "195",
            "8",
            ",",
            " first",
            " English",
            " translation",
            " published",
            " ",
            "201",
            "3",
            ")",
            " Resistance",
            ",",
            " Rebellion",
            ",",
            " and",
            " Death",
            " (",
            "collection",
            ",",
            " ",
            "196",
            "1",
            ")",
            " ",
            " Note",
            "books",
            " ",
            "193",
            "5",
            "–",
            "194",
            "2",
            " (",
            "C",
            "arn",
            "ets",
            ",",
            " mai"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "BackingField",
    "uner",
    "iesen",
    "eza",
    "ç½"
  ],
  "bottom_logits": [
    " Rooney",
    " base",
    " Abs",
    " Trem",
    "Abs"
  ],
  "act_min": -0.0,
  "act_max": 0.641
}