{
  "index": 65452,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " writings",
            ".",
            " During",
            " these",
            " visits",
            " Shelley",
            " wrote",
            " the",
            " poem",
            " \"",
            "Mont",
            " Blanc",
            "\",",
            " Byron",
            " wrote",
            " \"",
            "The",
            " Prison",
            "er",
            " of",
            " Ch",
            "illon",
            "\"",
            " and",
            " the",
            " dramatic",
            " poem",
            " Man",
            "fred",
            ",",
            " and",
            " Mary",
            " Shelley",
            ",",
            " who",
            " found",
            " the",
            " scenery",
            " overwhelming",
            ",",
            " conceived",
            " the",
            " idea",
            " for",
            " the",
            " novel",
            " Frank",
            "enstein",
            " in",
            " her",
            " villa",
            " on",
            " the",
            " shores",
            " of",
            " Lake",
            " Geneva",
            " amid",
            " a",
            " thunder",
            "storm",
            ".",
            " When"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " After",
            " his",
            " crew",
            " returned",
            " to",
            " Russia",
            " with",
            " sea",
            " ot",
            "ter",
            " pel",
            "ts",
            " judged",
            " to",
            " be",
            " the",
            " finest",
            " fur",
            " in",
            " the",
            " world",
            ",",
            " small",
            " associations",
            " of",
            " fur",
            " traders",
            " began",
            " to",
            " sail",
            " from",
            " the",
            " shores",
            " of",
            " Siber",
            "ia",
            " toward",
            " the",
            " Ale",
            "ut",
            "ian",
            " Islands",
            ".",
            " The",
            " first",
            " permanent",
            " European",
            " settlement",
            " was",
            " founded",
            " in",
            " ",
            "178",
            "4",
            ".",
            "Between",
            " ",
            "177",
            "4",
            " and",
            " ",
            "180",
            "0"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " After",
            " his",
            " crew",
            " returned",
            " to",
            " Russia",
            " with",
            " sea",
            " ot",
            "ter",
            " pel",
            "ts",
            " judged",
            " to",
            " be",
            " the",
            " finest",
            " fur",
            " in",
            " the",
            " world",
            ",",
            " small",
            " associations",
            " of",
            " fur",
            " traders",
            " began",
            " to",
            " sail",
            " from",
            " the",
            " shores",
            " of",
            " Siber",
            "ia",
            " toward",
            " the",
            " Ale",
            "ut",
            "ian",
            " Islands",
            ".",
            " The",
            " first",
            " permanent",
            " European",
            " settlement",
            " was",
            " founded",
            " in",
            " ",
            "178",
            "4",
            ".",
            "Between",
            " ",
            "177",
            "4",
            " and",
            " ",
            "180",
            "0"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " writings",
            ".",
            " During",
            " these",
            " visits",
            " Shelley",
            " wrote",
            " the",
            " poem",
            " \"",
            "Mont",
            " Blanc",
            "\",",
            " Byron",
            " wrote",
            " \"",
            "The",
            " Prison",
            "er",
            " of",
            " Ch",
            "illon",
            "\"",
            " and",
            " the",
            " dramatic",
            " poem",
            " Man",
            "fred",
            ",",
            " and",
            " Mary",
            " Shelley",
            ",",
            " who",
            " found",
            " the",
            " scenery",
            " overwhelming",
            ",",
            " conceived",
            " the",
            " idea",
            " for",
            " the",
            " novel",
            " Frank",
            "enstein",
            " in",
            " her",
            " villa",
            " on",
            " the",
            " shores",
            " of",
            " Lake",
            " Geneva",
            " amid",
            " a",
            " thunder",
            "storm",
            ".",
            " When"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.781,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " noted",
            " on",
            " the",
            " first",
            " maps",
            " that",
            " included",
            " the",
            " South",
            " Atlantic",
            " and",
            " it",
            " was",
            " also",
            " the",
            " subject",
            " of",
            " the",
            " first",
            " computer",
            "-ass",
            "isted",
            " plate",
            " t",
            "ect",
            "onic",
            " recon",
            "structions",
            " in",
            " ",
            "196",
            "5",
            ".",
            " This",
            " magnificent",
            " fit",
            ",",
            " however",
            ",",
            " has",
            " since",
            " then",
            " proven",
            " problematic",
            " and",
            " later",
            " recon",
            "structions",
            " have",
            " introduced",
            " various",
            " deformation",
            " zones",
            " along",
            " the",
            " shore",
            "lines",
            " to",
            " accommodate",
            " the",
            " north",
            "ward",
            "-prop"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.781,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " noted",
            " on",
            " the",
            " first",
            " maps",
            " that",
            " included",
            " the",
            " South",
            " Atlantic",
            " and",
            " it",
            " was",
            " also",
            " the",
            " subject",
            " of",
            " the",
            " first",
            " computer",
            "-ass",
            "isted",
            " plate",
            " t",
            "ect",
            "onic",
            " recon",
            "structions",
            " in",
            " ",
            "196",
            "5",
            ".",
            " This",
            " magnificent",
            " fit",
            ",",
            " however",
            ",",
            " has",
            " since",
            " then",
            " proven",
            " problematic",
            " and",
            " later",
            " recon",
            "structions",
            " have",
            " introduced",
            " various",
            " deformation",
            " zones",
            " along",
            " the",
            " shore",
            "lines",
            " to",
            " accommodate",
            " the",
            " north",
            "ward",
            "-prop"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " close",
            " to",
            " the",
            " shore",
            ".",
            " A",
            "arhus",
            " was",
            " founded",
            " at",
            " the",
            " mouth",
            " of",
            " a",
            " br",
            "ack",
            "ish",
            " water",
            " fj",
            "ord",
            ",",
            " but",
            " the",
            " original",
            " fj",
            "ord",
            " no",
            " longer",
            " exists",
            ",",
            " as",
            " it",
            " has",
            " gradually",
            " narrowed",
            " into",
            " what",
            " is",
            " now",
            " the",
            " A",
            "arhus",
            " River",
            " and",
            " the",
            " Br",
            "ab",
            "rand",
            " Lake",
            ",",
            " due",
            " to",
            " natural",
            " sediment",
            "ation",
            ".",
            " The",
            " land",
            " around",
            " A",
            "arhus",
            " was",
            " once"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            "irc",
            "led",
            " by",
            " the",
            " Gulf",
            " Stream",
            ",",
            " North",
            " Atlantic",
            " Dr",
            "ift",
            ",",
            " and",
            " North",
            " Equ",
            "atorial",
            " Current",
            ".",
            " This",
            " population",
            " of",
            " seaw",
            "eed",
            " probably",
            " originated",
            " from",
            " T",
            "ertiary",
            " ancestors",
            " on",
            " the",
            " European",
            " shores",
            " of",
            " the",
            " former",
            " T",
            "eth",
            "ys",
            " Ocean",
            " and",
            " has",
            ",",
            " if",
            " so",
            ",",
            " maintained",
            " itself",
            " by",
            " veget",
            "ative",
            " growth",
            ",",
            " floating",
            " in",
            " the",
            " ocean",
            " for",
            " millions",
            " of",
            " years",
            ".",
            "Other"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.275,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " early",
            " Viking",
            " Age",
            ",",
            " A",
            "arhus",
            " is",
            " one",
            " of",
            " the",
            " oldest",
            " cities",
            " in",
            " Denmark",
            ",",
            " along",
            " with",
            " R",
            "ibe",
            " and",
            " H",
            "ede",
            "by",
            ".",
            " The",
            " original",
            " A",
            "ros",
            " settlement",
            " was",
            " situated",
            " on",
            " the",
            " northern",
            " shores",
            " of",
            " a",
            " fj",
            "ord",
            " by",
            " the",
            " mouth",
            " of",
            " the",
            " A",
            "arhus",
            " River",
            ",",
            " right",
            " where",
            " the",
            " city",
            " center",
            " is",
            " today",
            ".",
            " It",
            " quickly",
            " became",
            " a",
            " hub",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "air",
            " fleets",
            " that",
            " rav",
            "aged",
            " the",
            " Byz",
            "antine",
            "-controlled",
            " shores",
            " of",
            " the",
            " Ae",
            "ge",
            "an",
            " Sea",
            ".",
            " Cre",
            "te",
            " returned",
            " to",
            " Byz",
            "antine",
            " rule",
            " under",
            " Nike",
            "ph",
            "or",
            "os",
            " Ph",
            "ok",
            "as",
            ",",
            " who",
            " launched",
            " a",
            " huge",
            " campaign",
            " against",
            " the",
            " Em",
            "irate",
            " of",
            " Cre",
            "te",
            " in",
            " ",
            "960",
            " to",
            " ",
            "961",
            ".",
            "Meanwhile",
            ",",
            " the",
            " Bulgarian",
            " Empire",
            " threatened",
            " Byz",
            "antine",
            " control",
            " of",
            " Northern"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "air",
            " fleets",
            " that",
            " rav",
            "aged",
            " the",
            " Byz",
            "antine",
            "-controlled",
            " shores",
            " of",
            " the",
            " Ae",
            "ge",
            "an",
            " Sea",
            ".",
            " Cre",
            "te",
            " returned",
            " to",
            " Byz",
            "antine",
            " rule",
            " under",
            " Nike",
            "ph",
            "or",
            "os",
            " Ph",
            "ok",
            "as",
            ",",
            " who",
            " launched",
            " a",
            " huge",
            " campaign",
            " against",
            " the",
            " Em",
            "irate",
            " of",
            " Cre",
            "te",
            " in",
            " ",
            "960",
            " to",
            " ",
            "961",
            ".",
            "Meanwhile",
            ",",
            " the",
            " Bulgarian",
            " Empire",
            " threatened",
            " Byz",
            "antine",
            " control",
            " of",
            " Northern"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "aph",
            "l",
            "agon",
            "ia",
            ",",
            " and",
            " Pont",
            "us",
            ";",
            " to",
            " the",
            " west",
            " were",
            " M",
            "ys",
            "ia",
            ",",
            " Lydia",
            ",",
            " and",
            " Car",
            "ia",
            ";",
            " and",
            " L",
            "yc",
            "ia",
            ",",
            " Pam",
            "phyl",
            "ia",
            ",",
            " and",
            " C",
            "il",
            "icia",
            " belonged",
            " to",
            " the",
            " southern",
            " shore",
            ".",
            " There",
            " were",
            " also",
            " several",
            " inland",
            " regions",
            ":",
            " Ph",
            "ry",
            "gia",
            ",",
            " C",
            "app",
            "ad",
            "oc",
            "ia",
            ",",
            " Pis",
            "idia",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "aph",
            "l",
            "agon",
            "ia",
            ",",
            " and",
            " Pont",
            "us",
            ";",
            " to",
            " the",
            " west",
            " were",
            " M",
            "ys",
            "ia",
            ",",
            " Lydia",
            ",",
            " and",
            " Car",
            "ia",
            ";",
            " and",
            " L",
            "yc",
            "ia",
            ",",
            " Pam",
            "phyl",
            "ia",
            ",",
            " and",
            " C",
            "il",
            "icia",
            " belonged",
            " to",
            " the",
            " southern",
            " shore",
            ".",
            " There",
            " were",
            " also",
            " several",
            " inland",
            " regions",
            ":",
            " Ph",
            "ry",
            "gia",
            ",",
            " C",
            "app",
            "ad",
            "oc",
            "ia",
            ",",
            " Pis",
            "idia",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.672,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "e",
            " tribe",
            " on",
            " the",
            " opposite",
            " shore",
            ".",
            " Crossing",
            " the",
            " river",
            " at",
            " night",
            ",",
            " he",
            " surprised",
            " them",
            " and",
            " forced",
            " their",
            " army",
            " to",
            " retreat",
            " after",
            " the",
            " first",
            " cavalry",
            " skirm",
            "ish",
            ".",
            "News",
            " then",
            " reached",
            " Alexander",
            " that",
            " the",
            " Il",
            "ly",
            "rian",
            " ch",
            "ie",
            "ft",
            "ain",
            " Cle",
            "itus",
            " and",
            " King",
            " Gl",
            "au",
            "k",
            "ias",
            " of",
            " the",
            " T",
            "aul",
            "ant",
            "ii",
            " were",
            " in",
            " open",
            " revolt",
            " against",
            " his",
            " authority"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.672,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "mos",
            "son",
            " in",
            " Val",
            "ais",
            ",",
            " Switzerland",
            ",",
            " dinosaur",
            " tracks",
            " were",
            " found",
            " in",
            " the",
            " ",
            "197",
            "0",
            "s",
            ",",
            " dating",
            " probably",
            " from",
            " the",
            " Tri",
            "assic",
            " Period",
            ".",
            "History",
            "Pre",
            "history",
            " ",
            "When",
            " the",
            " ice",
            " melted",
            " after",
            " the",
            " W",
            "Ã¼r",
            "m",
            " glac",
            "iation",
            ",",
            " pale",
            "olithic",
            " settlements",
            " were",
            " established",
            " along",
            " the",
            " lake",
            " shores",
            " and",
            " in",
            " cave",
            " systems",
            ".",
            " Evidence",
            " of",
            " human",
            " hab",
            "itation"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " importance",
            " are",
            " the",
            " Mediterranean",
            " monk",
            " seal",
            ",",
            " logger",
            "head",
            " sea",
            " turtle",
            " and",
            " green",
            " sea",
            " turtle",
            " that",
            " use",
            " to",
            " nest",
            " on",
            " the",
            " country",
            "'s",
            " coastal",
            " waters",
            " and",
            " shores",
            ".",
            "In",
            " terms",
            " of",
            " ph",
            "yt",
            "oge",
            "ography",
            ",",
            " Albania",
            " is",
            " part",
            " of",
            " the",
            " B",
            "oreal",
            " Kingdom",
            " and",
            " stretches",
            " specifically",
            " within",
            " the",
            " Il",
            "ly",
            "rian",
            " province",
            " of",
            " the",
            " Circ",
            "umb",
            "oreal",
            " and",
            " Mediterranean",
            " Region",
            ".",
            " Its"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " J",
            "ut",
            "land",
            ".",
            " ",
            " To",
            " the",
            " west",
            ",",
            " the",
            " Lim",
            "f",
            "j",
            "ord",
            " broad",
            "ens",
            " into",
            " an",
            " irregular",
            " lake",
            " (",
            "salt",
            " water",
            "),",
            " with",
            " low",
            ",",
            " marsh",
            "y",
            " shores",
            " and",
            " many",
            " islands",
            ".",
            " ",
            " Northwest",
            " is",
            " Store",
            " V",
            "ild",
            "m",
            "ose",
            " (\"",
            "Greater",
            " Wild",
            " bog",
            "\"),",
            " a",
            " swamp",
            " where",
            " a",
            " mir",
            "age",
            " is",
            " sometimes",
            " seen",
            " in",
            " summer",
            ".",
            " Southeast",
            " lies",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " municipality",
            " are",
            " called",
            " L",
            "anger",
            "ak",
            " to",
            " the",
            " east",
            " and",
            " G",
            "j",
            "Ã¸",
            "l",
            " B",
            "red",
            "ning",
            " to",
            " the",
            " west",
            ".",
            " ",
            " The",
            " island",
            " of",
            " E",
            "gh",
            "ol",
            "m",
            " is",
            " located",
            " in",
            " G",
            "j",
            "Ã¸",
            "l",
            " B",
            "red",
            "ning",
            ",",
            " and",
            " is",
            " connected",
            " by",
            " ferry",
            " to",
            " the",
            " city",
            " of",
            " A",
            "alborg",
            " at",
            " its",
            " southern",
            " shore",
            ".",
            "The",
            " area",
            " is",
            " typical",
            " for",
            " the",
            " north"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " shore",
            " under",
            " his",
            " adm",
            "iral",
            " N",
            "earch",
            "us",
            ",",
            " while",
            " he",
            " led",
            " the",
            " rest",
            " back",
            " to",
            " Pers",
            "ia",
            " through",
            " the",
            " more",
            " difficult",
            " southern",
            " route",
            " along",
            " the",
            " Ged",
            "ros",
            "ian",
            " Desert",
            " and",
            " Mak",
            "ran",
            ".",
            " Alexander",
            " reached",
            " Sus",
            "a",
            " in",
            " ",
            "324",
            " BC",
            ",",
            " but",
            " not",
            " before",
            " losing",
            " many",
            " men",
            " to",
            " the",
            " harsh",
            " desert",
            ".",
            "Last",
            " years",
            " in",
            " Pers",
            "ia",
            "Discover",
            "ing",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "1",
            " January",
            " ",
            "201",
            "1",
            ")",
            " is",
            " the",
            " second",
            "-largest",
            " city",
            " in",
            " Denmark",
            " and",
            " the",
            " seat",
            " of",
            " A",
            "arhus",
            " Municip",
            "ality",
            ".",
            " It",
            " is",
            " located",
            " on",
            " the",
            " eastern",
            " shore",
            " of",
            " J",
            "ut",
            "land",
            " in",
            " the",
            " Kat",
            "teg",
            "at",
            " sea",
            " and",
            " approximately",
            " ",
            " northwest",
            " of",
            " Copenhagen",
            ".",
            "Dating",
            " back",
            " to",
            " the",
            " late",
            " ",
            "8",
            "th",
            " century",
            ",",
            " A",
            "arhus",
            " was",
            " founded",
            " as",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " Red",
            " Deer",
            ",",
            " and",
            " Edmonton",
            ".",
            "North",
            " of",
            " Edmonton",
            ",",
            " the",
            " highway",
            " continues",
            " to",
            " Ath",
            "ab",
            "asca",
            ",",
            " then",
            " north",
            "w",
            "ester",
            "ly",
            " along",
            " the",
            " south",
            " shore",
            " of",
            " Lesser",
            " Slave",
            " Lake",
            " into",
            " High",
            " Prairie",
            ",",
            " north",
            " to",
            " Peace",
            " River",
            ",",
            " west",
            " to",
            " Fair",
            "view",
            " and",
            " finally",
            " south",
            " to",
            " Grande",
            " Prairie",
            ",",
            " where",
            " it",
            " ends",
            " at",
            " an",
            " interchange",
            " with",
            " Highway",
            "Âł",
            "43",
            ".",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.551,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " subject",
            " of",
            " oper",
            "as",
            ",",
            " ballet",
            "s",
            " and",
            " related",
            " genres",
            ".",
            " Oper",
            "as",
            " titled",
            " De",
            "id",
            "am",
            "ia",
            " were",
            " composed",
            " by",
            " Francesco",
            " Cav",
            "alli",
            " (",
            "164",
            "4",
            ")",
            " and",
            " George",
            " Fr",
            "ider",
            "ic",
            " Hand",
            "el",
            " (",
            "173",
            "9",
            ").",
            " Ach",
            "ille",
            " et",
            " Poly",
            "x",
            "Ã¨ne",
            " (",
            "Paris",
            " ",
            "168",
            "7",
            ")",
            " is",
            " an",
            " opera",
            " begun",
            " by",
            " Jean",
            "-B",
            "apt",
            "iste",
            " L",
            "ully"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " subject",
            " and",
            " object",
            ".",
            "A",
            " second",
            " category",
            ",",
            " which",
            " partially",
            " overlaps",
            " with",
            " case",
            ",",
            " is",
            " the",
            " AA",
            " linguistic",
            " category",
            " of",
            " \"",
            "state",
            ".\"",
            " Lingu",
            "ists",
            " use",
            " the",
            " term",
            " \"",
            "state",
            "\"",
            " to",
            " refer",
            " to",
            " different",
            " things",
            " in",
            " different",
            " languages",
            ".",
            " In",
            " Cush",
            "itic",
            " and",
            " Sem",
            "itic",
            ",",
            " nouns",
            " exist",
            " in",
            " the",
            " \"",
            "free",
            " state",
            "\"",
            " or",
            " the",
            " \"",
            "construct",
            " state",
            "\".",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " subject",
            " due",
            " to",
            " their",
            " sovereign",
            ".",
            " As",
            " the",
            " subject",
            " owed",
            " to",
            " their",
            " sovereign",
            " their",
            " true",
            " and",
            " faithful",
            " allegiance",
            " and",
            " obedience",
            ",",
            " so",
            " the",
            " sovereign",
            " ",
            " (",
            "Cal",
            "vin",
            "'s",
            " Case",
            " (",
            "160",
            "8",
            ")",
            " ",
            "7",
            " Co",
            " Rep",
            " ",
            "1",
            "a",
            ";",
            " Jen",
            "k",
            " ",
            "306",
            ";",
            " ",
            "2",
            " State",
            " Tr",
            " ",
            "559",
            ";",
            " ",
            "77",
            " ER",
            " ",
            "377",
            ")."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " Stage",
            " Mountain",
            ",",
            " Cas",
            "p",
            "ian",
            " Shore",
            " Defensive",
            " Con",
            "structions",
            ",",
            " Ord",
            "ub",
            "ad",
            " National",
            " Reserve",
            " and",
            " the",
            " Palace",
            " of",
            " Sh",
            "aki",
            " Kh",
            "ans",
            ".",
            "Among",
            " other",
            " architectural",
            " treasures",
            " are",
            " Quadr",
            "angular",
            " Castle",
            " in",
            " M",
            "ard",
            "akan",
            ",",
            " Par",
            "ig",
            "ala",
            " in",
            " Yuk",
            "h",
            "ary",
            " Ch",
            "ard",
            "ag",
            "lar",
            ",",
            " a",
            " number",
            " of",
            " bridges",
            " spanning",
            " the",
            " Ar",
            "as",
            " River",
            ",",
            " and",
            " several",
            " ma"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " subject",
            " due",
            " to",
            " their",
            " sovereign",
            ".",
            " As",
            " the",
            " subject",
            " owed",
            " to",
            " their",
            " sovereign",
            " their",
            " true",
            " and",
            " faithful",
            " allegiance",
            " and",
            " obedience",
            ",",
            " so",
            " the",
            " sovereign",
            " ",
            " (",
            "Cal",
            "vin",
            "'s",
            " Case",
            " (",
            "160",
            "8",
            ")",
            " ",
            "7",
            " Co",
            " Rep",
            " ",
            "1",
            "a",
            ";",
            " Jen",
            "k",
            " ",
            "306",
            ";",
            " ",
            "2",
            " State",
            " Tr",
            " ",
            "559",
            ";",
            " ",
            "77",
            " ER",
            " ",
            "377",
            ")."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " physical",
            " sciences",
            ",",
            " who",
            " examine",
            " the",
            " subject",
            " in",
            " terms",
            " of",
            " early",
            " chemistry",
            ",",
            " medicine",
            ",",
            " and",
            " char",
            "lat",
            "anism",
            ",",
            " and",
            " the",
            " philosophical",
            " and",
            " religious",
            " contexts",
            " in",
            " which",
            " these",
            " events",
            " occurred",
            ".",
            " The",
            " latter",
            " interests",
            " historians",
            " of",
            " es",
            "oteric",
            "ism",
            ",",
            " psychologists",
            ",",
            " and",
            " some",
            " philosophers",
            " and",
            " spiritual",
            "ists",
            ".",
            " The",
            " subject",
            " has",
            " also",
            " made",
            " an",
            " ongoing",
            " impact",
            " on",
            " literature",
            " and",
            " the",
            " arts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " physical",
            " sciences",
            ",",
            " who",
            " examine",
            " the",
            " subject",
            " in",
            " terms",
            " of",
            " early",
            " chemistry",
            ",",
            " medicine",
            ",",
            " and",
            " char",
            "lat",
            "anism",
            ",",
            " and",
            " the",
            " philosophical",
            " and",
            " religious",
            " contexts",
            " in",
            " which",
            " these",
            " events",
            " occurred",
            ".",
            " The",
            " latter",
            " interests",
            " historians",
            " of",
            " es",
            "oteric",
            "ism",
            ",",
            " psychologists",
            ",",
            " and",
            " some",
            " philosophers",
            " and",
            " spiritual",
            "ists",
            ".",
            " The",
            " subject",
            " has",
            " also",
            " made",
            " an",
            " ongoing",
            " impact",
            " on",
            " literature",
            " and",
            " the",
            " arts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " Napoleon",
            " I",
            " on",
            " his",
            " Imperial",
            " Throne",
            " is",
            " partly",
            " borrowed",
            " from",
            " the",
            " Statue",
            " of",
            " Zeus",
            " at",
            " Olympia",
            ".",
            " As",
            " evidenced",
            " by",
            " the",
            " title",
            ",",
            " the",
            " subject",
            " is",
            " Napoleon",
            ",",
            " and",
            " the",
            " content",
            " is",
            " In",
            "gres",
            "'s",
            " representation",
            " of",
            " Napoleon",
            " as",
            " \"",
            "Em",
            "peror",
            "-G",
            "od",
            " beyond",
            " time",
            " and",
            " space",
            "\".",
            " Similarly",
            " to",
            " extreme",
            " formal",
            "ism",
            ",",
            " philosophers",
            " typically",
            " reject",
            " extreme",
            " intentional",
            "ism",
            ",",
            " because"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " play",
            " called",
            " RÃ©",
            "vol",
            "te",
            " dans",
            " les",
            " Ast",
            "uries",
            " (",
            "Rev",
            "olt",
            " in",
            " the",
            " Ast",
            "ur",
            "ias",
            ")",
            " written",
            " with",
            " three",
            " friends",
            " in",
            " May",
            " ",
            "193",
            "6",
            ".",
            " The",
            " subject",
            " was",
            " the",
            " ",
            "193",
            "4",
            " revolt",
            " by",
            " Spanish",
            " miners",
            " that",
            " was",
            " brutally",
            " suppressed",
            " by",
            " the",
            " Spanish",
            " government",
            " resulting",
            " in",
            " ",
            "1",
            ",",
            "500",
            " to",
            " ",
            "2",
            ",",
            "000",
            " deaths",
            ".",
            " In",
            " May",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " contribution",
            " being",
            " the",
            " introduction",
            " of",
            " the",
            " subject",
            "ivist",
            " approach",
            " in",
            " economics",
            ".",
            "Despite",
            " such",
            " claim",
            ",",
            " John",
            " Stuart",
            " Mill",
            " had",
            " used",
            " value",
            " in",
            " use",
            " in",
            " this",
            " sense",
            " in",
            " ",
            "184",
            "8",
            " in",
            " Principles",
            " of",
            " Political",
            " Economy",
            ",",
            " where",
            " he",
            " wrote",
            ":",
            " \"",
            "Value",
            " in",
            " use",
            ",",
            " or",
            " as",
            " Mr",
            ".",
            " De",
            " Qu",
            "ince",
            "y",
            " calls",
            " it",
            ",",
            " tele",
            "ologic",
            " value",
            ",",
            " is",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " use",
            " of",
            " the",
            " term",
            " for",
            " some",
            " of",
            " the",
            " subject",
            " matter",
            " occurred",
            " subsequently",
            ",",
            " such",
            " as",
            " the",
            " use",
            " by",
            " Ãī",
            "t",
            "ienne",
            " Ser",
            "res",
            " in",
            " ",
            "183",
            "9",
            " to",
            " describe",
            " the",
            " natural",
            " history",
            ",",
            " or",
            " pale",
            "ontology",
            ",",
            " of",
            " man",
            ",",
            " based",
            " on",
            " comparative",
            " anatomy",
            ",",
            " and",
            " the",
            " creation",
            " of",
            " a",
            " chair",
            " in",
            " anthropology",
            " and",
            " ethn",
            "ography",
            " in",
            " ",
            "185",
            "0",
            " at",
            " the",
            " French"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            0.52,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "'s",
            " instruction",
            " of",
            " the",
            " M",
            "uses",
            " formed",
            " the",
            " subject",
            " of",
            " Igor",
            " Str",
            "av",
            "insky",
            "'s",
            " Ap",
            "oll",
            "on",
            " mus",
            "ag",
            "Ã¨te",
            " (",
            "192",
            "7",
            "–",
            "192",
            "8",
            ").",
            " In",
            " ",
            "197",
            "8",
            ",",
            " the",
            " Canadian",
            " band",
            " Rush",
            " released",
            " an",
            " album",
            " with",
            " songs",
            " \"",
            "Apollo",
            ":",
            " Br",
            "inger",
            " of",
            " Wisdom",
            "\"/",
            "\"",
            "D",
            "ion",
            "ys",
            "us",
            ":",
            " Br",
            "inger",
            " of",
            " Love",
            "\".",
            "Books",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "629",
            ";",
            " Opp",
            "en",
            "heimer",
            " v",
            " C",
            "atter",
            "m",
            "ole",
            " [",
            "197",
            "2",
            "]",
            " ",
            "3",
            " All",
            " ER",
            " ",
            "110",
            "6",
            ").",
            " The",
            " duty",
            " of",
            " the",
            " crown",
            " towards",
            " its",
            " subjects",
            " was",
            " to",
            " govern",
            " and",
            " protect",
            " them",
            ".",
            " The",
            " reciprocal",
            " duty",
            " of",
            " the",
            " subject",
            " towards",
            " the",
            " crown",
            " was",
            " that",
            " of",
            " allegiance",
            ".",
            "At",
            " common",
            " law",
            ",",
            " allegiance",
            " was",
            " a",
            " true",
            " and",
            " faithful",
            " obedience"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " Although",
            " the",
            " historical",
            " Fl",
            "amel",
            " existed",
            ",",
            " the",
            " writings",
            " and",
            " legends",
            " assigned",
            " to",
            " him",
            " only",
            " appeared",
            " in",
            " ",
            "161",
            "2",
            ".",
            " Fl",
            "amel",
            " was",
            " not",
            " a",
            " religious",
            " scholar",
            " as",
            " were",
            " many",
            " of",
            " his",
            " predecessors",
            ",",
            " and",
            " his",
            " entire",
            " interest",
            " in",
            " the",
            " subject",
            " rev",
            "olved",
            " around",
            " the",
            " pursuit",
            " of",
            " the",
            " philosopher",
            "'s",
            " stone",
            ".",
            " His",
            " work",
            " spends",
            " a",
            " great",
            " deal",
            " of",
            " time",
            " describing",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "14",
            ").",
            " The",
            " French",
            " dram",
            "at",
            "ist",
            " Thomas",
            " Cor",
            "ne",
            "ille",
            " wrote",
            " a",
            " tragedy",
            " La",
            " Mort",
            " d",
            "'A",
            "ch",
            "ille",
            " (",
            "167",
            "3",
            ").",
            " Achilles",
            " is",
            " the",
            " subject",
            " of",
            " the",
            " poem",
            " Ach",
            "ille",
            "is",
            " (",
            "179",
            "9",
            "),",
            " a",
            " fragment",
            " by",
            " Johann",
            " Wolfgang",
            " von",
            " Go",
            "ethe",
            ".",
            " In",
            " ",
            "189",
            "9",
            ",",
            " the",
            " Polish",
            " playwright",
            ",",
            " painter",
            " and",
            " poet",
            " Stan",
            "is",
            "ÅĤaw",
            " W"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " use",
            " of",
            " the",
            " term",
            " for",
            " some",
            " of",
            " the",
            " subject",
            " matter",
            " occurred",
            " subsequently",
            ",",
            " such",
            " as",
            " the",
            " use",
            " by",
            " Ãī",
            "t",
            "ienne",
            " Ser",
            "res",
            " in",
            " ",
            "183",
            "9",
            " to",
            " describe",
            " the",
            " natural",
            " history",
            ",",
            " or",
            " pale",
            "ontology",
            ",",
            " of",
            " man",
            ",",
            " based",
            " on",
            " comparative",
            " anatomy",
            ",",
            " and",
            " the",
            " creation",
            " of",
            " a",
            " chair",
            " in",
            " anthropology",
            " and",
            " ethn",
            "ography",
            " in",
            " ",
            "185",
            "0",
            " at",
            " the",
            " French"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "-",
            "211",
            " is",
            " the",
            " subject",
            " of",
            " ongoing",
            " research",
            " in",
            " nuclear",
            " medicine",
            ".",
            " It",
            " must",
            " be",
            " used",
            " quickly",
            " as",
            " it",
            " dec",
            "ays",
            " with",
            " a",
            " half",
            "-life",
            " of",
            " ",
            "7",
            ".",
            "2",
            "Âł",
            "hours",
            ";",
            " this",
            " is",
            " long",
            " enough",
            " to",
            " permit",
            " mult",
            "ist",
            "ep",
            " labeling",
            " strategies",
            ".",
            " A",
            "stat",
            "ine",
            "-",
            "211",
            " has",
            " potential",
            " for",
            " targeted",
            " alpha",
            "-p",
            "article",
            " therapy",
            ",",
            " since",
            " it",
            " dec",
            "ays"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " understand",
            " the",
            " subject",
            " about",
            " which",
            " they",
            " are",
            " lying",
            ".",
            " S",
            "ocrates",
            " uses",
            " various",
            " analog",
            "ies",
            ",",
            " discussing",
            " athletics",
            " and",
            " the",
            " sciences",
            " to",
            " prove",
            " his",
            " point",
            ".",
            " The",
            " two",
            " also",
            " reference",
            " Homer",
            " extensively",
            ".",
            " S",
            "ocrates",
            " and",
            " H",
            "ippi",
            "as",
            " agree",
            " that",
            " Od",
            "ys",
            "se",
            "us",
            ",",
            " who",
            " conco",
            "ct",
            "ed",
            " a",
            " number",
            " of",
            " lies",
            " throughout",
            " the",
            " Odyssey",
            " and",
            " other",
            " stories",
            " in",
            " the",
            " Trojan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.508,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " has",
            " been",
            " the",
            " subject",
            " of",
            " or",
            " inspiration",
            " for",
            " many",
            " novels",
            ",",
            " films",
            ",",
            " plays",
            ",",
            " and",
            " works",
            " of",
            " music",
            ".",
            " He",
            " is",
            " a",
            " favorite",
            " model",
            " for",
            " dep",
            "ictions",
            " of",
            " absent",
            "-minded",
            " professors",
            ";",
            " his",
            " expressive",
            " face",
            " and",
            " distinctive",
            " hairstyle",
            " have",
            " been",
            " widely",
            " copied",
            " and",
            " exaggerated",
            ".",
            " Time",
            " magazine",
            "'s",
            " Freder",
            "ic",
            " Golden",
            " wrote",
            " that",
            " Einstein",
            " was",
            " \"",
            "a",
            " cartoon",
            "ist",
            "'s",
            " dream",
            " come"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " ideas",
            ".",
            "Te",
            "levision",
            " shows",
            ",",
            " movies",
            ",",
            " songs",
            ",",
            " and",
            " video",
            " games",
            " have",
            " referred",
            " to",
            " Rand",
            " and",
            " her",
            " works",
            ".",
            " Throughout",
            " her",
            " life",
            " she",
            " was",
            " the",
            " subject",
            " of",
            " many",
            " articles",
            " in",
            " popular",
            " magazines",
            ",",
            " as",
            " well",
            " as",
            " book",
            "-length",
            " critiques",
            " by",
            " authors",
            " such",
            " as",
            " the",
            " psychologist",
            " Albert",
            " Ellis",
            " and",
            " Trinity",
            " Foundation",
            " president",
            " John",
            " W",
            ".",
            " Robbins",
            ".",
            " Rand",
            " or",
            " characters",
            " based",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " intellectual",
            " results",
            " of",
            " the",
            " comparative",
            " methods",
            " developed",
            " in",
            " the",
            " earlier",
            " ",
            "19",
            "th",
            " century",
            ".",
            " The",
            "or",
            "ists",
            " in",
            " such",
            " diverse",
            " fields",
            " as",
            " anatomy",
            ",",
            " lingu",
            "istics",
            ",",
            " and",
            " ethn",
            "ology",
            ",",
            " making",
            " feature",
            "-by",
            "-feature",
            " comparisons",
            " of",
            " their",
            " subject",
            " matters",
            ",",
            " were",
            " beginning",
            " to",
            " suspect",
            " that",
            " similarities",
            " between",
            " animals",
            ",",
            " languages",
            ",",
            " and",
            " folk",
            "ways",
            " were",
            " the",
            " result",
            " of",
            " processes",
            " or",
            " laws"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " capacities",
            ".",
            " These",
            " personnel",
            " are",
            " distributed",
            " across",
            " the",
            " line",
            " infantry",
            " regiment",
            ",",
            " the",
            " service",
            " and",
            " support",
            " unit",
            ",",
            " the",
            " air",
            " force",
            ",",
            " and",
            " the",
            " coast",
            " guard",
            ".",
            " In",
            " addition",
            " there",
            " is",
            " the",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " Cad",
            "et",
            " Corps",
            ",",
            " which",
            " is",
            " made",
            " up",
            " of",
            " two",
            " hundred",
            " young",
            " people",
            " between",
            " the",
            " ages",
            " of",
            " ",
            "12",
            " and",
            " ",
            "18",
            ".",
            " The",
            " Defence"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " displaced",
            " have",
            " now",
            " squ",
            "atted",
            " around",
            " the",
            " capital",
            ",",
            " in",
            " mus",
            "sequ",
            "es",
            " (",
            "sh",
            "ant",
            "y",
            " towns",
            ")",
            " the",
            " general",
            " situation",
            " for",
            " Ang",
            "ol",
            "ans",
            " remains",
            " desperate",
            ".",
            "A",
            " drought",
            " in",
            " ",
            "201",
            "6",
            " caused",
            " the",
            " worst",
            " food",
            " crisis",
            " in",
            " Southern",
            " Africa",
            " in",
            " ",
            "25",
            " years",
            ",",
            " affecting",
            " ",
            "1",
            ".",
            "4",
            " million",
            " people",
            " across",
            " seven",
            " of",
            " Angola",
            "'s",
            " eighteen",
            " provinces",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            "ard",
            "v",
            "ark",
            " to",
            " back",
            " out",
            " when",
            " the",
            " coast",
            " is",
            " clear",
            ".",
            "Movement",
            "The",
            " a",
            "ard",
            "v",
            "ark",
            " is",
            " known",
            " to",
            " be",
            " a",
            " good",
            " swim",
            "mer",
            " and",
            " has",
            " been",
            " witnessed",
            " successfully",
            " swimming",
            " in",
            " strong",
            " currents",
            ".",
            " It",
            " can",
            " dig",
            " a",
            " yard",
            " of",
            " tunnel",
            " in",
            " about",
            " five",
            " minutes",
            ",",
            " but",
            " otherwise",
            " moves",
            " fairly",
            " slowly",
            ".",
            "When",
            " leaving",
            " the",
            " bur",
            "row",
            " at",
            " night"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.354,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " It",
            " was",
            " a",
            " notable",
            " feature",
            " of",
            " the",
            " international",
            " synd",
            "ical",
            "ist",
            " movement",
            ".",
            " In",
            " China",
            ",",
            " small",
            " groups",
            " of",
            " students",
            " imported",
            " the",
            " human",
            "istic",
            " pro",
            "-sc",
            "ience",
            " version",
            " of",
            " an",
            "ar",
            "cho",
            "-comm",
            "un",
            "ism",
            ".",
            " Tokyo",
            " was",
            " a",
            " hotspot",
            " for",
            " rebell",
            "ious",
            " youth",
            " from",
            " East",
            " Asian",
            " countries",
            ",",
            " who",
            " moved",
            " to",
            " the",
            " Japanese",
            " capital",
            " to",
            " study",
            ".",
            " In",
            " Latin",
            " America",
            ",",
            " Argentina"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.334,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " show",
            " that",
            " he",
            " could",
            " have",
            " been",
            " successful",
            " without",
            " the",
            " aid",
            " of",
            " his",
            " family",
            "'s",
            " wealth",
            ".",
            " Later",
            ",",
            " Francisco",
            " bankrupt",
            "s",
            " the",
            " d",
            "'",
            "An",
            "con",
            "ia",
            " business",
            " to",
            " put",
            " it",
            " out",
            " of",
            " others",
            "'",
            " reach",
            ".",
            " His",
            " full",
            " name",
            " is",
            " given",
            " as",
            " \"",
            "Franc",
            "isco",
            " Doming",
            "o",
            " Carlos",
            " Andres",
            " Sebast",
            "i",
            "Ã¡n",
            " d",
            "'",
            "An",
            "con",
            "ia",
            "\".",
            "John",
            " G",
            "alt"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Communist",
            " Party",
            ",",
            " Gins",
            "berg",
            " objected",
            ":",
            " \"",
            "I",
            " am",
            " not",
            ",",
            " as",
            " a",
            " matter",
            " of",
            " fact",
            ",",
            " a",
            " member",
            " of",
            " the",
            " Communist",
            " party",
            ",",
            " nor",
            " am",
            " I",
            " dedicated",
            " to",
            " the",
            " overthrow",
            " of",
            " the",
            " U",
            ".S",
            ".",
            " government",
            " or",
            " any",
            " government",
            " by",
            " violence",
            "Âł",
            "...",
            " I",
            " must",
            " say",
            " that",
            " I",
            " see",
            " little",
            " difference",
            " between",
            " the",
            " armed",
            " and",
            " violent",
            " governments",
            " both",
            " Communist"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ube",
            ",",
            " while",
            " making",
            " plans",
            " to",
            " strike",
            " at",
            " Constantin",
            "ople",
            " once",
            " more",
            " to",
            " reclaim",
            " tribute",
            ".",
            "However",
            ",",
            " he",
            " died",
            " in",
            " the",
            " early",
            " months",
            " of",
            " ",
            "453",
            ".",
            "The",
            " conventional",
            " account",
            " from",
            " Pr",
            "isc",
            "us",
            " says",
            " that",
            " At",
            "til",
            "a",
            " was",
            " at",
            " a",
            " feast",
            " celebrating",
            " his",
            " latest",
            " marriage",
            ",",
            " this",
            " time",
            " to",
            " the",
            " beautiful",
            " young",
            " I",
            "ld",
            "ico",
            " (",
            "the",
            " name",
            " suggests",
            " Gothic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            "askan",
            " communities",
            " to",
            " legally",
            " incorporate",
            " as",
            " cities",
            " did",
            " not",
            " come",
            " about",
            " until",
            " ",
            "190",
            "0",
            ",",
            " and",
            " home",
            " rule",
            " for",
            " cities",
            " was",
            " extremely",
            " limited",
            " or",
            " unavailable",
            " until",
            " state",
            "hood",
            " took",
            " effect",
            " in",
            " ",
            "195",
            "9",
            ".",
            "Al",
            "aska",
            " as",
            " an",
            " incorporated",
            " U",
            ".S",
            ".",
            " territory",
            "Starting",
            " in",
            " the",
            " ",
            "189",
            "0",
            "s",
            " and",
            " stretching",
            " in",
            " some",
            " places",
            " to",
            " the",
            " early",
            " ",
            "191"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " speculate",
            " that",
            " asteroid",
            " impacts",
            " may",
            " have",
            " seeded",
            " the",
            " early",
            " Earth",
            " with",
            " the",
            " chemicals",
            " necessary",
            " to",
            " initiate",
            " life",
            ",",
            " or",
            " may",
            " have",
            " even",
            " brought",
            " life",
            " itself",
            " to",
            " Earth",
            " (",
            "an",
            " event",
            " called",
            " \"",
            "p",
            "ans",
            "perm",
            "ia",
            "\").",
            " In",
            " August",
            "Âł",
            "201",
            "1",
            ",",
            " a",
            " report",
            ",",
            " based",
            " on",
            " NASA",
            " studies",
            " with",
            " meteor",
            "ites",
            " found",
            " on",
            " Earth",
            ",",
            " was",
            " published",
            " suggesting",
            " DNA",
            " and",
            " RNA"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            ")",
            " ",
            " ",
            "192",
            "9",
            "  ",
            " –",
            " Christ",
            "os",
            " S",
            "artz",
            "et",
            "akis",
            ",",
            " Greek",
            " jur",
            "ist",
            ",",
            " supreme",
            " justice",
            " and",
            " President",
            " of",
            " Greece",
            " (",
            "d",
            ".",
            " ",
            "202",
            "2",
            ")",
            " ",
            "193",
            "1",
            " –",
            " Ram",
            " D",
            "ass",
            ",",
            " American",
            " author",
            " and",
            " educator",
            " (",
            "d",
            ".",
            " ",
            "201",
            "9",
            ")",
            " ",
            " ",
            "193",
            "1",
            "  ",
            " –",
            " Ivan",
            " Dixon",
            ",",
            " American",
            " actor",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Islamic",
            " art",
            ",",
            " where",
            " dep",
            "ictions",
            " of",
            " Muhammad",
            " remain",
            " especially",
            " controversial",
            ".",
            " Much",
            " art",
            " has",
            " been",
            " disliked",
            " purely",
            " because",
            " it",
            " depicted",
            " or",
            " otherwise",
            " stood",
            " for",
            " unpopular",
            " rulers",
            ",",
            " parties",
            " or",
            " other",
            " groups",
            ".",
            " Art",
            "istic",
            " conventions",
            " have",
            " often",
            " been",
            " conservative",
            " and",
            " taken",
            " very",
            " seriously",
            " by",
            " art",
            " critics",
            ",",
            " though",
            " often",
            " much",
            " less",
            " so",
            " by",
            " a",
            " wider",
            " public",
            ".",
            " The",
            " icon",
            "ographic",
            " content",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            "000",
            " paying",
            " guests",
            " in",
            " ",
            "201",
            "5",
            ".",
            " Spot",
            " festival",
            " is",
            " aiming",
            " to",
            " showcase",
            " up",
            "-and",
            "-",
            "coming",
            " Danish",
            " and",
            " Scandinavian",
            " talents",
            " at",
            " selected",
            " venues",
            " of",
            " the",
            " inner",
            " city",
            ".",
            " The",
            " outdoor",
            " Gr",
            "Ã¸",
            "n",
            " Kon",
            "cert",
            " music",
            " festival",
            " takes",
            " place",
            " every",
            " year",
            " in",
            " many",
            " cities",
            " across",
            " Denmark",
            ",",
            " including",
            " A",
            "arhus",
            ".",
            " Dan",
            "marks",
            " gr",
            "imm",
            "este",
            " festival",
            " (",
            "lit",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "-",
            "ordered",
            " only",
            " if",
            " the",
            " appeal",
            " reaches",
            " the",
            " Supreme",
            " Court",
            ".",
            "Direct",
            " or",
            " collateral",
            ":",
            " Appe",
            "aling",
            " criminal",
            " convictions",
            "Many",
            " jurisdictions",
            " recognize",
            " two",
            " types",
            " of",
            " appeals",
            ",",
            " particularly",
            " in",
            " the",
            " criminal",
            " context",
            ".",
            " The",
            " first",
            " is",
            " the",
            " traditional",
            " \"",
            "direct",
            "\"",
            " appeal",
            " in",
            " which",
            " the",
            " appellant",
            " files",
            " an",
            " appeal",
            " with",
            " the",
            " next",
            " higher",
            " court",
            " of",
            " review",
            ".",
            " ",
            " The",
            " second",
            " is",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "alborg",
    " Bylo",
    "alace",
    " gratuita",
    "Ã¡ch"
  ],
  "bottom_logits": [
    " di",
    "oper",
    "èĭ±æĸĩ",
    " dialect",
    " unb"
  ],
  "act_min": -0.0,
  "act_max": 0.844
}