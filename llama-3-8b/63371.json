{
  "index": 63371,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " agree",
            " that",
            " Vert",
            "igo",
            " is",
            " the",
            " director",
            "'s",
            " most",
            " personal",
            " and",
            " revealing",
            " film",
            ",",
            " dealing",
            " with",
            " the",
            " Py",
            "g",
            "mal",
            "ion",
            "-like",
            " obs",
            "essions",
            " of",
            " a",
            " man",
            " who",
            " mould",
            "s",
            " a",
            " woman",
            " into",
            " the",
            " person",
            " he",
            " desires",
            ".",
            " Vert",
            "igo",
            " explores",
            " more",
            " frankly",
            " and",
            " at",
            " greater",
            " length",
            " his",
            " interest",
            " in",
            " the",
            " relation",
            " between",
            " sex",
            " and",
            " death",
            ",",
            " than",
            " any",
            " other",
            " work",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " stiff",
            " military",
            " m",
            "oust",
            "ache",
            ".",
            " His",
            " apparel",
            " was",
            " neat",
            " to",
            " perfection",
            ",",
            " a",
            " little",
            " quaint",
            " and",
            " frankly",
            " d",
            "and",
            "ified",
            ".\"",
            " He",
            " was",
            " accompanied",
            " by",
            " Captain",
            " Harry",
            " Haven",
            ",",
            " who",
            " had",
            " returned",
            " to",
            " London",
            " from",
            " a",
            " Colombian",
            " business",
            " venture",
            " ended",
            " by",
            " a",
            " civil",
            " war",
            ".",
            " ",
            "A",
            " more",
            " obvious",
            " influence",
            " on",
            " the",
            " early",
            " P",
            "oi",
            "rot",
            " stories",
            " is",
            " that",
            " of",
            " Arthur",
            " Conan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " stiff",
            " military",
            " m",
            "oust",
            "ache",
            ".",
            " His",
            " apparel",
            " was",
            " neat",
            " to",
            " perfection",
            ",",
            " a",
            " little",
            " quaint",
            " and",
            " frankly",
            " d",
            "and",
            "ified",
            ".\"",
            " He",
            " was",
            " accompanied",
            " by",
            " Captain",
            " Harry",
            " Haven",
            ",",
            " who",
            " had",
            " returned",
            " to",
            " London",
            " from",
            " a",
            " Colombian",
            " business",
            " venture",
            " ended",
            " by",
            " a",
            " civil",
            " war",
            ".",
            " ",
            "A",
            " more",
            " obvious",
            " influence",
            " on",
            " the",
            " early",
            " P",
            "oi",
            "rot",
            " stories",
            " is",
            " that",
            " of",
            " Arthur",
            " Conan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.609,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Serbian",
            " elite",
            ",",
            " ignited",
            " allegations",
            " of",
            " bols",
            "hev",
            "ism",
            " from",
            " Bel",
            "grade",
            ".",
            " This",
            ",",
            " in",
            " turn",
            ",",
            " led",
            " to",
            " increased",
            " pressure",
            " from",
            " Italy",
            " and",
            " cul",
            "minated",
            " in",
            " Z",
            "og",
            "'s",
            " restoration",
            " to",
            " authority",
            ".",
            " Sub",
            "sequently",
            " in",
            " ",
            "192",
            "8",
            ",",
            " Z",
            "og",
            "u",
            " transition",
            "ed",
            " Albania",
            " from",
            " a",
            " republic",
            " to",
            " a",
            " monarchy",
            " that",
            " garnered",
            " backing",
            " from",
            " Fasc",
            "ist",
            " Italy"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "ab",
            "ets",
            " today",
            ",",
            " such",
            " as",
            " the",
            " Han",
            "uno",
            "'o",
            " script",
            ",",
            " are",
            " learned",
            " one",
            " letter",
            " at",
            " a",
            " time",
            ",",
            " in",
            " no",
            " particular",
            " order",
            ",",
            " and",
            " are",
            " not",
            " used",
            " for",
            " coll",
            "ation",
            " where",
            " a",
            " definite",
            " order",
            " is",
            " required",
            ".",
            " However",
            ",",
            " a",
            " dozen",
            " U",
            "gar",
            "itic",
            " tablets",
            " from",
            " the",
            " four",
            "teenth",
            " century",
            " BCE",
            " preserve",
            " the",
            " alphabet",
            " in",
            " two",
            " sequences",
            ".",
            " One",
            ",",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " dozen",
            " dens",
            ",",
            " which",
            " are",
            " occupied",
            " for",
            " six",
            " weeks",
            " at",
            " a",
            " time",
            ".",
            "There",
            " are",
            " two",
            " distinct",
            " populations",
            ":",
            " one",
            " in",
            " Southern",
            " Africa",
            ",",
            " and",
            " another",
            " in",
            " East",
            " and",
            " Northeast",
            " Africa",
            ".",
            " The",
            " species",
            " does",
            " not",
            " occur",
            " in",
            " the",
            " intermediary",
            " mi",
            "ombo",
            " forests",
            ".",
            "An",
            " adult",
            " pair",
            ",",
            " along",
            " with",
            " their",
            " most",
            "-re",
            "cent",
            " offspring",
            ",",
            " occupies",
            " a",
            " territory",
            " of",
            " .",
            "Behavior"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " ",
            "181",
            "4",
            " during",
            " the",
            " War",
            " of",
            " ",
            "181",
            "2",
            ".",
            " The",
            " formally",
            " acquired",
            " western",
            " American",
            " lands",
            " continued",
            " to",
            " be",
            " populated",
            " by",
            " a",
            " dozen",
            " or",
            " so",
            " American",
            " Indian",
            " tribes",
            " that",
            " had",
            " been",
            " British",
            " allies",
            " for",
            " the",
            " most",
            " part",
            ".",
            " Though",
            " British",
            " forts",
            " on",
            " their",
            " lands",
            " had",
            " been",
            " c",
            "eded",
            " to",
            " either",
            " the",
            " French",
            " or",
            " the",
            " British",
            " prior",
            " to",
            " the",
            " creation",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " get",
            " anything",
            " overt",
            "ly",
            " anti",
            "-Russian",
            " printed",
            ".",
            " Anti",
            "-Russian",
            " books",
            " do",
            " appear",
            ",",
            " but",
            " mostly",
            " from",
            " Catholic",
            " publishing",
            " firms",
            " and",
            " always",
            " from",
            " a",
            " religious",
            " or",
            " frankly",
            " reactionary",
            " angle",
            "\".",
            "The",
            " publisher",
            " Jonathan",
            " Cape",
            ",",
            " who",
            " had",
            " initially",
            " accepted",
            " Animal",
            " Farm",
            ",",
            " subsequently",
            " rejected",
            " the",
            " book",
            " after",
            " an",
            " official",
            " at",
            " the",
            " British",
            " Ministry",
            " of",
            " Information",
            " warned",
            " him",
            " off",
            "Âł",
            "–",
            " although",
            " the",
            " civil"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " who",
            " could",
            " not",
            " complete",
            " the",
            " university",
            " program",
            " of",
            " his",
            " youth",
            ",",
            " received",
            " at",
            " least",
            " a",
            " dozen",
            " honorary",
            " degrees",
            " from",
            " academic",
            " institutions",
            ",",
            " including",
            " eight",
            " honorary",
            " LL",
            ".D",
            ".s",
            " (",
            "Doctor",
            "ate",
            " of",
            " Laws",
            "),",
            " two",
            " Ph",
            ".D",
            ".s",
            ",",
            " a",
            " D",
            ".Sc",
            ".,",
            " and",
            " an",
            " M",
            ".D",
            ".",
            ":",
            " Gall",
            "aud",
            "et",
            " College",
            " (",
            "then",
            " named",
            " National",
            " De",
            "af",
            "-M",
            "ute",
            " College",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " heavy",
            " rainfall",
            " that",
            " triggered",
            " catastrophic",
            " flooding",
            " throughout",
            " much",
            " of",
            " the",
            " southern",
            " half",
            " of",
            " the",
            " province",
            " along",
            " the",
            " Bow",
            ",",
            " El",
            "bow",
            ",",
            " High",
            "wood",
            " and",
            " Old",
            "man",
            " rivers",
            " and",
            " trib",
            "ut",
            "aries",
            ".",
            " A",
            " dozen",
            " municipalities",
            " in",
            " Southern",
            " Alberta",
            " declared",
            " local",
            " states",
            " of",
            " emergency",
            " on",
            " June",
            " ",
            "21",
            " as",
            " water",
            " levels",
            " rose",
            " and",
            " numerous",
            " communities",
            " were",
            " placed",
            " under",
            " evacuation",
            " orders",
            ".",
            "In",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "ellation",
            ".\"",
            " It",
            " was",
            " public",
            "ized",
            " from",
            " the",
            " very",
            " beginning",
            " that",
            " \"",
            "Mary",
            " West",
            "mac",
            "ott",
            "\"",
            " was",
            " a",
            " pen",
            " name",
            " of",
            " a",
            " well",
            "-known",
            " author",
            ",",
            " although",
            " the",
            " identity",
            " behind",
            " the",
            " pen",
            " name",
            " was",
            " kept",
            " secret",
            ";",
            " the",
            " dust",
            " jacket",
            " of",
            " Giant",
            "'s",
            " Bread",
            " mentions",
            " that",
            " the",
            " author",
            " had",
            " previously",
            " written",
            " \"",
            "under",
            " her",
            " real",
            " name",
            "...",
            "half",
            " a",
            " dozen",
            " books",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " schools",
            ",",
            " including",
            " N",
            "en",
            "ana",
            " Student",
            " Living",
            " Center",
            " in",
            " N",
            "en",
            "ana",
            " and",
            " The",
            " Gal",
            "ena",
            " Interior",
            " Learning",
            " Academy",
            " in",
            " Gal",
            "ena",
            ".",
            "There",
            " are",
            " more",
            " than",
            " a",
            " dozen",
            " colleges",
            " and",
            " universities",
            " in",
            " Alaska",
            ".",
            " Accred",
            "ited",
            " universities",
            " in",
            " Alaska",
            " include",
            " the",
            " University",
            " of",
            " Alaska",
            " Anch",
            "orage",
            ",",
            " University",
            " of",
            " Alaska",
            " Fair",
            "banks",
            ",",
            " University",
            " of",
            " Alaska",
            " Southeast",
            ",",
            " and",
            " Alaska",
            " Pacific"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " King",
            " was",
            " in",
            " Italy",
            ",",
            " Da",
            "oud",
            " Khan",
            " launched",
            " a",
            " blood",
            "less",
            " coup",
            " and",
            " became",
            " the",
            " first",
            " president",
            " of",
            " Afghanistan",
            ",",
            " abol",
            "ishing",
            " the",
            " monarchy",
            ".",
            "Democratic",
            " Republic",
            " and",
            " Soviet",
            " war",
            "In",
            " April",
            " ",
            "197",
            "8",
            ",",
            " the",
            " communist",
            " People",
            "'s",
            " Democratic",
            " Party",
            " of",
            " Afghanistan",
            " (",
            "PD",
            "PA",
            ")",
            " seized",
            " power",
            " in",
            " a",
            " bloody",
            " coup",
            " d",
            "'Ã©t",
            "at",
            " against",
            " then",
            "-President"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " ones",
            " traditionally",
            " used",
            " to",
            " designate",
            " the",
            " planets",
            ".",
            " By",
            " ",
            "185",
            "5",
            " there",
            " were",
            " two",
            " dozen",
            " asteroid",
            " symbols",
            ",",
            " which",
            " often",
            " occurred",
            " in",
            " multiple",
            " variants",
            ".",
            "In",
            " ",
            "185",
            "1",
            ",",
            " after",
            " the",
            " fif",
            "teenth",
            " asteroid",
            ",",
            " E",
            "un",
            "om",
            "ia",
            ",",
            " had",
            " been",
            " discovered",
            ",",
            " Johann",
            " Franz",
            " En",
            "cke",
            " made",
            " a",
            " major",
            " change",
            " in",
            " the",
            " upcoming",
            " ",
            "185",
            "4",
            " edition",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            " official",
            " functions",
            " and",
            " cultural",
            " life",
            ".",
            " The",
            " country",
            " has",
            " never",
            " had",
            " an",
            " official",
            " religion",
            " either",
            " as",
            " a",
            " republic",
            " or",
            " as",
            " a",
            " kingdom",
            ".",
            " In",
            " the",
            " ",
            "20",
            "th",
            " century",
            ",",
            " the",
            " clergy",
            " of",
            " all",
            " faith",
            "s",
            " was",
            " weakened",
            " under",
            " the",
            " monarchy",
            " and",
            " ultimately",
            " erad",
            "icated",
            " during",
            " the",
            " ",
            "195",
            "0",
            "s",
            " and",
            " ",
            "196",
            "0",
            "s",
            ",",
            " under",
            " the",
            " state",
            " policy",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " also",
            " strengthened",
            " the",
            " position",
            " of",
            " merchant",
            " groups",
            " at",
            " the",
            " expense",
            " of",
            " monarch",
            "s",
            ".",
            " Growth",
            " was",
            " more",
            " rapid",
            " in",
            " non",
            "-abs",
            "olut",
            "ist",
            " countries",
            ",",
            " such",
            " as",
            " Britain",
            " and",
            " the",
            " Netherlands",
            ",",
            " and",
            " more",
            " limited",
            " in",
            " absolut",
            "ist",
            " monarch",
            "ies",
            ",",
            " such",
            " as",
            " Portugal",
            ",",
            " Spain",
            ",",
            " and",
            " France",
            ",",
            " where",
            " profit",
            " mostly",
            " or",
            " exclusively",
            " benefited",
            " the",
            " monarchy",
            " and",
            " its",
            " allies",
            ".",
            "Trans"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " Manitoba",
            " and",
            " grew",
            " up",
            " in",
            " heavily",
            " immigrant",
            " communities",
            ".",
            " ",
            " Until",
            " he",
            " was",
            " four",
            ",",
            " van",
            " Vog",
            "t",
            " spoke",
            " only",
            " Pl",
            "aut",
            "di",
            "ets",
            "ch",
            " at",
            " home",
            ".",
            "For",
            " the",
            " first",
            " dozen",
            " or",
            " so",
            " years",
            " of",
            " his",
            " life",
            ",",
            " van",
            " Vog",
            "t",
            "'s",
            " father",
            ",",
            " Henry",
            " Vog",
            "t",
            ",",
            " a",
            " lawyer",
            ",",
            " moved",
            " his",
            " family",
            " several",
            " times",
            " within",
            " western",
            " Canada",
            ",",
            " moving",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "th",
            " century",
            ",",
            " while",
            " fully",
            " subordinate",
            " to",
            " Sass",
            "an",
            "id",
            " Iran",
            ",",
            " and",
            " retained",
            " its",
            " monarchy",
            ".",
            " Despite",
            " being",
            " one",
            " of",
            " the",
            " chief",
            " v",
            "ass",
            "als",
            " of",
            " the",
            " S",
            "asan",
            "ian",
            " emperor",
            ",",
            " the",
            " Alban",
            "ian",
            " king",
            " had",
            " only",
            " a",
            " sembl",
            "ance",
            " of",
            " authority",
            ",",
            " and",
            " the",
            " S",
            "asan",
            "ian",
            " mar",
            "z",
            "ban",
            " (",
            "m",
            "ilitary",
            " governor",
            ")",
            " held",
            " most",
            " civil",
            ",",
            " religious"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " birth",
            " to",
            " religious",
            " anarch",
            "ism",
            ".",
            " In",
            " the",
            " S",
            "asan",
            "ian",
            " Empire",
            ",",
            " Maz",
            "d",
            "ak",
            " called",
            " for",
            " an",
            " egal",
            "itarian",
            " society",
            " and",
            " the",
            " abolition",
            " of",
            " monarchy",
            ",",
            " only",
            " to",
            " be",
            " soon",
            " executed",
            " by",
            " Emperor",
            " K",
            "avad",
            " I",
            ".",
            "In",
            " Bas",
            "ra",
            ",",
            " religious",
            " sect",
            "s",
            " preached",
            " against",
            " the",
            " state",
            ".",
            " In",
            " Europe",
            ",",
            " various",
            " sect",
            "s",
            " developed",
            " anti",
            "-state",
            " and",
            " libertarian",
            " tendencies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " theory",
            " of",
            " it",
            " in",
            " any",
            " way",
            "\".",
            "E",
            "ug",
            "ene",
            " Eric",
            " Kim",
            " and",
            " Betty",
            " Alexandra",
            " To",
            "ole",
            " consider",
            " it",
            " \"",
            "incorrect",
            "\"",
            " to",
            " regard",
            " Lov",
            "el",
            "ace",
            " as",
            " the",
            " first",
            " computer",
            " programmer",
            ",",
            " as",
            " B",
            "abbage",
            " wrote",
            " the",
            " initial",
            " programs",
            " for",
            " his",
            " Analy",
            "tical",
            " Engine",
            ",",
            " although",
            " the",
            " majority",
            " were",
            " never",
            " published",
            ".",
            " Brom",
            "ley",
            " notes",
            " several",
            " dozen",
            " sample",
            " programs",
            " prepared",
            " by",
            " B"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Han",
            " (",
            "111",
            " CE",
            ")",
            " several",
            " dozen",
            " Proto",
            "-T",
            "ur",
            "k",
            "ic",
            " ex",
            "ot",
            "isms",
            " in",
            " Chinese",
            " Han",
            " trans",
            "criptions",
            ".",
            " Lan",
            "hai",
            " Wei",
            " and",
            " H",
            "ui",
            " Li",
            " reconstruct",
            " the",
            " name",
            " of",
            " the",
            " Xi",
            "Åį",
            "ng",
            "n",
            "Ãº",
            " ruling",
            " house",
            " as",
            "PT",
            " *",
            "Al",
            "ay",
            "und",
            "luÄŁ",
            " /",
            "al",
            "aj",
            "unt",
            "Ë",
            "Ī",
            "lu",
            "Î³",
            "/",
            " '",
            "pie",
            "b",
            "ald",
            " horse"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " realm",
            ";",
            " it",
            " is",
            " a",
            " constitutional",
            " monarchy",
            " with",
            " Charles",
            " III",
            " as",
            " its",
            " head",
            " of",
            " state",
            ".",
            "The",
            " economy",
            " of",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " is",
            " largely",
            " dependent",
            " on",
            " tourism",
            ",",
            " which",
            " accounts",
            " for",
            " ",
            "80",
            "%",
            " of",
            " its",
            " GDP",
            ".",
            " Like",
            " other",
            " island",
            " nations",
            ",",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " is",
            " vulnerable",
            " to",
            " the",
            " effects",
            " of",
            " climate",
            " change",
            ",",
            " such",
            " as",
            " sea",
            " level"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " a",
            " higher",
            " priority",
            " to",
            " anarch",
            "a",
            "-f",
            "emin",
            "ists",
            " than",
            " an",
            "ar",
            "cho",
            "-comm",
            "un",
            "ists",
            ".",
            "An",
            "arch",
            "ists",
            " are",
            " generally",
            " committed",
            " against",
            " coerc",
            "ive",
            " authority",
            " in",
            " all",
            " forms",
            ",",
            " namely",
            " \"",
            "all",
            " centralized",
            " and",
            " hierarchical",
            " forms",
            " of",
            " government",
            " (",
            "e",
            ".g",
            ".,",
            " monarchy",
            ",",
            " representative",
            " democracy",
            ",",
            " state",
            " socialism",
            ",",
            " etc",
            ".),",
            " economic",
            " class",
            " systems",
            " (",
            "e",
            ".g",
            ".,",
            " capitalism"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " dozen",
            " other",
            " minority",
            " languages",
            " spoken",
            " n",
            "atively",
            " in",
            " the",
            " country",
            ".",
            " A",
            "var",
            ",",
            " Bud",
            "uk",
            "h",
            ",",
            " Georgian",
            ",",
            " J",
            "uh",
            "uri",
            ",",
            " Kh",
            "inal",
            "ug",
            ",",
            " K",
            "ry",
            "ts",
            ",",
            " Le",
            "z",
            "gin",
            ",",
            " Rut",
            "ul",
            ",",
            " T",
            "aly",
            "sh",
            ",",
            " Tat",
            ",",
            " Ts",
            "akh",
            "ur",
            ",",
            " and",
            " U",
            "di",
            " are",
            " all",
            " spoken",
            " by",
            " small",
            " minorities",
            ".",
            " Some",
            " of",
            " these",
            " language"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " nour",
            "ishment",
            " from",
            " the",
            " adult",
            ".",
            " The",
            " larvae",
            " emerge",
            " at",
            " varying",
            " stages",
            " of",
            " their",
            " growth",
            ",",
            " either",
            " before",
            " or",
            " after",
            " metam",
            "orph",
            "osis",
            ",",
            " according",
            " to",
            " their",
            " species",
            ".",
            " The",
            " to",
            "ad",
            " genus",
            " N",
            "ect",
            "oph",
            "ryn",
            "oid",
            "es",
            " exhibits",
            " all",
            " of",
            " these",
            " developmental",
            " patterns",
            " among",
            " its",
            " dozen",
            " or",
            " so",
            " members",
            ".",
            " Amph",
            "ib",
            "ian",
            " larvae",
            " are",
            " known",
            " as",
            " tad",
            "po",
            "les",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " dozen",
            " languages",
            " worldwide",
            ".",
            " In",
            " ",
            "200",
            "3",
            ",",
            " Spir",
            "ited",
            " Away",
            ",",
            " a",
            " Studio",
            " G",
            "hib",
            "li",
            " feature",
            " film",
            " directed",
            " by",
            " Hay",
            "ao",
            " Miy",
            "az",
            "aki",
            ",",
            " won",
            " the",
            " Academy",
            " Award",
            " for",
            " Best",
            " Animated",
            " Feature",
            " at",
            " the",
            " ",
            "75",
            "th",
            " Academy",
            " Awards",
            ".",
            " It",
            " later",
            " became",
            " the",
            " highest",
            "-g",
            "ross",
            "ing",
            " anime",
            " film",
            ",",
            " earning",
            " more",
            " than",
            " $",
            "355",
            "Âł",
            "million",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " dozen",
            " languages",
            " worldwide",
            ".",
            " In",
            " ",
            "200",
            "3",
            ",",
            " Spir",
            "ited",
            " Away",
            ",",
            " a",
            " Studio",
            " G",
            "hib",
            "li",
            " feature",
            " film",
            " directed",
            " by",
            " Hay",
            "ao",
            " Miy",
            "az",
            "aki",
            ",",
            " won",
            " the",
            " Academy",
            " Award",
            " for",
            " Best",
            " Animated",
            " Feature",
            " at",
            " the",
            " ",
            "75",
            "th",
            " Academy",
            " Awards",
            ".",
            " It",
            " later",
            " became",
            " the",
            " highest",
            "-g",
            "ross",
            "ing",
            " anime",
            " film",
            ",",
            " earning",
            " more",
            " than",
            " $",
            "355",
            "Âł",
            "million",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " Players",
            " of",
            " Ä",
            "Ģ",
            " (",
            "later",
            " re",
            "-t",
            "itled",
            " The",
            " P",
            "awns",
            " of",
            " Null",
            "-A",
            ")",
            " was",
            " serialized",
            " in",
            " ",
            "194",
            "8",
            "–",
            "49",
            ".",
            "At",
            " the",
            " same",
            " time",
            ",",
            " in",
            " his",
            " fiction",
            ",",
            " van",
            " Vog",
            "t",
            " was",
            " consistently",
            " sympathetic",
            " to",
            " absolute",
            " monarchy",
            " as",
            " a",
            " form",
            " of",
            " government",
            ".",
            " This",
            " was",
            " the",
            " case",
            ",",
            " for",
            " instance",
            ",",
            " in",
            " the",
            " Weapon",
            " Shop",
            " series"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " Third",
            " Anglo",
            "-A",
            "f",
            "ghan",
            " War",
            " in",
            " ",
            "191",
            "9",
            ",",
            " Afghanistan",
            " became",
            " free",
            " of",
            " foreign",
            " political",
            " heg",
            "emony",
            ",",
            " and",
            " emerged",
            " as",
            " the",
            " independent",
            " Kingdom",
            " of",
            " Afghanistan",
            " in",
            " June",
            " ",
            "192",
            "6",
            " under",
            " A",
            "man",
            "ullah",
            " Khan",
            ".",
            " This",
            " monarchy",
            " lasted",
            " almost",
            " half",
            " a",
            " century",
            ",",
            " until",
            " Z",
            "ahir",
            " Shah",
            " was",
            " over",
            "thrown",
            " in",
            " ",
            "197",
            "3",
            ",",
            " following",
            " which",
            " the",
            " Republic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " alphabet",
            "\".",
            "However",
            ",",
            " Daniels",
            "'s",
            " terminology",
            " has",
            " found",
            " acceptance",
            " in",
            " the",
            " linguistic",
            " community",
            ".",
            "Orig",
            "ins",
            "The",
            " first",
            " ab",
            "jad",
            " to",
            " gain",
            " widespread",
            " usage",
            " was",
            " the",
            " Ph",
            "oen",
            "ician",
            " ab",
            "jad",
            ".",
            " Unlike",
            " other",
            " contemporary",
            " scripts",
            ",",
            " such",
            " as",
            " c",
            "une",
            "iform",
            " and",
            " Egyptian",
            " hier",
            "og",
            "lyph",
            "s",
            ",",
            " the",
            " Ph",
            "oen",
            "ician",
            " script",
            " consisted",
            " of",
            " only",
            " a",
            " few",
            " dozen",
            " symbols"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " over",
            " two",
            " dozen",
            " Alban",
            "ian",
            " Grand",
            " V",
            "iz",
            "iers",
            ".",
            " Others",
            " included",
            " members",
            " of",
            " the",
            " prominent",
            " KÃ¶",
            "pr",
            "Ã¼lÃ¼",
            " family",
            ",",
            " Z",
            "agan",
            " P",
            "asha",
            ",",
            " Muhammad",
            " Ali",
            " of",
            " Egypt",
            " and",
            " Ali",
            " P",
            "asha",
            " of",
            " Tep",
            "el",
            "ena",
            ".",
            " Furthermore",
            ",",
            " two",
            " s",
            "ult",
            "ans",
            ",",
            " Bay",
            "ez",
            "id",
            " II",
            " and",
            " Meh",
            "med",
            " III",
            ",",
            " both",
            " had",
            " mothers",
            " of",
            " Alban",
            "ian",
            " origin",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " desert",
            "ers",
            ";",
            " among",
            " those",
            " labeled",
            " German",
            " desert",
            "ers",
            ",",
            " however",
            ",",
            " it",
            " is",
            " estimated",
            " that",
            " ",
            "1",
            ",",
            "800",
            " were",
            " killed",
            " in",
            " combat",
            ".",
            "Legacy",
            "The",
            " American",
            " Revolution",
            " established",
            " the",
            " United",
            " States",
            " with",
            " its",
            " numerous",
            " civil",
            " liberties",
            " and",
            " set",
            " an",
            " example",
            " to",
            " overthrow",
            " both",
            " monarchy",
            " and",
            " colonial",
            " governments",
            ".",
            " The",
            " United",
            " States",
            " has",
            " the",
            " world",
            "'s",
            " oldest",
            " written",
            " constitution",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            "ication",
            " at",
            " the",
            " nearby",
            " Royal",
            " Palace",
            " of",
            " Amsterdam",
            ".",
            " Normally",
            ",",
            " however",
            ",",
            " the",
            " Parliament",
            " sits",
            " in",
            " The",
            " Hague",
            ",",
            " the",
            " city",
            " which",
            " has",
            " historically",
            " been",
            " the",
            " seat",
            " of",
            " the",
            " Dutch",
            " government",
            ",",
            " the",
            " Dutch",
            " monarchy",
            ",",
            " and",
            " the",
            " Dutch",
            " supreme",
            " court",
            ".",
            " Foreign",
            " emb",
            "ass",
            "ies",
            " are",
            " also",
            " located",
            " in",
            " The",
            " Hague",
            ".",
            "Symbols",
            "The",
            " coat",
            " of",
            " arms",
            " of",
            " Amsterdam",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "181",
            "3",
            ".",
            "Thomas",
            " Bass",
            "ett",
            ",",
            " a",
            " loyal",
            "ist",
            " to",
            " the",
            " British",
            " monarchy",
            " during",
            " the",
            " Revolutionary",
            " era",
            ",",
            " was",
            " one",
            " of",
            " the",
            " earliest",
            " white",
            " settlers",
            " in",
            " the",
            " state",
            " outside",
            " Mobile",
            ".",
            " He",
            " settled",
            " in",
            " the",
            " Tomb",
            "ig",
            "bee",
            " District",
            " during",
            " the",
            " early",
            " ",
            "177",
            "0",
            "s",
            ".",
            " The",
            " district",
            "'s",
            " boundaries",
            " were",
            " roughly",
            " limited",
            " to",
            " the",
            " area",
            " within",
            " a",
            " few"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " which",
            " placed",
            " the",
            " colony",
            " under",
            " the",
            " British",
            " monarchy",
            "'s",
            " direct",
            " governance",
            ".",
            "These",
            " measures",
            " rallied",
            " the",
            " other",
            " colonies",
            ",",
            " ",
            "12",
            " of",
            " which",
            " sent",
            " delegates",
            " to",
            " the",
            " First",
            " Continental",
            " Congress",
            " in",
            " Philadelphia",
            " in",
            " early",
            " September",
            " ",
            "177",
            "4",
            " to",
            " protest",
            " the",
            " measures",
            " and",
            " deliberate",
            " on",
            " potential",
            " responses",
            ".",
            " The",
            " Congress",
            " drafted",
            " a",
            " Pet",
            "ition",
            " to",
            " the",
            " King",
            " asking",
            " for",
            " peace",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.451,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " Metropolitan",
            " Opera",
            ",",
            " where",
            " he",
            " was",
            " cheered",
            " by",
            " the",
            " audience",
            " on",
            " his",
            " arrival",
            ".",
            " During",
            " the",
            " days",
            " following",
            ",",
            " he",
            " was",
            " given",
            " the",
            " keys",
            " to",
            " the",
            " city",
            " by",
            " Mayor",
            " Jimmy",
            " Walker",
            " and",
            " met",
            " the",
            " president",
            " of",
            " Columbia",
            " University",
            ",",
            " who",
            " described",
            " Einstein",
            " as",
            " \"",
            "the",
            " ruling",
            " monarch",
            " of",
            " the",
            " mind",
            "\".",
            " Harry",
            " Emerson",
            " F",
            "os",
            "d",
            "ick",
            ",",
            " pastor",
            " at",
            " New",
            " York",
            "'s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Nobel",
            " citation",
            " displayed",
            " a",
            " degree",
            " of",
            " doubt",
            " even",
            " about",
            " the",
            " work",
            " on",
            " photo",
            "electric",
            "ity",
            " that",
            " it",
            " acknowledged",
            ":",
            " it",
            " did",
            " not",
            " ass",
            "ent",
            " to",
            " Einstein",
            "'s",
            " notion",
            " of",
            " the",
            " partic",
            "ulate",
            " nature",
            " of",
            " light",
            ",",
            " which",
            " only",
            " won",
            " over",
            " the",
            " entire",
            " scientific",
            " community",
            " when",
            " S",
            ".",
            " N",
            ".",
            " Bose",
            " derived",
            " the",
            " Plan",
            "ck",
            " spectrum",
            " in",
            " ",
            "192",
            "4",
            ".",
            " That",
            " same"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " spectacular",
            " wealth",
            " of",
            " visible",
            " mur",
            "als",
            " and",
            " fres",
            "cos",
            ".",
            " Perhaps",
            " the",
            " best",
            " known",
            " examples",
            " can",
            " be",
            " found",
            " in",
            " the",
            " southern",
            " Alban",
            "ian",
            " cities",
            " and",
            " surroundings",
            " of",
            " Kor",
            "Ã§",
            "Ã«",
            ",",
            " Ber",
            "at",
            ",",
            " V",
            "os",
            "kop",
            "oj",
            "Ã«",
            " and",
            " G",
            "ji",
            "rok",
            "ast",
            "Ã«r",
            ".",
            " In",
            "vol",
            "ving",
            " the",
            " introduction",
            " of",
            " Ottoman",
            " architecture",
            " there",
            " was",
            " a",
            " development",
            " of",
            " mosques",
            " and",
            " other",
            " Islamic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " program",
            " requires",
            " ",
            "70",
            "%",
            " of",
            " that",
            " subsidy",
            " to",
            " go",
            " to",
            " carriers",
            " who",
            " offer",
            " passenger",
            " service",
            " to",
            " the",
            " communities",
            ".",
            "Many",
            " communities",
            " have",
            " small",
            " air",
            " taxi",
            " services",
            ".",
            " These",
            " operations",
            " originated",
            " from",
            " the",
            " demand",
            " for",
            " customized",
            " transport",
            " to",
            " remote",
            " areas",
            ".",
            " Perhaps",
            " the",
            " most",
            " quint",
            "ess",
            "entially",
            " Al",
            "askan",
            " plane",
            " is",
            " the",
            " bush",
            " se",
            "ap",
            "lane",
            ".",
            " The",
            " world",
            "'s",
            " busiest"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " One",
            " approach",
            " attempting",
            " to",
            " overcome",
            " the",
            " divide",
            " between",
            " consequential",
            "ism",
            " and",
            " de",
            "ontology",
            " is",
            " case",
            "-based",
            " reasoning",
            ",",
            " also",
            " known",
            " as",
            " cas",
            "u",
            "istry",
            ".",
            " Cas",
            "u",
            "istry",
            " does",
            " not",
            " begin",
            " with",
            " theory",
            ",",
            " rather",
            " it",
            " starts",
            " with",
            " the",
            " immediate",
            " facts",
            " of",
            " a",
            " real",
            " and",
            " concrete",
            " case",
            ".",
            " While",
            " cas",
            "u",
            "istry",
            " makes",
            " use",
            " of",
            " ethical",
            " theory",
            ",",
            " it",
            " does",
            " not",
            " view"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.35,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "ammad",
            " (",
            "capital",
            " of",
            " the",
            " H",
            "ammad",
            "id",
            " Em",
            "irate",
            "),",
            " as",
            " they",
            " had",
            " done",
            " in",
            " K",
            "air",
            "ou",
            "an",
            " a",
            " few",
            " decades",
            " ago",
            ".",
            " From",
            " there",
            " they",
            " gradually",
            " gained",
            " the",
            " upper",
            " Alg",
            "iers",
            " and",
            " Or",
            "an",
            " plains",
            ".",
            " Some",
            " of",
            " these",
            " territories",
            " were",
            " forcibly",
            " taken",
            " back",
            " by",
            " the",
            " Al",
            "m",
            "oh",
            "ads",
            " in",
            " the",
            " second",
            " half",
            " of",
            " the",
            " ",
            "12",
            "th",
            " century"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.334,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " fifth",
            "-century",
            " form",
            " resembling",
            " the",
            " Greek",
            " letter",
            " tau",
            " in",
            " the",
            " hands",
            " of",
            " medieval",
            " Irish",
            " and",
            " English",
            " writers",
            ".",
            " The",
            " Roman",
            " form",
            " is",
            " used",
            " in",
            " most",
            " printed",
            " material",
            ";",
            " it",
            " consists",
            " of",
            " a",
            " small",
            " loop",
            " with",
            " an",
            " arc",
            " over",
            " it",
            " (\"",
            "a",
            "\").",
            " Both",
            " derive",
            " from",
            " the",
            " maj",
            "usc",
            "ule",
            " (",
            "capital",
            ")",
            " form",
            ".",
            " In",
            " Greek",
            " handwriting",
            ",",
            " it",
            " was",
            " common",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " on",
            " his",
            " sister",
            ",",
            " D",
            "agn",
            "y",
            " Tag",
            "gart",
            ",",
            " to",
            " actually",
            " run",
            " the",
            " railroad",
            ",",
            " but",
            " nonetheless",
            " opposes",
            " her",
            " in",
            " almost",
            " every",
            " endeavor",
            " because",
            " of",
            " his",
            " various",
            " anti",
            "-capital",
            "ist",
            " moral",
            " and",
            " political",
            " beliefs",
            ".",
            " In",
            " a",
            " sense",
            ",",
            " he",
            " is",
            " the",
            " ant",
            "ith",
            "esis",
            " of",
            " D",
            "agn",
            "y",
            ".",
            " This",
            " contradiction",
            " leads",
            " to",
            " the",
            " recurring",
            " absurd",
            "ity",
            " of",
            " his",
            " life",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " century",
            " with",
            " the",
            " British",
            " Agricultural",
            " Revolution",
            ",",
            " allowing",
            " global",
            " population",
            " to",
            " rise",
            " significantly",
            ".",
            " Since",
            " ",
            "190",
            "0",
            ",",
            " agriculture",
            " in",
            " developed",
            " nations",
            ",",
            " and",
            " to",
            " a",
            " lesser",
            " extent",
            " in",
            " the",
            " developing",
            " world",
            ",",
            " has",
            " seen",
            " large",
            " rises",
            " in",
            " productivity",
            " as",
            " mechan",
            "ization",
            " replaces",
            " human",
            " labor",
            ",",
            " and",
            " assisted",
            " by",
            " synthetic",
            " fertil",
            "izers",
            ",",
            " pesticides",
            ",",
            " and",
            " selective",
            " breeding",
            ".",
            " The",
            " Hab",
            "er"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " religious",
            " resistance",
            " heavily",
            " opposed",
            " this",
            " tendency",
            ",",
            " but",
            " in",
            " contrast",
            " to",
            " the",
            " other",
            " colon",
            "ized",
            " countries",
            "'",
            " path",
            " in",
            " central",
            " Asia",
            " and",
            " Caucas",
            "us",
            ",",
            " Algeria",
            " kept",
            " its",
            " individual",
            " skills",
            " and",
            " a",
            " relatively",
            " human",
            "-capital",
            " intensive",
            " agriculture",
            ".",
            "During",
            " the",
            " Second",
            " World",
            " War",
            ",",
            " Algeria",
            " came",
            " under",
            " V",
            "ich",
            "y",
            " control",
            " before",
            " being",
            " liberated",
            " by",
            " the",
            " Allies",
            " in",
            " Operation",
            " Torch",
            ",",
            " which",
            " saw"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " small",
            "-complete",
            " category",
            " which",
            " satisfies",
            " the",
            " appropriate",
            " solution",
            " set",
            " condition",
            " has",
            " a",
            " left",
            "-",
            "adj",
            "oint",
            " (",
            "the",
            " Fre",
            "yd",
            " adj",
            "oint",
            " functor",
            " theorem",
            ").",
            "We",
            "aker",
            " forms",
            "There",
            " are",
            " several",
            " weaker",
            " statements",
            " that",
            " are",
            " not",
            " equivalent",
            " to",
            " the",
            " axiom",
            " of",
            " choice",
            ",",
            " but",
            " are",
            " closely",
            " related",
            ".",
            " One",
            " example",
            " is",
            " the",
            " axiom",
            " of",
            " dependent",
            " choice",
            " (",
            "DC",
            ").",
            " A",
            " still"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.205,
            -0.0,
            -0.0,
            0.207,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " ca",
            "esium",
            " chloride",
            " in",
            " mice",
            " is",
            " ",
            "2",
            ".",
            "3",
            "Âł",
            "g",
            " per",
            " kil",
            "ogram",
            ",",
            " which",
            " is",
            " comparable",
            " to",
            " the",
            " LD",
            "50",
            " values",
            " of",
            " potassium",
            " chloride",
            " and",
            " sodium",
            " chloride",
            ".",
            " Ca",
            "esium",
            " chloride",
            " has",
            " been",
            " promoted",
            " as",
            " an",
            " alternative",
            " cancer",
            " therapy",
            ",",
            " but",
            " has",
            " been",
            " linked",
            " to",
            " the",
            " deaths",
            " of",
            " over",
            " ",
            "50",
            " patients",
            ",",
            " on",
            " whom",
            " it",
            " was",
            " used",
            " as",
            " part"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.228,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "tones",
            " ...",
            " evidence",
            " of",
            " an",
            " exceptionally",
            " force",
            "ful",
            " mind",
            ".\"",
            " Jonathan",
            " Barnes",
            " wrote",
            " that",
            " \"",
            "an",
            " account",
            " of",
            " Aristotle",
            "'s",
            " intellectual",
            " after",
            "life",
            " would",
            " be",
            " little",
            " less",
            " than",
            " a",
            " history",
            " of",
            " European",
            " thought",
            "\".",
            "A",
            "rist",
            "otle",
            " has",
            " been",
            " called",
            " the",
            " father",
            " of",
            " logic",
            ",",
            " biology",
            ",",
            " political",
            " science",
            ",",
            " zo",
            "ology",
            ",",
            " embry",
            "ology",
            ",",
            " natural",
            " law",
            ",",
            " scientific",
            " method",
            ",",
            " rhetoric"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.157,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "ides",
            " and",
            " many",
            " act",
            "in",
            "ides",
            ",",
            " act",
            "inium",
            " assumes",
            " oxidation",
            " state",
            " +",
            "3",
            " in",
            " nearly",
            " all",
            " its",
            " chemical",
            " compounds",
            ".",
            " Act",
            "inium",
            " is",
            " found",
            " only",
            " in",
            " traces",
            " in",
            " uranium",
            " and",
            " thor",
            "ium",
            " ores",
            " as",
            " the",
            " is",
            "otope",
            " ",
            "227",
            "Ac",
            ",",
            " which",
            " dec",
            "ays",
            " with",
            " a",
            " half",
            "-life",
            " of",
            " ",
            "21",
            ".",
            "772",
            " years",
            ",",
            " predominantly",
            " emitting",
            " beta",
            " and",
            " sometimes",
            " alpha",
            " particles"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.157,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " effects",
            ",",
            " which",
            " are",
            " predicted",
            " to",
            " have",
            " a",
            " large",
            " influence",
            " on",
            " the",
            " chemical",
            " properties",
            " of",
            " super",
            "heavy",
            " elements",
            ";",
            " even",
            " if",
            " it",
            " does",
            " turn",
            " out",
            " to",
            " be",
            " an",
            " alk",
            "ali",
            " metal",
            ",",
            " it",
            " is",
            " predicted",
            " to",
            " have",
            " some",
            " differences",
            " in",
            " physical",
            " and",
            " chemical",
            " properties",
            " from",
            " its",
            " lighter",
            " hom",
            "olog",
            "ues",
            ".",
            "Most",
            " alk",
            "ali",
            " metals",
            " have",
            " many",
            " different",
            " applications",
            ".",
            " One",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " located",
            " to",
            " the",
            " right",
            " of",
            " plut",
            "onium",
            ",",
            " to",
            " the",
            " left",
            " of",
            " cur",
            "ium",
            ",",
            " and",
            " below",
            " the",
            " lan",
            "than",
            "ide",
            " europ",
            "ium",
            ",",
            " with",
            " which",
            " it",
            " shares",
            " many",
            " physical",
            " and",
            " chemical",
            " properties",
            ".",
            " Americ",
            "ium",
            " is",
            " a",
            " highly",
            " radioactive",
            " element",
            ".",
            " When",
            " freshly",
            " prepared",
            ",",
            " it",
            " has",
            " a",
            " sil",
            "very",
            "-white",
            " metallic",
            " lust",
            "re",
            ",",
            " but",
            " then",
            " slowly",
            " tarn",
            "ishes",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " believed",
            " in",
            " a",
            " blind",
            ",",
            " powerful",
            ",",
            " inex",
            "orable",
            " and",
            " ins",
            "ensible",
            " fate",
            " over",
            " which",
            " man",
            " had",
            " no",
            " control",
            ".",
            " This",
            " was",
            " replaced",
            " with",
            " the",
            " Islamic",
            " notion",
            " of",
            " a",
            " powerful",
            " but",
            " provid",
            "ent",
            " and",
            " merc",
            "iful",
            " God",
            ".",
            "According",
            " to",
            " Francis",
            " Edward",
            " Peters",
            ",",
            " \"",
            "The",
            " Qur",
            "’",
            "Äģn",
            " insists",
            ",",
            " Muslims",
            " believe",
            ",",
            " and",
            " historians",
            " affirm",
            " that",
            " Muhammad",
            " and",
            " his",
            " followers",
            " worship"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "cast",
            ".",
            "TV",
            " ratings",
            " ",
            "Histor",
            "ically",
            ",",
            " the",
            " tele",
            "cast",
            "'s",
            " viewers",
            "hip",
            " is",
            " higher",
            " when",
            " box",
            "-office",
            " hits",
            " are",
            " favored",
            " to",
            " win",
            " the",
            " Best",
            " Picture",
            " award",
            ".",
            " More",
            " than",
            " ",
            "57",
            ".",
            "25",
            "Âł",
            "million",
            " viewers",
            " tuned",
            " to",
            " the",
            " tele",
            "cast",
            " for",
            " the",
            " ",
            "70",
            "th",
            " Academy",
            " Awards",
            " in",
            " ",
            "199",
            "8",
            ",",
            " the",
            " year",
            " of",
            " Titanic",
            ",",
            " which",
            " generated",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "aceae",
            " at",
            " the",
            " Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Website",
            " Com",
            "posit",
            "ae",
            ".org",
            " –",
            " Com",
            "posit",
            "ae",
            " Working",
            " Group",
            " (",
            "CW",
            "G",
            ")",
            " and",
            " Global",
            " Com",
            "posit",
            "ae",
            " Database",
            " (",
            "G",
            "CD",
            ")",
            " ",
            "Ast",
            "era",
            "les",
            " families",
            "Ext",
            "ant",
            " Cam",
            "pan",
            "ian",
            " first",
            " appearances",
            "<|begin_of_text|>",
            "Api",
            "aceae",
            " ()",
            " or",
            " Umb",
            "ell",
            "if",
            "era",
            "e",
            " is",
            " a",
            " family",
            " of",
            " mostly"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " high",
            " lattice",
            " ent",
            "hal",
            "py",
            " of",
            " lithium",
            " fluoride",
            " is",
            " due",
            " to",
            " the",
            " small",
            " sizes",
            " of",
            " the",
            " Li",
            "+",
            " and",
            " F",
            "âĪĴ",
            " ions",
            ",",
            " causing",
            " the",
            " electro",
            "static",
            " interactions",
            " between",
            " them",
            " to",
            " be",
            " strong",
            ":",
            " a",
            " similar",
            " effect",
            " occurs",
            " for",
            " magnesium",
            " fluoride",
            ",",
            " consistent",
            " with",
            " the",
            " diagonal",
            " relationship",
            " between",
            " lithium",
            " and",
            " magnesium",
            ".",
            "The",
            " alk",
            "ali",
            " metals",
            " also",
            " react",
            " similarly",
            " with",
            " hydrogen",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "ump",
    "umble",
    "rlen",
    "asn",
    "reff"
  ],
  "bottom_logits": [
    " hem",
    " Cz",
    "n",
    "757",
    "arem"
  ],
  "act_min": -0.0,
  "act_max": 0.633
}