{
  "index": 44627,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.91,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.114,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.088,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "TP",
            " class",
            ",",
            " and",
            " finished",
            " the",
            " race",
            " in",
            " the",
            " top",
            " two",
            " positions",
            ",",
            " while",
            " the",
            " Champion",
            " Racing",
            " R",
            "8",
            " finished",
            " third",
            " overall",
            ",",
            " and",
            " first",
            " in",
            " the",
            " L",
            "MP",
            "900",
            " class",
            ".",
            " Audi",
            " returned",
            " to",
            " the",
            " winner",
            "'s",
            " podium",
            " at",
            " the",
            " ",
            "200",
            "4",
            " race",
            ",",
            " with",
            " the",
            " top",
            " three",
            " finish",
            "ers",
            " all",
            " driving",
            " R",
            "8",
            "s",
            ":",
            " Audi",
            " Sport",
            " Japan",
            " Team",
            " G"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.898,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Cl",
            ".",
            " The",
            " first",
            " two",
            " compounds",
            " may",
            " also",
            " be",
            " produced",
            " in",
            " water",
            "Âł",
            "–",
            " a",
            "stat",
            "ine",
            " reacts",
            " with",
            " iod",
            "ine",
            "/",
            "iod",
            "ide",
            " solution",
            " to",
            " form",
            " At",
            "I",
            ",",
            " whereas",
            " At",
            "Br",
            " requires",
            " (",
            "aside",
            " from",
            " a",
            "stat",
            "ine",
            ")",
            " an",
            " iod",
            "ine",
            "/",
            "iod",
            "ine",
            " mon",
            "ob",
            "rom",
            "ide",
            "/b",
            "rom",
            "ide",
            " solution",
            ".",
            " The",
            " excess",
            " of",
            " iod",
            "ides",
            " or",
            " brom"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.844,
            0.207,
            -0.0,
            -0.0,
            0.254,
            0.142,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            0.35,
            0.093,
            -0.0,
            -0.0,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            0.222,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Marg",
            "aret",
            " Me",
            "ad",
            "M",
            "erv",
            "yn",
            " Meg",
            "g",
            "itt",
            "Jose",
            "f",
            " M",
            "enge",
            "le",
            "Nich",
            "olas",
            " Mik",
            "lou",
            "ho",
            "-M",
            "acl",
            "ay",
            "Emily",
            " Martin",
            "Hor",
            "ace",
            " Mitchell",
            " Miner",
            "Sid",
            "ney",
            " Mint",
            "z",
            "Ash",
            "ley",
            " Mont",
            "agu",
            "James",
            " Mo",
            "oney",
            "Hen",
            "ri",
            "etta",
            " L",
            ".",
            " Moore",
            "John",
            " H",
            ".",
            " Moore",
            "Lewis",
            " H"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.84,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            0.204,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "-Saharan",
            " Africa",
            ",",
            " a",
            " rate",
            " that",
            " has",
            " not",
            " changed",
            " significantly",
            " in",
            " the",
            " past",
            " few",
            " decades",
            ".",
            " However",
            ",",
            " the",
            " Food",
            " and",
            " Agriculture",
            " Organization",
            " of",
            " the",
            " United",
            " Nations",
            " (",
            "FA",
            "O",
            ")",
            " pos",
            "its",
            " that",
            " the",
            " roles",
            " and",
            " responsibilities",
            " of",
            " women",
            " in",
            " agriculture",
            " may",
            " be",
            " changing",
            " –",
            " for",
            " example",
            ",",
            " from",
            " subs",
            "istence",
            " farming",
            " to",
            " wage",
            " employment",
            ",",
            " and",
            " from",
            " contributing",
            " household",
            " members",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.82,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ag",
            "assi",
            " was",
            " raised",
            " on",
            " hard",
            "cour",
            "ts",
            ",",
            " but",
            " found",
            " much",
            " of",
            " his",
            " early",
            " major",
            "-t",
            "ournament",
            " success",
            " on",
            " the",
            " red",
            " clay",
            " of",
            " Roland",
            " Gar",
            "ros",
            ",",
            " reaching",
            " two",
            " consecutive",
            " finals",
            " there",
            " early",
            " in",
            " his",
            " career",
            ".",
            " Despite",
            " grass",
            " being",
            " his",
            " worst",
            " surface",
            ",",
            " his",
            " first",
            " major",
            " win",
            " was",
            " at",
            " the",
            " slick",
            " grass",
            " of",
            " Wimbledon",
            " in",
            " ",
            "199",
            "2",
            ",",
            " a",
            " tournament"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.812,
            0.183,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Document",
            "ary",
            " category",
            " for",
            " that",
            " award",
            ",",
            " and",
            " Animation",
            ",",
            " Narrative",
            ",",
            " Alternative",
            ",",
            " or",
            " International",
            " for",
            " the",
            " other",
            " awards",
            ").",
            " The",
            " requirements",
            " for",
            " the",
            " qualifying",
            " theatrical",
            " run",
            " are",
            " also",
            " different",
            " from",
            " those",
            " for",
            " other",
            " awards",
            ".",
            " Only",
            " one",
            " screening",
            " per",
            " day",
            " is",
            " required",
            ".",
            " For",
            " the",
            " Documentary",
            " award",
            ",",
            " the",
            " screening",
            " must",
            " start",
            " between",
            " noon",
            " and",
            " ",
            "10",
            "Âł",
            "pm",
            " local",
            " time"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.809,
            0.124,
            -0.0,
            0.205,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.1,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.112,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Print",
            "able",
            " characters",
            "Codes",
            " ",
            "20",
            "hex",
            " to",
            " ",
            "7",
            "E",
            "hex",
            ",",
            " known",
            " as",
            " the",
            " printable",
            " characters",
            ",",
            " represent",
            " letters",
            ",",
            " digits",
            ",",
            " punctuation",
            " marks",
            ",",
            " and",
            " a",
            " few",
            " miscellaneous",
            " symbols",
            ".",
            " There",
            " are",
            " ",
            "95",
            " printable",
            " characters",
            " in",
            " total",
            ".",
            "Code",
            " ",
            "20",
            "hex",
            ",",
            " the",
            " \"",
            "space",
            "\"",
            " character",
            ",",
            " denotes",
            " the",
            " space",
            " between",
            " words",
            ",",
            " as",
            " produced",
            " by"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.809,
            -0.0,
            0.088,
            0.137,
            0.314,
            -0.0,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            0.268
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Print",
            " ",
            " E",
            "-book",
            "External",
            " links",
            "All",
            "an",
            " D",
            "wan",
            " profile",
            ",",
            " virtual",
            "-history",
            ".com",
            ";",
            " accessed",
            " June",
            " ",
            "16",
            ",",
            " ",
            "201",
            "4",
            "188",
            "5",
            " births",
            "198",
            "1",
            " deaths",
            "20",
            "th",
            "-century",
            " American",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " American",
            " screen",
            "writers",
            "American",
            " film",
            " directors",
            "American",
            " film",
            " producers",
            "American",
            " male",
            " screen",
            "writers",
            "Bur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.801,
            0.175,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            0.085,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ar",
            "uba",
            " boasts",
            " a",
            " diverse",
            " culture",
            ".",
            " According",
            " to",
            " the",
            " Bureau",
            " Burg",
            "elijke",
            " Stand",
            " en",
            " Be",
            "vol",
            "k",
            "ings",
            "register",
            " (",
            "B",
            "BS",
            "B",
            ",",
            " Civil",
            " Registry",
            " and",
            " Population",
            " Register",
            "),",
            " in",
            " ",
            "200",
            "5",
            ",",
            " the",
            " island",
            " was",
            " home",
            " to",
            " pe",
            "ope",
            " from",
            " ninety",
            "-two",
            " different",
            " national",
            "ities",
            ".",
            " Dutch",
            " influence",
            " is",
            " still",
            " evident",
            " in",
            " traditions",
            " like",
            " the",
            " celebration",
            " of",
            " S",
            "inter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.801,
            0.334,
            -0.0,
            -0.0,
            -0.0,
            0.239,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Con",
            "cent",
            "rate",
            " (",
            ",",
            " Kont",
            "sent",
            "rat",
            ")",
            " is",
            " a",
            " never",
            "-f",
            "il",
            "med",
            " ",
            "195",
            "8",
            " screenplay",
            " by",
            " T",
            "ark",
            "ovsky",
            ".",
            " The",
            " screenplay",
            " is",
            " based",
            " on",
            " T",
            "ark",
            "ovsky",
            "'s",
            " year",
            " in",
            " the",
            " ta",
            "iga",
            " as",
            " a",
            " member",
            " of",
            " a",
            " research",
            " expedition",
            ",",
            " prior",
            " to",
            " his",
            " enrollment",
            " in",
            " film",
            " school",
            ".",
            " It",
            "'s",
            " about",
            " the",
            " leader",
            " of",
            " a",
            " geological",
            " expedition"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.801,
            0.223,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ar",
            "ab",
            ").",
            "Modern",
            " Standard",
            " Arabic",
            " (",
            "MS",
            "A",
            ")",
            " largely",
            " follows",
            " the",
            " gramm",
            "atical",
            " standards",
            " of",
            " Classical",
            " Arabic",
            " and",
            " uses",
            " much",
            " of",
            " the",
            " same",
            " vocabulary",
            ".",
            " However",
            ",",
            " it",
            " has",
            " discarded",
            " some",
            " gramm",
            "atical",
            " constructions",
            " and",
            " vocabulary",
            " that",
            " no",
            " longer",
            " have",
            " any",
            " counterpart",
            " in",
            " the",
            " spoken",
            " varieties",
            " and",
            " has",
            " adopted",
            " certain",
            " new",
            " constructions",
            " and",
            " vocabulary",
            " from",
            " the",
            " spoken",
            " varieties",
            ".",
            " Much",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.801,
            0.12,
            0.061,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            0.01,
            -0.0,
            0.131,
            -0.0,
            0.114,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ar",
            "men",
            "ian",
            " TV",
            " series",
            "),",
            " a",
            " ",
            "201",
            "7",
            " melod",
            "rama",
            " series",
            " Alien",
            " (",
            "sc",
            "ulpt",
            "ure",
            "),",
            " a",
            " ",
            "201",
            "2",
            " work",
            " by",
            " David",
            " Bre",
            "uer",
            "-",
            "We",
            "il",
            ",",
            " in",
            " M",
            "ott",
            "is",
            "font",
            ",",
            " Hampshire",
            ",",
            " England",
            " Ali",
            "ens",
            " (",
            "Dark",
            " Horse",
            " Comics",
            " line",
            ")",
            " The",
            " Ali",
            "ens",
            " (",
            "TV",
            " series",
            "),",
            " ",
            "201",
            "6",
            " British",
            " sci"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Acc",
            "using",
            " Evidence",
            " (",
            "191",
            "6",
            ")",
            "Pan",
            "the",
            "a",
            " (",
            "191",
            "7",
            ")",
            "A",
            " Modern",
            " Musk",
            "ete",
            "er",
            " (",
            "191",
            "7",
            ")",
            "Bound",
            " in",
            " Morocco",
            " (",
            "191",
            "8",
            ")",
            "Head",
            "in",
            "'",
            " South",
            " (",
            "191",
            "8",
            ")",
            "Mr",
            ".",
            " Fix",
            "-",
            "It",
            " (",
            "191",
            "8",
            ")",
            "He",
            " Comes",
            " Up",
            " Sm",
            "iling",
            " (",
            "191",
            "8",
            ")",
            "Che",
            "ating",
            " Che",
            "aters",
            " (",
            "191",
            "9"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.793,
            0.059,
            0.074,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            0.112,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "NH",
            "3",
            " âĨĴ",
            " Na",
            "+",
            " +",
            " e",
            "(N",
            "H",
            "3",
            ")x",
            "âĪĴ",
            "Due",
            " to",
            " the",
            " presence",
            " of",
            " sol",
            "v",
            "ated",
            " electrons",
            ",",
            " these",
            " solutions",
            " are",
            " very",
            " powerful",
            " reducing",
            " agents",
            " used",
            " in",
            " organic",
            " synthesis",
            ".",
            "Reaction",
            " ",
            "1",
            ")",
            " is",
            " known",
            " as",
            " Birch",
            " reduction",
            ".",
            "Other",
            " reductions",
            " that",
            " can",
            " be",
            " carried",
            " by",
            " these",
            " solutions",
            " are",
            ":",
            "S",
            "8",
            " +",
            " ",
            "2",
            "e",
            "âĪĴ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Sid",
            "well",
            " ",
            "201",
            "5",
            "b",
            ")",
            " suggests",
            " that",
            " Aust",
            "ro",
            "asi",
            "atic",
            " branches",
            " may",
            " have",
            " a",
            " loosely",
            " nested",
            " structure",
            " rather",
            " than",
            " a",
            " completely",
            " rake",
            "-like",
            " structure",
            ",",
            " with",
            " an",
            " east",
            "–",
            "west",
            " division",
            " (",
            "cons",
            "isting",
            " of",
            " M",
            "unda",
            ",",
            " K",
            "has",
            "ic",
            ",",
            " P",
            "ala",
            "ung",
            "ic",
            ",",
            " and",
            " Kh",
            "mu",
            "ic",
            " forming",
            " a",
            " western",
            " group",
            " as",
            " opposed",
            " to",
            " all",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.789,
            0.266,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Scient",
            "ific",
            " research",
            "Liquid",
            " arg",
            "on",
            " is",
            " used",
            " as",
            " the",
            " target",
            " for",
            " neutr",
            "ino",
            " experiments",
            " and",
            " direct",
            " dark",
            " matter",
            " searches",
            ".",
            " The",
            " interaction",
            " between",
            " the",
            " hypothetical",
            " W",
            "IM",
            "Ps",
            " and",
            " an",
            " arg",
            "on",
            " nucleus",
            " produces",
            " sc",
            "int",
            "illation",
            " light",
            " that",
            " is",
            " detected",
            " by",
            " phot",
            "om",
            "ulti",
            "plier",
            " tubes",
            ".",
            " Two",
            "-phase",
            " detectors",
            " containing",
            " arg",
            "on",
            " gas",
            " are",
            " used",
            " to",
            " detect",
            " the",
            " ion"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Media",
            " franchises",
            " ",
            "In",
            " Japanese",
            " culture",
            " and",
            " entertainment",
            ",",
            " media",
            " mix",
            " is",
            " a",
            " strategy",
            " to",
            " dis",
            "perse",
            " content",
            " across",
            " multiple",
            " representations",
            ":",
            " different",
            " broadcast",
            " media",
            ",",
            " gaming",
            " technologies",
            ",",
            " cell",
            " phones",
            ",",
            " toys",
            ",",
            " amusement",
            " parks",
            ",",
            " and",
            " other",
            " methods",
            ".",
            " It",
            " is",
            " the",
            " Japanese",
            " term",
            " for",
            " a",
            " trans",
            "media",
            " franchise",
            ".",
            " The",
            " term",
            " gained",
            " its",
            " circulation",
            " in",
            " late",
            " ",
            "198",
            "0",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Histor",
            "ic",
            " motors",
            "port",
            " or",
            " vintage",
            " motors",
            "port",
            " uses",
            " vehicles",
            " limited",
            " to",
            " a",
            " particular",
            " era",
            ".",
            " Only",
            " safety",
            " precautions",
            " are",
            " modern",
            "ized",
            " in",
            " these",
            " hobby",
            "ist",
            " races",
            ".",
            " A",
            " historical",
            " event",
            " can",
            " be",
            " of",
            " various",
            " types",
            " of",
            " motors",
            "port",
            " disciplines",
            ",",
            " from",
            " road",
            " racing",
            " to",
            " rallying",
            ".",
            " Because",
            " it",
            " is",
            " based",
            " on",
            " a",
            " particular",
            " era",
            " it",
            " is",
            " more",
            " hobby",
            "ist",
            "-oriented",
            ",",
            " reducing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Histor",
            "ic",
            " motors",
            "port",
            " or",
            " vintage",
            " motors",
            "port",
            " uses",
            " vehicles",
            " limited",
            " to",
            " a",
            " particular",
            " era",
            ".",
            " Only",
            " safety",
            " precautions",
            " are",
            " modern",
            "ized",
            " in",
            " these",
            " hobby",
            "ist",
            " races",
            ".",
            " A",
            " historical",
            " event",
            " can",
            " be",
            " of",
            " various",
            " types",
            " of",
            " motors",
            "port",
            " disciplines",
            ",",
            " from",
            " road",
            " racing",
            " to",
            " rallying",
            ".",
            " Because",
            " it",
            " is",
            " based",
            " on",
            " a",
            " particular",
            " era",
            " it",
            " is",
            " more",
            " hobby",
            "ist",
            "-oriented",
            ",",
            " reducing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            0.266,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Scient",
            "ific",
            " research",
            "Liquid",
            " arg",
            "on",
            " is",
            " used",
            " as",
            " the",
            " target",
            " for",
            " neutr",
            "ino",
            " experiments",
            " and",
            " direct",
            " dark",
            " matter",
            " searches",
            ".",
            " The",
            " interaction",
            " between",
            " the",
            " hypothetical",
            " W",
            "IM",
            "Ps",
            " and",
            " an",
            " arg",
            "on",
            " nucleus",
            " produces",
            " sc",
            "int",
            "illation",
            " light",
            " that",
            " is",
            " detected",
            " by",
            " phot",
            "om",
            "ulti",
            "plier",
            " tubes",
            ".",
            " Two",
            "-phase",
            " detectors",
            " containing",
            " arg",
            "on",
            " gas",
            " are",
            " used",
            " to",
            " detect",
            " the",
            " ion"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "BA",
            " songs",
            ".",
            " In",
            " June",
            " ",
            "199",
            "2",
            ",",
            " he",
            " and",
            " Ul",
            "vae",
            "us",
            " appeared",
            " with",
            " U",
            "2",
            " at",
            " a",
            " Stockholm",
            " concert",
            ",",
            " singing",
            " the",
            " chorus",
            " of",
            " \"",
            "D",
            "ancing",
            " Queen",
            "\",",
            " and",
            " a",
            " few",
            " years",
            " later",
            " during",
            " the",
            " final",
            " performance",
            " of",
            " the",
            " B",
            " &",
            " B",
            " in",
            " Concert",
            " in",
            " Stockholm",
            ",",
            " Anders",
            "son",
            " joined",
            " the",
            " cast",
            " for",
            " an",
            " encore",
            " at",
            " the",
            " piano",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "BA",
            " members",
            " reunited",
            " at",
            " E",
            "aling",
            " Studios",
            " in",
            " London",
            " to",
            " continue",
            " working",
            " on",
            " the",
            " avatar",
            " project",
            " and",
            " filming",
            " for",
            " the",
            " tour",
            ".",
            " Ul",
            "vae",
            "us",
            " confirmed",
            " that",
            " the",
            " avatar",
            " tour",
            " would",
            " be",
            " scheduled",
            " for",
            " ",
            "202",
            "2",
            ".",
            " When",
            " questioned",
            " if",
            " the",
            " new",
            " recordings",
            " were",
            " definitely",
            " coming",
            " out",
            " in",
            " ",
            "202",
            "1",
            ",",
            " Bj",
            "Ã¶r",
            "n",
            " said",
            " \"",
            "There",
            " will",
            " be",
            " new",
            " music"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.099,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "BA",
            " songs",
            ".",
            " Al",
            "ain",
            " and",
            " Daniel",
            " Bou",
            "bl",
            "il",
            ",",
            " who",
            " wrote",
            " Les",
            " Mis",
            "Ã©r",
            "ables",
            ",",
            " had",
            " been",
            " in",
            " touch",
            " with",
            " St",
            "ig",
            " Anderson",
            " about",
            " the",
            " project",
            ",",
            " and",
            " the",
            " TV",
            " musical",
            " was",
            " aired",
            " over",
            " Christmas",
            " on",
            " French",
            " TV",
            " and",
            " later",
            " a",
            " Dutch",
            " version",
            " was",
            " also",
            " broadcast",
            ".",
            " Bou",
            "bl",
            "il",
            " previously",
            " also",
            " wrote",
            " the",
            " French",
            " lyric",
            " for",
            " M",
            "ire",
            "ille"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "BA",
            "'s",
            " nine",
            " number",
            "-one",
            " singles",
            " in",
            " Germany",
            ",",
            " this",
            " being",
            " in",
            " December",
            " ",
            "198",
            "1",
            ";",
            " and",
            " the",
            " sw",
            "ans",
            "ong",
            " of",
            " their",
            " sixteen",
            " Top",
            " ",
            "5",
            " singles",
            " on",
            " the",
            " South",
            " African",
            " chart",
            ".",
            " \"",
            "One",
            " of",
            " Us",
            "\"",
            " was",
            " also",
            " AB",
            "BA",
            "'s",
            " final",
            " Top",
            " ",
            "3",
            " hit",
            " in",
            " the",
            " UK",
            ",",
            " reaching",
            " number",
            "-three",
            " on",
            " the",
            " UK",
            " Singles",
            " Chart",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            0.049,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.099,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "RG",
            ")",
            " The",
            " Beg",
            "u",
            "iled",
            " (",
            "197",
            "1",
            ",",
            " US",
            ")",
            " The",
            " Out",
            "law",
            " Jose",
            "y",
            " Wales",
            " (",
            "197",
            "6",
            ",",
            " US",
            ")",
            " Glory",
            " (",
            "198",
            "9",
            ",",
            " US",
            ")",
            " The",
            " Civil",
            " War",
            " (",
            "199",
            "0",
            ",",
            " US",
            ")",
            " Get",
            "t",
            "ys",
            "burg",
            " (",
            "199",
            "3",
            ",",
            " US",
            ")",
            " The",
            " Last",
            " Out",
            "law",
            " (",
            "199",
            "3",
            ",",
            " US",
            ")",
            " Cold",
            " Mountain",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.206,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "DI",
            " Clean",
            " Diesel",
            ".",
            " In",
            " May",
            " ",
            "201",
            "2",
            ",",
            " Audi",
            " reported",
            " a",
            " ",
            "10",
            "%",
            " increase",
            " in",
            " its",
            " sales",
            "—from",
            " ",
            "408",
            " units",
            " to",
            " ",
            "480",
            " in",
            " the",
            " last",
            " year",
            " alone",
            ".",
            "A",
            "udi",
            " manufactures",
            " vehicles",
            " in",
            " seven",
            " plants",
            " around",
            " the",
            " world",
            ",",
            " some",
            " of",
            " which",
            " are",
            " shared",
            " with",
            " other",
            " VW",
            " Group",
            " mar",
            "ques",
            " although",
            " many",
            " sub",
            "-as",
            "semblies",
            " such",
            " as",
            " engines"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            0.272,
            0.193,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.149,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Th",
            "erm",
            "odynamic",
            " fluctuations",
            " and",
            " statistical",
            " physics",
            " ",
            "E",
            "instein",
            "'s",
            " first",
            " paper",
            " submitted",
            " in",
            " ",
            "190",
            "0",
            " to",
            " Ann",
            "alen",
            " der",
            " Phys",
            "ik",
            " was",
            " on",
            " cap",
            "illary",
            " attraction",
            ".",
            " It",
            " was",
            " published",
            " in",
            " ",
            "190",
            "1",
            " with",
            " the",
            " title",
            " \"",
            "F",
            "ol",
            "ger",
            "ungen",
            " aus",
            " den",
            " Cap",
            "ill",
            "ar",
            "itÃ¤",
            "ts",
            "ers",
            "chein",
            "ungen",
            "\",",
            " which",
            " translates",
            " as",
            " \"",
            "Con",
            "clusions",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Anc",
            "ient",
            " Egyptian",
            " physicians",
            " were",
            " renowned",
            " in",
            " the",
            " ancient",
            " Near",
            " East",
            " for",
            " their",
            " healing",
            " skills",
            ",",
            " and",
            " some",
            ",",
            " such",
            " as",
            " Im",
            "hot",
            "ep",
            ",",
            " remained",
            " famous",
            " long",
            " after",
            " their",
            " deaths",
            ".",
            " Her",
            "od",
            "ot",
            "us",
            " remarked",
            " that",
            " there",
            " was",
            " a",
            " high",
            " degree",
            " of",
            " specialization",
            " among",
            " Egyptian",
            " physicians",
            ",",
            " with",
            " some",
            " treating",
            " only",
            " the",
            " head",
            " or",
            " the",
            " stomach",
            ",",
            " while",
            " others",
            " were",
            " eye"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            0.132,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            0.126,
            -0.0,
            -0.0,
            0.287,
            0.108,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            0.172,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            0.107,
            -0.0,
            -0.0,
            0.101,
            -0.0,
            -0.0,
            0.101,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Anc",
            "ient",
            " Greek",
            " political",
            " philosophers",
            "Anc",
            "ient",
            " Greek",
            " political",
            " refugees",
            "Anc",
            "ient",
            " Greek",
            " philosophers",
            " of",
            " art",
            "Anc",
            "ient",
            " literary",
            " critics",
            "Anc",
            "ient",
            " St",
            "ag",
            "ir",
            "ites",
            "A",
            "ph",
            "or",
            "ists",
            "A",
            "rist",
            "otel",
            "ian",
            " philosophers",
            "Att",
            "ic",
            " Greek",
            " writers",
            "Anc",
            "ient",
            " Greek",
            " cosm",
            "ologists",
            "Greek",
            " male",
            " writers",
            "Greek",
            " ge",
            "ologists",
            "Greek",
            " meteor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            -0.0,
            0.231,
            0.083,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "SA",
            ".",
            "V",
            "ow",
            "els",
            " ",
            "Modern",
            " Standard",
            " Arabic",
            " has",
            " six",
            " pure",
            " vowels",
            " (",
            "while",
            " most",
            " modern",
            " dialect",
            "s",
            " have",
            " eight",
            " pure",
            " vowels",
            " which",
            " includes",
            " the",
            " long",
            " vowels",
            " ),",
            " with",
            " short",
            " ",
            " and",
            " corresponding",
            " long",
            " vowels",
            " .",
            " There",
            " are",
            " also",
            " two",
            " d",
            "iph",
            "th",
            "ongs",
            ":",
            " ",
            " and",
            " .",
            "The",
            " pronunciation",
            " of",
            " the",
            " vowels",
            " differs",
            " from",
            " speaker",
            " to",
            " speaker",
            ",",
            " in",
            " a",
            " way"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            0.219,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            0.063,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.089,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Th",
            "ailand",
            ")",
            " (",
            "Th",
            "ailand",
            ")",
            " Pan",
            " American",
            " Day",
            " (",
            "se",
            "veral",
            " countries",
            " in",
            " the",
            " Americas",
            ")",
            " The",
            " first",
            " day",
            " of",
            " Tak",
            "ay",
            "ama",
            " Spring",
            " Festival",
            " (",
            "Tak",
            "ay",
            "ama",
            ",",
            " G",
            "ifu",
            ",",
            " Japan",
            ")",
            " V",
            "ais",
            "akh",
            " (",
            "P",
            "un",
            "jab",
            " (",
            "region",
            ")),",
            " (",
            "India",
            " and",
            " Pakistan",
            ")",
            " Youth",
            " Day",
            " (",
            "Ang",
            "ola",
            ")",
            " April",
            " ",
            "15",
            " Day"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.781,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.205,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "SA",
            ".",
            " Arabic",
            " vern",
            "acular",
            "s",
            " do",
            " not",
            " descend",
            " from",
            " M",
            "SA",
            " or",
            " Classical",
            " Arabic",
            ".",
            " Combined",
            ",",
            " Arabic",
            " dialect",
            "s",
            " have",
            " ",
            "362",
            " million",
            " native",
            " speakers",
            ",",
            " while",
            " M",
            "SA",
            " is",
            " spoken",
            " by",
            " ",
            "274",
            " million",
            " L",
            "2",
            " speakers",
            ",",
            " making",
            " it",
            " the",
            " sixth",
            " most",
            " spoken",
            " language",
            " in",
            " the",
            " world",
            ",",
            " and",
            " the",
            " most",
            " spoken",
            " that",
            " is",
            " neither",
            " Chinese",
            " nor",
            " Indo",
            "-European"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.754,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "-Al",
            "ger",
            "ian",
            " relations",
            " and",
            " sparked",
            " the",
            " Alger",
            "ian",
            " War",
            " which",
            " concluded",
            " with",
            " Algeria",
            " gaining",
            " its",
            " independence",
            " on",
            " ",
            "5",
            " July",
            " ",
            "196",
            "2",
            " and",
            " the",
            " proclamation",
            " of",
            " the",
            " People",
            "'s",
            " Democratic",
            " Republic",
            " on",
            " ",
            "25",
            " September",
            " of",
            " that",
            " year",
            ".",
            "The",
            " official",
            " languages",
            " of",
            " Algeria",
            " are",
            " Arabic",
            " and",
            " Ber",
            "ber",
            ".",
            " The",
            " majority",
            " of",
            " Algeria",
            "'s",
            " population",
            " is",
            " Arab",
            ",",
            " practicing",
            " Islam"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.75,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Al",
            "pine",
            " plants",
            " such",
            " as",
            " the",
            " Alpine",
            " gent",
            "ian",
            " grow",
            " in",
            " abundance",
            " in",
            " areas",
            " such",
            " as",
            " the",
            " me",
            "adows",
            " above",
            " the",
            " L",
            "aut",
            "er",
            "br",
            "unn",
            "ental",
            ".",
            " Gent",
            "ians",
            " are",
            " named",
            " after",
            " the",
            " Il",
            "ly",
            "rian",
            " king",
            " Gent",
            "ius",
            ",",
            " and",
            " ",
            "40",
            " species",
            " of",
            " the",
            " early",
            "-s",
            "pring",
            " blo",
            "oming",
            " flower",
            " grow",
            " in",
            " the",
            " Alps",
            ",",
            " in",
            " a",
            " range",
            " of",
            " ."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.742,
            0.097,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Fr",
            "anks",
            " asserts",
            " without",
            " supporting",
            " evidence",
            " that",
            " most",
            " major",
            " forms",
            " of",
            " individual",
            "ist",
            " anarch",
            "ism",
            " have",
            " been",
            " largely",
            " an",
            "ar",
            "cho",
            "-capital",
            "ist",
            " in",
            " content",
            ",",
            " and",
            " concludes",
            " from",
            " this",
            " premise",
            " that",
            " most",
            " forms",
            " of",
            " individual",
            "ism",
            " are",
            " incompatible",
            " with",
            " anarch",
            "ism",
            "\".",
            " Davis",
            " argues",
            " that",
            " \"",
            "the",
            " conclusion",
            " is",
            " unsustainable",
            " because",
            " the",
            " premise",
            " is",
            " false",
            ",",
            " depending",
            " as",
            " it",
            " does",
            " for",
            " any"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.644,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ",",
            " she",
            " participated",
            " in",
            " three",
            " events",
            " organized",
            " by",
            " El",
            "ton",
            " John",
            " and",
            " by",
            " fellow",
            " tennis",
            " players",
            " Serena",
            " Williams",
            " and",
            " Andy",
            " Rod",
            "d",
            "ick",
            ".",
            " In",
            " January",
            " ",
            "200",
            "5",
            ",",
            " she",
            " played",
            " in",
            " a",
            " doubles",
            " charity",
            " event",
            " for",
            " the",
            " Indian",
            " Ocean",
            " tsunami",
            " with",
            " John",
            " Mc",
            "En",
            "roe",
            ",",
            " Andy",
            " Rod",
            "d",
            "ick",
            ",",
            " and",
            " Chris",
            " E",
            "vert",
            ".",
            " In",
            " November",
            " ",
            "200"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.644,
            0.236,
            0.295,
            0.006,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.079,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " –",
            " Deng",
            " Xia",
            "oping",
            ",",
            " Chinese",
            " soldier",
            " and",
            " politician",
            ",",
            " ",
            "1",
            "st",
            " Vice",
            " Premier",
            " of",
            " the",
            " People",
            "'s",
            " Republic",
            " of",
            " China",
            " (",
            "d",
            ".",
            " ",
            "199",
            "7",
            ")",
            "190",
            "8",
            " –",
            " Henri",
            " Cart",
            "ier",
            "-B",
            "ress",
            "on",
            ",",
            " French",
            " photographer",
            " and",
            " painter",
            " (",
            "d",
            ".",
            " ",
            "200",
            "4",
            ")",
            " ",
            " ",
            "190",
            "8",
            "  ",
            " –",
            " Er",
            "win",
            " Th",
            "ies",
            "ies",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.586,
            0.113,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "11",
            " November",
            " ",
            "202",
            "1",
            " that",
            " Afghanistan",
            " was",
            " facing",
            " widespread",
            " famine",
            " due",
            " to",
            " an",
            " economic",
            " and",
            " banking",
            " crisis",
            ".",
            " The",
            " Taliban",
            " have",
            " significantly",
            " tackled",
            " corruption",
            ",",
            " now",
            " being",
            " placed",
            " as",
            " ",
            "150",
            "th",
            " on",
            " the",
            " corruption",
            " watchdog",
            " perception",
            " index",
            ".",
            " The",
            " Taliban",
            " have",
            " also",
            " reportedly",
            " reduced",
            " bribery",
            " and",
            " extortion",
            " in",
            " public",
            " service",
            " areas",
            ".",
            " At",
            " the",
            " same",
            " time",
            ",",
            " the",
            " human",
            " rights",
            " situation"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.578,
            0.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "-looking",
            " humanoid",
            " robots",
            "),",
            " a",
            " robot",
            " with",
            " a",
            " female",
            " appearance",
            " can",
            " also",
            " be",
            " referred",
            " to",
            " as",
            " a",
            " g",
            "yn",
            "oid",
            ".",
            " Besides",
            " one",
            " can",
            " refer",
            " to",
            " robots",
            " without",
            " all",
            "uding",
            " to",
            " their",
            " sexual",
            " appearance",
            " by",
            " calling",
            " them",
            " anth",
            "robots",
            " (",
            "a",
            " port",
            "m",
            "ante",
            "au",
            " of",
            " anth",
            "r",
            "Åį",
            "pos",
            " and",
            " robot",
            ";",
            " see",
            " anth",
            "robot",
            "ics",
            ")",
            " or",
            " anthrop",
            "oids",
            " (",
            "short"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.57,
            0.175,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "193",
            "7",
            ")",
            "201",
            "7",
            " –",
            " Don",
            " Rick",
            "les",
            ",",
            " American",
            " actor",
            " and",
            " comedian",
            " (",
            "b",
            ".",
            " ",
            "192",
            "6",
            ")",
            "201",
            "9",
            " –",
            " Michael",
            " O",
            "'D",
            "onn",
            "ell",
            ",",
            " British",
            " physician",
            ",",
            " journalist",
            ",",
            " author",
            " and",
            " broadcaster",
            " (",
            "b",
            ".",
            " ",
            "192",
            "8",
            ")",
            " ",
            "202",
            "0",
            " –",
            " Al",
            " Kal",
            "ine",
            ",",
            " American",
            " baseball",
            " player",
            ",",
            " broadcaster",
            " and",
            " executive",
            " (",
            "b",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Romans",
            " built",
            " settlements",
            " in",
            " the",
            " Alps",
            ".",
            " In",
            " towns",
            " such",
            " as",
            " A",
            "osta",
            ",",
            " Mart",
            "ign",
            "y",
            ",",
            " La",
            "us",
            "anne",
            ",",
            " and",
            " Part",
            "en",
            "kir",
            "chen",
            " remains",
            " of",
            " vill",
            "as",
            ",",
            " arenas",
            ",",
            " and",
            " temples",
            " have",
            " been",
            " discovered",
            ".",
            "Christian",
            "ity",
            ",",
            " feudal",
            "ism",
            ",",
            " and",
            " Nap",
            "ole",
            "onic",
            " wars",
            " ",
            "Christian",
            "ity",
            " was",
            " established",
            " in",
            " the",
            " Alps",
            " by",
            " the",
            " Roman",
            " people"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.547,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Media",
            ",",
            " Par",
            "th",
            "ia",
            ",",
            " A",
            "ria",
            " (",
            "West",
            " Afghanistan",
            "),",
            " Dr",
            "ang",
            "iana",
            ",",
            " Ar",
            "ach",
            "os",
            "ia",
            " (",
            "South",
            " and",
            " Central",
            " Afghanistan",
            "),",
            " B",
            "act",
            "ria",
            " (",
            "North",
            " and",
            " Central",
            " Afghanistan",
            "),",
            " and",
            " Sc",
            "yth",
            "ia",
            ".",
            "In",
            " ",
            "329",
            " BC",
            ",",
            " Spit",
            "am",
            "enes",
            ",",
            " who",
            " held",
            " an",
            " undefined",
            " position",
            " in",
            " the",
            " sat",
            "rapy",
            " of",
            " S",
            "og",
            "d",
            "iana"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.434,
            0.132,
            -0.0,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.195,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.073,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.168,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "194",
            "6",
            ")",
            " The",
            " Un",
            "con",
            "qu",
            "ered",
            " (",
            "per",
            "formed",
            " ",
            "194",
            "0",
            ",",
            " published",
            " ",
            "201",
            "4",
            ")",
            " The",
            " Fountain",
            "head",
            " (",
            "194",
            "3",
            ")",
            " Atlas",
            " Shr",
            "ugged",
            " (",
            "195",
            "7",
            ")",
            " The",
            " Early",
            " A",
            "yn",
            " Rand",
            " (",
            "198",
            "4",
            ")",
            " Ideal",
            " (",
            "201",
            "5",
            ")",
            "Non",
            "-fiction",
            ":",
            " Pol",
            "a",
            " Neg",
            "ri",
            " (",
            "192",
            "5",
            ")",
            " For",
            " the",
            " New",
            " Intellectual"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.434,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ic",
            " (\"",
            "Tr",
            "anse",
            "uras",
            "ian",
            "\")",
            " speakers",
            " would",
            " have",
            " migrated",
            " south",
            " into",
            " the",
            " modern",
            " Lia",
            "oning",
            " province",
            ",",
            " where",
            " they",
            " would",
            " have",
            " been",
            " mostly",
            " assim",
            "ilated",
            " by",
            " an",
            " agricultural",
            " community",
            " with",
            " an",
            " Austr",
            "ones",
            "ian",
            "-like",
            " language",
            ".",
            " The",
            " fusion",
            " of",
            " the",
            " two",
            " languages",
            " would",
            " have",
            " resulted",
            " in",
            " proto",
            "-J",
            "apanese",
            " and",
            " proto",
            "-K",
            "orean",
            ".",
            "In",
            " a",
            " typ",
            "ological",
            " study",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.393,
            0.15,
            -0.0,
            0.215,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            0.235,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ichten",
            "stein",
            " (",
            "L",
            "ichten",
            "stein",
            " was",
            " also",
            " a",
            " friend",
            " whom",
            " he",
            " met",
            " at",
            " one",
            " of",
            " his",
            " mother",
            "'s",
            " parties",
            " in",
            " We",
            "imar",
            ").",
            "Early",
            " work",
            " ",
            "Sch",
            "openh",
            "auer",
            " left",
            " Berlin",
            " in",
            " a",
            " rush",
            " in",
            " ",
            "181",
            "3",
            ",",
            " fearing",
            " that",
            " the",
            " city",
            " could",
            " be",
            " attacked",
            " and",
            " that",
            " he",
            " could",
            " be",
            " pressed",
            " into",
            " military",
            " service",
            " as",
            " Pr",
            "ussia",
            " had",
            " just",
            " joined",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            0.381,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ists",
            ")",
            " opposed",
            " state",
            " socialism",
            ",",
            " advocating",
            " political",
            " abst",
            "ention",
            "ism",
            " and",
            " small",
            " property",
            " holdings",
            ".",
            " After",
            " bitter",
            " disputes",
            ",",
            " the",
            " Bak",
            "un",
            "in",
            "ists",
            " were",
            " expelled",
            " from",
            " the",
            " International",
            " by",
            " the",
            " Marx",
            "ists",
            " at",
            " the",
            " ",
            "187",
            "2",
            " Hague",
            " Congress",
            ".",
            " An",
            "arch",
            "ists",
            " were",
            " treated",
            " similarly",
            " in",
            " the",
            " Second",
            " International",
            ",",
            " being",
            " ultimately",
            " expelled",
            " in",
            " ",
            "189",
            "6",
            ".",
            " Bak",
            "un"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ed",
            " or",
            " t",
            "illed",
            ")",
            " regularly",
            ",",
            " generally",
            " under",
            " a",
            " system",
            " of",
            " crop",
            " rotation",
            "\".",
            " In",
            " Britain",
            ",",
            " ar",
            "able",
            " land",
            " has",
            " traditionally",
            " been",
            " contrast",
            "ed",
            " with",
            " past",
            "urable",
            " land",
            " such",
            " as",
            " he",
            "aths",
            ",",
            " which",
            " could",
            " be",
            " used",
            " for",
            " sheep",
            "-re",
            "aring",
            " but",
            " not",
            " as",
            " far",
            "mland",
            ".",
            "Ar",
            "able",
            " land",
            " is",
            " vulnerable",
            " to",
            " land",
            " degradation",
            " and",
            " some",
            " types",
            " of",
            " un",
            "-ar"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.085,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.12,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Apollo",
            " spoke",
            " again",
            " from",
            " the",
            " womb",
            ",",
            " asking",
            " Let",
            "o",
            " to",
            " take",
            " look",
            " at",
            " the",
            " floating",
            " island",
            " in",
            " front",
            " of",
            " her",
            " and",
            " expressing",
            " his",
            " wish",
            " to",
            " be",
            " born",
            " there",
            ".",
            " When",
            " Let",
            "o",
            " approached",
            " Aster",
            "ia",
            ",",
            " all",
            " the",
            " other",
            " islands",
            " fled",
            ".",
            " But",
            " Aster",
            "ia",
            " welcomed",
            " Let",
            "o",
            " without",
            " any",
            " fear",
            " of",
            " Hera",
            ".",
            " Walking",
            " on",
            " the",
            " island",
            ",",
            " she",
            " sat"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.291,
            0.194,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.071,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " virtue",
            " of",
            " being",
            " loosely",
            " based",
            " on",
            " CP",
            "/M",
            ",",
            " and",
            " Windows",
            " in",
            " turn",
            " inherited",
            " it",
            " from",
            " MS",
            "-D",
            "OS",
            ".",
            "Re",
            "quiring",
            " two",
            " characters",
            " to",
            " mark",
            " the",
            " end",
            " of",
            " a",
            " line",
            " introduces",
            " unnecessary",
            " complexity",
            " and",
            " ambiguity",
            " as",
            " to",
            " how",
            " to",
            " interpret",
            " each",
            " character",
            " when",
            " encountered",
            " by",
            " itself",
            ".",
            " To",
            " simplify",
            " matters",
            ",",
            " plain",
            " text",
            " data",
            " streams",
            ",",
            " including",
            " files",
            ",",
            " on",
            " Mult"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.25,
            0.04,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            0.168,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.149,
            0.135,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            0.131,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.085,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "0",
            ".",
            "1",
            " solar",
            " masses",
            " not",
            " associated",
            " with",
            " a",
            " young",
            " cluster",
            ".",
            "G",
            "J",
            " ",
            "347",
            "0",
            " c",
            " (",
            "202",
            "0",
            ")",
            " is",
            " the",
            " first",
            " ex",
            "oplan",
            "et",
            " candidate",
            " completely",
            " discovered",
            " by",
            " amateurs",
            ".",
            " Unlike",
            " Peter",
            " Jal",
            "ow",
            "icz",
            "or",
            ",",
            " K",
            "oj",
            "ima",
            "-",
            "1",
            "L",
            "b",
            " and",
            " XO",
            "-",
            "1",
            "b",
            ",",
            " G",
            "J",
            " ",
            "347",
            "0",
            " c",
            " was",
            " fully"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.149,
            0.135,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            0.131,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.085,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "0",
            ".",
            "1",
            " solar",
            " masses",
            " not",
            " associated",
            " with",
            " a",
            " young",
            " cluster",
            ".",
            "G",
            "J",
            " ",
            "347",
            "0",
            " c",
            " (",
            "202",
            "0",
            ")",
            " is",
            " the",
            " first",
            " ex",
            "oplan",
            "et",
            " candidate",
            " completely",
            " discovered",
            " by",
            " amateurs",
            ".",
            " Unlike",
            " Peter",
            " Jal",
            "ow",
            "icz",
            "or",
            ",",
            " K",
            "oj",
            "ima",
            "-",
            "1",
            "L",
            "b",
            " and",
            " XO",
            "-",
            "1",
            "b",
            ",",
            " G",
            "J",
            " ",
            "347",
            "0",
            " c",
            " was",
            " fully"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.175,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " number",
            " of",
            " low",
            "-profile",
            " penc",
            "iling",
            " jobs",
            " followed",
            " by",
            " a",
            " run",
            " on",
            " New",
            " Warriors",
            " in",
            " ",
            "199",
            "0",
            ".",
            " Bag",
            "ley",
            " penc",
            "iled",
            " the",
            " flagship",
            " Spider",
            "-Man",
            " title",
            " from",
            " ",
            "199",
            "1",
            " to",
            " ",
            "199",
            "6",
            ".",
            " During",
            " that",
            " time",
            ",",
            " Bag",
            "ley",
            "'s",
            " rendition",
            " of",
            " Spider",
            "-Man",
            " was",
            " used",
            " extensively",
            " for",
            " licensed",
            " material",
            " and",
            " merchandise",
            ".",
            "Issues",
            " #",
            "361",
            "–",
            "363",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " any",
            " ill",
            " or",
            " damage",
            " intended",
            " him",
            " without",
            " defending",
            " him",
            " there",
            "from",
            ".\"",
            " This",
            " was",
            " thought",
            " to",
            " favour",
            " the",
            " doctrine",
            " of",
            " absolute",
            " non",
            "-res",
            "istance",
            ",",
            " and",
            ",",
            " accordingly",
            ",",
            " the",
            " Convention",
            " Parliament",
            " enacted",
            " the",
            " form",
            " that",
            " has",
            " been",
            " in",
            " use",
            " since",
            " that",
            " time",
            " –",
            " \"",
            "I",
            " do",
            " sincerely",
            " promise",
            " and",
            " swear",
            " that",
            " I",
            " will",
            " be",
            " faithful",
            " and",
            " bear",
            " true",
            " allegiance",
            " to",
            " His",
            " Majesty"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.094,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " this",
            " community",
            " with",
            " various",
            " solutions",
            " to",
            " auditory",
            " communication",
            " needs",
            " by",
            " providing",
            " higher",
            " sound",
            " (",
            "for",
            " those",
            " who",
            " are",
            " hard",
            " of",
            " hearing",
            "),",
            " tactile",
            " feedback",
            ",",
            " visual",
            " cues",
            " and",
            " improved",
            " technology",
            " access",
            ".",
            " Individuals",
            " who",
            " are",
            " deaf",
            " or",
            " hard",
            " of",
            " hearing",
            " utilize",
            " a",
            " variety",
            " of",
            " assist",
            "ive",
            " technologies",
            " that",
            " provide",
            " them",
            " with",
            " different",
            " access",
            " to",
            " information",
            " in",
            " numerous",
            " environments",
            ".",
            " Most",
            " devices",
            " either",
            " provide"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "achuset",
    "/WebAPI",
    "shima",
    "å¯¸",
    "-REAL"
  ],
  "bottom_logits": [
    "otton",
    " Lloyd",
    " King",
    "ecn",
    "âĢ¦\n"
  ],
  "act_min": -0.0,
  "act_max": 0.91
}