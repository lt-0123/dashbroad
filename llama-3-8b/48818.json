{
  "index": 48818,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "rez",
            ",",
            " Salah",
            " Assad",
            " and",
            " Dj",
            "amel",
            " Z",
            "id",
            "ane",
            ".",
            " The",
            " Algeria",
            " national",
            " football",
            " team",
            " qualified",
            " for",
            " the",
            " ",
            "198",
            "2",
            " FIFA",
            " World",
            " Cup",
            ",",
            " ",
            "198",
            "6",
            " FIFA",
            " World",
            " Cup",
            ",",
            " ",
            "201",
            "0",
            " FIFA",
            " World",
            " Cup",
            " and",
            " ",
            "201",
            "4",
            " FIFA",
            " World",
            " Cup",
            ".",
            " In",
            " addition",
            ",",
            " several",
            " football",
            " clubs",
            " have",
            " won",
            " continental",
            " and",
            " international",
            " trophies",
            " as",
            " the",
            " club",
            " ES"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            " Dallas",
            ",",
            " Low",
            "nd",
            "es",
            ",",
            " Mare",
            "ngo",
            " and",
            " Perry",
            ".\"",
            "In",
            " ",
            "197",
            "2",
            ",",
            " for",
            " the",
            " first",
            " time",
            " since",
            " ",
            "190",
            "1",
            ",",
            " the",
            " legislature",
            " completed",
            " the",
            " congressional",
            " red",
            "istrict",
            "ing",
            " based",
            " on",
            " the",
            " dec",
            "ennial",
            " census",
            ".",
            " This",
            " benefited",
            " the",
            " urban",
            " areas",
            " that",
            " had",
            " developed",
            ",",
            " as",
            " well",
            " as",
            " all",
            " in",
            " the",
            " population",
            " who",
            " had",
            " been",
            " under",
            "represented",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            " Dallas",
            ",",
            " Low",
            "nd",
            "es",
            ",",
            " Mare",
            "ngo",
            " and",
            " Perry",
            ".\"",
            "In",
            " ",
            "197",
            "2",
            ",",
            " for",
            " the",
            " first",
            " time",
            " since",
            " ",
            "190",
            "1",
            ",",
            " the",
            " legislature",
            " completed",
            " the",
            " congressional",
            " red",
            "istrict",
            "ing",
            " based",
            " on",
            " the",
            " dec",
            "ennial",
            " census",
            ".",
            " This",
            " benefited",
            " the",
            " urban",
            " areas",
            " that",
            " had",
            " developed",
            ",",
            " as",
            " well",
            " as",
            " all",
            " in",
            " the",
            " population",
            " who",
            " had",
            " been",
            " under",
            "represented",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ",",
            " was",
            " renamed",
            " Cedar",
            " Crest",
            " Elementary",
            ".",
            " Johnston",
            " Middle",
            " School",
            " in",
            " Houston",
            ",",
            " Texas",
            ",",
            " was",
            " also",
            " renamed",
            " Meyer",
            "land",
            " Middle",
            " School",
            ".",
            " Three",
            " other",
            " elementary",
            " schools",
            " named",
            " for",
            " Confederate",
            " veterans",
            " were",
            " renamed",
            " simultaneously",
            ".",
            "See",
            " also",
            " Albert",
            " Sidney",
            " Johnston",
            " High",
            " School",
            ",",
            " a",
            " def",
            "unct",
            " public",
            " high",
            " school",
            " in",
            " Austin",
            ",",
            " Texas",
            " Statue",
            " of",
            " Albert",
            " Sidney"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "Music",
            " ",
            "War",
            "hol",
            " strongly",
            " influenced",
            " the",
            " new",
            " wave",
            "/p",
            "unk",
            " rock",
            " band",
            " De",
            "vo",
            ",",
            " as",
            " well",
            " as",
            " David",
            " Bowie",
            ".",
            " Bowie",
            " recorded",
            " a",
            " song",
            " called",
            " \"",
            "Andy",
            " War",
            "hol",
            "\"",
            " for",
            " his",
            " ",
            "197",
            "1",
            " album",
            " H",
            "unky",
            " D",
            "ory",
            ".",
            " Lou",
            " Reed",
            " wrote",
            " the",
            " song",
            " \"",
            "Andy",
            "'s",
            " Chest",
            "\",",
            " about",
            " Valerie",
            " Sol",
            "anas",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "Music",
            " ",
            "War",
            "hol",
            " strongly",
            " influenced",
            " the",
            " new",
            " wave",
            "/p",
            "unk",
            " rock",
            " band",
            " De",
            "vo",
            ",",
            " as",
            " well",
            " as",
            " David",
            " Bowie",
            ".",
            " Bowie",
            " recorded",
            " a",
            " song",
            " called",
            " \"",
            "Andy",
            " War",
            "hol",
            "\"",
            " for",
            " his",
            " ",
            "197",
            "1",
            " album",
            " H",
            "unky",
            " D",
            "ory",
            ".",
            " Lou",
            " Reed",
            " wrote",
            " the",
            " song",
            " \"",
            "Andy",
            "'s",
            " Chest",
            "\",",
            " about",
            " Valerie",
            " Sol",
            "anas",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.057,
            -0.0,
            0.017
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            " that",
            " of",
            " other",
            " industries",
            "\".",
            " Pro",
            "ponents",
            " believe",
            " that",
            " private",
            " systems",
            " of",
            " justice",
            " and",
            " defense",
            " already",
            " exist",
            ",",
            " naturally",
            " forming",
            " where",
            " the",
            " market",
            " is",
            " allowed",
            " to",
            " \"",
            "comp",
            "ens",
            "ate",
            " for",
            " the",
            " failure",
            " of",
            " the",
            " state",
            "\",",
            " namely",
            " private",
            " arbitration",
            ",",
            " security",
            " guards",
            ",",
            " neighborhood",
            " watch",
            " groups",
            " and",
            " so",
            " on",
            ".",
            " These",
            " private",
            " courts",
            " and",
            " police",
            " are",
            " sometimes",
            " referred",
            " to",
            " gener",
            "ically"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " Bran",
            "iff",
            "'s",
            " chairman",
            " and",
            " president",
            " Harding",
            " Lawrence",
            ",",
            " was",
            " representing",
            " the",
            " Dallas",
            "-based",
            " carrier",
            " at",
            " that",
            " time",
            ".",
            " Lois",
            " succeeded",
            " Wells",
            " Rich",
            " Greene",
            " Agency",
            " on",
            " December",
            " ",
            "1",
            ",",
            " ",
            "196",
            "8",
            ".",
            " The",
            " rights",
            " to",
            " War",
            "hol",
            "'s",
            " films",
            " for",
            " Bran",
            "iff",
            " and",
            " his",
            " signed",
            " contracts",
            " are",
            " owned",
            " by",
            " a",
            " private",
            " trust",
            " and",
            " are",
            " administered",
            " by",
            " Bran",
            "iff",
            " Airways",
            " Foundation"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.457,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "0",
            " T",
            "DI",
            ",",
            " Audi",
            " A",
            "3",
            " Sport",
            "back",
            " ",
            "2",
            ".",
            "0",
            " T",
            "DI",
            " with",
            " S",
            " tr",
            "onic",
            " transmission",
            ")",
            " travelling",
            " across",
            " the",
            " American",
            " continent",
            " from",
            " New",
            " York",
            " to",
            " Los",
            " Angeles",
            ",",
            " passing",
            " major",
            " cities",
            " like",
            " Chicago",
            ",",
            " Dallas",
            " and",
            " Las",
            " Vegas",
            " during",
            " the",
            " ",
            "13",
            " daily",
            " stages",
            ",",
            " as",
            " well",
            " as",
            " natural",
            " wonders",
            " including",
            " the",
            " Rocky",
            " Mountains",
            ",",
            " Death",
            " Valley"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.451,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " moved",
            " to",
            " Cah",
            "aba",
            " in",
            " Dallas",
            " County",
            ".",
            "C",
            "ah",
            "aba",
            ",",
            " now",
            " a",
            " ghost",
            " town",
            ",",
            " was",
            " the",
            " first",
            " permanent",
            " state",
            " capital",
            " from",
            " ",
            "182",
            "0",
            " to",
            " ",
            "182",
            "5",
            ".",
            " The",
            " Alabama",
            " Fever",
            " land",
            " rush",
            " was",
            " underway",
            " when",
            " the",
            " state",
            " was",
            " admitted",
            " to",
            " the",
            " Union",
            ",",
            " with",
            " settlers",
            " and",
            " land",
            " spec",
            "ulators",
            " pouring",
            " into",
            " the",
            " state",
            " to",
            " take",
            " advantage",
            " of",
            " fertile"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " entirely",
            " un",
            "ham",
            "pered",
            " by",
            " violence",
            " or",
            " threats",
            " of",
            " violence",
            "\"",
            " The",
            " system",
            " relies",
            " on",
            " contracts",
            " between",
            " individuals",
            " as",
            " the",
            " legal",
            " framework",
            " which",
            " would",
            " be",
            " enforced",
            " by",
            " private",
            " police",
            " and",
            " security",
            " forces",
            " as",
            " well",
            " as",
            " private",
            " arbitr",
            "ations",
            ".",
            "R",
            "oth",
            "bard",
            " argues",
            " that",
            " limited",
            " liability",
            " for",
            " corporations",
            " could",
            " also",
            " exist",
            " through",
            " contract",
            ",",
            " arguing",
            " that",
            " \"[",
            "c",
            "]",
            "orpor",
            "ations",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " World",
            " Cup",
            " last",
            " July",
            " at",
            " BC",
            " Place",
            " in",
            " Vancouver",
            ",",
            " British",
            " Columbia",
            ",",
            " Canada",
            " before",
            " an",
            " undis",
            "puted",
            " AT",
            "&T",
            " Stadium",
            " audience",
            " of",
            " ",
            "101",
            ",",
            "763",
            " to",
            " open",
            " Wrestle",
            "Man",
            "ia",
            " ",
            "32",
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "In",
            " ",
            "201",
            "7",
            ",",
            " Jackie",
            " Evan",
            "cho",
            " released",
            " Together",
            " We",
            " Stand",
            ",",
            " a",
            " disc",
            " containing",
            " three",
            " patriotic",
            " songs",
            " including",
            " \"",
            "America",
            " the",
            " Beautiful"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " the",
            " barbar",
            "ians",
            ",",
            " to",
            " look",
            " after",
            " the",
            " former",
            " as",
            " after",
            " friends",
            " and",
            " relatives",
            ",",
            " and",
            " to",
            " deal",
            " with",
            " the",
            " latter",
            " as",
            " with",
            " beasts",
            " or",
            " plants",
            "\".",
            " By",
            " ",
            "335",
            "Âł",
            "BC",
            ",",
            " Aristotle",
            " had",
            " returned",
            " to",
            " Athens",
            ",",
            " establishing",
            " his",
            " own",
            " school",
            " there",
            " known",
            " as",
            " the",
            " Ly",
            "ce",
            "um",
            ".",
            " Aristotle",
            " conducted",
            " courses",
            " at",
            " the",
            " school",
            " for",
            " the",
            " next",
            " twelve",
            " years"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "iled",
            " beds",
            " threaten",
            " to",
            " make",
            " barbar",
            "ians",
            " of",
            " us",
            " all",
            "\"",
            " they",
            " are",
            " not",
            " advancing",
            " a",
            " definition",
            " or",
            " theory",
            " about",
            " art",
            ",",
            " but",
            " questioning",
            " the",
            " value",
            " of",
            " H",
            "irst",
            "'s",
            " and",
            " Emin",
            "'s",
            " work",
            ".",
            " In",
            " ",
            "199",
            "8",
            ",",
            " Arthur",
            " D",
            "anto",
            ",",
            " suggested",
            " a",
            " thought",
            " experiment",
            " showing",
            " that",
            " \"",
            "the",
            " status",
            " of",
            " an",
            " artifact",
            " as",
            " work",
            " of",
            " art",
            " results",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " \"",
            "bar",
            "bar",
            "ous",
            "\"",
            " authors",
            " for",
            " \"",
            "fine",
            " powder",
            ".\"",
            " V",
            "igo",
            " wrote",
            ":",
            " \"",
            "the",
            " barbar",
            "ous",
            " a",
            "uct",
            "ours",
            " use",
            " alcohol",
            ",",
            " or",
            " (",
            "as",
            " I",
            " fy",
            "nde",
            " it",
            " som",
            "et",
            "ymes",
            " w",
            "ry",
            "ten",
            ")",
            " al",
            "co",
            "f",
            "oll",
            ",",
            " for",
            " mo",
            "ost",
            " fine",
            " p",
            "oud",
            "re",
            ".\"",
            "The",
            " ",
            "165",
            "7",
            " Lex",
            "icon",
            " Ch",
            "ym",
            "icum",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " \"",
            "bar",
            "bar",
            "ous",
            "\"",
            " authors",
            " for",
            " \"",
            "fine",
            " powder",
            ".\"",
            " V",
            "igo",
            " wrote",
            ":",
            " \"",
            "the",
            " barbar",
            "ous",
            " a",
            "uct",
            "ours",
            " use",
            " alcohol",
            ",",
            " or",
            " (",
            "as",
            " I",
            " fy",
            "nde",
            " it",
            " som",
            "et",
            "ymes",
            " w",
            "ry",
            "ten",
            ")",
            " al",
            "co",
            "f",
            "oll",
            ",",
            " for",
            " mo",
            "ost",
            " fine",
            " p",
            "oud",
            "re",
            ".\"",
            "The",
            " ",
            "165",
            "7",
            " Lex",
            "icon",
            " Ch",
            "ym",
            "icum",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.434,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ra",
            ",",
            " the",
            " oath",
            " of",
            " barbar",
            "ians",
            " of",
            " Bou",
            "alem",
            " Sans",
            "al",
            ",",
            " memory",
            " of",
            " the",
            " flesh",
            " of",
            " Ah",
            "lam",
            " Most",
            "eg",
            "han",
            "emi",
            " and",
            " the",
            " last",
            " novel",
            " by",
            " Ass",
            "ia",
            " D",
            "je",
            "bar",
            " nowhere",
            " in",
            " my",
            " father",
            "'s",
            " House",
            ".",
            "Music",
            "Cha",
            "Ã¢",
            "bi",
            " music",
            " is",
            " a",
            " typically",
            " Alger",
            "ian",
            " musical",
            " genre",
            " characterized",
            " by",
            " specific",
            " rhythms",
            " and",
            " of",
            " Q",
            "acid",
            "ate"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.434,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " statue",
            ",",
            " with",
            " an",
            " inscription",
            " describing",
            " Julian",
            " as",
            " \"",
            "Lord",
            " of",
            " the",
            " whole",
            " world",
            " from",
            " the",
            " British",
            " Ocean",
            " to",
            " the",
            " barbar",
            "ian",
            " nations",
            "\",",
            " can",
            " still",
            " be",
            " seen",
            ",",
            " built",
            " into",
            " the",
            " eastern",
            " side",
            " of",
            " the",
            " inner",
            " circuit",
            " of",
            " the",
            " walls",
            " of",
            " Ankara",
            " Castle",
            ".",
            " The",
            " Column",
            " of",
            " Julian",
            " which",
            " was",
            " erected",
            " in",
            " honor",
            " of",
            " the",
            " emperor",
            "'s",
            " visit",
            " to",
            " the",
            " city"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            0.245,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " simulation",
            "?",
            " Van",
            " Em",
            "de",
            " Bo",
            "as",
            " observes",
            " \"",
            "even",
            " if",
            " we",
            " base",
            " complexity",
            " theory",
            " on",
            " abstract",
            " instead",
            " of",
            " concrete",
            " machines",
            ",",
            " the",
            " arbit",
            "rar",
            "iness",
            " of",
            " the",
            " choice",
            " of",
            " a",
            " model",
            " remains",
            ".",
            " It",
            " is",
            " at",
            " this",
            " point",
            " that",
            " the",
            " notion",
            " of",
            " simulation",
            " enters",
            "\".",
            " When",
            " speed",
            " is",
            " being",
            " measured",
            ",",
            " the",
            " instruction",
            " set",
            " matters",
            ".",
            " For",
            " example",
            ",",
            " the",
            " sub"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            0.247,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " invent",
            "iveness",
            " of",
            " the",
            " language",
            ",",
            " but",
            " expressed",
            " une",
            "ase",
            " at",
            " the",
            " violent",
            " subject",
            " matter",
            ".",
            " The",
            " Spect",
            "ator",
            " praised",
            " Burgess",
            "'s",
            " \"",
            "extra",
            "ordinary",
            " technical",
            " feat",
            "\"",
            " but",
            " was",
            " uncomfortable",
            " with",
            " \"",
            "a",
            " certain",
            " arbit",
            "rar",
            "iness",
            " about",
            " the",
            " plot",
            " which",
            " is",
            " slightly",
            " irritating",
            "\".",
            " New",
            " States",
            "man",
            " acclaimed",
            " Burgess",
            " for",
            " addressing",
            " \"",
            "ac",
            "utely",
            " and",
            " sav",
            "ag",
            "ely",
            " the",
            " tendencies",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            0.247,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " invent",
            "iveness",
            " of",
            " the",
            " language",
            ",",
            " but",
            " expressed",
            " une",
            "ase",
            " at",
            " the",
            " violent",
            " subject",
            " matter",
            ".",
            " The",
            " Spect",
            "ator",
            " praised",
            " Burgess",
            "'s",
            " \"",
            "extra",
            "ordinary",
            " technical",
            " feat",
            "\"",
            " but",
            " was",
            " uncomfortable",
            " with",
            " \"",
            "a",
            " certain",
            " arbit",
            "rar",
            "iness",
            " about",
            " the",
            " plot",
            " which",
            " is",
            " slightly",
            " irritating",
            "\".",
            " New",
            " States",
            "man",
            " acclaimed",
            " Burgess",
            " for",
            " addressing",
            " \"",
            "ac",
            "utely",
            " and",
            " sav",
            "ag",
            "ely",
            " the",
            " tendencies",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "ath",
            ",",
            " which",
            " is",
            " portrayed",
            " as",
            " \"",
            "a",
            " body",
            " of",
            " persons",
            " voluntarily",
            " united",
            " for",
            " socially",
            " beneficial",
            " purposes",
            "\"",
            " with",
            " its",
            " territorial",
            " claim",
            " being",
            " limited",
            " to",
            " \"",
            "the",
            " sum",
            " total",
            " of",
            " the",
            " landed",
            " properties",
            " of",
            " its",
            " members",
            "\".",
            " Civil",
            " disputes",
            " were",
            " settled",
            " by",
            " private",
            " arbit",
            "ers",
            " called",
            " \"",
            "b",
            "reh",
            "ons",
            "\"",
            " and",
            " the",
            " compensation",
            " to",
            " be",
            " paid",
            " to",
            " the",
            " wrong",
            "ed",
            " party",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "ath",
            ",",
            " which",
            " is",
            " portrayed",
            " as",
            " \"",
            "a",
            " body",
            " of",
            " persons",
            " voluntarily",
            " united",
            " for",
            " socially",
            " beneficial",
            " purposes",
            "\"",
            " with",
            " its",
            " territorial",
            " claim",
            " being",
            " limited",
            " to",
            " \"",
            "the",
            " sum",
            " total",
            " of",
            " the",
            " landed",
            " properties",
            " of",
            " its",
            " members",
            "\".",
            " Civil",
            " disputes",
            " were",
            " settled",
            " by",
            " private",
            " arbit",
            "ers",
            " called",
            " \"",
            "b",
            "reh",
            "ons",
            "\"",
            " and",
            " the",
            " compensation",
            " to",
            " be",
            " paid",
            " to",
            " the",
            " wrong",
            "ed",
            " party",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Barb",
            "uda",
            " Dock",
            "yard",
            " Museum",
            " Betty",
            "'s",
            " Hope",
            " ",
            " Dow",
            "'s",
            " Hill",
            " Interpret",
            "ation",
            " Centre",
            "Sports",
            "Cr",
            "icket",
            " is",
            " the",
            " most",
            " popular",
            " sport",
            " in",
            " the",
            " islands",
            ".",
            " With",
            " Sir",
            " Isaac",
            " Viv",
            "ian",
            " Alexander",
            " Richards",
            " ",
            " who",
            " represented",
            " the",
            " West",
            " Indies",
            " cricket",
            " team",
            " between",
            " ",
            "197",
            "4",
            " and",
            " ",
            "199",
            "1",
            ",",
            " Ant",
            "igua",
            " had",
            " one",
            " of",
            " the",
            " world",
            "'s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Barb",
            "uda",
            " is",
            " only",
            " present",
            " on",
            " the",
            " island",
            " of",
            " Barb",
            "uda",
            " at",
            " the",
            " present",
            " time",
            ";",
            " however",
            ",",
            " there",
            " is",
            " legislation",
            " in",
            " place",
            " for",
            " a",
            " system",
            " of",
            " village",
            " councils",
            " on",
            " the",
            " island",
            " of",
            " Ant",
            "igua",
            ";",
            " however",
            ",",
            " village",
            " councils",
            " have",
            " not",
            " been",
            " active",
            " since",
            " the",
            " ",
            "194",
            "0",
            "s",
            " and",
            " ",
            "195",
            "0",
            "s",
            ".",
            "Human",
            " rights",
            "As",
            " of",
            " July",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Barb",
            "uda",
            " is",
            " thought",
            " to",
            " refer",
            " either",
            " to",
            " the",
            " male",
            " inhabitants",
            " of",
            " the",
            " island",
            ",",
            " or",
            " the",
            " be",
            "arded",
            " fig",
            " trees",
            " present",
            " there",
            ".",
            "History",
            "Pre",
            "-col",
            "on",
            "ial",
            " period",
            "Ant",
            "igua",
            " was",
            " first",
            " settled",
            " by",
            " arch",
            "aic",
            " age",
            " hunter",
            "-g",
            "ather",
            "er",
            " Native",
            " Americans",
            " called",
            " the",
            " C",
            "ib",
            "oney",
            ".",
            " Carbon",
            " dating",
            " has",
            " established",
            " the",
            " earliest",
            " settlements",
            " started",
            " around",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.4,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " dish",
            " pol",
            "enta",
            ".",
            " Other",
            " popular",
            " dishes",
            " from",
            " this",
            " region",
            " include",
            " d",
            "uc",
            "ana",
            ",",
            " salt",
            "fish",
            ",",
            " seasoned",
            " rice",
            ",",
            " and",
            " lobster",
            " (",
            "from",
            " Barb",
            "uda",
            ").",
            " In",
            " addition",
            ",",
            " there",
            " are",
            " sweets",
            " that",
            " are",
            " made",
            " locally",
            ",",
            " such",
            " as",
            " peanut",
            " brittle",
            ",",
            " sugar",
            " cake",
            ",",
            " f",
            "udge",
            ",",
            " raspberry",
            " and",
            " t",
            "amar",
            "ind",
            " stew",
            ",",
            " and",
            " other",
            " similar",
            " dishes",
            ".",
            "Despite"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " signed",
            " the",
            " UN",
            " treaty",
            " on",
            " the",
            " Pro",
            "hibition",
            " of",
            " Nuclear",
            " Weapons",
            ".",
            "Administr",
            "ative",
            " divisions",
            "Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " is",
            " divided",
            " into",
            " six",
            " par",
            "ishes",
            " and",
            " two",
            " dependencies",
            ":",
            "Although",
            " they",
            " are",
            " referred",
            " to",
            " as",
            " dependencies",
            ",",
            " both",
            " Barb",
            "uda",
            " and",
            " Red",
            "onda",
            " are",
            " actually",
            " integral",
            " parts",
            " of",
            " the",
            " state",
            " and",
            " can",
            " be",
            " thought",
            " of",
            " as",
            " administrative",
            " divisions",
            ".",
            " Simply",
            " put"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.396,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " infrastructure",
            ",",
            " leaving",
            " Barb",
            "uda",
            " \"",
            "bare",
            "ly",
            " habit",
            "able",
            "\"",
            " according",
            " to",
            " Prime",
            " Minister",
            " Gast",
            "on",
            " Brow",
            "ne",
            ".",
            " Nearly",
            " everyone",
            " on",
            " the",
            " island",
            " was",
            " evacuated",
            " to",
            " Ant",
            "igua",
            ".",
            "Am",
            "id",
            "st",
            " the",
            " following",
            " rebuilding",
            " efforts",
            " on",
            " Barb",
            "uda",
            " that",
            " were",
            " estimated",
            " to",
            " cost",
            " at",
            " least",
            " $",
            "100",
            " million",
            ",",
            " the",
            " government",
            " announced",
            " plans",
            " to",
            " revoke",
            " a",
            " century",
            "-old",
            " law"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " mostly",
            " of",
            " its",
            " two",
            " names",
            "ake",
            " islands",
            ",",
            " Ant",
            "igua",
            ",",
            " and",
            " Barb",
            "uda",
            ".",
            " Other",
            " than",
            " that",
            ",",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            "'s",
            " biggest",
            " islands",
            " are",
            " Gu",
            "iana",
            " Island",
            " and",
            " Long",
            " Island",
            " off",
            " the",
            " coast",
            " of",
            " Ant",
            "igua",
            ",",
            " and",
            " Red",
            "onda",
            " island",
            ",",
            " which",
            " is",
            " far",
            " from",
            " both",
            " of",
            " the",
            " main",
            " islands",
            ".",
            "Climate",
            " ",
            "Rain",
            "fall",
            " averages",
            " ",
            " per"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " island",
            " of",
            " Ant",
            "igua",
            " was",
            " originally",
            " called",
            " ",
            " by",
            " the",
            " A",
            "raw",
            "aks",
            " and",
            " is",
            " locally",
            " known",
            " by",
            " that",
            " name",
            " today",
            ";",
            " the",
            " Car",
            "ibs",
            " possibly",
            " called",
            " Barb",
            "uda",
            " .",
            " Christopher",
            " Columbus",
            ",",
            " while",
            " sailing",
            " by",
            " in",
            " ",
            "149",
            "3",
            ",",
            " may",
            " have",
            " named",
            " it",
            " ,",
            " after",
            " an",
            " icon",
            " in",
            " the",
            " Spanish",
            " Se",
            "ville",
            " Cathedral",
            ".",
            " The",
            " \"",
            "be",
            "arded",
            "\"",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            "201",
            "3",
            ")",
            "192",
            "5",
            " –",
            " David",
            " A",
            ".",
            " Huffman",
            ",",
            " American",
            " computer",
            " scientist",
            ",",
            " developed",
            " Huffman",
            " coding",
            " (",
            "d",
            ".",
            " ",
            "199",
            "9",
            ")",
            "192",
            "6",
            " –",
            " Denis",
            " At",
            "kinson",
            ",",
            " Barb",
            "adian",
            " cr",
            "ick",
            "eter",
            " (",
            "d",
            ".",
            " ",
            "200",
            "1",
            ")",
            "192",
            "7",
            " –",
            " Daniel",
            " Key",
            "es",
            ",",
            " American",
            " short",
            " story",
            " writer",
            " and",
            " novelist",
            " (",
            "d",
            ".",
            " ",
            "201",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " \"",
            "dependency",
            "\"",
            " is",
            " just",
            " a",
            " title",
            ".",
            " The",
            " Red",
            "onda",
            " is",
            " a",
            " second",
            "-level",
            " administrative",
            " division",
            " that",
            " is",
            " part",
            " of",
            " the",
            " Saint",
            " John",
            " Parish",
            "'s",
            " District",
            " \"",
            "A",
            ".\"",
            " Barb",
            "uda",
            " is",
            " a",
            " local",
            " administrative",
            " division",
            " on",
            " the",
            " same",
            " level",
            " as",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " and",
            " its",
            " council",
            " is",
            " the",
            " name",
            " of",
            " its",
            " local",
            " governing",
            " body",
            ".",
            " In",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Leg",
            "itimate",
            " Acts",
            " and",
            " Illegal",
            " Enc",
            "ounters",
            ":",
            " Law",
            " and",
            " Society",
            " in",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            " Riley",
            ",",
            " J",
            ".",
            " H",
            ".",
            " –",
            " Catalog",
            "ue",
            " of",
            " a",
            " Collection",
            " of",
            " Birds",
            " from",
            " Barb",
            "uda",
            " and",
            " Ant",
            "igua",
            ",",
            " British",
            " West",
            " Indies",
            ".",
            " R",
            "ouse",
            ",",
            " Irving",
            " and",
            " Bir",
            "git",
            " Fab",
            "er",
            " Morse",
            " –",
            " Exc",
            "av",
            "ations",
            " at",
            " the",
            " Indian",
            " Creek",
            " Site",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " mac",
            "a",
            "ques",
            " are",
            " the",
            " sole",
            " native",
            " monkey",
            ".",
            " Sn",
            "akes",
            ",",
            " monitor",
            " l",
            "izards",
            ",",
            " and",
            " numerous",
            " other",
            " rept",
            "iles",
            " can",
            " be",
            " found",
            " living",
            " among",
            " an",
            " array",
            " of",
            " rodents",
            " throughout",
            " the",
            " semi",
            " ar",
            "id",
            " regions",
            " of",
            " Algeria",
            ".",
            " Many",
            " animals",
            " are",
            " now",
            " extinct",
            ",",
            " including",
            " the",
            " Barb",
            "ary",
            " lions",
            ",",
            " Atlas",
            " bears",
            " and",
            " cro",
            "cod",
            "iles",
            ".",
            "In",
            " the",
            " north",
            ",",
            " some"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Ant",
            "igua",
            " Labour",
            " Party",
            " (",
            "AL",
            "P",
            ")",
            " to",
            " the",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " Labour",
            " Party",
            " (",
            "AB",
            "LP",
            ").",
            " This",
            " was",
            " done",
            " to",
            " officially",
            " include",
            " the",
            " party",
            "'s",
            " presence",
            " on",
            " the",
            " sister",
            " island",
            " of",
            " Barb",
            "uda",
            " in",
            " its",
            " organisation",
            ",",
            " the",
            " only",
            " political",
            " party",
            " on",
            " the",
            " mainland",
            " to",
            " have",
            " a",
            " physical",
            " branch",
            " in",
            " Barb",
            "uda",
            ".",
            "Jud",
            "iciary",
            " ",
            "The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " World",
            " Trade",
            " Organization",
            " and",
            " the",
            " Eastern",
            " Caribbean",
            "'s",
            " Regional",
            " Security",
            " System",
            ".",
            "Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " is",
            " also",
            " a",
            " member",
            " of",
            " the",
            " International",
            " Criminal",
            " Court",
            " (",
            "with",
            " a",
            " Bil",
            "ateral",
            " Imm",
            "unity",
            " Agreement",
            " of",
            " Protection",
            " for",
            " the",
            " US",
            " military",
            " as",
            " covered",
            " under",
            " Article",
            " ",
            "98",
            " of",
            " the",
            " Rome",
            " Stat",
            "ute",
            ").",
            "In",
            " ",
            "201",
            "3",
            ",",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " called"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " describe",
            " the",
            " culinary",
            " traditions",
            " of",
            " the",
            " islands",
            " of",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " in",
            " the",
            " Caribbean",
            ".",
            " F",
            "ung",
            "ie",
            ",",
            " pronounced",
            " \"",
            "foon",
            "-j",
            "ee",
            "\",",
            " and",
            " pepper",
            "pot",
            " are",
            " the",
            " country",
            "'s",
            " official",
            " dish",
            " and",
            " dish",
            " of",
            " pride",
            ".",
            " Corn",
            "meal",
            " is",
            " the",
            " main",
            " ingredient",
            " in",
            " fung",
            "ie",
            ",",
            " which",
            " is",
            " a",
            " dish",
            " that",
            " is",
            " very",
            " similar",
            " to",
            " the",
            " Italian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " breakup",
            " of",
            " the",
            " federation",
            " in",
            " ",
            "196",
            "2",
            ",",
            " it",
            " became",
            " one",
            " of",
            " the",
            " West",
            " Indies",
            " Associated",
            " States",
            " in",
            " ",
            "196",
            "7",
            ".",
            " Following",
            " a",
            " period",
            " of",
            " internal",
            " self",
            "-g",
            "overn",
            "ance",
            ",",
            " it",
            " gained",
            " full",
            " independence",
            " from",
            " the",
            " United",
            " Kingdom",
            " on",
            " ",
            "1",
            " November",
            " ",
            "198",
            "1",
            ".",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " is",
            " a",
            " member",
            " of",
            " the",
            " Commonwealth",
            " and",
            " a",
            " Commonwealth"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " BBC",
            " News",
            " World",
            " Bank",
            "'s",
            " country",
            " data",
            " profile",
            " for",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " Archae",
            "ology",
            "Ant",
            "igua",
            ".org",
            " –",
            " ",
            "201",
            "0",
            "March",
            "13",
            " source",
            " of",
            " archaeological",
            " information",
            " for",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " Ant",
            "igua",
            " &",
            " Barb",
            "uda",
            " Official",
            " Business",
            " Hub",
            " ",
            "Countries",
            " in",
            " the",
            " Caribbean",
            "Is",
            "land",
            " countries",
            "Common",
            "wealth",
            " realms",
            "Countries",
            " in"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " using",
            " it",
            ".",
            " In",
            " Kub",
            "rick",
            "'s",
            " opinion",
            " –",
            " as",
            " in",
            " the",
            " opinion",
            " of",
            " other",
            " readers",
            ",",
            " including",
            " the",
            " original",
            " American",
            " editor",
            " –",
            " the",
            " final",
            " chapter",
            " was",
            " un",
            "conv",
            "inc",
            "ing",
            " and",
            " inconsistent",
            " with",
            " the",
            " book",
            ".",
            " Kub",
            "rick",
            "'s",
            " stance",
            " was",
            " unusual",
            " when",
            " compared",
            " to",
            " the",
            " standard",
            " Hollywood",
            " practice",
            " of",
            " producing",
            " films",
            " with",
            " the",
            " familiar",
            " trop",
            "es",
            " of",
            " resolving",
            " moral",
            " messages"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " em",
            "pires",
            " of",
            " their",
            " time",
            ",",
            " as",
            " well",
            " as",
            " forming",
            " part",
            " of",
            " a",
            " conf",
            "eder",
            "ated",
            " support",
            " and",
            " trade",
            " network",
            " with",
            " other",
            " Islamic",
            " states",
            " during",
            " the",
            " Islamic",
            " Era",
            ".",
            "The",
            " Ber",
            "ber",
            " people",
            " historically",
            " consisted",
            " of",
            " several",
            " tribes",
            ".",
            " The",
            " two",
            " main",
            " branches",
            " were",
            " the",
            " Bot",
            "r",
            " and",
            " Barn",
            "Ã¨s",
            " tribes",
            ",",
            " who",
            " were",
            " divided",
            " into",
            " tribes",
            ",",
            " and",
            " again",
            " into",
            " sub",
            "-"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.275,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Hutchinson",
            ".",
            " ",
            " (",
            "extract",
            "s",
            " quoted",
            " here",
            ")",
            "External",
            " links",
            " ",
            " A",
            " Clock",
            "work",
            " Orange",
            " at",
            " Spark",
            "Notes",
            " A",
            " Clock",
            "work",
            " Orange",
            " ",
            " at",
            " Liter",
            "ap",
            "edia",
            " A",
            " Clock",
            "work",
            " Orange",
            " (",
            "196",
            "2",
            ")",
            " |",
            "Last",
            " chapter",
            " |",
            "Anthony",
            " Burgess",
            " (",
            "191",
            "7",
            "–",
            "199",
            "3",
            ")",
            "Compar",
            "isons",
            " with",
            " the",
            " Kub",
            "rick",
            " film",
            " adaptation",
            " Dal"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " strongly",
            " with",
            " the",
            " borders",
            " of",
            " the",
            " states",
            " and",
            " also",
            " with",
            " the",
            " border",
            " with",
            " Bav",
            "aria",
            ",",
            " with",
            " Bav",
            "arians",
            " having",
            " a",
            " markedly",
            " different",
            " rhythm",
            " of",
            " speech",
            " in",
            " spite",
            " of",
            " the",
            " linguistic",
            " similarities",
            ".",
            "References",
            "Notes",
            "C",
            "itations",
            "Works",
            " cited",
            "Further",
            " reading",
            " Am",
            "mon",
            ",",
            " Ul",
            "rich",
            ":",
            " Die",
            " deutsche",
            " Spr",
            "ache",
            " in",
            " Deutschland",
            ",",
            " Ãĸ",
            "sterreich",
            " und",
            " der"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " annual",
            " production",
            " of",
            " aluminium",
            " exceeded",
            " ",
            "50",
            ",",
            "000",
            ",",
            "000",
            " metric",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            "The",
            " real",
            " price",
            " for",
            " aluminium",
            " declined",
            " from",
            " $",
            "14",
            ",",
            "000",
            " per",
            " metric",
            " ton",
            " in",
            " ",
            "190",
            "0",
            " to",
            " $",
            "2",
            ",",
            "340",
            " in",
            " ",
            "194",
            "8",
            " (",
            "in",
            " ",
            "199",
            "8",
            " United",
            " States",
            " dollars",
            ").",
            " Extraction",
            " and",
            " processing",
            " costs",
            " were",
            " lowered",
            " over"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.184,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " My",
            " Favorites",
            " in",
            " Susp",
            "ense",
            " (",
            "195",
            "9",
            ").",
            " He",
            " hired",
            " T",
            "ippi",
            " Hed",
            "ren",
            " to",
            " play",
            " the",
            " lead",
            " role",
            ".",
            " It",
            " was",
            " her",
            " first",
            " role",
            ";",
            " she",
            " had",
            " been",
            " a",
            " model",
            " in",
            " New",
            " York",
            " when",
            " Hitch",
            "cock",
            " saw",
            " her",
            ",",
            " in",
            " October",
            " ",
            "196",
            "1",
            ",",
            " in",
            " an",
            " NBC",
            " television",
            " advert",
            " for",
            " S",
            "ego",
            ",",
            " a",
            " diet",
            " drink",
            ":",
            " \"",
            "I"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.17,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            "\",",
            ".",
            " Aqu",
            "inas",
            " concluded",
            " that",
            " though",
            " we",
            " are",
            " not",
            " bound",
            " to",
            " love",
            " others",
            " more",
            " than",
            " ourselves",
            ",",
            " we",
            " naturally",
            " seek",
            " the",
            " common",
            " good",
            ",",
            " the",
            " good",
            " of",
            " the",
            " whole",
            ",",
            " more",
            " than",
            " any",
            " private",
            " good",
            ",",
            " the",
            " good",
            " of",
            " a",
            " part",
            ".",
            " However",
            ",",
            " he",
            " thought",
            " we",
            " should",
            " love",
            " God",
            " more",
            " than",
            " ourselves",
            " and",
            " our",
            " neighbours",
            ",",
            " and",
            " more",
            " than",
            " our",
            " bodily"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.137,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " about",
            " a",
            " medieval",
            " princess",
            ",",
            " her",
            " loyal",
            " general",
            ",",
            " and",
            " two",
            " peasants",
            " who",
            " all",
            " need",
            " to",
            " travel",
            " through",
            " enemy",
            " lines",
            " in",
            " order",
            " to",
            " reach",
            " their",
            " home",
            " region",
            ".",
            " Released",
            " in",
            " December",
            " ",
            "195",
            "8",
            ",",
            " The",
            " Hidden",
            " Fortress",
            " became",
            " an",
            " enormous",
            " box",
            "-office",
            " success",
            " in",
            " Japan",
            " and",
            " was",
            " warmly",
            " received",
            " by",
            " critics",
            " both",
            " in",
            " Japan",
            " and",
            " abroad",
            ".",
            " Today",
            ",",
            " the",
            " film",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.122,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " months",
            " are",
            " normally",
            " independent",
            ",",
            " though",
            " they",
            " often",
            " share",
            " a",
            " den",
            " with",
            " their",
            " mother",
            " until",
            " the",
            " next",
            " breeding",
            " season",
            ".",
            " By",
            " the",
            " time",
            " the",
            " next",
            " set",
            " of",
            " c",
            "ubs",
            " is",
            " born",
            ",",
            " the",
            " older",
            " c",
            "ubs",
            " have",
            " moved",
            " on",
            ".",
            " A",
            "ard",
            "w",
            "olves",
            " generally",
            " achieve",
            " sexual",
            " maturity",
            " at",
            " one",
            " and",
            " a",
            " half",
            " to",
            " two",
            " years",
            " of",
            " age",
            ".",
            "Cons",
            "ervation",
            "The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.109,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " Paramount",
            " Pictures",
            " where",
            " he",
            " was",
            " responsible",
            " for",
            " films",
            " including",
            " John",
            " Trav",
            "olta",
            "'s",
            " Urban",
            " Cowboy",
            ".",
            " His",
            " boyfriend",
            " of",
            " ",
            "12",
            " years",
            " was",
            " Jed",
            " Johnson",
            ",",
            " whom",
            " he",
            " met",
            " in",
            " ",
            "196",
            "8",
            ",",
            " and",
            " who",
            " later",
            " achieved",
            " fame",
            " as",
            " an",
            " interior",
            " designer",
            ".",
            "The",
            " fact",
            " that",
            " War",
            "hol",
            "'s",
            " homosexuality",
            " influenced",
            " his",
            " work",
            " and",
            " shaped",
            " his",
            " relationship",
            " to",
            " the",
            " art",
            " world",
            " is"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " maintain",
            " a",
            " computer",
            " called",
            " Y",
            "gg",
            "dr",
            "asil",
            " in",
            " Ah",
            "!",
            " My",
            " Goddess",
            ".",
            " Genre",
            " crossing",
            " in",
            " anime",
            " is",
            " also",
            " prevalent",
            ",",
            " such",
            " as",
            " the",
            " blend",
            " of",
            " fantasy",
            " and",
            " comedy",
            " in",
            " Dragon",
            " Half",
            ",",
            " and",
            " the",
            " incorporation",
            " of",
            " slap",
            "stick",
            " humor",
            " in",
            " the",
            " crime",
            " anime",
            " film",
            " Castle",
            " of",
            " C",
            "agli",
            "ost",
            "ro",
            ".",
            " Other",
            " sub",
            "genres",
            " found",
            " in",
            " anime",
            " include",
            " magical",
            " girl",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " story",
            " saw",
            " Kind",
            "red",
            "'s",
            " plans",
            " come",
            " to",
            " fruition",
            " as",
            " he",
            " tor",
            "mented",
            " Spider",
            "-Man",
            ".",
            " The",
            " story",
            " has",
            " also",
            " seen",
            " five",
            " \".",
            "LR",
            "\"",
            " for",
            " issues",
            " ",
            "50",
            ",",
            " ",
            "51",
            ",",
            " ",
            "52",
            ",",
            " ",
            "53",
            ",",
            " and",
            " ",
            "54",
            " which",
            " focused",
            " on",
            " The",
            " Order",
            " of",
            " the",
            " Web",
            ",",
            " a",
            " new",
            " faction",
            " of",
            " Spider",
            "-P",
            "e",
            "ople",
            " consisting",
            " of",
            " Julia",
            " Carpenter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Latin",
            "-spe",
            "akers",
            " adopted",
            " the",
            " Greek",
            " term",
            " as",
            " ,",
            " which",
            " in",
            " French",
            " ultimately",
            " became",
            " ,",
            " whence",
            " the",
            " English",
            " word",
            " \"",
            "ars",
            "enic",
            "\".",
            "Ars",
            "enic",
            " sulf",
            "ides",
            " (",
            "orp",
            "iment",
            ",",
            " real",
            "gar",
            ")",
            " and",
            " ox",
            "ides",
            " have",
            " been",
            " known",
            " and",
            " used",
            " since",
            " ancient",
            " times",
            ".",
            " Z",
            "os",
            "imos",
            " ()",
            " describes",
            " ro",
            "asting",
            " sand",
            "ar",
            "ach",
            " (",
            "real",
            "gar",
            ")",
            " to",
            " obtain",
            " cloud"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " chosen",
            " by",
            " the",
            " ",
            "38",
            " provinces",
            ".",
            " The",
            " body",
            " has",
            " a",
            " permanent",
            " secret",
            "ariat",
            ",",
            " the",
            " Anglic",
            "an",
            " Comm",
            "union",
            " Office",
            ",",
            " of",
            " which",
            " the",
            " arch",
            "bishop",
            " of",
            " Canterbury",
            " is",
            " president",
            ".",
            " The",
            " Prim",
            "ates",
            "'",
            " Meeting",
            " (",
            "first",
            " met",
            " in",
            " ",
            "197",
            "9",
            ")",
            " is",
            " the",
            " most",
            " recent",
            " manifestation",
            " of",
            " international",
            " consultation",
            " and",
            " deliber",
            "ation",
            ",",
            " having",
            " been",
            " first",
            " convened",
            " by",
            " Archbishop"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "EU",
            ")",
            " membership",
            ",",
            " active",
            " efforts",
            " are",
            " being",
            " made",
            " to",
            " achieve",
            " substantial",
            " improvements",
            " in",
            " these",
            " areas",
            " to",
            " align",
            " with",
            " EU",
            " criteria",
            " and",
            " standards",
            ".",
            "Foreign",
            " relations",
            " ",
            "Emer",
            "ging",
            " from",
            " decades",
            " of",
            " isolation",
            " during",
            " the",
            " communism",
            ",",
            " Albania",
            " has",
            " adopted",
            " a",
            " foreign",
            " policy",
            " orientation",
            " centered",
            " on",
            " active",
            " cooperation",
            " and",
            " engagement",
            " in",
            " international",
            " affairs",
            ".",
            " At",
            " the",
            " core",
            " of",
            " Albania",
            "'s",
            " foreign",
            " policies",
            " lie"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "InParameter",
    "à¹ģà¸£à¸¡",
    "pine",
    "OSE",
    "ewan"
  ],
  "bottom_logits": [
    "svc",
    "ettle",
    "elif",
    "âĢİ",
    "aced"
  ],
  "act_min": -0.0,
  "act_max": 0.516
}