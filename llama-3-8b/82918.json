{
  "index": 82918,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " relating",
            " to",
            " FFT",
            " algorithms",
            " (",
            "used",
            " heavily",
            " in",
            " the",
            " field",
            " of",
            " image",
            " processing",
            "),",
            " can",
            " decrease",
            " processing",
            " time",
            " up",
            " to",
            " ",
            "1",
            ",",
            "000",
            " times",
            " for",
            " applications",
            " like",
            " medical",
            " imaging",
            ".",
            " In",
            " general",
            ",",
            " speed",
            " improvements",
            " depend",
            " on",
            " special",
            " properties",
            " of",
            " the",
            " problem",
            ",",
            " which",
            " are",
            " very",
            " common",
            " in",
            " practical",
            " applications",
            ".",
            " Speed",
            "ups",
            " of",
            " this",
            " magnitude",
            " enable",
            " computing",
            " devices",
            " that",
            " make",
            " extensive"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.055,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.114,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " S",
            "-type",
            ".",
            " These",
            " were",
            " named",
            " after",
            " and",
            " are",
            " generally",
            " identified",
            " with",
            " carbon",
            "aceous",
            " (",
            "carbon",
            "-rich",
            "),",
            " metallic",
            ",",
            " and",
            " sil",
            "ic",
            "aceous",
            " (",
            "st",
            "ony",
            ")",
            " compositions",
            ",",
            " respectively",
            ".",
            " The",
            " physical",
            " composition",
            " of",
            " asteroids",
            " is",
            " varied",
            " and",
            " in",
            " most",
            " cases",
            " poorly",
            " understood",
            ".",
            " C",
            "eres",
            " appears",
            " to",
            " be",
            " composed",
            " of",
            " a",
            " rocky",
            " core",
            " covered",
            " by",
            " an",
            " icy",
            " mantle",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.051,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.047,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.863,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " solution",
            " of",
            " hydrogen",
            " chloride",
            " that",
            " is",
            " found",
            " in",
            " gastric",
            " acid",
            " in",
            " the",
            " stomach",
            " and",
            " activates",
            " digestive",
            " enzymes",
            "),",
            " ac",
            "etic",
            " acid",
            " (",
            "vine",
            "gar",
            " is",
            " a",
            " dil",
            "ute",
            " aque",
            "ous",
            " solution",
            " of",
            " this",
            " liquid",
            "),",
            " sulfur",
            "ic",
            " acid",
            " (",
            "used",
            " in",
            " car",
            " batteries",
            "),",
            " and",
            " cit",
            "ric",
            " acid",
            " (",
            "found",
            " in",
            " citrus",
            " fruits",
            ").",
            " As",
            " these",
            " examples",
            " show",
            ",",
            " acids",
            " (",
            "in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.051,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.047,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.863,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " solution",
            " of",
            " hydrogen",
            " chloride",
            " that",
            " is",
            " found",
            " in",
            " gastric",
            " acid",
            " in",
            " the",
            " stomach",
            " and",
            " activates",
            " digestive",
            " enzymes",
            "),",
            " ac",
            "etic",
            " acid",
            " (",
            "vine",
            "gar",
            " is",
            " a",
            " dil",
            "ute",
            " aque",
            "ous",
            " solution",
            " of",
            " this",
            " liquid",
            "),",
            " sulfur",
            "ic",
            " acid",
            " (",
            "used",
            " in",
            " car",
            " batteries",
            "),",
            " and",
            " cit",
            "ric",
            " acid",
            " (",
            "found",
            " in",
            " citrus",
            " fruits",
            ").",
            " As",
            " these",
            " examples",
            " show",
            ",",
            " acids",
            " (",
            "in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.031,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "fish",
            ")",
            " are",
            " the",
            " sister",
            " lineage",
            " of",
            " all",
            " other",
            " act",
            "in",
            "opt",
            "ery",
            "g",
            "ians",
            ",",
            " the",
            " Ac",
            "ip",
            "enser",
            "iform",
            "es",
            " (",
            "st",
            "urge",
            "ons",
            " and",
            " paddle",
            "fish",
            "es",
            ")",
            " are",
            " the",
            " sister",
            " lineage",
            " of",
            " Ne",
            "opt",
            "ery",
            "gii",
            ",",
            " and",
            " Hol",
            "ost",
            "ei",
            " (",
            "bow",
            "fin",
            " and",
            " g",
            "ars",
            ")",
            " are",
            " the",
            " sister",
            " lineage",
            " of",
            " tele",
            "ost",
            "s",
            ".",
            " The",
            " E"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " up",
            " to",
            " ",
            "3",
            ".",
            "5",
            " to",
            " ",
            "4",
            " t",
            "/",
            "ha",
            " with",
            " irrigation",
            ".",
            " In",
            " contrast",
            ",",
            " the",
            " average",
            " wheat",
            " yield",
            " in",
            " countries",
            " such",
            " as",
            " France",
            " is",
            " over",
            " ",
            "8",
            " t",
            "/",
            "ha",
            ".",
            " Vari",
            "ations",
            " in",
            " yields",
            " are",
            " due",
            " mainly",
            " to",
            " variation",
            " in",
            " climate",
            ",",
            " genetics",
            ",",
            " and",
            " the",
            " level",
            " of",
            " intensive",
            " farming",
            " techniques",
            " (",
            "use",
            " of",
            " fertil",
            "izers",
            ",",
            " chemical"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.023,
            0.118,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.758,
            0.018,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "s",
            " of",
            " marine",
            " in",
            "verte",
            "brates",
            " of",
            " the",
            " order",
            " S",
            "cl",
            "er",
            "act",
            "inia",
            " (",
            "st",
            "ony",
            " cor",
            "als",
            ").",
            " These",
            " animals",
            " metabol",
            "ize",
            " sugar",
            " and",
            " oxygen",
            " to",
            " obtain",
            " energy",
            " for",
            " their",
            " cell",
            "-building",
            " processes",
            ",",
            " including",
            " secretion",
            " of",
            " the",
            " ex",
            "os",
            "keleton",
            ",",
            " with",
            " water",
            " and",
            " carbon",
            " dioxide",
            " as",
            " by",
            "products",
            ".",
            " Din",
            "of",
            "lag",
            "ell",
            "ates",
            " (",
            "al",
            "gal",
            " prot",
            "ists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            1.023,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "rons",
            " (",
            "tr",
            "it",
            "ium",
            ")",
            " and",
            " more",
            " than",
            " two",
            " neut",
            "rons",
            ".",
            " The",
            " known",
            " elements",
            " form",
            " a",
            " set",
            " of",
            " atomic",
            " numbers",
            ",",
            " from",
            " the",
            " single",
            "-pro",
            "ton",
            " element",
            " hydrogen",
            " up",
            " to",
            " the",
            " ",
            "118",
            "-pro",
            "ton",
            " element",
            " og",
            "an",
            "esson",
            ".",
            " All",
            " known",
            " isot",
            "opes",
            " of",
            " elements",
            " with",
            " atomic",
            " numbers",
            " greater",
            " than",
            " ",
            "82",
            " are",
            " radioactive",
            ",",
            " although",
            " the",
            " radio",
            "activity",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.132,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " existence",
            " of",
            " a",
            " special",
            " set",
            " of",
            " \"",
            "in",
            "dependent",
            "\"",
            " pron",
            "ouns",
            ",",
            " which",
            " are",
            " distinct",
            " from",
            " subject",
            " pron",
            "ouns",
            ".",
            " They",
            " can",
            " occur",
            " together",
            " with",
            " subject",
            " pron",
            "ouns",
            " but",
            " cannot",
            " fulfill",
            " an",
            " object",
            " function",
            ".",
            " Also",
            " common",
            " are",
            " dependent",
            "/",
            "aff",
            "ix",
            " pron",
            "ouns",
            " (",
            "used",
            " for",
            " direct",
            " objects",
            " and",
            " to",
            " mark",
            " possession",
            ").",
            " For",
            " most",
            " branches",
            ",",
            " the",
            " first",
            " person"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "or",
            "od",
            " dimensions",
            " Shape",
            " factor",
            " (",
            "image",
            " analysis",
            " and",
            " microscopy",
            ")",
            " Finite",
            " Element",
            " Analysis",
            "Aspect",
            " ratios",
            " of",
            " simple",
            " shapes",
            "Rect",
            "angles",
            "For",
            " a",
            " rectangle",
            ",",
            " the",
            " aspect",
            " ratio",
            " denotes",
            " the",
            " ratio",
            " of",
            " the",
            " width",
            " to",
            " the",
            " height",
            " of",
            " the",
            " rectangle",
            ".",
            " A",
            " square",
            " has",
            " the",
            " smallest",
            " possible",
            " aspect",
            " ratio",
            " of",
            " ",
            "1",
            ":",
            "1",
            ".",
            "Examples",
            ":",
            " ",
            "4"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.996,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " council",
            " of",
            " Amsterdam",
            " voted",
            " to",
            " maintain",
            " the",
            " borough",
            " system",
            " by",
            " replacing",
            " the",
            " district",
            " councils",
            " with",
            " smaller",
            ",",
            " but",
            " still",
            " directly",
            " elected",
            " district",
            " committees",
            " (",
            "best",
            "u",
            "urs",
            "com",
            "miss",
            "ies",
            ").",
            " Under",
            " a",
            " municipal",
            " ordinance",
            ",",
            " the",
            " new",
            " district",
            " committees",
            " were",
            " granted",
            " responsibilities",
            " through",
            " delegation",
            " of",
            " regulatory",
            " and",
            " executive",
            " powers",
            " by",
            " the",
            " central",
            " municipal",
            " council",
            ".",
            "Met",
            "ropolitan",
            " area",
            "\"",
            "Am",
            "sterdam"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.996,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " use",
            " one",
            " character",
            " to",
            " erase",
            " the",
            " previous",
            " character",
            ";",
            " this",
            " could",
            " be",
            " set",
            " to",
            " BS",
            " or",
            " DEL",
            ",",
            " but",
            " not",
            " both",
            ",",
            " resulting",
            " in",
            " recurring",
            " situations",
            " of",
            " ambiguity",
            " where",
            " users",
            " had",
            " to",
            " decide",
            " depending",
            " on",
            " what",
            " terminal",
            " they",
            " were",
            " using",
            " (",
            "sh",
            "ells",
            " that",
            " allow",
            " line",
            " editing",
            ",",
            " such",
            " as",
            " k",
            "sh",
            ",",
            " bash",
            ",",
            " and",
            " z",
            "sh",
            ",",
            " understand",
            " both",
            ").",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.988,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            0.816,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.672,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "ch",
            "ic",
            "ory",
            "),",
            " C",
            "yn",
            "ara",
            " sc",
            "ol",
            "ym",
            "us",
            " (",
            "g",
            "lobe",
            " art",
            "ich",
            "oke",
            "),",
            " Hel",
            "ian",
            "thus",
            " ann",
            "u",
            "us",
            " (",
            "sun",
            "flower",
            "),",
            " Small",
            "anth",
            "us",
            " son",
            "ch",
            "if",
            "ol",
            "ius",
            " (",
            "y",
            "ac",
            "Ã³n",
            "),",
            " Car",
            "th",
            "amus",
            " tin",
            "ctor",
            "ius",
            " (",
            "s",
            "aff",
            "lower",
            ")",
            " and",
            " Hel",
            "ian",
            "thus",
            " tub",
            "eros",
            "us",
            " (",
            "Jer",
            "usalem"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.988,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " plastic",
            " films",
            ",",
            " are",
            " not",
            " easy",
            " to",
            " recycle",
            " because",
            " of",
            " high",
            " contamination",
            " levels",
            " (",
            "up",
            " to",
            " ",
            "40",
            "–",
            "50",
            "%",
            " by",
            " weight",
            " contamination",
            " by",
            " pesticides",
            ",",
            " fertil",
            "izers",
            ",",
            " soil",
            " and",
            " debris",
            ",",
            " moist",
            " vegetation",
            ",",
            " sil",
            "age",
            " juice",
            " water",
            ",",
            " and",
            " UV",
            " stabil",
            "izers",
            ")",
            " and",
            " collection",
            " difficulties",
            " .",
            " Therefore",
            ",",
            " they",
            " are",
            " often",
            " buried",
            " or",
            " abandoned",
            " in",
            " fields",
            " and",
            " water"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.984,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " drought",
            " tolerance",
            ",",
            " eased",
            " harvest",
            " and",
            " improved",
            " the",
            " taste",
            " and",
            " nutritional",
            " value",
            " of",
            " crop",
            " plants",
            ".",
            " Care",
            "ful",
            " selection",
            " and",
            " breeding",
            " have",
            " had",
            " enormous",
            " effects",
            " on",
            " the",
            " characteristics",
            " of",
            " crop",
            " plants",
            ".",
            " Plant",
            " selection",
            " and",
            " breeding",
            " in",
            " the",
            " ",
            "192",
            "0",
            "s",
            " and",
            " ",
            "193",
            "0",
            "s",
            " improved",
            " pasture",
            " (",
            "gr",
            "asses",
            " and",
            " clo",
            "ver",
            ")",
            " in",
            " New",
            " Zealand",
            ".",
            " Ext",
            "ensive"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.984,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " stability",
            ",",
            " ",
            " and",
            " ,",
            " which",
            " lie",
            " ",
            "60",
            "Â°",
            " ahead",
            " of",
            " and",
            " behind",
            " the",
            " larger",
            " body",
            ".",
            "In",
            " the",
            " Solar",
            " System",
            ",",
            " most",
            " known",
            " tro",
            "j",
            "ans",
            " share",
            " the",
            " orbit",
            " of",
            " Jupiter",
            ".",
            " They",
            " are",
            " divided",
            " into",
            " the",
            " Greek",
            " camp",
            " at",
            " ",
            " (",
            "ahead",
            " of",
            " Jupiter",
            ")",
            " and",
            " the",
            " Trojan",
            " camp",
            " at",
            " ",
            " (",
            "tr",
            "ailing",
            " Jupiter",
            ").",
            " More",
            " than",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.984,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " broader",
            " businesses",
            " that",
            " support",
            " the",
            " farms",
            " and",
            " farming",
            " populations",
            ".",
            "The",
            " major",
            " agricultural",
            " products",
            " can",
            " be",
            " broadly",
            " grouped",
            " into",
            " foods",
            ",",
            " fibers",
            ",",
            " fuels",
            ",",
            " and",
            " raw",
            " materials",
            " (",
            "such",
            " as",
            " rubber",
            ").",
            " Food",
            " classes",
            " include",
            " cere",
            "als",
            " (",
            "gr",
            "ains",
            "),",
            " vegetables",
            ",",
            " fruits",
            ",",
            " cooking",
            " oils",
            ",",
            " meat",
            ",",
            " milk",
            ",",
            " eggs",
            ",",
            " and",
            " fungi",
            ".",
            " Global",
            " agricultural",
            " production",
            " amounts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.334,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.672,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.98,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "loyd",
            " from",
            " Forbidden",
            "\"The",
            " Animal",
            "\"",
            "\"The",
            " Animal",
            "\"",
            " (",
            "Dist",
            "urbed",
            " song",
            "),",
            " ",
            "201",
            "0",
            "\"The",
            " Animal",
            "\",",
            " by",
            " Steve",
            " V",
            "ai",
            " from",
            " Passion",
            " and",
            " Warfare",
            "Other",
            " uses",
            " AN",
            "IMAL",
            " (",
            "computer",
            " worm",
            "),",
            " an",
            " early",
            " self",
            "-re",
            "p",
            "lic",
            "ating",
            " computer",
            " program",
            " AN",
            "IMAL",
            " (",
            "image",
            " processing",
            "),",
            " an",
            " interactive",
            " software",
            " environment",
            " for",
            " image",
            " processing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " books",
            " on",
            " fashion",
            " as",
            " well",
            " as",
            " paintings",
            " with",
            " fashion",
            " (",
            "sh",
            "oes",
            ")",
            " as",
            " a",
            " subject",
            ".",
            " War",
            "hol",
            " himself",
            " has",
            " been",
            " described",
            " as",
            " a",
            " modern",
            " d",
            "andy",
            ",",
            " whose",
            " authority",
            " \"",
            "rest",
            "ed",
            " more",
            " on",
            " presence",
            " than",
            " on",
            " words",
            "\".",
            " Performance",
            " Art",
            ":",
            " War",
            "hol",
            " and",
            " his",
            " friends",
            " staged",
            " theatrical",
            " multimedia",
            " happen",
            "ings",
            " at",
            " parties",
            " and",
            " public",
            " venues",
            ",",
            " combining",
            " music",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Lum",
            "bar",
            " punct",
            "ure",
            " (",
            "sp",
            "inal",
            " tap",
            ")",
            " -",
            " A",
            " needle",
            " is",
            " inserted",
            " into",
            " the",
            " lower",
            " back",
            " (",
            "lum",
            "bar",
            " region",
            ")",
            " between",
            " two",
            " lum",
            "bar",
            " vert",
            "ebra",
            "e",
            " to",
            " obtain",
            " a",
            " sample",
            " of",
            " cere",
            "bro",
            "sp",
            "inal",
            " fluid",
            " for",
            " testing",
            ".",
            " Genetic",
            " testing",
            " -",
            " Determines",
            " whether",
            " the",
            " mutation",
            " that",
            " causes",
            " one",
            " of",
            " the",
            " her",
            "editary",
            " at",
            "ax",
            "ic",
            " conditions",
            " is"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.902,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.05,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.973,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.766
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " sampled",
            " from",
            " the",
            " environment",
            " and",
            " maintained",
            " in",
            " laboratories",
            " with",
            " relative",
            " ease",
            ".",
            "On",
            " the",
            " basis",
            " of",
            " their",
            " habitat",
            ",",
            " algae",
            " can",
            " be",
            " categorized",
            " as",
            ":",
            " aquatic",
            " (",
            "pl",
            "ank",
            "ton",
            "ic",
            ",",
            " b",
            "enth",
            "ic",
            ",",
            " marine",
            ",",
            " freshwater",
            ",",
            " l",
            "entic",
            ",",
            " lot",
            "ic",
            "),",
            " terrestrial",
            ",",
            " aerial",
            " (",
            "sub",
            "a",
            "erial",
            "),",
            " lith",
            "oph",
            "ytic",
            ",",
            " hal",
            "oph",
            "ytic",
            " (",
            "or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "asi",
            "atic",
            " languages",
            ".",
            " Ch",
            "á»¯",
            " N",
            "Ã´m",
            " Kh",
            "mer",
            " alphabet",
            " Kh",
            "om",
            " script",
            " (",
            "used",
            " for",
            " a",
            " short",
            " period",
            " in",
            " the",
            " early",
            " ",
            "20",
            "th",
            " century",
            " for",
            " indigenous",
            " languages",
            " in",
            " Laos",
            ")",
            " Old",
            " Mon",
            " script",
            " Mon",
            " script",
            " P",
            "ah",
            "aw",
            "h",
            " H",
            "mong",
            " was",
            " once",
            " used",
            " to",
            " write",
            " Kh",
            "mu",
            ",",
            " under",
            " the",
            " name",
            " \"",
            "P",
            "ah",
            "aw"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " largest",
            " family",
            " of",
            " all",
            " ang",
            "ios",
            "perms",
            " (",
            "only",
            " Aster",
            "aceae",
            " might",
            " -",
            " or",
            " might",
            " not",
            " -",
            " be",
            " more",
            " spec",
            "io",
            "se",
            ")",
            " and",
            " hence",
            " by",
            " far",
            " the",
            " largest",
            " in",
            " the",
            " order",
            ".",
            " The",
            " Dah",
            "lg",
            "ren",
            " system",
            " recognized",
            " three",
            " families",
            " of",
            " orch",
            "ids",
            ",",
            " but",
            " DNA",
            " sequence",
            " analysis",
            " later",
            " showed",
            " that",
            " these",
            " families",
            " are",
            " poly",
            "ph",
            "yle",
            "tic",
            " and",
            " so",
            " should",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " speaking",
            " through",
            " the",
            " hero",
            " of",
            " his",
            " third",
            " play",
            " The",
            " A",
            "char",
            "n",
            "ians",
            " (",
            "st",
            "aged",
            " at",
            " the",
            " Lena",
            "ia",
            ",",
            " where",
            " there",
            " were",
            " few",
            " or",
            " no",
            " foreign",
            " dign",
            "it",
            "aries",
            "),",
            " the",
            " poet",
            " carefully",
            " distingu",
            "ishes",
            " between",
            " the",
            " polis",
            " and",
            " the",
            " real",
            " targets",
            " of",
            " his",
            " ac",
            "erb",
            "ic",
            " wit",
            ":",
            "A",
            "rist",
            "oph",
            "anes",
            " repeatedly",
            " sav",
            "ages",
            " Cle",
            "on",
            " in",
            " his",
            " later"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.965,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " observed",
            " asteroids",
            ".",
            " Such",
            " observations",
            " are",
            " limited",
            " as",
            " they",
            " provide",
            " information",
            " about",
            " only",
            " the",
            " thin",
            " layer",
            " on",
            " the",
            " surface",
            " (",
            "up",
            " to",
            " several",
            " mic",
            "rom",
            "eters",
            ").",
            " As",
            " planet",
            "ologist",
            " Patrick",
            " Michel",
            " writes",
            ":",
            "Mid",
            "-",
            " to",
            " thermal",
            "-in",
            "frared",
            " observations",
            ",",
            " along",
            " with",
            " polar",
            "im",
            "etry",
            " measurements",
            ",",
            " are",
            " probably",
            " the",
            " only",
            " data",
            " that",
            " give",
            " some",
            " indication",
            " of",
            " actual",
            " physical",
            " properties"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.965,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "ness",
            ",",
            " heat",
            ",",
            " swelling",
            ",",
            " pain",
            ",",
            " and",
            " loss",
            " of",
            " function",
            ".",
            " There",
            " may",
            " also",
            " be",
            " high",
            " temperature",
            " (",
            "fe",
            "ver",
            ")",
            " and",
            " ch",
            "ills",
            ".",
            " If",
            " superficial",
            ",",
            " abs",
            "cess",
            "es",
            " may",
            " be",
            " fluct",
            "uant",
            " when",
            " palp",
            "ated",
            ";",
            " this",
            " wave",
            "-like",
            " motion",
            " is",
            " caused",
            " by",
            " movement",
            " of",
            " the",
            " pus",
            " inside",
            " the",
            " abs",
            "cess",
            ".",
            "An",
            " internal",
            " abs",
            "cess",
            " is",
            " more"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.965,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " only",
            " one",
            " (",
            "up",
            " to",
            " is",
            "om",
            "orphism",
            ",",
            " but",
            " not",
            " unique",
            " is",
            "om",
            "orphism",
            ")",
            " which",
            " is",
            " an",
            " algebra",
            "ic",
            " extension",
            " of",
            " F",
            ";",
            " it",
            " is",
            " called",
            " the",
            " algebra",
            "ic",
            " closure",
            " of",
            " F",
            ".",
            "The",
            " theory",
            " of",
            " algebra",
            "ically",
            " closed",
            " fields",
            " has",
            " quant",
            "ifier",
            " elimination",
            ".",
            "Notes",
            "References",
            " ",
            "  ",
            " ",
            "Field",
            " (",
            "math",
            "ematics",
            ")",
            "<|begin_of_text|>",
            "Events"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.961,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " period",
            " of",
            " weakness",
            " and",
            " disorder",
            " in",
            " the",
            " Roman",
            " Empire",
            " to",
            " set",
            " up",
            " a",
            " short",
            "-lived",
            " state",
            " of",
            " her",
            " own",
            ".",
            "The",
            " town",
            " was",
            " reinc",
            "orpor",
            "ated",
            " into",
            " the",
            " Roman",
            " Empire",
            " under",
            " Emperor",
            " Aure",
            "lian",
            " in",
            " ",
            "272",
            ".",
            " The",
            " t",
            "etr",
            "archy",
            ",",
            " a",
            " system",
            " of",
            " multiple",
            " (",
            "up",
            " to",
            " four",
            ")",
            " em",
            "per",
            "ors",
            " introduced",
            " by",
            " Di",
            "oc",
            "let",
            "ian",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.961,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " it",
            ",",
            " such",
            " as",
            " being",
            " treated",
            " as",
            " an",
            " EU",
            " member",
            " for",
            " trade",
            " in",
            " manufactured",
            " goods",
            " (",
            "no",
            " tariffs",
            ")",
            " and",
            " as",
            " a",
            " non",
            "-E",
            "U",
            " member",
            " for",
            " agricultural",
            " products",
            ".",
            " And",
            "orra",
            " lacked",
            " a",
            " currency",
            " of",
            " its",
            " own",
            " and",
            " used",
            " both",
            " the",
            " French",
            " franc",
            " and",
            " the",
            " Spanish",
            " pes",
            "eta",
            " in",
            " banking",
            " transactions",
            " until",
            " ",
            "31",
            " December",
            " ",
            "199",
            "9",
            ",",
            " when",
            " both",
            " currencies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.957,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " increased",
            " cloud",
            " dro",
            "plet",
            " number",
            " concentrations",
            ",",
            " which",
            " in",
            " turn",
            " leads",
            " to",
            " increased",
            " cloud",
            " al",
            "bedo",
            ",",
            " increased",
            " light",
            " scattering",
            " and",
            " radi",
            "ative",
            " cooling",
            " (",
            "first",
            " indirect",
            " effect",
            "),",
            " but",
            " also",
            " leads",
            " to",
            " reduced",
            " precipitation",
            " efficiency",
            " and",
            " increased",
            " lifetime",
            " of",
            " the",
            " cloud",
            " (",
            "second",
            " indirect",
            " effect",
            ").",
            "In",
            " extremely",
            " polluted",
            " cities",
            " like",
            " Delhi",
            ",",
            " aeros",
            "ol",
            " pollutants",
            " influence",
            " local",
            " weather",
            " and",
            " induce"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.953,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "ality",
            " and",
            " was",
            " commanded",
            " by",
            " two",
            " officials",
            " (",
            "ve",
            "gu",
            "ers",
            ")",
            " appointed",
            " by",
            " France",
            " and",
            " the",
            " Bishop",
            " of",
            " Urg",
            "ell",
            ".",
            "In",
            " the",
            " modern",
            " era",
            ",",
            " the",
            " army",
            " has",
            " consisted",
            " of",
            " a",
            " very",
            " small",
            " body",
            " of",
            " volunteers",
            " willing",
            " to",
            " undertake",
            " ceremonial",
            " duties",
            ".",
            " Uniform",
            "s",
            " and",
            " weaponry",
            " were",
            " handed",
            " down",
            " from",
            " generation",
            " to",
            " generation",
            " within",
            " families",
            " and",
            " communities",
            ".",
            "The",
            " army",
            "'s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.879,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.863,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " which",
            " a",
            " single",
            " symbol",
            " denotes",
            " the",
            " combination",
            " of",
            " one",
            " conson",
            "ant",
            " and",
            " one",
            " vowel",
            ".",
            "Related",
            " concepts",
            " were",
            " introduced",
            " independently",
            " in",
            " ",
            "194",
            "8",
            " by",
            " James",
            " Ger",
            "main",
            " FÃ©",
            "vrier",
            " (",
            "using",
            " the",
            " term",
            " )",
            " and",
            " David",
            " Dir",
            "inger",
            " (",
            "using",
            " the",
            " term",
            " sem",
            "is",
            "yll",
            "ab",
            "ary",
            "),",
            " then",
            " in",
            " ",
            "195",
            "9",
            " by",
            " Fred",
            " House",
            "holder",
            " (",
            "int",
            "roducing",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.867,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " metal",
            " group",
            ",",
            " due",
            " to",
            " them",
            " sharing",
            " the",
            " characteristic",
            " s",
            "1",
            " electron",
            " configuration",
            " of",
            " the",
            " alk",
            "ali",
            " metals",
            " (",
            "group",
            " ",
            "1",
            ":",
            " p",
            "6",
            "s",
            "1",
            ";",
            " group",
            " ",
            "11",
            ":",
            " d",
            "10",
            "s",
            "1",
            ").",
            " However",
            ",",
            " the",
            " similarities",
            " are",
            " largely",
            " confined",
            " to",
            " the",
            " sto",
            "ichi",
            "omet",
            "ries",
            " of",
            " the",
            " +",
            "1",
            " compounds",
            " of",
            " both",
            " groups",
            ",",
            " and",
            " not",
            " their",
            " chemical"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            0.863,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " constraint",
            " is",
            " against",
            " two",
            " different",
            " lab",
            "ial",
            " conson",
            "ants",
            " (",
            "other",
            " than",
            " w",
            ")",
            " occurring",
            " together",
            " in",
            " a",
            " root",
            ",",
            " a",
            " constraint",
            " which",
            " can",
            " be",
            " found",
            " in",
            " all",
            " branches",
            " but",
            " Om",
            "otic",
            ".",
            " Another",
            " widespread",
            " constraint",
            " is",
            " against",
            " two",
            " non",
            "-",
            "ident",
            "ical",
            " lateral",
            " ob",
            "stru",
            "ents",
            ",",
            " which",
            " can",
            " be",
            " found",
            " in",
            " Egyptian",
            ",",
            " Ch",
            "adic",
            ",",
            " Sem",
            "itic",
            ",",
            " and",
            " probably"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.859,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            "M",
            "ement",
            "os",
            " ",
            "The",
            " astronauts",
            " had",
            " personal",
            " preference",
            " kits",
            " (",
            "PP",
            "K",
            "s",
            "),",
            " small",
            " bags",
            " containing",
            " personal",
            " items",
            " of",
            " significance",
            " they",
            " wanted",
            " to",
            " take",
            " with",
            " them",
            " on",
            " the",
            " mission",
            ".",
            " Five",
            " ",
            " P",
            "PK",
            "s",
            " were",
            " carried",
            " on",
            " Apollo",
            " ",
            "11",
            ":",
            " three",
            " (",
            "one",
            " for",
            " each",
            " astronaut",
            ")",
            " were",
            " st",
            "owed",
            " on",
            " Columbia",
            " before",
            " launch",
            ",",
            " and",
            " two",
            " on",
            " Eagle"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.824,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            0.108,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " –",
            " James",
            " J",
            ".",
            " Jeff",
            "ries",
            ",",
            " American",
            " boxer",
            " and",
            " promoter",
            " (",
            "d",
            ".",
            " ",
            "195",
            "3",
            ")",
            "187",
            "7",
            " –",
            " Georg",
            " Kol",
            "be",
            ",",
            " German",
            " sculpt",
            "or",
            " (",
            "d",
            ".",
            " ",
            "194",
            "7",
            ")",
            "187",
            "7",
            " –",
            " William",
            " David",
            " Ross",
            ",",
            " Scottish",
            " philosopher",
            " (",
            "d",
            ".",
            " ",
            "197",
            "1",
            ")",
            "187",
            "8",
            " –",
            " Robert",
            " Wal",
            "ser",
            ",",
            " Swiss",
            " author",
            " and",
            " playwright",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            0.087,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.812,
            0.069,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " ",
            "199",
            "4",
            ")",
            "194",
            "1",
            " –",
            " Ces",
            "Ã¡ria",
            " Ãī",
            "v",
            "ora",
            ",",
            " Cape",
            " Verde",
            "an",
            " singer",
            " (",
            "d",
            ".",
            " ",
            "201",
            "1",
            ")",
            " ",
            " ",
            "194",
            "1",
            "  ",
            " –",
            " J",
            "Ã¡n",
            "os",
            " Kon",
            "rÃ¡",
            "d",
            ",",
            " Hungarian",
            " water",
            " polo",
            " player",
            " and",
            " swim",
            "mer",
            " (",
            "d",
            ".",
            " ",
            "201",
            "4",
            ")",
            " ",
            " ",
            "194",
            "1",
            "  ",
            " –",
            " Harrison",
            " Page",
            ",",
            " American",
            " actor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "196",
            "7",
            "  ",
            " –",
            " Alfred",
            " G",
            "ough",
            ",",
            " American",
            " screen",
            "writer",
            " and",
            " producer",
            " ",
            " ",
            "196",
            "7",
            "  ",
            " –",
            " Lay",
            "ne",
            " St",
            "aley",
            ",",
            " American",
            " singer",
            "-song",
            "writer",
            " (",
            "d",
            ".",
            " ",
            "200",
            "2",
            ")",
            "196",
            "8",
            " –",
            " Cas",
            "per",
            " Christ",
            "ensen",
            ",",
            " Danish",
            " comedian",
            ",",
            " actor",
            ",",
            " and",
            " screen",
            "writer",
            " ",
            " ",
            "196",
            "8",
            "  ",
            " –",
            " Rich",
            " Low",
            "ry"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.789,
            0.05,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " dis",
            "section",
            " and",
            " inspection",
            " of",
            " cad",
            "avers",
            ".",
            " The",
            " study",
            " of",
            " microscopic",
            " anatomy",
            " (",
            "or",
            " hist",
            "ology",
            ")",
            " can",
            " be",
            " aided",
            " by",
            " practical",
            " experience",
            " examining",
            " hist",
            "ological",
            " preparations",
            " (",
            "or",
            " slides",
            ")",
            " under",
            " a",
            " microscope",
            ".",
            "Human",
            " anatomy",
            ",",
            " physiology",
            " and",
            " bio",
            "chemistry",
            " are",
            " complementary",
            " basic",
            " medical",
            " sciences",
            ",",
            " which",
            " are",
            " generally",
            " taught",
            " to",
            " medical",
            " students",
            " in",
            " their",
            " first",
            " year",
            " at",
            " medical",
            " school"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            "\".",
            " To",
            " contrast",
            " with",
            " the",
            " more",
            " stere",
            "ot",
            "yped",
            " descriptions",
            ",",
            " Christie",
            " portrayed",
            " some",
            " \"",
            "foreign",
            "\"",
            " characters",
            " as",
            " victims",
            ",",
            " or",
            " potential",
            " victims",
            ",",
            " at",
            " the",
            " hands",
            " of",
            " English",
            " male",
            "f",
            "actors",
            ",",
            " such",
            " as",
            ",",
            " respectively",
            ",",
            " Ol",
            "ga",
            " Sem",
            "in",
            "off",
            " (",
            "H",
            "allow",
            "e",
            "'en",
            " Party",
            ")",
            " and",
            " Katrina",
            " Re",
            "iger",
            " (",
            "in",
            " the",
            " short",
            " story",
            " \"",
            "How",
            " Does"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.766,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.766,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            0.762,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            "198",
            "9",
            "  ",
            " –",
            " Sugar",
            " Ray",
            " Robinson",
            ",",
            " American",
            " boxer",
            " (",
            "b",
            ".",
            " ",
            "192",
            "1",
            ")",
            "199",
            "2",
            " –",
            " Il",
            "ario",
            " Band",
            "ini",
            ",",
            " Italian",
            " racing",
            " driver",
            " and",
            " businessman",
            " (",
            "b",
            ".",
            " ",
            "191",
            "1",
            ")",
            "199",
            "7",
            " –",
            " George",
            " Wald",
            ",",
            " American",
            " neuro",
            "log",
            "ist",
            " and",
            " academic",
            ",",
            " Nobel",
            " Prize",
            " laure",
            "ate",
            " (",
            "b",
            ".",
            " ",
            "190",
            "6",
            ")",
            "199",
            "8"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.734,
            0.034,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            "AR",
            " of",
            " .",
            "If",
            " the",
            " dimension",
            " d",
            " is",
            " fixed",
            ",",
            " then",
            " all",
            " reasonable",
            " definitions",
            " of",
            " aspect",
            " ratio",
            " are",
            " equivalent",
            " to",
            " within",
            " constant",
            " factors",
            ".",
            "Not",
            "ations",
            "Aspect",
            " ratios",
            " are",
            " math",
            "em",
            "atically",
            " expressed",
            " as",
            " x",
            ":y",
            " (",
            "pron",
            "ounced",
            " \"",
            "x",
            "-to",
            "-y",
            "\").",
            "C",
            "in",
            "emat",
            "ographic",
            " aspect",
            " ratios",
            " are",
            " usually",
            " den",
            "oted",
            " as",
            " a",
            " (",
            "rounded",
            ")",
            " decimal",
            " multiple",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            0.037,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "simple",
            " past",
            ")",
            " is",
            " very",
            " rarely",
            " used",
            " in",
            " Austria",
            ",",
            " especially",
            " in",
            " the",
            " spoken",
            " language",
            ",",
            " with",
            " the",
            " exception",
            " of",
            " some",
            " modal",
            " verbs",
            " (",
            "ich",
            " sollte",
            ",",
            " ich",
            " wollte",
            ").",
            "V",
            "ocabulary",
            "There",
            " are",
            " many",
            " official",
            " terms",
            " that",
            " differ",
            " in",
            " Austrian",
            " German",
            " from",
            " their",
            " usage",
            " in",
            " most",
            " parts",
            " of",
            " Germany",
            ".",
            " Words",
            " used",
            " in",
            " Austria",
            " are",
            " J",
            "Ã¤nner",
            " (",
            "January",
            ")",
            " rather"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " post",
            "hum",
            "ously",
            " as",
            " a",
            " charter",
            " member",
            " of",
            " the",
            " World",
            " Academy",
            " of",
            " Art",
            " and",
            " Science",
            " (",
            "WA",
            "AS",
            "),",
            " an",
            " organization",
            " founded",
            " by",
            " distinguished",
            " scientists",
            " and",
            " intellectuals",
            " who",
            " committed",
            " themselves",
            " to",
            " the",
            " responsible",
            " and",
            " ethical",
            " advances",
            " of",
            " science",
            ",",
            " particularly",
            " in",
            " light",
            " of",
            " the",
            " development",
            " of",
            " nuclear",
            " weapons",
            ".",
            "US",
            " citizenship",
            " ",
            "E",
            "instein",
            " became",
            " an",
            " American",
            " citizen",
            " in",
            " ",
            "194",
            "0",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            " (",
            "cod",
            "s",
            ")",
            " Division",
            " Pol",
            "ym",
            "ix",
            "i",
            "acea",
            " Bet",
            "anc",
            "ur",
            "-R",
            "od",
            "rig",
            "uez",
            " et",
            " al",
            ".",
            " ",
            "201",
            "3",
            " (",
            "Pol",
            "ym",
            "y",
            "xi",
            "omor",
            "pha",
            ";",
            " Pol",
            "ym",
            "ix",
            "ii",
            "pt",
            "ery",
            "gii",
            ")",
            " Order",
            " âĢł",
            "P",
            "att",
            "erson",
            "ich",
            "thy",
            "iform",
            "es",
            " G",
            "aud",
            "ant",
            " ",
            "197",
            "6",
            " Order",
            " âĢł",
            "C",
            "ten",
            "oth",
            "r",
            "iss"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " Philipp",
            "opol",
            "is",
            " (",
            "P",
            "lov",
            "div",
            "),",
            " and",
            " Arc",
            "adi",
            "opol",
            "is",
            " (",
            "L",
            "Ã¼le",
            "burg",
            "az",
            ").",
            " They",
            " encountered",
            " and",
            " destroyed",
            " a",
            " Roman",
            " army",
            " outside",
            " Constantin",
            "ople",
            " but",
            " were",
            " stopped",
            " by",
            " the",
            " double",
            " walls",
            " of",
            " the",
            " Eastern",
            " capital",
            ".",
            " They",
            " defeated",
            " a",
            " second",
            " army",
            " near",
            " Call",
            "ip",
            "olis",
            " (",
            "G",
            "elib",
            "olu",
            ").",
            "The",
            "odos",
            "ius",
            ",",
            " unable",
            " to",
            " make",
            " effective"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.198,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.25,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.215,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Names",
            "day",
            " of",
            " Queen",
            " Sil",
            "via",
            " of",
            " Sweden",
            ",",
            " (",
            "Sweden",
            ")",
            " N",
            "ane",
            " N",
            "ane",
            " Day",
            " (",
            "T",
            "anz",
            "ania",
            ")",
            " Signal",
            " Tro",
            "ops",
            " Day",
            " (",
            "U",
            "kraine",
            ")",
            " August",
            " ",
            "9",
            " Battle",
            " of",
            " Gang",
            "ut",
            " Day",
            " (",
            "Russia",
            ")",
            " International",
            " Day",
            " of",
            " the",
            " World",
            "'s",
            " Indigenous",
            " People",
            " (",
            "United",
            " Nations",
            ")",
            " National",
            " Day",
            " (",
            "Singapore",
            ")",
            " National",
            " Women",
            "'s",
            " Day"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " ox",
            "alic",
            " acid",
            ".",
            " As",
            "cor",
            "bic",
            " acid",
            " (",
            "V",
            "itamin",
            " C",
            ")",
            " is",
            " an",
            " essential",
            " vitamin",
            " for",
            " the",
            " human",
            " body",
            " and",
            " is",
            " present",
            " in",
            " such",
            " foods",
            " as",
            " a",
            "ml",
            "a",
            " (",
            "Indian",
            " goose",
            "berry",
            "),",
            " lemon",
            ",",
            " citrus",
            " fruits",
            ",",
            " and",
            " gu",
            "ava",
            ".",
            "Many",
            " acids",
            " can",
            " be",
            " found",
            " in",
            " various",
            " kinds",
            " of",
            " food",
            " as",
            " additives",
            ",",
            " as",
            " they",
            " alter",
            " their",
            " taste"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " observ",
            "ances",
            " ",
            " Children",
            "'s",
            " Day",
            " (",
            "B",
            "ol",
            "ivia",
            ")",
            " Christian",
            " feast",
            " day",
            ":",
            " Ad",
            "on",
            "iram",
            " Jud",
            "son",
            " (",
            "Ep",
            "iscopal",
            " Church",
            ")",
            " Al",
            "fer",
            "ius",
            " Blessed",
            " Angelo",
            " Car",
            "let",
            "ti",
            " di",
            " Ch",
            "iv",
            "asso",
            " Erk",
            "emb",
            "ode",
            " Pope",
            " Julius",
            " I",
            " Teresa",
            " of",
            " the",
            " And",
            "es",
            " Z",
            "eno",
            " of",
            " Ver",
            "ona",
            " April",
            " ",
            "12",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " more",
            " rings",
            ".",
            "Simple",
            " cyc",
            "lo",
            "alk",
            "anes",
            " have",
            " a",
            " prefix",
            " \"",
            "c",
            "yc",
            "lo",
            "-\"",
            " to",
            " distinguish",
            " them",
            " from",
            " al",
            "kan",
            "es",
            ".",
            " Cyc",
            "lo",
            "alk",
            "anes",
            " are",
            " named",
            " as",
            " per",
            " their",
            " ac",
            "yclic",
            " counterparts",
            " with",
            " respect",
            " to",
            " the",
            " number",
            " of",
            " carbon",
            " atoms",
            " in",
            " their",
            " back",
            "bones",
            ",",
            " e",
            ".g",
            ".,",
            " cyc",
            "lop",
            "ent",
            "ane",
            " (",
            "C",
            "5",
            "H",
            "10",
            ")",
            " is"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ba",
            "ai",
            " (",
            "Command",
            "er",
            "'s",
            " Bay",
            ")",
            " in",
            " Sav",
            "an",
            "eta",
            ",",
            " and",
            " S",
            "int",
            " Nicola",
            "as",
            " Ba",
            "ai",
            " in",
            " San",
            " Nicola",
            "as",
            ".",
            " Pa",
            "arden",
            "ba",
            "ai",
            " services",
            " all",
            " the",
            " cruise",
            "-",
            "ship",
            " lines",
            " such",
            " as",
            " Royal",
            " Caribbean",
            ",",
            " Carnival",
            ",",
            " N",
            "CL",
            ",",
            " Holland",
            " America",
            ",",
            " MSC",
            " Cru",
            "ises",
            ",",
            " Costa",
            " Cru",
            "ises",
            ",",
            " P",
            "&",
            "O",
            " Cru",
            "ises",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.173,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "use",
            "\",",
            " and",
            " ",
            " \"",
            "leader",
            "\"",
            " ",
            "M",
            "use",
            "get",
            "es",
            " (",
            " ;",
            " ,",
            " M",
            "ous",
            "Äĵ",
            "get",
            "Äĵ",
            "s",
            "),",
            " as",
            " the",
            " preceding",
            "Arch",
            "ery",
            "A",
            "phet",
            "or",
            " (",
            " ;",
            " ,",
            " Aph",
            "Äĵ",
            "t",
            "Åį",
            "r",
            "),",
            " from",
            " ,",
            " \"",
            "to",
            " let",
            " loose",
            "\"",
            "A",
            "phet",
            "orus",
            " (",
            " ;",
            " ,",
            " Aph",
            "Äĵ",
            "tor",
            "os",
            "),",
            " as",
            " the",
            " preceding"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " order",
            ",",
            " and",
            " reason",
            "—",
            "character",
            "istics",
            " contrast",
            "ed",
            " with",
            " those",
            " of",
            " Dion",
            "ys",
            "us",
            ",",
            " god",
            " of",
            " wine",
            ",",
            " who",
            " represents",
            " ecstasy",
            " and",
            " disorder",
            ".",
            " The",
            " contrast",
            " between",
            " the",
            " roles",
            " of",
            " these",
            " gods",
            " is",
            " reflected",
            " in",
            " the",
            " ad",
            "jectives",
            " Ap",
            "oll",
            "onian",
            " and",
            " Dion",
            "ys",
            "ian",
            ".",
            " However",
            ",",
            " the",
            " Greeks",
            " thought",
            " of",
            " the",
            " two",
            " qualities",
            " as",
            " complementary",
            ":",
            " the",
            " two"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " Underground",
            ",",
            " Andy",
            " would",
            " have",
            " them",
            " dressed",
            " in",
            " all",
            " black",
            " to",
            " perform",
            " in",
            " front",
            " of",
            " movies",
            " that",
            " he",
            " was",
            " also",
            " presenting",
            ".",
            " In",
            " ",
            "196",
            "6",
            ",",
            " he",
            " \"",
            "produ",
            "ced",
            "\"",
            " their",
            " first",
            " album",
            " The",
            " Velvet",
            " Underground",
            " &",
            " Nico",
            ",",
            " as",
            " well",
            " as",
            " providing",
            " its",
            " album",
            " art",
            ".",
            " His",
            " actual",
            " participation",
            " in",
            " the",
            " album",
            "'s",
            " production",
            " amounted",
            " to",
            " simply",
            " paying",
            " for",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iter",
            "acy",
            " rate",
            " for",
            " people",
            " over",
            " ",
            "10",
            " was",
            " ",
            "22",
            ".",
            "3",
            "%,",
            " ",
            "15",
            ".",
            "6",
            "%",
            " for",
            " men",
            " and",
            " ",
            "29",
            ".",
            "0",
            "%",
            " for",
            " women",
            ".",
            " The",
            " province",
            " with",
            " the",
            " lowest",
            " rate",
            " of",
            " ill",
            "iter",
            "acy",
            " was",
            " Alg",
            "iers",
            " Province",
            " at",
            " ",
            "11",
            ".",
            "6",
            "%,",
            " while",
            " the",
            " province",
            " with",
            " the",
            " highest",
            " rate",
            " was",
            " Dj",
            "elf",
            "a",
            " Province",
            " at"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "acula",
    " stripslashes",
    ".spotify",
    "agnar",
    "á»ĩ"
  ],
  "bottom_logits": [
    " commercial",
    " great",
    "\n",
    " cover",
    ","
  ],
  "act_min": -0.0,
  "act_max": 1.07
}