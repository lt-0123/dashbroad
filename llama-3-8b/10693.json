{
  "index": 10693,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            ":",
            " Serbia",
            " declares",
            " war",
            " on",
            " Germany",
            ";",
            " Austria",
            " declares",
            " war",
            " on",
            " Russia",
            ".",
            "191",
            "5",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " S",
            "ari",
            " Bair",
            ":",
            " The",
            " Allies",
            " mount",
            " a",
            " diversion",
            "ary",
            " attack",
            " timed",
            " to",
            " coincide",
            " with",
            " a",
            " major",
            " Allied",
            " landing",
            " of",
            " reinforcements",
            " at",
            " S",
            "uv",
            "la",
            " Bay",
            ".",
            "191",
            "7",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " M",
            "Äĥr",
            "Äĥ",
            "ÈĻ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            ":",
            " Serbia",
            " declares",
            " war",
            " on",
            " Germany",
            ";",
            " Austria",
            " declares",
            " war",
            " on",
            " Russia",
            ".",
            "191",
            "5",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " S",
            "ari",
            " Bair",
            ":",
            " The",
            " Allies",
            " mount",
            " a",
            " diversion",
            "ary",
            " attack",
            " timed",
            " to",
            " coincide",
            " with",
            " a",
            " major",
            " Allied",
            " landing",
            " of",
            " reinforcements",
            " at",
            " S",
            "uv",
            "la",
            " Bay",
            ".",
            "191",
            "7",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " M",
            "Äĥr",
            "Äĥ",
            "ÈĻ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "ides",
            ":",
            " b",
            "ayer",
            "ite",
            ",",
            " gib",
            "bsite",
            ",",
            " and",
            " nord",
            "strand",
            "ite",
            ",",
            " which",
            " differ",
            " in",
            " their",
            " crystall",
            "ine",
            " structure",
            " (",
            "pol",
            "ym",
            "orph",
            "s",
            ").",
            " Many",
            " other",
            " intermediate",
            " and",
            " related",
            " structures",
            " are",
            " also",
            " known",
            ".",
            " Most",
            " are",
            " produced",
            " from",
            " ores",
            " by",
            " a",
            " variety",
            " of",
            " wet",
            " processes",
            " using",
            " acid",
            " and",
            " base",
            ".",
            " Heating",
            " the",
            " hydro",
            "x",
            "ides",
            " leads",
            " to",
            " formation",
            " of",
            " cor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "ides",
            ":",
            " b",
            "ayer",
            "ite",
            ",",
            " gib",
            "bsite",
            ",",
            " and",
            " nord",
            "strand",
            "ite",
            ",",
            " which",
            " differ",
            " in",
            " their",
            " crystall",
            "ine",
            " structure",
            " (",
            "pol",
            "ym",
            "orph",
            "s",
            ").",
            " Many",
            " other",
            " intermediate",
            " and",
            " related",
            " structures",
            " are",
            " also",
            " known",
            ".",
            " Most",
            " are",
            " produced",
            " from",
            " ores",
            " by",
            " a",
            " variety",
            " of",
            " wet",
            " processes",
            " using",
            " acid",
            " and",
            " base",
            ".",
            " Heating",
            " the",
            " hydro",
            "x",
            "ides",
            " leads",
            " to",
            " formation",
            " of",
            " cor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            ":",
            " on",
            " arrival",
            " to",
            " Bren",
            "ner",
            " Pass",
            ",",
            " he",
            " failed",
            " to",
            " declare",
            " his",
            " film",
            " stock",
            " to",
            " customs",
            " and",
            " it",
            " was",
            " confiscated",
            ";",
            " one",
            " actress",
            " could",
            " not",
            " enter",
            " the",
            " water",
            " for",
            " a",
            " scene",
            " because",
            " she",
            " was",
            " on",
            " her",
            " period",
            ";",
            " budget",
            " over",
            "runs",
            " meant",
            " that",
            " he",
            " had",
            " to",
            " borrow",
            " money",
            " from",
            " the",
            " actors",
            ".",
            " Hitch",
            "cock",
            " also",
            " needed",
            " a",
            " translator",
            " to",
            " give",
            " instructions"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            ":",
            " on",
            " arrival",
            " to",
            " Bren",
            "ner",
            " Pass",
            ",",
            " he",
            " failed",
            " to",
            " declare",
            " his",
            " film",
            " stock",
            " to",
            " customs",
            " and",
            " it",
            " was",
            " confiscated",
            ";",
            " one",
            " actress",
            " could",
            " not",
            " enter",
            " the",
            " water",
            " for",
            " a",
            " scene",
            " because",
            " she",
            " was",
            " on",
            " her",
            " period",
            ";",
            " budget",
            " over",
            "runs",
            " meant",
            " that",
            " he",
            " had",
            " to",
            " borrow",
            " money",
            " from",
            " the",
            " actors",
            ".",
            " Hitch",
            "cock",
            " also",
            " needed",
            " a",
            " translator",
            " to",
            " give",
            " instructions"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " cannot",
            " be",
            " made",
            " by",
            " heating",
            " their",
            " \"",
            "hydr",
            "ates",
            "\":",
            " hydrated",
            " aluminium",
            " chloride",
            " is",
            " in",
            " fact",
            " not",
            " Al",
            "Cl",
            "3",
            "Â·",
            "6",
            "H",
            "2",
            "O",
            " but",
            " [",
            "Al",
            "(H",
            "2",
            "O",
            ")",
            "6",
            "]",
            "Cl",
            "3",
            ",",
            " and",
            " the",
            " Al",
            "–",
            "O",
            " bonds",
            " are",
            " so",
            " strong",
            " that",
            " heating",
            " is",
            " not",
            " sufficient",
            " to",
            " break",
            " them",
            " and",
            " form",
            " Al",
            "–",
            "Cl",
            " bonds",
            " instead",
            ":",
            "2"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " note",
            " saying",
            " to",
            " retain",
            " the",
            " letters",
            " in",
            " case",
            " she",
            " had",
            " to",
            " use",
            " them",
            " to",
            " show",
            " maternal",
            " concern",
            ".",
            " In",
            " one",
            " letter",
            " to",
            " Lady",
            " Mil",
            "ban",
            "ke",
            ",",
            " she",
            " referred",
            " to",
            " her",
            " daughter",
            " as",
            " \"",
            "it",
            "\":",
            " \"",
            "I",
            " talk",
            " to",
            " it",
            " for",
            " your",
            " satisfaction",
            ",",
            " not",
            " my",
            " own",
            ",",
            " and",
            " shall",
            " be",
            " very",
            " glad",
            " when",
            " you",
            " have",
            " it",
            " under",
            " your",
            " own",
            ".\"",
            " Lady"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "S",
            "peer",
            " Myth",
            "\":",
            " the",
            " perception",
            " of",
            " him",
            " as",
            " an",
            " ap",
            "ol",
            "itical",
            " techn",
            "ocrat",
            " responsible",
            " for",
            " revolution",
            "izing",
            " the",
            " German",
            " war",
            " machine",
            ".",
            " The",
            " myth",
            " began",
            " to",
            " fall",
            " apart",
            " in",
            " the",
            " ",
            "198",
            "0",
            "s",
            ",",
            " when",
            " the",
            " arm",
            "aments",
            " miracle",
            " was",
            " attributed",
            " to",
            " Nazi",
            " propaganda",
            ".",
            " Adam",
            " Too",
            "ze",
            " wrote",
            " in",
            " The",
            " W",
            "ages",
            " of",
            " Destruction",
            " that",
            " the",
            " idea",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " '",
            "To",
            " a",
            " gas",
            " chamber",
            "—",
            "go",
            "!.",
            "Rand",
            "'s",
            " non",
            "fiction",
            " received",
            " far",
            " fewer",
            " reviews",
            " than",
            " her",
            " novels",
            ".",
            " The",
            " ten",
            "or",
            " of",
            " the",
            " criticism",
            " for",
            " her",
            " first",
            " non",
            "fiction",
            " book",
            ",",
            " For",
            " the",
            " New",
            " Intellectual",
            ",",
            " was",
            " similar",
            " to",
            " that",
            " for",
            " Atlas",
            " Shr",
            "ugged",
            ".",
            " Phil",
            "osopher",
            " Sidney",
            " Hook",
            " liken",
            "ed",
            " her",
            " certainty",
            " to",
            " \"",
            "the",
            " way",
            " philosophy",
            " is",
            " written"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "00",
            " UTC",
            " on",
            " July",
            " ",
            "28",
            ".",
            " Columbia",
            " was",
            " taken",
            " to",
            " Ford",
            " Island",
            " for",
            " de",
            "activation",
            ",",
            " and",
            " its",
            " py",
            "rote",
            "chn",
            "ics",
            " made",
            " safe",
            ".",
            " It",
            " was",
            " then",
            " taken",
            " to",
            " Hick",
            "ham",
            " Air",
            " Force",
            " Base",
            ",",
            " from",
            " whence",
            " it",
            " was",
            " flown",
            " to",
            " Houston",
            " in",
            " a",
            " Douglas",
            " C",
            "-",
            "133",
            " C",
            "arg",
            "om",
            "aster",
            ",",
            " reaching",
            " the",
            " Lunar",
            " Re",
            "ceiving",
            " Laboratory",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " UTC",
            ",",
            " GPS",
            ",",
            " L",
            "OR",
            "AN",
            " and",
            " T",
            "AI",
            "Time",
            " scales",
            "<|begin_of_text|>",
            "Al",
            "tru",
            "ism",
            " is",
            " the",
            " principle",
            " and",
            " practice",
            " of",
            " concern",
            " for",
            " the",
            " well",
            "-being",
            " and",
            "/or",
            " happiness",
            " of",
            " other",
            " humans",
            " or",
            " animals",
            ".",
            " While",
            " objects",
            " of",
            " altru",
            "istic",
            " concern",
            " vary",
            ",",
            " it",
            " is",
            " an",
            " important",
            " moral",
            " value",
            " in",
            " many",
            " cultures",
            " and",
            " religions",
            ".",
            " It",
            " may",
            " be",
            " considered",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.115,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.182,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.125,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Sh",
            "attered",
            " Web",
            " [#",
            "56",
            "–",
            "60",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "13",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " [#",
            "61",
            "–",
            "65",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " #",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "14",
            ":",
            " Ch",
            "ameleon",
            " Conspiracy",
            " [#",
            "66",
            "–",
            "69",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Ch",
            "ameleon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " glob",
            "ular",
            " clusters",
            " Mess",
            "ier",
            "Âł",
            "2",
            ",",
            " Mess",
            "ier",
            "Âł",
            "72",
            ",",
            " and",
            " the",
            " aster",
            "ism",
            " Mess",
            "ier",
            "Âł",
            "73",
            ".",
            " While",
            " M",
            "73",
            " was",
            " originally",
            " catalog",
            "ued",
            " as",
            " a",
            " sp",
            "ars",
            "ely",
            " populated",
            " open",
            " cluster",
            ",",
            " modern",
            " analysis",
            " indicates",
            " the",
            " ",
            "6",
            " main",
            " stars",
            " are",
            " not",
            " close",
            " enough",
            " together",
            " to",
            " fit",
            " this",
            " definition",
            ",",
            " re",
            "class",
            "ifying",
            " M",
            "73"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " potassium",
            "-",
            "40",
            ",",
            " van",
            "adium",
            "-",
            "50",
            ",",
            " lan",
            "than",
            "um",
            "-",
            "138",
            ",",
            " and",
            " lut",
            "et",
            "ium",
            "-",
            "176",
            ".",
            " Most",
            " odd",
            "-",
            "odd",
            " nuclei",
            " are",
            " highly",
            " unstable",
            " with",
            " respect",
            " to",
            " beta",
            " decay",
            ",",
            " because",
            " the",
            " decay",
            " products",
            " are",
            " even",
            "-even",
            ",",
            " and",
            " are",
            " therefore",
            " more",
            " strongly",
            " bound",
            ",",
            " due",
            " to",
            " nuclear",
            " pairing",
            " effects",
            ".",
            "Mass",
            " ",
            "The",
            " large",
            " majority"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.129,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.172,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "57",
            "Âł",
            "minutes",
            ".",
            " Wimbledon",
            ":",
            " The",
            " Record",
            " Break",
            "ers",
            " (",
            "200",
            "5",
            ")",
            " St",
            "arring",
            ":",
            " Andre",
            " Ag",
            "assi",
            ",",
            " Boris",
            " Becker",
            ";",
            " Standing",
            " Room",
            " Only",
            ",",
            " DVD",
            " Release",
            " Date",
            ":",
            " August",
            " ",
            "16",
            ",",
            " ",
            "200",
            "5",
            ",",
            " Run",
            " Time",
            ":",
            " ",
            "52",
            "Âł",
            "minutes",
            ",",
            " .",
            "Video",
            " games",
            " Andre",
            " Ag",
            "assi",
            " Tennis",
            " for",
            " the",
            " Super",
            " Nintendo",
            " Entertainment"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " annual",
            " production",
            " of",
            " aluminium",
            " exceeded",
            " ",
            "50",
            ",",
            "000",
            ",",
            "000",
            " metric",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            "The",
            " real",
            " price",
            " for",
            " aluminium",
            " declined",
            " from",
            " $",
            "14",
            ",",
            "000",
            " per",
            " metric",
            " ton",
            " in",
            " ",
            "190",
            "0",
            " to",
            " $",
            "2",
            ",",
            "340",
            " in",
            " ",
            "194",
            "8",
            " (",
            "in",
            " ",
            "199",
            "8",
            " United",
            " States",
            " dollars",
            ").",
            " Extraction",
            " and",
            " processing",
            " costs",
            " were",
            " lowered",
            " over"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.098,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " DÃ¼nya",
            ".",
            "D",
            "us",
            "in",
            "ber",
            "re",
            ",",
            " El",
            "sp",
            "eth",
            " R",
            ".",
            " M",
            ".",
            " ",
            "201",
            "3",
            ".",
            " Empire",
            ",",
            " Authority",
            ",",
            " and",
            " Aut",
            "onomy",
            " In",
            " A",
            "cha",
            "emen",
            "id",
            " Anat",
            "olia",
            ".",
            " Cambridge",
            ":",
            " Cambridge",
            " University",
            " Press",
            ".",
            "G",
            "ates",
            ",",
            " Charles",
            ",",
            " Jacques",
            " Mor",
            "in",
            ",",
            " and",
            " Thomas",
            " Zimmer",
            "mann",
            ".",
            " ",
            "200",
            "9",
            ".",
            " Sacred",
            " Land",
            "sc",
            "apes"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Become",
            " Human",
            " also",
            " explores",
            " how",
            " android",
            "s",
            " are",
            " treated",
            " as",
            " second",
            " class",
            " citizens",
            " in",
            " a",
            " near",
            " future",
            " society",
            ".",
            "Female",
            " android",
            "s",
            ",",
            " or",
            " \"",
            "g",
            "yn",
            "oids",
            "\",",
            " are",
            " often",
            " seen",
            " in",
            " science",
            " fiction",
            ",",
            " and",
            " can",
            " be",
            " viewed",
            " as",
            " a",
            " continuation",
            " of",
            " the",
            " long",
            " tradition",
            " of",
            " men",
            " attempting",
            " to",
            " create",
            " the",
            " stere",
            "otypical",
            " \"",
            "perfect",
            " woman",
            "\".",
            " Examples",
            " include",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.124,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "34",
            ")",
            " or",
            ",",
            " \"",
            "have",
            " taken",
            " place",
            "\"",
            " (",
            "Luke",
            " ",
            "21",
            ":",
            "32",
            ").",
            " Similarly",
            ",",
            " in",
            " ",
            "1",
            "st",
            " Peter",
            " ",
            "1",
            ":",
            "20",
            ",",
            " \"",
            "Christ",
            ",",
            " who",
            " ver",
            "ily",
            " was",
            " fore",
            "ord",
            "ained",
            " before",
            " the",
            " foundation",
            " of",
            " the",
            " world",
            " but",
            " was",
            " manifest",
            " in",
            " these",
            " last",
            " times",
            " for",
            " you",
            "\",",
            " as",
            " well",
            " as",
            " \"",
            "But",
            " the",
            " end",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "21",
            " languages",
            " of",
            " Burma",
            ",",
            " southern",
            " China",
            ",",
            " and",
            " Thailand",
            " Nuclear",
            " Mon",
            "–",
            "Kh",
            "mer",
            " languages",
            " Kh",
            "mer",
            "o",
            "-V",
            "iet",
            "ic",
            " languages",
            " (",
            "Eastern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " Viet",
            "o",
            "-K",
            "atu",
            "ic",
            " languages",
            " ?",
            " Viet",
            "ic",
            ":",
            " ",
            "10",
            " languages",
            " of",
            " Vietnam",
            " and",
            " Laos",
            ",",
            " including",
            " Mu",
            "ong",
            " and",
            " Vietnamese",
            ",",
            " which",
            " has",
            " the",
            " most",
            " speakers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "2",
            " languages",
            ",",
            " the",
            " Mon",
            " language",
            " of",
            " Burma",
            " and",
            " the",
            " Ny",
            "ah",
            "kur",
            " language",
            " of",
            " Thailand",
            ".",
            "Sid",
            "well",
            " (",
            "200",
            "9",
            "–",
            "201",
            "5",
            ")",
            " ",
            "Paul",
            " Sid",
            "well",
            " (",
            "200",
            "9",
            "),",
            " in",
            " a",
            " le",
            "xic",
            "ostat",
            "istical",
            " comparison",
            " of",
            " ",
            "36",
            " languages",
            " which",
            " are",
            " well",
            " known",
            " enough",
            " to",
            " exclude",
            " loan",
            "words",
            ",",
            " finds",
            " little",
            " evidence",
            " for",
            " internal",
            " branching"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " they",
            " concentrated",
            " on",
            " looking",
            " for",
            " the",
            " philosophers",
            "'",
            " stone",
            ".",
            " Bernard",
            " Tre",
            "vis",
            "an",
            " and",
            " George",
            " Rip",
            "ley",
            " made",
            " similar",
            " contributions",
            ".",
            " Their",
            " crypt",
            "ic",
            " all",
            "usions",
            " and",
            " symbolism",
            " led",
            " to",
            " wide",
            " variations",
            " in",
            " interpretation",
            " of",
            " the",
            " art",
            ".",
            "A",
            " common",
            " idea",
            " in",
            " European",
            " al",
            "chemy",
            " in",
            " the",
            " medieval",
            " era",
            " was",
            " a",
            " metaph",
            "ysical",
            " \"",
            "Hom",
            "eric",
            " chain",
            " of",
            " wise",
            " men",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.219,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.127,
            -0.0,
            -0.0,
            -0.0,
            0.299
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            " Birth",
            " of",
            " the",
            " Newton",
            ":",
            " ",
            " The",
            " Newton",
            " Hall",
            " of",
            " Fame",
            ":",
            " People",
            " behind",
            " the",
            " Newton",
            ":",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Why",
            " did",
            " Apple",
            " kill",
            " the",
            " Newton",
            "?:",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Newton",
            " Notes",
            " column",
            " archive",
            ":",
            " ",
            " A",
            ".I",
            ".",
            " Magazine",
            " article",
            " by",
            " Ya",
            "eger",
            " on",
            " Newton",
            " H",
            "WR",
            " design",
            ",",
            " algorithms",
            ",",
            " &",
            " quality",
            ":",
            " ",
            " Associated",
            " slides",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " On",
            " This",
            " Day",
            " ",
            " Historical",
            " Events",
            " on",
            " April",
            " ",
            "12",
            "Days",
            " of",
            " the",
            " year",
            "April",
            "<|begin_of_text|>",
            "Events",
            "Pre",
            "-",
            "160",
            "0",
            " ",
            "769",
            " –",
            " The",
            " Later",
            "an",
            " Council",
            " ends",
            " by",
            " condemning",
            " the",
            " Council",
            " of",
            " Hier",
            "ia",
            " and",
            " an",
            "ath",
            "emat",
            "izing",
            " its",
            " icon",
            "oc",
            "lastic",
            " rulings",
            ".",
            "107",
            "1",
            " –",
            " B",
            "ari",
            ",",
            " the",
            " last",
            " Byz",
            "antine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " character",
            " who",
            " first",
            " questions",
            " whether",
            " it",
            " is",
            " moral",
            " to",
            " turn",
            " a",
            " violent",
            " person",
            " into",
            " a",
            " behavioural",
            " autom",
            "aton",
            " who",
            " can",
            " make",
            " no",
            " choice",
            " in",
            " such",
            " matters",
            ".",
            " This",
            " is",
            " the",
            " only",
            " character",
            " who",
            " is",
            " truly",
            " concerned",
            " about",
            " Alex",
            "'s",
            " welfare",
            ";",
            " he",
            " is",
            " not",
            " taken",
            " seriously",
            " by",
            " Alex",
            ",",
            " though",
            ".",
            " He",
            " is",
            " nicknamed",
            " by",
            " Alex",
            " \"",
            "pr",
            "ison",
            " char",
            "lie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " two",
            " lengths",
            " must",
            " not",
            " be",
            " prime",
            " to",
            " one",
            " another",
            ".",
            " Eu",
            "clid",
            " stip",
            "ulated",
            " this",
            " so",
            " that",
            " he",
            " could",
            " construct",
            " a",
            " re",
            "duct",
            "io",
            " ad",
            " absurd",
            "um",
            " proof",
            " that",
            " the",
            " two",
            " numbers",
            "'",
            " common",
            " measure",
            " is",
            " in",
            " fact",
            " the",
            " greatest",
            ".",
            " While",
            " Nic",
            "om",
            "ach",
            "us",
            "'",
            " algorithm",
            " is",
            " the",
            " same",
            " as",
            " Eu",
            "clid",
            "'s",
            ",",
            " when",
            " the",
            " numbers",
            " are",
            " prime"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " (",
            "1",
            ")",
            " Korean",
            " did",
            " not",
            " belong",
            " with",
            " the",
            " other",
            " three",
            " gene",
            "alog",
            "ically",
            ",",
            " but",
            " had",
            " been",
            " influenced",
            " by",
            " an",
            " Alta",
            "ic",
            " substr",
            "atum",
            ";",
            " (",
            "2",
            ")",
            " Korean",
            " was",
            " related",
            " to",
            " the",
            " other",
            " three",
            " at",
            " the",
            " same",
            " level",
            " they",
            " were",
            " related",
            " to",
            " each",
            " other",
            ";",
            " (",
            "3",
            ")",
            " Korean",
            " had",
            " split",
            " off",
            " from",
            " the",
            " other",
            " three",
            " before",
            " they",
            " underwent",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.247,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Post",
            "card",
            " Book",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " (",
            "200",
            "2",
            ").",
            " ",
            " H",
            "re",
            "ben",
            "i",
            "ak",
            ",",
            " Michael",
            ".",
            " Action",
            " Writing",
            ":",
            " Jack",
            " Ker",
            "ou",
            "ac",
            "'s",
            " Wild",
            " Form",
            ",",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " UP",
            ",",
            " ",
            "200",
            "6",
            ".",
            " Kash",
            "ner",
            ",",
            " Sam",
            ",",
            " When",
            " I",
            " Was",
            " Cool",
            ",",
            " My",
            " Life",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " \"",
            "If",
            " you",
            " see",
            " something",
            " horrible",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ",",
            " and",
            " if",
            " you",
            " see",
            " something",
            " beautiful",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ".\"",
            "After",
            " returning",
            " to",
            " the",
            " United",
            " States",
            ",",
            " a",
            " chance",
            " encounter",
            " on",
            " a",
            " New",
            " York",
            " City",
            " street",
            " with",
            " Ch",
            "Ã¶",
            "gy",
            "am",
            " Trung",
            "pa",
            " Rin",
            "po",
            "che",
            " (",
            "they",
            " both",
            " tried",
            " to",
            " catch",
            " the",
            " same",
            " cab",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.132,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.232,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " S",
            "ins",
            " of",
            " Norman",
            " Os",
            "born",
            " #",
            "1",
            ",",
            " FC",
            "BD",
            " ",
            "202",
            "0",
            ":",
            " Spider",
            "-Man",
            "/V",
            "en",
            "om",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "11",
            ":",
            " Last",
            " Rem",
            "ains",
            " [#",
            "50",
            "–",
            "55",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            ":",
            " Last",
            " Rem",
            "ains",
            " Companion",
            " [#",
            "50",
            ".",
            "1",
            "–",
            "54",
            ".",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "12"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.244,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "32",
            " UTC",
            ",",
            " and",
            " it",
            " was",
            " the",
            " fifth",
            " crew",
            "ed",
            " mission",
            " of",
            " NASA",
            "'s",
            " Apollo",
            " program",
            ".",
            " The",
            " Apollo",
            " spacecraft",
            " had",
            " three",
            " parts",
            ":",
            " a",
            " command",
            " module",
            " (",
            "CM",
            ")",
            " with",
            " a",
            " cabin",
            " for",
            " the",
            " three",
            " astronauts",
            ",",
            " the",
            " only",
            " part",
            " that",
            " returned",
            " to",
            " Earth",
            ";",
            " a",
            " service",
            " module",
            " (",
            "SM",
            "),",
            " which",
            " supported",
            " the",
            " command",
            " module",
            " with",
            " propulsion",
            ",",
            " electrical",
            " power"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Po",
            "et",
            " in",
            " the",
            " Cinema",
            " (",
            "198",
            "4",
            "):",
            " directed",
            " by",
            " Don",
            "at",
            "ella",
            " Bag",
            "l",
            "ivo",
            ".",
            " Moscow",
            " E",
            "leg",
            "y",
            " (",
            "198",
            "7",
            "),",
            " a",
            " documentary",
            "/h",
            "om",
            "age",
            " to",
            " T",
            "ark",
            "ovsky",
            " by",
            " Aleks",
            "andr",
            " Sok",
            "uro",
            "v",
            ".",
            " Auf",
            " der",
            " Suche",
            " nach",
            " der",
            " ver",
            "lo",
            "ren",
            "en",
            " Zeit",
            " (",
            "198",
            "8",
            "):",
            " Andre",
            "j",
            " T",
            "ark",
            "owski"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " temple",
            " was",
            " narrow",
            ",",
            " and",
            " the",
            " number",
            " of",
            " p",
            "ter",
            "on",
            " columns",
            " (",
            "probably",
            " wooden",
            ")",
            " was",
            " ",
            "5",
            " x",
            " ",
            "15",
            ".",
            " There",
            " was",
            " a",
            " single",
            " row",
            " of",
            " inner",
            " columns",
            ".",
            " It",
            " measures",
            " ",
            "12",
            ".",
            "13",
            " x",
            " ",
            "38",
            ".",
            "23",
            " m",
            " at",
            " the",
            " sty",
            "lob",
            "ate",
            ",",
            " which",
            " was",
            " made",
            " from",
            " stones",
            ".",
            "  ",
            "Cor",
            "inth",
            ":",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " any",
            " Aust",
            "ro",
            "asi",
            "atic",
            " language",
            ".",
            " Kat",
            "u",
            "ic",
            ":",
            " ",
            "19",
            " languages",
            " of",
            " Laos",
            ",",
            " Vietnam",
            ",",
            " and",
            " Thailand",
            ".",
            " Kh",
            "mer",
            "o",
            "-B",
            "ahn",
            "ar",
            "ic",
            " languages",
            " B",
            "ahn",
            "ar",
            "ic",
            ":",
            " ",
            "40",
            " languages",
            " of",
            " Vietnam",
            ",",
            " Laos",
            ",",
            " and",
            " Cambodia",
            ".",
            "Kh",
            "meric",
            " languages",
            " The",
            " Kh",
            "mer",
            " dialect",
            "s",
            " of",
            " Cambodia",
            ",",
            " Thailand",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " franc",
            "ium",
            " (",
            "Fr",
            ").",
            " Together",
            " with",
            " hydrogen",
            " they",
            " constitute",
            " group",
            " ",
            "1",
            ",",
            " which",
            " lies",
            " in",
            " the",
            " s",
            "-block",
            " of",
            " the",
            " periodic",
            " table",
            ".",
            " All",
            " alk",
            "ali",
            " metals",
            " have",
            " their",
            " outer",
            "most",
            " electron",
            " in",
            " an",
            " s",
            "-or",
            "b",
            "ital",
            ":",
            " this",
            " shared",
            " electron",
            " configuration",
            " results",
            " in",
            " their",
            " having",
            " very",
            " similar",
            " characteristic",
            " properties",
            ".",
            " Indeed",
            ",",
            " the",
            " alk",
            "ali",
            " metals",
            " provide",
            " the",
            " best"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " list",
            ".",
            " Polynomial",
            " time",
            ":",
            " if",
            " the",
            " time",
            " is",
            " a",
            " power",
            " of",
            " the",
            " input",
            " size",
            ".",
            " E",
            ".g",
            ".",
            " the",
            " bubble",
            " sort",
            " algorithm",
            " has",
            " quadratic",
            " time",
            " complexity",
            ".",
            " Ex",
            "ponential",
            " time",
            ":",
            " if",
            " the",
            " time",
            " is",
            " an",
            " exponential",
            " function",
            " of",
            " the",
            " input",
            " size",
            ".",
            " E",
            ".g",
            ".",
            " Br",
            "ute",
            "-force",
            " search",
            ".",
            "Some",
            " problems",
            " may",
            " have",
            " multiple",
            " algorithms",
            " of",
            " differing",
            " complexity",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.232,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " manner",
            ".",
            "Mapping",
            " is",
            " possible",
            " for",
            " select",
            " species",
            " only",
            ":",
            " \"",
            "there",
            " are",
            " many",
            " valid",
            " examples",
            " of",
            " confined",
            " distribution",
            " patterns",
            ".\"",
            " For",
            " example",
            ",",
            " Cl",
            "ath",
            "rom",
            "orph",
            "um",
            " is",
            " an",
            " ar",
            "ctic",
            " genus",
            " and",
            " is",
            " not",
            " mapped",
            " far",
            " south",
            " of",
            " there",
            ".",
            " However",
            ",",
            " scientists",
            " regard",
            " the",
            " overall",
            " data",
            " as",
            " insufficient",
            " due",
            " to",
            " the",
            " \"",
            "diff",
            "icult",
            "ies",
            " of",
            " undertaking",
            " such",
            " studies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "ÃŃa",
            ",",
            " tur",
            "ismo",
            " y",
            " de",
            " via",
            "jes",
            " History",
            " of",
            " And",
            "orra",
            ":",
            " Primary",
            " Documents",
            " from",
            " Euro",
            "Docs",
            " A",
            " New",
            " Path",
            " for",
            " And",
            "orra",
            " –",
            " slideshow",
            " by",
            " The",
            " New",
            " York",
            " Times",
            " ",
            " ",
            " ",
            "127",
            "8",
            " establishments",
            " in",
            " Europe",
            "C",
            "atal",
            "an",
            " Countries",
            "Christian",
            " states",
            "Countries",
            " in",
            " Europe",
            "Di",
            "arch",
            "ies",
            "D",
            "uty",
            "-free",
            " zones",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.218,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " On",
            " ",
            "27",
            " February",
            " ",
            "184",
            "1",
            ",",
            " Ada",
            " wrote",
            " to",
            " her",
            " mother",
            ":",
            " \"",
            "I",
            " am",
            " not",
            " in",
            " the",
            " least",
            " astonished",
            ".",
            " In",
            " fact",
            ",",
            " you",
            " merely",
            " confirm",
            " what",
            " I",
            " have",
            " for",
            " years",
            " and",
            " years",
            " felt",
            " scarcely",
            " a",
            " doubt",
            " about",
            ",",
            " but",
            " should",
            " have",
            " considered",
            " it",
            " most",
            " improper",
            " in",
            " me",
            " to",
            " hint",
            " to",
            " you",
            " that",
            " I",
            " in",
            " any",
            " way",
            " suspected",
            ".\""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "Works",
            " cited",
            " Edmund",
            " White",
            ",",
            " ",
            " AndrÃ©",
            " G",
            "ide",
            ":",
            " A",
            " Life",
            " in",
            " the",
            " Present",
            ".",
            " Cambridge",
            ",",
            " MA",
            ":",
            " Harvard",
            " University",
            " Press",
            ",",
            " ",
            "199",
            "8",
            ".]",
            "Further",
            " reading",
            " Noel",
            " I",
            ".",
            " Gar",
            "de",
            " [",
            "Ed",
            "gar",
            " H",
            ".",
            " Le",
            "oni",
            "],",
            " Jonathan",
            " to",
            " G",
            "ide",
            ":",
            " The",
            " Hom",
            "osexual",
            " in",
            " History",
            ".",
            " New",
            " York",
            ":",
            "V",
            "ang",
            "ard"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " diagnosis",
            " of",
            " AHL",
            " are",
            ":",
            " peripheral",
            " leuk",
            "oc",
            "yt",
            "osis",
            ",",
            " cere",
            "bro",
            "sp",
            "inal",
            " fluid",
            " (",
            "CS",
            "F",
            ")",
            " ple",
            "oc",
            "yt",
            "osis",
            " associated",
            " with",
            " normal",
            " glucose",
            " and",
            " increased",
            " protein",
            ".",
            " On",
            " magnetic",
            " resonance",
            " imaging",
            " (",
            "MRI",
            "),",
            " lesions",
            " of",
            " AHL",
            " typically",
            " show",
            " extensive",
            " T",
            "2",
            "-weight",
            "ed",
            " and",
            " fluid",
            "-",
            "att",
            "enu",
            "ated",
            " inversion",
            " recovery",
            " (",
            "FL",
            "AIR",
            ")",
            " white",
            " matter"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.204,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.191,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.207,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            "Âµ",
            "g",
            "/L",
            " Japan",
            ":",
            " ",
            "15",
            "Âł",
            "Âµ",
            "g",
            "/L",
            " United",
            " States",
            " Environmental",
            " Protection",
            " Agency",
            ",",
            " Health",
            " Canada",
            " and",
            " the",
            " Ontario",
            " Ministry",
            " of",
            " Environment",
            ":",
            " ",
            "6",
            "Âł",
            "Âµ",
            "g",
            "/L",
            " EU",
            " and",
            " German",
            " Federal",
            " Ministry",
            " of",
            " Environment",
            ":",
            " ",
            "5",
            "Âł",
            "Âµ",
            "g",
            "/L",
            "The",
            " toler",
            "able",
            " daily",
            " intake",
            " (",
            "TD",
            "I",
            ")",
            " proposed",
            " by",
            " WHO",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            "iba",
            ".",
            "In",
            " ",
            "184",
            "8",
            ",",
            " From",
            "ental",
            " Hal",
            "Ã©",
            "vy",
            " had",
            " premiered",
            " the",
            " opera",
            " Le",
            " Val",
            " d",
            "'",
            "And",
            "orre",
            " to",
            " great",
            " success",
            " in",
            " Europe",
            ",",
            " where",
            " the",
            " national",
            " consciousness",
            " of",
            " the",
            " valleys",
            " was",
            " exposed",
            " in",
            " the",
            " romantic",
            " work",
            " during",
            " the",
            " Pen",
            "ins",
            "ular",
            " War",
            ".",
            "20",
            "th",
            " and",
            " ",
            "21",
            "st",
            " century",
            ":",
            " Modern",
            "isation",
            " of",
            " the",
            " country",
            " and",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.162,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " radi",
            "ated",
            " out",
            " from",
            " the",
            " central",
            " Mek",
            "ong",
            " river",
            " valley",
            " relatively",
            " quickly",
            ".",
            "Sub",
            "sequently",
            ",",
            " Sid",
            "well",
            " (",
            "201",
            "5",
            "a",
            ":",
            " ",
            "179",
            ")",
            " proposed",
            " that",
            " Nic",
            "ob",
            "are",
            "se",
            " sub",
            "groups",
            " with",
            " As",
            "lian",
            ",",
            " just",
            " as",
            " how",
            " K",
            "has",
            "ian",
            " and",
            " P",
            "ala",
            "ung",
            "ic",
            " subgroup",
            " with",
            " each",
            " other",
            ".",
            "A",
            " subsequent",
            " computational",
            " phy",
            "logen",
            "etic",
            " analysis",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Apollo",
            " ",
            "11",
            ":",
            " As",
            " It",
            " Happ",
            "ened",
            ",",
            " a",
            " ",
            "199",
            "4",
            " six",
            "-hour",
            " documentary",
            " on",
            " ABC",
            " News",
            "'",
            " coverage",
            " of",
            " the",
            " event",
            " First",
            " Man",
            ",",
            " ",
            "201",
            "8",
            " film",
            " by",
            " Damien",
            " Ch",
            "az",
            "elle",
            " based",
            " on",
            " the",
            " ",
            "200",
            "5",
            " James",
            " R",
            ".",
            " Hansen",
            " book",
            " First",
            " Man",
            ":",
            " The",
            " Life",
            " of",
            " Neil",
            " A",
            ".",
            " Armstrong",
            ".",
            " Apollo",
            " ",
            "11",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "eding",
            " of",
            " Cult",
            "ivated",
            " Plants",
            ".",
            " Selected",
            " W",
            "rit",
            "ings",
            ",",
            " in",
            " Chron",
            "ica",
            " bot",
            "an",
            "ica",
            ",",
            " ",
            "13",
            ":",
            " ",
            "1",
            "–",
            "6",
            ",",
            " W",
            "alth",
            "am",
            ",",
            " Mass",
            ".,",
            " ",
            "194",
            "9",
            "–",
            "50",
            "V",
            "av",
            "il",
            "ov",
            " Nicol",
            "ai",
            " I",
            ".,",
            " World",
            " Resources",
            " of",
            " C",
            "ere",
            "als",
            ",",
            " Leg",
            "umin",
            "ous",
            " Seed",
            " C",
            "rops",
            " and",
            " Fl",
            "ax",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "pp",
            "b",
            " (",
            "i",
            ".e",
            ".,",
            " above",
            " the",
            " ",
            "10",
            "Âł",
            "pp",
            "b",
            " drinking",
            " water",
            " standard",
            ")",
            " compromises",
            " the",
            " initial",
            " immune",
            " response",
            " to",
            " H",
            "1",
            "N",
            "1",
            " or",
            " sw",
            "ine",
            " flu",
            " infection",
            " according",
            " to",
            " N",
            "IE",
            "HS",
            "-supported",
            " scientists",
            ".",
            " The",
            " study",
            ",",
            " conducted",
            " in",
            " laboratory",
            " mice",
            ",",
            " suggests",
            " that",
            " people",
            " exposed",
            " to",
            " arsen",
            "ic",
            " in",
            " their",
            " drinking",
            " water",
            " may",
            " be",
            " at",
            " increased"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "188",
            "1",
            ")",
            " ",
            " (",
            "The",
            " As",
            "pir",
            "ations",
            " of",
            " Jean",
            " Serv",
            "ien",
            ")",
            " (",
            "188",
            "2",
            ")",
            " ",
            " (",
            "H",
            "oney",
            "-B",
            "ee",
            ")",
            " (",
            "188",
            "3",
            ")",
            " ",
            " (",
            "188",
            "9",
            ")",
            " ",
            " (",
            "189",
            "0",
            ")",
            " ",
            " (",
            "Mother",
            " of",
            " Pearl",
            ")",
            " (",
            "189",
            "2",
            ")",
            " ",
            " (",
            "At",
            " the",
            " Sign",
            " of",
            " the",
            " Re",
            "ine",
            " P",
            "Ã©d",
            "au",
            "que",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " debris",
            " disk",
            " detected",
            " in",
            " a",
            " moving",
            " group",
            " at",
            " the",
            " time",
            " of",
            " the",
            " discovery",
            ".",
            "PS",
            "R",
            " J",
            "191",
            "3",
            "+",
            "110",
            "2",
            " (",
            "201",
            "6",
            ")",
            " is",
            " a",
            " binary",
            " neutron",
            " star",
            " with",
            " the",
            " highest",
            " total",
            " mass",
            " at",
            " the",
            " time",
            " of",
            " the",
            " discovery",
            ".",
            "Don",
            "ati",
            "ello",
            " I",
            " (",
            "201",
            "6",
            ")",
            " a",
            " nearby",
            " s",
            "pher",
            "oidal",
            " dwarf",
            " galaxy",
            " discovered",
            " by",
            " the",
            " Italian",
            " amateur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Hitch",
            "cock",
            " was",
            " required",
            " to",
            " wear",
            " put",
            "tees",
            ".",
            " He",
            " could",
            " never",
            " master",
            " wrapping",
            " them",
            " around",
            " his",
            " legs",
            ",",
            " and",
            " they",
            " repeatedly",
            " fell",
            " down",
            " around",
            " his",
            " ankles",
            ".",
            "After",
            " the",
            " war",
            ",",
            " Hitch",
            "cock",
            " took",
            " an",
            " interest",
            " in",
            " creative",
            " writing",
            ".",
            " In",
            " June",
            " ",
            "191",
            "9",
            ",",
            " he",
            " became",
            " a",
            " founding",
            " editor",
            " and",
            " business",
            " manager",
            " of",
            " Hen",
            "ley",
            "'s",
            " in",
            "-house",
            " publication",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " tac",
            "it",
            " American",
            " and",
            " Z",
            "aire",
            "an",
            " support",
            " the",
            " FN",
            "LA",
            " began",
            " mass",
            "ing",
            " large",
            " numbers",
            " of",
            " troops",
            " in",
            " northern",
            " Angola",
            " in",
            " an",
            " attempt",
            " to",
            " gain",
            " military",
            " superiority",
            ".",
            " Meanwhile",
            ",",
            " the",
            " MPL",
            "A",
            " began",
            " securing",
            " control",
            " of",
            " Lu",
            "anda",
            ",",
            " a",
            " traditional",
            " Amb",
            "und",
            "u",
            " stronghold",
            ".",
            " Spor",
            "adic",
            " violence",
            " broke",
            " out",
            " in",
            " Lu",
            "anda",
            " over",
            " the",
            " next",
            " few",
            " months",
            " after",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "RAINT",
    "orda",
    "oulos",
    "CTYPE",
    "olet"
  ],
  "bottom_logits": [
    "ely",
    " Power",
    "-power",
    " hol",
    "Ã¶t"
  ],
  "act_min": -0.0,
  "act_max": 0.496
}