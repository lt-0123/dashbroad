{
  "index": 74111,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alps",
            ":",
            " Regional",
            " Perspectives",
            " on",
            " Climate",
            " Change",
            ".",
            " Cambridge",
            " MA",
            ":",
            " MIT",
            " Press",
            ".",
            " ",
            " Chat",
            "rÃ©",
            ",",
            " Bapt",
            "iste",
            ",",
            " et",
            " al",
            ".",
            " (",
            "201",
            "0",
            ").",
            " The",
            " Alps",
            ":",
            " People",
            " and",
            " Press",
            "ures",
            " in",
            " the",
            " Mountains",
            ",",
            " the",
            " Facts",
            " at",
            " a",
            " Gl",
            "ance",
            ".",
            " Permanent",
            " Secret",
            "ariat",
            " of",
            " the",
            " Alpine",
            " Convention",
            " (",
            "al",
            "p",
            "conv",
            ".org",
            ").",
            " Retrieved",
            " August",
            " ",
            "4"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alps",
            ":",
            " Regional",
            " Perspectives",
            " on",
            " Climate",
            " Change",
            ".",
            " Cambridge",
            " MA",
            ":",
            " MIT",
            " Press",
            ".",
            " ",
            " Chat",
            "rÃ©",
            ",",
            " Bapt",
            "iste",
            ",",
            " et",
            " al",
            ".",
            " (",
            "201",
            "0",
            ").",
            " The",
            " Alps",
            ":",
            " People",
            " and",
            " Press",
            "ures",
            " in",
            " the",
            " Mountains",
            ",",
            " the",
            " Facts",
            " at",
            " a",
            " Gl",
            "ance",
            ".",
            " Permanent",
            " Secret",
            "ariat",
            " of",
            " the",
            " Alpine",
            " Convention",
            " (",
            "al",
            "p",
            "conv",
            ".org",
            ").",
            " Retrieved",
            " August",
            " ",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.609,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Yale",
            " Collection",
            " of",
            " Western",
            " Americ",
            "ana",
            ",",
            " Be",
            "ine",
            "cke",
            " Rare",
            " Book",
            " and",
            " Manus",
            "cript",
            " Library",
            ".",
            "U",
            ".S",
            ".",
            " federal",
            " government",
            " Alaska",
            " State",
            " Guide",
            " from",
            " the",
            " Library",
            " of",
            " Congress",
            " Energy",
            " &",
            " Environmental",
            " Data",
            " for",
            " Alaska",
            " US",
            "GS",
            " real",
            "-time",
            ",",
            " geographic",
            ",",
            " and",
            " other",
            " scientific",
            " resources",
            " of",
            " Alaska",
            " ",
            " US",
            " Census",
            " Bureau",
            " Alaska",
            " State",
            " Facts",
            " Alaska"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.609,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Yale",
            " Collection",
            " of",
            " Western",
            " Americ",
            "ana",
            ",",
            " Be",
            "ine",
            "cke",
            " Rare",
            " Book",
            " and",
            " Manus",
            "cript",
            " Library",
            ".",
            "U",
            ".S",
            ".",
            " federal",
            " government",
            " Alaska",
            " State",
            " Guide",
            " from",
            " the",
            " Library",
            " of",
            " Congress",
            " Energy",
            " &",
            " Environmental",
            " Data",
            " for",
            " Alaska",
            " US",
            "GS",
            " real",
            "-time",
            ",",
            " geographic",
            ",",
            " and",
            " other",
            " scientific",
            " resources",
            " of",
            " Alaska",
            " ",
            " US",
            " Census",
            " Bureau",
            " Alaska",
            " State",
            " Facts",
            " Alaska"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "onaut",
            "ica",
            ":",
            " Phantom",
            " cos",
            "mon",
            "aut",
            "s",
            " collect",
            "SPACE",
            ":",
            " Astr",
            "onaut",
            " appearances",
            " calendar",
            " space",
            "facts",
            " Space",
            "facts",
            ".de",
            " M",
            "anned",
            " astronaut",
            "ics",
            ":",
            " facts",
            " and",
            " figures",
            " Astr",
            "onaut",
            " Candidate",
            " Bro",
            "chure",
            " online",
            " ",
            " ",
            "Science",
            " occupations",
            "195",
            "9",
            " introdu",
            "ctions",
            "<|begin_of_text|>",
            "A",
            " Mod",
            "est",
            " Proposal",
            " For",
            " preventing",
            " the",
            " Children",
            " of",
            " Poor",
            " People",
            " From",
            " being",
            " a",
            " Bur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "onaut",
            "ica",
            ":",
            " Phantom",
            " cos",
            "mon",
            "aut",
            "s",
            " collect",
            "SPACE",
            ":",
            " Astr",
            "onaut",
            " appearances",
            " calendar",
            " space",
            "facts",
            " Space",
            "facts",
            ".de",
            " M",
            "anned",
            " astronaut",
            "ics",
            ":",
            " facts",
            " and",
            " figures",
            " Astr",
            "onaut",
            " Candidate",
            " Bro",
            "chure",
            " online",
            " ",
            " ",
            "Science",
            " occupations",
            "195",
            "9",
            " introdu",
            "ctions",
            "<|begin_of_text|>",
            "A",
            " Mod",
            "est",
            " Proposal",
            " For",
            " preventing",
            " the",
            " Children",
            " of",
            " Poor",
            " People",
            " From",
            " being",
            " a",
            " Bur"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "onaut",
            "ica",
            ":",
            " Phantom",
            " cos",
            "mon",
            "aut",
            "s",
            " collect",
            "SPACE",
            ":",
            " Astr",
            "onaut",
            " appearances",
            " calendar",
            " space",
            "facts",
            " Space",
            "facts",
            ".de",
            " M",
            "anned",
            " astronaut",
            "ics",
            ":",
            " facts",
            " and",
            " figures",
            " Astr",
            "onaut",
            " Candidate",
            " Bro",
            "chure",
            " online",
            " ",
            " ",
            "Science",
            " occupations",
            "195",
            "9",
            " introdu",
            "ctions",
            "<|begin_of_text|>",
            "A",
            " Mod",
            "est",
            " Proposal",
            " For",
            " preventing",
            " the",
            " Children",
            " of",
            " Poor",
            " People",
            " From",
            " being",
            " a",
            " Bur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "onaut",
            "ica",
            ":",
            " Phantom",
            " cos",
            "mon",
            "aut",
            "s",
            " collect",
            "SPACE",
            ":",
            " Astr",
            "onaut",
            " appearances",
            " calendar",
            " space",
            "facts",
            " Space",
            "facts",
            ".de",
            " M",
            "anned",
            " astronaut",
            "ics",
            ":",
            " facts",
            " and",
            " figures",
            " Astr",
            "onaut",
            " Candidate",
            " Bro",
            "chure",
            " online",
            " ",
            " ",
            "Science",
            " occupations",
            "195",
            "9",
            " introdu",
            "ctions",
            "<|begin_of_text|>",
            "A",
            " Mod",
            "est",
            " Proposal",
            " For",
            " preventing",
            " the",
            " Children",
            " of",
            " Poor",
            " People",
            " From",
            " being",
            " a",
            " Bur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " inside",
            " a",
            " glass",
            " of",
            " milk",
            ",",
            " perhaps",
            " poisoned",
            ",",
            " that",
            " Grant",
            " is",
            " bringing",
            " to",
            " his",
            " wife",
            ";",
            " the",
            " light",
            " ensures",
            " that",
            " the",
            " audience",
            "'s",
            " attention",
            " is",
            " on",
            " the",
            " glass",
            ".",
            " Grant",
            "'s",
            " character",
            " is",
            " actually",
            " a",
            " killer",
            ",",
            " as",
            " per",
            " written",
            " in",
            " the",
            " book",
            ",",
            " Before",
            " the",
            " Fact",
            " by",
            " Francis",
            " I",
            "les",
            ",",
            " but",
            " the",
            " studio",
            " felt",
            " that",
            " Grant",
            "'s",
            " image",
            " would",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-century",
            " American",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " American",
            " novel",
            "ists",
            "20",
            "th",
            "-century",
            " American",
            " short",
            " story",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " short",
            " story",
            " writers",
            "American",
            " male",
            " novel",
            "ists",
            "American",
            " male",
            " short",
            " story",
            " writers",
            "American",
            " science",
            " fiction",
            " writers",
            "An",
            "alog",
            " Science",
            " Fiction",
            " and",
            " Fact",
            " people",
            "Canadian",
            " M",
            "ennon",
            "ites"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " This",
            " caric",
            "ature",
            " wholly",
            " mis",
            "rep",
            "resents",
            " and",
            " dist",
            "orts",
            " the",
            " facts",
            " of",
            " the",
            " matter",
            "\"",
            " in",
            " every",
            " instance",
            ".",
            " The",
            " Lost",
            " Cause",
            " myth",
            " was",
            " formal",
            "ized",
            " by",
            " Charles",
            " A",
            ".",
            " Beard",
            " and",
            " Mary",
            " R",
            ".",
            " Beard",
            ",",
            " whose",
            " The",
            " Rise",
            " of",
            " American",
            " Civilization",
            " (",
            "192",
            "7",
            ")",
            " spawned",
            " \"",
            "Be",
            "ard",
            "ian",
            " histor",
            "i",
            "ography",
            "\".",
            " The",
            " Be",
            "ards",
            " down",
            "played"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " This",
            " caric",
            "ature",
            " wholly",
            " mis",
            "rep",
            "resents",
            " and",
            " dist",
            "orts",
            " the",
            " facts",
            " of",
            " the",
            " matter",
            "\"",
            " in",
            " every",
            " instance",
            ".",
            " The",
            " Lost",
            " Cause",
            " myth",
            " was",
            " formal",
            "ized",
            " by",
            " Charles",
            " A",
            ".",
            " Beard",
            " and",
            " Mary",
            " R",
            ".",
            " Beard",
            ",",
            " whose",
            " The",
            " Rise",
            " of",
            " American",
            " Civilization",
            " (",
            "192",
            "7",
            ")",
            " spawned",
            " \"",
            "Be",
            "ard",
            "ian",
            " histor",
            "i",
            "ography",
            "\".",
            " The",
            " Be",
            "ards",
            " down",
            "played"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " much",
            " of",
            " the",
            " world",
            ",",
            " court",
            " systems",
            " are",
            " divided",
            " into",
            " at",
            " least",
            " three",
            " levels",
            ":",
            " the",
            " trial",
            " court",
            ",",
            " which",
            " initially",
            " hears",
            " cases",
            " and",
            " reviews",
            " evidence",
            " and",
            " testimony",
            " to",
            " determine",
            " the",
            " facts",
            " of",
            " the",
            " case",
            ";",
            " at",
            " least",
            " one",
            " intermediate",
            " appellate",
            " court",
            ";",
            " and",
            " a",
            " supreme",
            " court",
            " (",
            "or",
            " court",
            " of",
            " last",
            " resort",
            ")",
            " which",
            " primarily",
            " reviews",
            " the",
            " decisions",
            " of",
            " the",
            " intermediate",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " much",
            " of",
            " the",
            " world",
            ",",
            " court",
            " systems",
            " are",
            " divided",
            " into",
            " at",
            " least",
            " three",
            " levels",
            ":",
            " the",
            " trial",
            " court",
            ",",
            " which",
            " initially",
            " hears",
            " cases",
            " and",
            " reviews",
            " evidence",
            " and",
            " testimony",
            " to",
            " determine",
            " the",
            " facts",
            " of",
            " the",
            " case",
            ";",
            " at",
            " least",
            " one",
            " intermediate",
            " appellate",
            " court",
            ";",
            " and",
            " a",
            " supreme",
            " court",
            " (",
            "or",
            " court",
            " of",
            " last",
            " resort",
            ")",
            " which",
            " primarily",
            " reviews",
            " the",
            " decisions",
            " of",
            " the",
            " intermediate",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            0.488,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " particular",
            " case",
            " itself",
            " and",
            " then",
            " ask",
            " what",
            " morally",
            " significant",
            " features",
            " (",
            "including",
            " both",
            " theory",
            " and",
            " practical",
            " considerations",
            ")",
            " ought",
            " to",
            " be",
            " considered",
            " for",
            " that",
            " particular",
            " case",
            ".",
            " In",
            " their",
            " observations",
            " of",
            " medical",
            " ethics",
            " committees",
            ",",
            " J",
            "ons",
            "en",
            " and",
            " T",
            "oul",
            "min",
            " note",
            " that",
            " a",
            " consensus",
            " on",
            " particularly",
            " problematic",
            " moral",
            " cases",
            " often",
            " emerges",
            " when",
            " participants",
            " focus",
            " on",
            " the",
            " facts",
            " of",
            " the",
            " case"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "agnet",
            "ism",
            ",",
            " Einstein",
            "'s",
            " equation",
            " in",
            " general",
            " rel",
            "ativity",
            ",",
            " Mend",
            "el",
            "'s",
            " laws",
            " of",
            " genetics",
            ",",
            " Darwin",
            "'s",
            " Natural",
            " selection",
            " law",
            ",",
            " etc",
            ".",
            " These",
            " founding",
            " assertions",
            " are",
            " usually",
            " called",
            " principles",
            " or",
            " post",
            "ulates",
            " so",
            " as",
            " to",
            " distinguish",
            " from",
            " mathematical",
            " ax",
            "ioms",
            ".",
            "As",
            " a",
            " matter",
            " of",
            " facts",
            ",",
            " the",
            " role",
            " of",
            " ax",
            "ioms",
            " in",
            " mathematics",
            " and",
            " post",
            "ulates",
            " in",
            " experimental"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "In",
            " the",
            " United",
            " States",
            ",",
            " both",
            " state",
            " and",
            " federal",
            " appellate",
            " courts",
            " are",
            " usually",
            " restricted",
            " to",
            " examining",
            " whether",
            " the",
            " lower",
            " court",
            " made",
            " the",
            " correct",
            " legal",
            " determin",
            "ations",
            ",",
            " rather",
            " than",
            " hearing",
            " direct",
            " evidence",
            " and",
            " determining",
            " what",
            " the",
            " facts",
            " of",
            " the",
            " case",
            " were",
            ".",
            " Furthermore",
            ",",
            " U",
            ".S",
            ".",
            " appellate",
            " courts",
            " are",
            " usually",
            " restricted",
            " to",
            " hearing",
            " appeals",
            " based",
            " on",
            " matters",
            " that",
            " were",
            " originally",
            " brought",
            " up"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "In",
            " the",
            " United",
            " States",
            ",",
            " both",
            " state",
            " and",
            " federal",
            " appellate",
            " courts",
            " are",
            " usually",
            " restricted",
            " to",
            " examining",
            " whether",
            " the",
            " lower",
            " court",
            " made",
            " the",
            " correct",
            " legal",
            " determin",
            "ations",
            ",",
            " rather",
            " than",
            " hearing",
            " direct",
            " evidence",
            " and",
            " determining",
            " what",
            " the",
            " facts",
            " of",
            " the",
            " case",
            " were",
            ".",
            " Furthermore",
            ",",
            " U",
            ".S",
            ".",
            " appellate",
            " courts",
            " are",
            " usually",
            " restricted",
            " to",
            " hearing",
            " appeals",
            " based",
            " on",
            " matters",
            " that",
            " were",
            " originally",
            " brought",
            " up"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Azerbaijan",
            " at",
            " University",
            " of",
            " Colorado",
            " at",
            " Boulder",
            " Country",
            " profile",
            " from",
            " BBC",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Azerbaijan",
            " from",
            " International",
            " Futures",
            " V",
            "isions",
            " of",
            " Azerbaijan",
            " Journal",
            " of",
            " The",
            " European",
            " Azerbaijan",
            " Society",
            "Major",
            " government",
            " resources",
            " President",
            " of",
            " Azerbaijan",
            " website",
            " Azerbaijan",
            " State",
            " Statistical",
            " Committee",
            " United",
            " Nations",
            " Office",
            " in",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " marked",
            " by",
            " the",
            " fact",
            " of",
            " having",
            " sensation",
            " and",
            " being",
            " alive",
            ".\"",
            "In",
            " the",
            " Catholic",
            " Church",
            ",",
            " opinion",
            " was",
            " divided",
            " on",
            " how",
            " serious",
            " abortion",
            " was",
            " in",
            " comparison",
            " with",
            " such",
            " acts",
            " as",
            " contraception",
            ",",
            " oral",
            " sex",
            ",",
            " and",
            " sex",
            " in",
            " marriage",
            " for",
            " pleasure",
            " rather",
            " than",
            " pro",
            "creation",
            ".",
            " The",
            " Catholic",
            " Church",
            " did",
            " not",
            " begin",
            " vigorously",
            " opposing",
            " abortion",
            " until",
            " the",
            " ",
            "19",
            "th",
            " century",
            ".",
            " As"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alabama",
            " ",
            " Alabama",
            " Quick",
            "F",
            "acts",
            " from",
            " the",
            " U",
            ".S",
            ".",
            " Census",
            " Bureau",
            " Alabama",
            " State",
            " Fact",
            " Sheet",
            " ",
            " ",
            "181",
            "9",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "Southern",
            " United",
            " States",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "181",
            "9",
            "States",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            "States",
            " of",
            " the",
            " Gulf",
            " Coast",
            " of",
            " the",
            " United",
            " States",
            "States",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alabama",
            " ",
            " Alabama",
            " Quick",
            "F",
            "acts",
            " from",
            " the",
            " U",
            ".S",
            ".",
            " Census",
            " Bureau",
            " Alabama",
            " State",
            " Fact",
            " Sheet",
            " ",
            " ",
            "181",
            "9",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "Southern",
            " United",
            " States",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "181",
            "9",
            "States",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            "States",
            " of",
            " the",
            " Gulf",
            " Coast",
            " of",
            " the",
            " United",
            " States",
            "States",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.059,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " One",
            " approach",
            " attempting",
            " to",
            " overcome",
            " the",
            " divide",
            " between",
            " consequential",
            "ism",
            " and",
            " de",
            "ontology",
            " is",
            " case",
            "-based",
            " reasoning",
            ",",
            " also",
            " known",
            " as",
            " cas",
            "u",
            "istry",
            ".",
            " Cas",
            "u",
            "istry",
            " does",
            " not",
            " begin",
            " with",
            " theory",
            ",",
            " rather",
            " it",
            " starts",
            " with",
            " the",
            " immediate",
            " facts",
            " of",
            " a",
            " real",
            " and",
            " concrete",
            " case",
            ".",
            " While",
            " cas",
            "u",
            "istry",
            " makes",
            " use",
            " of",
            " ethical",
            " theory",
            ",",
            " it",
            " does",
            " not",
            " view"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ant",
            "igua",
            ",",
            " West",
            " Indies",
            ".",
            " Thomas",
            " Hear",
            "ne",
            ".",
            " Southampton",
            ".",
            "External",
            " links",
            "  ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "rav",
            "els",
            ":",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " the",
            " L",
            "us",
            "aka",
            " Peace",
            " Process",
            ".",
            " New",
            " York",
            " and",
            " London",
            ",",
            " UK",
            ",",
            " Human",
            " Rights",
            " Watch",
            ".",
            "External",
            " links",
            " ",
            "Ang",
            "ola",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "Ang",
            "ola",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            ".",
            "Ang",
            "ola",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            ".",
            "Key",
            " Development",
            " Fore"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "196",
            "0",
            ".",
            "External",
            " links",
            " Govern",
            " d",
            "'",
            "And",
            "orra",
            " Official",
            " governmental",
            " site",
            " ",
            " And",
            "orra",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Port",
            "als",
            " to",
            " the",
            " World",
            " from",
            " the",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " And",
            "orra",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " And",
            "orra",
            " from",
            " the",
            " BBC",
            " News",
            " And",
            "orra",
            " –",
            " Gu"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            "Further",
            " reading",
            "External",
            " links",
            " ",
            " Afghanistan",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " ",
            " Research",
            " Guide",
            " to",
            " Afghanistan",
            " ",
            " ",
            "170",
            "9",
            " establishments",
            " in",
            " Asia",
            "Central",
            " Asian",
            " countries",
            "Countries",
            " in",
            " Asia",
            "Em",
            "irates",
            "Iran",
            "ian",
            " Plate",
            "au",
            "Islamic",
            " states",
            "Land",
            "locked",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " First",
            " Ministry",
            " Portal",
            " of",
            " the",
            " First",
            " Ministry",
            " Algeria",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " Algeria",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            " ",
            " ",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Algeria",
            " from",
            " International",
            " Futures",
            " EU",
            " Ne",
            "ighbour",
            "hood",
            " Info",
            " Centre",
            ":",
            " Algeria",
            " ",
            "North",
            " African",
            " countries",
            "Mag",
            "h",
            "re",
            "bi",
            " countries",
            "S",
            "ah",
            "aran",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "1",
            ".",
            "02",
            " male",
            "(s",
            ")/",
            "female",
            " (",
            "201",
            "1",
            " est",
            ".)",
            "Urban",
            "ization",
            "urban",
            " population",
            ":",
            " ",
            "68",
            ".",
            "1",
            "%",
            " of",
            " total",
            " population",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "4",
            ".",
            "04",
            "%",
            " annual",
            " rate",
            " of",
            " change",
            " (",
            "202",
            "0",
            "-",
            "202",
            "5",
            " est",
            ".)",
            "Health",
            "According",
            " to",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "2",
            "%",
            " of",
            " adults",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ban",
            "ia",
            ".al",
            "pres",
            "ident",
            ".al",
            "k",
            "ry",
            "emin",
            "ist",
            "ria",
            ".al",
            "par",
            "lament",
            ".al",
            " ",
            "Al",
            "ban",
            "ia",
            " at",
            " The",
            " World",
            " Fact",
            "book",
            " by",
            " Central",
            " Intelligence",
            " Agency",
            " (",
            "C",
            "IA",
            ")",
            " ",
            "Countries",
            " and",
            " territories",
            " where",
            " Alban",
            "ian",
            " is",
            " an",
            " official",
            " language",
            "B",
            "alk",
            "an",
            " countries",
            "Countries",
            " in",
            " Europe",
            "Member",
            " states",
            " of",
            " NATO",
            "Member",
            " states"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ban",
            "ia",
            ".al",
            "pres",
            "ident",
            ".al",
            "k",
            "ry",
            "emin",
            "ist",
            "ria",
            ".al",
            "par",
            "lament",
            ".al",
            " ",
            "Al",
            "ban",
            "ia",
            " at",
            " The",
            " World",
            " Fact",
            "book",
            " by",
            " Central",
            " Intelligence",
            " Agency",
            " (",
            "C",
            "IA",
            ")",
            " ",
            "Countries",
            " and",
            " territories",
            " where",
            " Alban",
            "ian",
            " is",
            " an",
            " official",
            " language",
            "B",
            "alk",
            "an",
            " countries",
            "Countries",
            " in",
            " Europe",
            "Member",
            " states",
            " of",
            " NATO",
            "Member",
            " states"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " most",
            " are",
            " thought",
            " to",
            " adhere",
            " to",
            " the",
            " Sunni",
            " Han",
            "afi",
            " school",
            ".",
            " According",
            " to",
            " Pew",
            " Research",
            " Center",
            ",",
            " as",
            " much",
            " as",
            " ",
            "90",
            "%",
            " are",
            " of",
            " the",
            " Sunni",
            " denomination",
            ",",
            " ",
            "7",
            "%",
            " Shia",
            " and",
            " ",
            "3",
            "%",
            " non",
            "-den",
            "omin",
            "ational",
            ".",
            " The",
            " CIA",
            " Fact",
            "book",
            " various",
            "ly",
            " estimates",
            " up",
            " to",
            " ",
            "89",
            ".",
            "7",
            "%",
            " Sunni",
            " or",
            " up",
            " to",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Afghanistan",
            ".",
            "Af",
            "ghan",
            " Christians",
            ",",
            " who",
            " number",
            " ",
            "500",
            "–",
            "8",
            ",",
            "000",
            ",",
            " practice",
            " their",
            " faith",
            " secretly",
            " due",
            " to",
            " intense",
            " societal",
            " opposition",
            ",",
            " and",
            " there",
            " are",
            " no",
            " public",
            " churches",
            ".",
            "Urban",
            "ization",
            "As",
            " estimated",
            " by",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "26",
            "%",
            " of",
            " the",
            " population",
            " was",
            " urban",
            "ized",
            " as",
            " of",
            " ",
            "202",
            "0",
            ".",
            " This",
            " is",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Pas",
            "ht",
            "uns",
            ",",
            " although",
            " many",
            " of",
            " them",
            " are",
            " also",
            " fluent",
            " in",
            " D",
            "ari",
            " while",
            " some",
            " non",
            "-P",
            "as",
            "ht",
            "uns",
            " are",
            " fluent",
            " in",
            " Pas",
            "ht",
            "o",
            ".",
            " Despite",
            " the",
            " Pas",
            "ht",
            "uns",
            " having",
            " been",
            " dominant",
            " in",
            " Afghan",
            " politics",
            " for",
            " centuries",
            ",",
            " D",
            "ari",
            " remained",
            " the",
            " preferred",
            " language",
            " for",
            " government",
            " and",
            " bureaucracy",
            ".",
            " ",
            "According",
            " to",
            " CIA",
            " World",
            " Fact",
            "book",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " various",
            " authorities",
            ",",
            " institutions",
            ",",
            " and",
            " countries",
            ",",
            " see",
            " for",
            " example",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ".",
            " Correspond",
            "ingly",
            ",",
            " the",
            " extent",
            " and",
            " number",
            " of",
            " oceans",
            " and",
            " seas",
            " vary",
            ".",
            "The",
            " Atlantic",
            " Ocean",
            " is",
            " bounded",
            " on",
            " the",
            " west",
            " by",
            " North",
            " and",
            " South",
            " America",
            ".",
            " It",
            " connects",
            " to",
            " the",
            " Arctic",
            " Ocean",
            " through",
            " the",
            " Denmark",
            " Strait",
            ",",
            " Greenland",
            " Sea",
            ",",
            " Norwegian",
            " Sea",
            " and",
            " B",
            "arent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " various",
            " authorities",
            ",",
            " institutions",
            ",",
            " and",
            " countries",
            ",",
            " see",
            " for",
            " example",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ".",
            " Correspond",
            "ingly",
            ",",
            " the",
            " extent",
            " and",
            " number",
            " of",
            " oceans",
            " and",
            " seas",
            " vary",
            ".",
            "The",
            " Atlantic",
            " Ocean",
            " is",
            " bounded",
            " on",
            " the",
            " west",
            " by",
            " North",
            " and",
            " South",
            " America",
            ".",
            " It",
            " connects",
            " to",
            " the",
            " Arctic",
            " Ocean",
            " through",
            " the",
            " Denmark",
            " Strait",
            ",",
            " Greenland",
            " Sea",
            ",",
            " Norwegian",
            " Sea",
            " and",
            " B",
            "arent"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.336,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " about",
            " using",
            " this",
            " approach",
            " in",
            " Paper",
            "man",
            ",",
            " John",
            " Kah",
            "rs",
            " said",
            " that",
            " \"",
            "Our",
            " anim",
            "ators",
            " can",
            " change",
            " things",
            ",",
            " actually",
            " erase",
            " away",
            " the",
            " CG",
            " under",
            "layer",
            " if",
            " they",
            " want",
            ",",
            " and",
            " change",
            " the",
            " profile",
            " of",
            " the",
            " arm",
            ".\"",
            "3",
            "D",
            "3",
            "D",
            " animation",
            " is",
            " digitally",
            " modeled",
            " and",
            " manipulated",
            " by",
            " an",
            " animator",
            ".",
            " The",
            " ",
            "3",
            "D",
            " model",
            " maker",
            " usually",
            " starts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.326,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " latter",
            " type",
            " of",
            " ambiguity",
            " with",
            " notable",
            " effect",
            " in",
            " his",
            " novel",
            " The",
            " Great",
            " G",
            "atsby",
            ".",
            "Math",
            "ematic",
            "al",
            " notation",
            " ",
            "Math",
            "ematic",
            "al",
            " notation",
            " is",
            " a",
            " helpful",
            " tool",
            " that",
            " eliminates",
            " a",
            " lot",
            " of",
            " misunderstand",
            "ings",
            " associated",
            " with",
            " natural",
            " language",
            " in",
            " physics",
            " and",
            " other",
            " sciences",
            ".",
            " Nonetheless",
            ",",
            " there",
            " are",
            " still",
            " some",
            " inherent",
            " ambigu",
            "ities",
            " due",
            " to",
            " lexical",
            ",",
            " synt",
            "actic",
            ",",
            " and",
            " semantic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "-B",
            "act",
            "rians",
            " and",
            " the",
            " M",
            "ugh",
            "als",
            ",",
            " amongst",
            " others",
            ",",
            " rose",
            " to",
            " form",
            " major",
            " em",
            "pires",
            ".",
            " The",
            " various",
            " conquest",
            "s",
            " and",
            " periods",
            " in",
            " both",
            " the",
            " Iranian",
            " and",
            " Indian",
            " cultural",
            " spheres",
            " made",
            " the",
            " area",
            " a",
            " center",
            " for",
            " Z",
            "oro",
            "ast",
            "rian",
            "ism",
            ",",
            " Buddhism",
            ",",
            " Hindu",
            "ism",
            ",",
            " and",
            " later",
            " Islam",
            ".",
            " The",
            " modern",
            " state",
            " of",
            " Afghanistan",
            " began",
            " with",
            " the",
            " D"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " made",
            " it",
            " easy",
            " for",
            " readers",
            " of",
            " the",
            " book",
            " to",
            " misunderstand",
            " what",
            " it",
            " was",
            " about",
            ",",
            " and",
            " the",
            " misunderstanding",
            " will",
            " pursue",
            " me",
            " until",
            " I",
            " die",
            ".",
            " I",
            " should",
            " not",
            " have",
            " written",
            " the",
            " book",
            " because",
            " of",
            " this",
            " danger",
            " of",
            " mis",
            "interpret",
            "ation",
            ",",
            " and",
            " the",
            " same",
            " may",
            " be",
            " said",
            " of",
            " Lawrence",
            " and",
            " Lady",
            " Ch",
            "atter",
            "ley",
            "'s",
            " Lover",
            ".\"",
            "A",
            "wards",
            " and",
            " nominations",
            " and",
            " rankings"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " his",
            " B",
            "act",
            "rian",
            " sat",
            "rap",
            " and",
            " k",
            "ins",
            "man",
            ".",
            " As",
            " Alexander",
            " approached",
            ",",
            " B",
            "ess",
            "us",
            " had",
            " his",
            " men",
            " fatally",
            " stab",
            " the",
            " Great",
            " King",
            " and",
            " then",
            " declared",
            " himself",
            " D",
            "arius",
            "'s",
            " successor",
            " as",
            " Art",
            "ax",
            "er",
            "xes",
            " V",
            ",",
            " before",
            " ret",
            "reating",
            " into",
            " Central",
            " Asia",
            " to",
            " launch",
            " a",
            " guerr",
            "illa",
            " campaign",
            " against",
            " Alexander",
            ".",
            " Alexander",
            " buried",
            " D",
            "arius",
            "'s",
            " remains"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.247,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " primary",
            " school",
            ",",
            " compared",
            " to",
            " just",
            " one",
            " in",
            " five",
            " ",
            "40",
            " years",
            " ago",
            ".",
            " Hong",
            " Kong",
            " ranked",
            " highest",
            " among",
            " the",
            " countries",
            " grouped",
            " on",
            " the",
            " HD",
            "I",
            " (",
            "number",
            " ",
            "7",
            " in",
            " the",
            " world",
            ",",
            " which",
            " is",
            " in",
            " the",
            " \"",
            "very",
            " high",
            " human",
            " development",
            "\"",
            " category",
            "),",
            " followed",
            " by",
            " Singapore",
            " (",
            "9",
            "),",
            " Japan",
            " (",
            "19",
            ")",
            " and",
            " South",
            " Korea",
            " (",
            "22",
            ").",
            " Afghanistan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.235,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.106,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Hermes",
            " and",
            " Dem",
            "oc",
            "rit",
            "us",
            " and",
            " unnamed",
            " al",
            "chem",
            "ists",
            " of",
            " his",
            " time",
            ".",
            " Albert",
            "us",
            " critically",
            " compared",
            " these",
            " to",
            " the",
            " writings",
            " of",
            " Aristotle",
            " and",
            " Av",
            "ic",
            "enna",
            ",",
            " where",
            " they",
            " concerned",
            " the",
            " trans",
            "mutation",
            " of",
            " metals",
            ".",
            " From",
            " the",
            " time",
            " shortly",
            " after",
            " his",
            " death",
            " through",
            " to",
            " the",
            " ",
            "15",
            "th",
            " century",
            ",",
            " more",
            " than",
            " ",
            "28",
            " al",
            "chemical",
            " tr",
            "acts",
            " were"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.221,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " relatively",
            " low",
            " performance",
            " in",
            " the",
            " international",
            " arena",
            " compared",
            " to",
            " the",
            " nation",
            " football",
            " clubs",
            ".",
            " The",
            " most",
            " successful",
            " Azerbai",
            "j",
            "ani",
            " football",
            " clubs",
            " are",
            " Ne",
            "ft",
            "Ã§i",
            ",",
            " Q",
            "ar",
            "aba",
            "ÄŁ",
            ",",
            " and",
            " Gab",
            "ala",
            ".",
            " In",
            " ",
            "201",
            "2",
            ",",
            " Ne",
            "ft",
            "chi",
            " B",
            "aku",
            " became",
            " the",
            " first",
            " Azerbai",
            "j",
            "ani",
            " team",
            " to",
            " advance",
            " to",
            " the",
            " group",
            " stage",
            " of",
            " a",
            " European",
            " competition"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.22,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " Walter",
            " (",
            "French",
            ":",
            " Les",
            " Cah",
            "iers",
            " d",
            "'",
            "And",
            "rÃ©",
            " Walter",
            "),",
            " in",
            " ",
            "189",
            "1",
            ",",
            " at",
            " the",
            " age",
            " of",
            " twenty",
            "-one",
            ".",
            "In",
            " ",
            "189",
            "3",
            " and",
            " ",
            "189",
            "4",
            ",",
            " G",
            "ide",
            " traveled",
            " in",
            " Northern",
            " Africa",
            ".",
            " There",
            " he",
            " came",
            " to",
            " accept",
            " his",
            " attraction",
            " to",
            " boys",
            " and",
            " youths",
            ".",
            "G",
            "ide",
            " bef",
            "ri",
            "ended",
            " Irish",
            " playwright",
            " Oscar",
            " Wilde",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.218,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " ceremonial",
            " trans",
            "continental",
            " telephone",
            " call",
            ".",
            " Calling",
            " from",
            " the",
            " AT",
            "&T",
            " head",
            " office",
            " at",
            " ",
            "15",
            " D",
            "ey",
            " Street",
            " in",
            " New",
            " York",
            " City",
            ",",
            " Bell",
            " was",
            " heard",
            " by",
            " Thomas",
            " Watson",
            " at",
            " ",
            "333",
            " Grant",
            " Avenue",
            " in",
            " San",
            " Francisco",
            ".",
            " The",
            " New",
            " York",
            " Times",
            " reported",
            ":",
            "Compet",
            "itors",
            "As",
            " is",
            " sometimes",
            " common",
            " in",
            " scientific",
            " discoveries",
            ",",
            " simultaneous",
            " developments",
            " can",
            " occur",
            ",",
            " as",
            " evidenced"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.201,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " the",
            " general",
            " population",
            ".",
            " Other",
            " pr",
            "airie",
            " flora",
            " known",
            " to",
            " flower",
            " early",
            " are",
            " the",
            " golden",
            " bean",
            " (",
            "Th",
            "erm",
            "opsis",
            " rh",
            "omb",
            "if",
            "olia",
            ")",
            " and",
            " wild",
            " rose",
            " (",
            "R",
            "osa",
            " ac",
            "icular",
            "is",
            ").",
            " Members",
            " of",
            " the",
            " sun",
            "flower",
            " (",
            "Hel",
            "ian",
            "thus",
            ")",
            " family",
            " blossom",
            " on",
            " the",
            " pr",
            "airie",
            " in",
            " the",
            " summer",
            " months",
            " between",
            " July",
            " and",
            " September",
            ".",
            " The",
            " southern"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "):",
            " It",
            " was",
            " only",
            " with",
            " the",
            " development",
            ",",
            " beginning",
            " in",
            " the",
            " ",
            "193",
            "0",
            "s",
            ",",
            " of",
            " electrom",
            "ech",
            "anical",
            " calcul",
            "ators",
            " using",
            " electrical",
            " rel",
            "ays",
            ",",
            " that",
            " machines",
            " were",
            " built",
            " having",
            " the",
            " scope",
            " B",
            "abbage",
            " had",
            " envisioned",
            ".\"",
            "Math",
            "ematics",
            " during",
            " the",
            " ",
            "19",
            "th",
            " century",
            " up",
            " to",
            " the",
            " mid",
            "-",
            "20",
            "th",
            " century",
            " ",
            "Symbols",
            " and",
            " rules",
            ":",
            " In",
            " rapid",
            " succession"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.046,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " Union",
            " soldiers",
            ".",
            " President",
            " Johnson",
            " was",
            " sworn",
            " in",
            " later",
            " that",
            " same",
            " day",
            ".",
            "Two",
            " weeks",
            " later",
            ",",
            " Booth",
            ",",
            " refusing",
            " to",
            " surrender",
            ",",
            " was",
            " tracked",
            " to",
            " a",
            " farm",
            " in",
            " Virginia",
            ",",
            " and",
            " was",
            " mort",
            "ally",
            " shot",
            " by",
            " Sergeant",
            " Boston",
            " Cor",
            "bett",
            " and",
            " died",
            " on",
            " April",
            " ",
            "26",
            ".",
            " Secretary",
            " of",
            " War",
            " Stanton",
            " had",
            " issued",
            " orders",
            " that",
            " Booth",
            " be",
            " taken",
            " alive",
            ",",
            " so"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " constituent",
            " of",
            " Deb",
            "ier",
            "ne",
            "'s",
            " ",
            "189",
            "9",
            " and",
            " ",
            "190",
            "0",
            " results",
            ";",
            " in",
            " fact",
            ",",
            " the",
            " chemical",
            " properties",
            " he",
            " reported",
            " make",
            " it",
            " likely",
            " that",
            " he",
            " had",
            ",",
            " instead",
            ",",
            " accidentally",
            " identified",
            " prot",
            "act",
            "inium",
            ",",
            " which",
            " would",
            " not",
            " be",
            " discovered",
            " for",
            " another",
            " fourteen",
            " years",
            ",",
            " only",
            " to",
            " have",
            " it",
            " disappear",
            " due",
            " to",
            " its",
            " hydro",
            "ly",
            "sis",
            " and",
            " ads",
            "orption",
            " onto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "Ê",
            "¾",
            " actually",
            " means",
            " \"",
            "the",
            " Egyptian",
            " [",
            "science",
            "]\",",
            " borrowing",
            " from",
            " the",
            " C",
            "optic",
            " word",
            " for",
            " \"",
            "Egypt",
            "\",",
            " ",
            " (",
            "or",
            " its",
            " equivalent",
            " in",
            " the",
            " Media",
            "eval",
            " Boh",
            "air",
            "ic",
            " dialect",
            " of",
            " C",
            "optic",
            ",",
            " ).",
            " This",
            " C",
            "optic",
            " word",
            " derives",
            " from",
            " Dem",
            "otic",
            " ,",
            " itself",
            " from",
            " ancient",
            " Egyptian",
            " .",
            " The",
            " ancient",
            " Egyptian",
            " word",
            " referred",
            " to",
            " both",
            " the",
            " country",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "irut",
    "etti",
    "ucci",
    "ULA",
    "orida"
  ],
  "bottom_logits": [
    " Neutral",
    "iman",
    " Alliance",
    "n",
    "ÑĸÑģÑĤ"
  ],
  "act_min": -0.0,
  "act_max": 0.719
}