{
  "index": 40142,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "s",
            " (",
            "192",
            "0",
            ",",
            " Min",
            "erva",
            " Films",
            ")",
            " The",
            " Great",
            " Bro",
            "x",
            "opp",
            " (",
            "192",
            "1",
            ")",
            " The",
            " Dover",
            " Road",
            " (",
            "192",
            "1",
            ")",
            " The",
            " Lucky",
            " One",
            " (",
            "192",
            "2",
            ")",
            " The",
            " Truth",
            " About",
            " Bl",
            "ay",
            "ds",
            " (",
            "192",
            "2",
            ")",
            " The",
            " Artist",
            ":",
            " A",
            " Du",
            "ologue",
            " (",
            "192",
            "3",
            ")",
            " Give",
            " Me",
            " Yesterday",
            " (",
            "192",
            "3",
            ")",
            " (",
            "a",
            ".k",
            ".a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.35,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.196,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            "graphic",
            " Case",
            " Study",
            ".",
            " O",
            "xb",
            "ow",
            " Books",
            ".",
            "Further",
            " reading",
            " Bl",
            "aÅ¾",
            "ek",
            ",",
            " VÃ¡clav",
            ".",
            " \"",
            "Al",
            "ta",
            "ic",
            " numer",
            "als",
            "\".",
            " In",
            ":",
            " Bl",
            "aÅ¾",
            "ek",
            ",",
            " VÃ¡clav",
            ".",
            " Numer",
            "als",
            ":",
            " comparative",
            "-et",
            "ym",
            "ological",
            " analyses",
            " of",
            " numeral",
            " systems",
            " and",
            " their",
            " implications",
            ":",
            " (",
            "S",
            "ah",
            "aran",
            ",",
            " N",
            "ub",
            "ian",
            ",",
            " Egyptian",
            ",",
            " Ber",
            "ber",
            ",",
            " Kart"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " most",
            " common",
            " form",
            " of",
            " trans",
            "missible",
            " s",
            "pong",
            "iform",
            " en",
            "ceph",
            "al",
            "opath",
            "ies",
            " caused",
            " by",
            " pr",
            "ions",
            ".",
            " E",
            "pon",
            "ym",
            " introduced",
            " by",
            " Wal",
            "ther",
            " Spi",
            "elm",
            "eyer",
            " in",
            " ",
            "192",
            "2",
            ".",
            "B",
            "ibli",
            "ography",
            " Die",
            " extr",
            "apy",
            "ram",
            "idal",
            "en",
            " Er",
            "kr",
            "ank",
            "ungen",
            ".",
            " In",
            ":",
            " Mon",
            "ograph",
            "ien",
            " aus",
            " dem",
            " Ges",
            "amt",
            "geb",
            "iete",
            " der",
            " Neuro",
            "log"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "ie",
            " und",
            " Psychiatry",
            ",",
            " Berlin",
            ",",
            " ",
            "192",
            "3",
            " Norm",
            "ale",
            " und",
            " path",
            "olog",
            "ische",
            " An",
            "atom",
            "ie",
            " und",
            " Hist",
            "ologie",
            " des",
            " Gross",
            "hir",
            "ns",
            ".",
            " Separate",
            " printing",
            " of",
            " Hand",
            "buch",
            " der",
            " Psychiatry",
            ".",
            " Leipzig",
            ",",
            " ",
            "192",
            "7",
            "–",
            "192",
            "8",
            " Das",
            " Kle",
            "inh",
            "ir",
            "n",
            ".",
            " In",
            ":",
            " Hand",
            "buch",
            " der",
            " mik",
            "ros",
            "kop",
            "ischen",
            " An",
            "atom",
            "ie",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "ie",
            " und",
            " Psychiatry",
            ",",
            " Berlin",
            ",",
            " ",
            "192",
            "3",
            " Norm",
            "ale",
            " und",
            " path",
            "olog",
            "ische",
            " An",
            "atom",
            "ie",
            " und",
            " Hist",
            "ologie",
            " des",
            " Gross",
            "hir",
            "ns",
            ".",
            " Separate",
            " printing",
            " of",
            " Hand",
            "buch",
            " der",
            " Psychiatry",
            ".",
            " Leipzig",
            ",",
            " ",
            "192",
            "7",
            "–",
            "192",
            "8",
            " Das",
            " Kle",
            "inh",
            "ir",
            "n",
            ".",
            " In",
            ":",
            " Hand",
            "buch",
            " der",
            " mik",
            "ros",
            "kop",
            "ischen",
            " An",
            "atom",
            "ie",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " this",
            " property",
            ".",
            " In",
            " mathematical",
            " logic",
            ",",
            " Î±",
            " is",
            " sometimes",
            " used",
            " as",
            " a",
            " placeholder",
            " for",
            " ordinal",
            " numbers",
            ".",
            "The",
            " proportion",
            "ality",
            " operator",
            " \"",
            "âĪ",
            "Ŀ",
            "\"",
            " (",
            "in",
            " Unicode",
            ":",
            " U",
            "+",
            "221",
            "D",
            ")",
            " is",
            " sometimes",
            " mistaken",
            " for",
            " alpha",
            ".",
            "The",
            " uppercase",
            " letter",
            " alpha",
            " is",
            " not",
            " generally",
            " used",
            " as",
            " a",
            " symbol",
            " because",
            " it",
            " tends",
            " to",
            " be",
            " rendered",
            " ident",
            "ically",
            " to",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " this",
            " property",
            ".",
            " In",
            " mathematical",
            " logic",
            ",",
            " Î±",
            " is",
            " sometimes",
            " used",
            " as",
            " a",
            " placeholder",
            " for",
            " ordinal",
            " numbers",
            ".",
            "The",
            " proportion",
            "ality",
            " operator",
            " \"",
            "âĪ",
            "Ŀ",
            "\"",
            " (",
            "in",
            " Unicode",
            ":",
            " U",
            "+",
            "221",
            "D",
            ")",
            " is",
            " sometimes",
            " mistaken",
            " for",
            " alpha",
            ".",
            "The",
            " uppercase",
            " letter",
            " alpha",
            " is",
            " not",
            " generally",
            " used",
            " as",
            " a",
            " symbol",
            " because",
            " it",
            " tends",
            " to",
            " be",
            " rendered",
            " ident",
            "ically",
            " to",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " In",
            ":",
            " Jo",
            "ach",
            "im",
            " Gr",
            "z",
            "ega",
            ":",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " ohne",
            " Fach",
            "ch",
            "ines",
            "isch",
            ".",
            " Sh",
            "aker",
            ",",
            " A",
            "achen",
            " ",
            "200",
            "1",
            ",",
            " S",
            ".",
            " ",
            "7",
            "–",
            "26",
            ".",
            " .",
            " Gr",
            "z",
            "ega",
            ",",
            " Jo",
            "ach",
            "im",
            ":",
            " \"",
            "On",
            " the",
            " Description",
            " of",
            " National",
            " Vari",
            "eties",
            ":",
            " Examples",
            " from",
            " (",
            "German",
            " and",
            " Austrian",
            ")",
            " German",
            " and",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Light",
            "\",",
            " a",
            " film",
            " essay",
            " by",
            " Oliver",
            " H",
            "ock",
            "enh",
            "ull",
            " ",
            " Raymond",
            " Fraser",
            ",",
            " George",
            " Wick",
            "es",
            " (",
            "Spring",
            " ",
            "196",
            "0",
            ").",
            " \"",
            "Interview",
            ":",
            " Ald",
            "ous",
            " H",
            "ux",
            "ley",
            ":",
            " The",
            " Art",
            " of",
            " Fiction",
            " No",
            ".",
            " ",
            "24",
            "\".",
            " The",
            " Paris",
            " Review",
            ".",
            " BBC",
            " discussion",
            " programme",
            " In",
            " our",
            " time",
            ":",
            " \"",
            "Br",
            "ave",
            " New",
            " World",
            "\".",
            " H",
            "ux",
            "ley"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Light",
            "\",",
            " a",
            " film",
            " essay",
            " by",
            " Oliver",
            " H",
            "ock",
            "enh",
            "ull",
            " ",
            " Raymond",
            " Fraser",
            ",",
            " George",
            " Wick",
            "es",
            " (",
            "Spring",
            " ",
            "196",
            "0",
            ").",
            " \"",
            "Interview",
            ":",
            " Ald",
            "ous",
            " H",
            "ux",
            "ley",
            ":",
            " The",
            " Art",
            " of",
            " Fiction",
            " No",
            ".",
            " ",
            "24",
            "\".",
            " The",
            " Paris",
            " Review",
            ".",
            " BBC",
            " discussion",
            " programme",
            " In",
            " our",
            " time",
            ":",
            " \"",
            "Br",
            "ave",
            " New",
            " World",
            "\".",
            " H",
            "ux",
            "ley"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.196,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " Berlin",
            ",",
            " ",
            "192",
            "8",
            " Die",
            " Sy",
            "phil",
            "is",
            " des",
            " Geh",
            "ir",
            "ns",
            " und",
            " seiner",
            " HÃ¤",
            "ute",
            ".",
            " In",
            ":",
            " Oswald",
            " B",
            "um",
            "ke",
            " (",
            "edit",
            ".",
            "):",
            " Hand",
            "buch",
            " der",
            " Ge",
            "istes",
            "kr",
            "ank",
            "heiten",
            ",",
            " Berlin",
            ",",
            " ",
            "193",
            "0",
            ".",
            "References",
            " ",
            "People",
            " from",
            " As",
            "ch",
            "aff",
            "enburg",
            "Ac",
            "ademic",
            " staff",
            " of",
            " the",
            " University",
            " of",
            " Hamburg",
            "German"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " suburbs",
            " which",
            " were",
            " built",
            " in",
            " that",
            " period",
            " are",
            " collectively",
            " called",
            " the",
            " West",
            "elijke",
            " Tu",
            "inst",
            "eden",
            ".",
            " The",
            " area",
            " to",
            " the",
            " southeast",
            " of",
            " the",
            " city",
            " built",
            " during",
            " the",
            " same",
            " period",
            " is",
            " known",
            " as",
            " the",
            " Bij",
            "l",
            "mer",
            ".",
            "Architecture",
            "Am",
            "sterdam",
            " has",
            " a",
            " rich",
            " architectural",
            " history",
            ".",
            " The",
            " oldest",
            " building",
            " in",
            " Amsterdam",
            " is",
            " the",
            " O",
            "ude",
            " K",
            "erk",
            " (",
            "English",
            ":",
            " Old"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.069,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ic",
            "'",
            " languages",
            "\".",
            " In",
            ":",
            " Central",
            " Asi",
            "atic",
            " Journal",
            " ",
            "53",
            " (",
            "1",
            "):",
            " ",
            "105",
            "–",
            "147",
            ".",
            "External",
            " links",
            "Al",
            "ta",
            "ic",
            " at",
            " the",
            " Lingu",
            "ist",
            " List",
            " Multi",
            "Tree",
            " Project",
            " (",
            "not",
            " functional",
            " as",
            " of",
            " ",
            "201",
            "4",
            "):",
            " Gene",
            "alog",
            "ical",
            " trees",
            " attributed",
            " to",
            " Ram",
            "sted",
            "t",
            " ",
            "195",
            "7",
            ",",
            " Miller",
            " ",
            "197",
            "1",
            ",",
            " and",
            " Pop"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " artists",
            " like",
            " W",
            "im",
            " Kan",
            ",",
            " W",
            "im",
            " Son",
            "nev",
            "eld",
            " and",
            " To",
            "on",
            " Herm",
            "ans",
            " were",
            " pioneers",
            " of",
            " this",
            " form",
            " of",
            " art",
            " in",
            " the",
            " Netherlands",
            ".",
            " In",
            " Amsterdam",
            " is",
            " the",
            " Kle",
            "ink",
            "un",
            "st",
            "acad",
            "emie",
            " (",
            "English",
            ":",
            " Cab",
            "aret",
            " Academy",
            ")",
            " and",
            " N",
            "eder",
            "lied",
            " Kle",
            "ink",
            "un",
            "st",
            "ko",
            "or",
            " (",
            "English",
            ":",
            " Cab",
            "aret",
            " Choir",
            ").",
            " Contemporary"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.239,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "Memory",
            ":",
            " Allen",
            " Gins",
            "berg",
            ")",
            " Cherry",
            " Valley",
            " Edition",
            "s",
            ",",
            " ",
            "198",
            "2",
            "  ",
            " ",
            " Morgan",
            ",",
            " Bill",
            " (",
            "ed",
            ".),",
            " I",
            " G",
            "reet",
            " You",
            " at",
            " the",
            " Beginning",
            " of",
            " a",
            " Great",
            " Career",
            ":",
            " The",
            " Selected",
            " Correspond",
            "ence",
            " of",
            " Lawrence",
            " Fer",
            "ling",
            "h",
            "etti",
            " and",
            " Allen",
            " Gins",
            "berg",
            ",",
            " ",
            "195",
            "5",
            "–",
            "199",
            "7",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " Publishers"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " artists",
            " like",
            " W",
            "im",
            " Kan",
            ",",
            " W",
            "im",
            " Son",
            "nev",
            "eld",
            " and",
            " To",
            "on",
            " Herm",
            "ans",
            " were",
            " pioneers",
            " of",
            " this",
            " form",
            " of",
            " art",
            " in",
            " the",
            " Netherlands",
            ".",
            " In",
            " Amsterdam",
            " is",
            " the",
            " Kle",
            "ink",
            "un",
            "st",
            "acad",
            "emie",
            " (",
            "English",
            ":",
            " Cab",
            "aret",
            " Academy",
            ")",
            " and",
            " N",
            "eder",
            "lied",
            " Kle",
            "ink",
            "un",
            "st",
            "ko",
            "or",
            " (",
            "English",
            ":",
            " Cab",
            "aret",
            " Choir",
            ").",
            " Contemporary"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "202",
            "1",
            ".",
            " pp",
            ".",
            " ",
            "57",
            "–",
            "84",
            ".",
            " https",
            "://",
            "doi",
            "-org",
            ".w",
            "ikip",
            "ed",
            "ial",
            "ibrary",
            ".id",
            "m",
            ".oc",
            "lc",
            ".org",
            "/",
            "10",
            ".",
            "151",
            "5",
            "/",
            "978",
            "311",
            "074",
            "878",
            "9",
            "-",
            "008",
            "Green",
            "berg",
            ",",
            " Joseph",
            " H",
            ".",
            " ",
            "199",
            "7",
            ".",
            " \"",
            "Does",
            " Alta",
            "ic",
            " exist",
            "?",
            "\".",
            " In",
            ":",
            " Ir",
            "Ã©n",
            " Heg",
            "ed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " known",
            " as",
            " De",
            " Wall",
            "en",
            " (",
            "English",
            ":",
            " \"",
            "The",
            " Qu",
            "ays",
            "\").",
            " It",
            " lies",
            " to",
            " the",
            " east",
            " of",
            " Dam",
            "rak",
            " and",
            " contains",
            " the",
            " city",
            "'s",
            " famous",
            " red",
            "-light",
            " district",
            ".",
            " To",
            " the",
            " south",
            " of",
            " De",
            " Wall",
            "en",
            " is",
            " the",
            " old",
            " Jewish",
            " quarter",
            " of",
            " Water",
            "lo",
            "ople",
            "in",
            ".",
            "The",
            " medieval",
            " and",
            " colonial",
            " age",
            " can",
            "als",
            " of",
            " Amsterdam",
            ",",
            " known",
            " as",
            " gr",
            "achten"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " groundwater",
            " by",
            " I",
            "AH",
            "'s",
            " Netherlands",
            " Chapter",
            " and",
            " the",
            " Netherlands",
            " Hydro",
            "logical",
            " Society",
            " Cont",
            "amin",
            "ant",
            " Focus",
            ":",
            " Ar",
            "sen",
            "ic",
            " ",
            " by",
            " the",
            " EPA",
            ".",
            " Environmental",
            " Health",
            " Criteria",
            " for",
            " Ar",
            "sen",
            "ic",
            " and",
            " Ar",
            "sen",
            "ic",
            " Comp",
            "ounds",
            ",",
            " ",
            "200",
            "1",
            " by",
            " the",
            " WHO",
            ".",
            " ",
            " National",
            " Institute",
            " for",
            " Occupational",
            " Safety",
            " and",
            " Health",
            " –",
            " Ar",
            "sen",
            "ic",
            " Page",
            " Ar"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "vel",
            "ian",
            ",",
            " U",
            "ral",
            "ic",
            ",",
            " Alta",
            "ic",
            " and",
            " Indo",
            "-European",
            " languages",
            ").",
            " V",
            "yd",
            ".",
            " ",
            "1",
            ".",
            " V",
            " Br",
            "nÄĽ",
            ":",
            " Mas",
            "ary",
            "kova",
            " univerz",
            "ita",
            ",",
            " ",
            "199",
            "9",
            ",",
            " pp",
            ".",
            "Âł",
            "102",
            "–",
            "140",
            ".",
            " ;",
            " Dy",
            "bo",
            ",",
            " Anna",
            ".",
            " \"",
            "New",
            " trends",
            " in",
            " European",
            " studies",
            " on",
            " the",
            " Alta",
            "ic",
            " problem",
            "\".",
            " In",
            ":",
            " Journal",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.117,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "English",
            " and",
            " American",
            ")",
            " English",
            "\".",
            " In",
            ":",
            " Lingu",
            "istik",
            " Online",
            " ",
            "7",
            " (",
            "200",
            "0",
            ").",
            " Gr",
            "z",
            "ega",
            ",",
            " Jo",
            "ach",
            "im",
            ":",
            " \"",
            "Non",
            "ch",
            "alance",
            " als",
            " Merk",
            "mal",
            " des",
            " Ãĸ",
            "sterreich",
            "ischen",
            " Deutsch",
            "\".",
            " In",
            ":",
            " Mut",
            "ters",
            "pr",
            "ache",
            " ",
            "113",
            " (",
            "200",
            "3",
            "):",
            " ",
            "242",
            "–",
            "254",
            ".",
            " Mu",
            "hr",
            ",",
            " Rud",
            "olf",
            " /",
            " Schro",
            "dt"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.117,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "English",
            " and",
            " American",
            ")",
            " English",
            "\".",
            " In",
            ":",
            " Lingu",
            "istik",
            " Online",
            " ",
            "7",
            " (",
            "200",
            "0",
            ").",
            " Gr",
            "z",
            "ega",
            ",",
            " Jo",
            "ach",
            "im",
            ":",
            " \"",
            "Non",
            "ch",
            "alance",
            " als",
            " Merk",
            "mal",
            " des",
            " Ãĸ",
            "sterreich",
            "ischen",
            " Deutsch",
            "\".",
            " In",
            ":",
            " Mut",
            "ters",
            "pr",
            "ache",
            " ",
            "113",
            " (",
            "200",
            "3",
            "):",
            " ",
            "242",
            "–",
            "254",
            ".",
            " Mu",
            "hr",
            ",",
            " Rud",
            "olf",
            " /",
            " Schro",
            "dt"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.117,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "English",
            " and",
            " American",
            ")",
            " English",
            "\".",
            " In",
            ":",
            " Lingu",
            "istik",
            " Online",
            " ",
            "7",
            " (",
            "200",
            "0",
            ").",
            " Gr",
            "z",
            "ega",
            ",",
            " Jo",
            "ach",
            "im",
            ":",
            " \"",
            "Non",
            "ch",
            "alance",
            " als",
            " Merk",
            "mal",
            " des",
            " Ãĸ",
            "sterreich",
            "ischen",
            " Deutsch",
            "\".",
            " In",
            ":",
            " Mut",
            "ters",
            "pr",
            "ache",
            " ",
            "113",
            " (",
            "200",
            "3",
            "):",
            " ",
            "242",
            "–",
            "254",
            ".",
            " Mu",
            "hr",
            ",",
            " Rud",
            "olf",
            " /",
            " Schro",
            "dt"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spotlight",
            ":",
            " Spider",
            "-Man",
            " –",
            " One",
            " More",
            " Day",
            "/",
            "Brand",
            " New",
            " Day",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " ",
            "1",
            " [#",
            "546",
            "–",
            "551",
            ";",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Swing",
            " Shift",
            " (",
            "Director",
            "'s",
            " Cut",
            ");",
            " Venom",
            " Super",
            "-S",
            "pecial",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " ",
            "2",
            " [#",
            "552",
            "–",
            "558",
            "]",
            " ()",
            "Brand",
            " New",
            " Day",
            " Vol",
            ".",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.124,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "3",
            ".",
            " \"",
            "The",
            " Unity",
            " and",
            " Diversity",
            " of",
            " Alta",
            "ic",
            "\",",
            " Annual",
            " Review",
            " of",
            " Lingu",
            "istics",
            " ",
            "9",
            ":",
            "135",
            "–",
            "154",
            " (",
            "January",
            " ",
            "202",
            "3",
            ")",
            " ",
            "J",
            "oh",
            "anson",
            ",",
            " Lars",
            ".",
            " ",
            "199",
            "9",
            ".",
            " \"",
            "C",
            "ogn",
            "ates",
            " and",
            " copies",
            " in",
            " Alta",
            "ic",
            " verb",
            " derivation",
            "\".",
            " In",
            ":",
            " Language",
            " and",
            " Literature",
            " –",
            " Japanese",
            " and",
            " the",
            " Other",
            " Alta",
            "ic"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " plant",
            " of",
            " Z",
            "wick",
            "au",
            " became",
            " the",
            " V",
            "EB",
            " (",
            "for",
            " \"",
            "People",
            " Owned",
            " Enterprise",
            "\")",
            " ",
            " or",
            " AW",
            "Z",
            " (",
            "in",
            " English",
            ":",
            " Automobile",
            " Works",
            " Z",
            "wick",
            "au",
            ").",
            "With",
            " no",
            " prospect",
            " of",
            " continuing",
            " production",
            " in",
            " Soviet",
            "-controlled",
            " East",
            " Germany",
            ",",
            " Auto",
            " Union",
            " executives",
            " began",
            " the",
            " process",
            " of",
            " reloc",
            "ating",
            " what",
            " was",
            " left",
            " of",
            " the",
            " company",
            " to",
            " West",
            " Germany",
            ".",
            " A",
            " site"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " centrally",
            " based",
            " companies",
            " have",
            " increasingly",
            " relocated",
            " outside",
            " Amsterdam",
            "'s",
            " city",
            " centre",
            ".",
            " Consequently",
            ",",
            " the",
            " Z",
            "uid",
            "as",
            " (",
            "English",
            ":",
            " South",
            " Axis",
            ")",
            " has",
            " become",
            " the",
            " new",
            " financial",
            " and",
            " legal",
            " hub",
            " of",
            " Amsterdam",
            ",",
            " with",
            " the",
            " country",
            "'s",
            " five",
            " largest",
            " law",
            " firms",
            " and",
            " several",
            " subsidiaries",
            " of",
            " large",
            " consulting",
            " firms",
            ",",
            " such",
            " as",
            " Boston",
            " Consulting",
            " Group",
            " and",
            " Accent",
            "ure",
            ",",
            " as",
            " well",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " centrally",
            " based",
            " companies",
            " have",
            " increasingly",
            " relocated",
            " outside",
            " Amsterdam",
            "'s",
            " city",
            " centre",
            ".",
            " Consequently",
            ",",
            " the",
            " Z",
            "uid",
            "as",
            " (",
            "English",
            ":",
            " South",
            " Axis",
            ")",
            " has",
            " become",
            " the",
            " new",
            " financial",
            " and",
            " legal",
            " hub",
            " of",
            " Amsterdam",
            ",",
            " with",
            " the",
            " country",
            "'s",
            " five",
            " largest",
            " law",
            " firms",
            " and",
            " several",
            " subsidiaries",
            " of",
            " large",
            " consulting",
            " firms",
            ",",
            " such",
            " as",
            " Boston",
            " Consulting",
            " Group",
            " and",
            " Accent",
            "ure",
            ",",
            " as",
            " well",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " research",
            " findings",
            " Joint",
            " Program",
            "matic",
            " Commission",
            " UNESCO",
            "-",
            "ONG",
            " Science",
            " and",
            " Ethics",
            ",",
            " ",
            "24",
            " March",
            " ",
            "200",
            "3",
            " (",
            "Com",
            "itÃ©",
            " de",
            " Lia",
            "ison",
            " O",
            "NG",
            "-",
            "UN",
            "ESCO",
            ")",
            " Bor",
            "ghi",
            " L",
            ".",
            " (",
            "201",
            "5",
            ")",
            " \"",
            "Heart",
            " Matters",
            ".",
            " The",
            " Collaboration",
            " Between",
            " Surge",
            "ons",
            " and",
            " Engineers",
            " in",
            " the",
            " Rise",
            " of",
            " Card",
            "iac",
            " Surgery",
            "\".",
            " In",
            ":",
            " Pis",
            "ano"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "ies",
            "inger",
            ",",
            " Peter",
            ":",
            " Die",
            " deutsche",
            " Spr",
            "ache",
            " in",
            " Ãĸ",
            "sterreich",
            ".",
            " Eine",
            " E",
            "inf",
            "Ã¼hrung",
            ",",
            " In",
            ":",
            " W",
            "ies",
            "inger",
            " (",
            "H",
            "g",
            ".",
            "):",
            " Das",
            " Ã¶",
            "sterreich",
            "ische",
            " Deutsch",
            ".",
            " Sch",
            "rif",
            "ten",
            " zur",
            " deutschen",
            " Spr",
            "ache",
            ".",
            " Band",
            " ",
            "12",
            ".''",
            " (",
            "W",
            "ien",
            ",",
            " KÃ¶",
            "ln",
            ",",
            " Graz",
            ",",
            " ",
            "198",
            "8",
            ",",
            " Ver",
            "lag",
            ",",
            " B"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.162,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "199",
            "9",
            ".",
            " ATP",
            " Player",
            " of",
            " the",
            " Year",
            ":",
            " ",
            "199",
            "9",
            ".",
            " ATP",
            " Most",
            " Improved",
            " Player",
            ":",
            " ",
            "198",
            "8",
            ",",
            " ",
            "199",
            "8",
            "Recognition",
            " In",
            " ",
            "199",
            "2",
            ",",
            " Ag",
            "assi",
            " was",
            " named",
            " the",
            " BBC",
            " Overse",
            "as",
            " Sports",
            " Personality",
            " of",
            " the",
            " Year",
            ".",
            " In",
            " ",
            "201",
            "0",
            ",",
            " Sports",
            " Illustrated",
            " named",
            " Ag",
            "assi",
            " the",
            " ",
            "7",
            "th"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.396,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.228,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Best",
            " Casting",
            ":",
            " rejected",
            " in",
            " ",
            "199",
            "9",
            " Best",
            " Popular",
            " Film",
            ":",
            " proposed",
            " in",
            " ",
            "201",
            "8",
            " for",
            " presentation",
            " at",
            " the",
            " ",
            "201",
            "9",
            " ceremony",
            ";",
            " postponed",
            " until",
            " the",
            " ",
            "202",
            "0",
            " ceremony",
            " at",
            " the",
            " earliest",
            " (",
            "yet",
            " to",
            " be",
            " implemented",
            ")",
            " Best",
            " St",
            "unt",
            " Coord",
            "ination",
            ":",
            " rejected",
            " every",
            " year",
            " from",
            " ",
            "199",
            "1",
            " to",
            " ",
            "201",
            "2",
            " Best"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " silver",
            " my",
            "lar",
            ",",
            " pillow",
            "-shaped",
            " balloons",
            ".",
            " A",
            " Silver",
            " Cloud",
            " was",
            " included",
            " in",
            " the",
            " traveling",
            " exhibition",
            " Air",
            " Art",
            " (",
            "196",
            "8",
            "–",
            "196",
            "9",
            ")",
            " curated",
            " by",
            " Will",
            "ough",
            "by",
            " Sharp",
            ".",
            " Cloud",
            "s",
            " was",
            " also",
            " adapted",
            " by",
            " War",
            "hol",
            " for",
            " avant",
            "-g",
            "arde",
            " chore",
            "ographer",
            " Mer",
            "ce",
            " Cunningham",
            "'s",
            " dance",
            " piece",
            " Rain",
            "Forest",
            " (",
            "196",
            "8",
            ").",
            " Audio",
            ":",
            " At"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " silver",
            " my",
            "lar",
            ",",
            " pillow",
            "-shaped",
            " balloons",
            ".",
            " A",
            " Silver",
            " Cloud",
            " was",
            " included",
            " in",
            " the",
            " traveling",
            " exhibition",
            " Air",
            " Art",
            " (",
            "196",
            "8",
            "–",
            "196",
            "9",
            ")",
            " curated",
            " by",
            " Will",
            "ough",
            "by",
            " Sharp",
            ".",
            " Cloud",
            "s",
            " was",
            " also",
            " adapted",
            " by",
            " War",
            "hol",
            " for",
            " avant",
            "-g",
            "arde",
            " chore",
            "ographer",
            " Mer",
            "ce",
            " Cunningham",
            "'s",
            " dance",
            " piece",
            " Rain",
            "Forest",
            " (",
            "196",
            "8",
            ").",
            " Audio",
            ":",
            " At"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "3",
            " [#",
            "559",
            "–",
            "563",
            "]",
            " ()",
            "Kr",
            "aven",
            "'s",
            " First",
            " Hunt",
            " [#",
            "564",
            "–",
            "567",
            ";",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Extra",
            "!",
            " #",
            "1",
            " (",
            "story",
            " #",
            "2",
            ")]",
            " ()",
            "New",
            " Ways",
            " to",
            " Die",
            " [#",
            "568",
            "–",
            "573",
            ";",
            " Marvel",
            " Spotlight",
            ":",
            " Spider",
            "-Man",
            " –",
            " Brand",
            " New",
            " Day",
            "]",
            " ()",
            "Crime",
            " and",
            " Pun",
            "isher",
            " [#",
            "574",
            "–",
            "577",
            ";",
            " The",
            " Amazing"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "3",
            " [#",
            "559",
            "–",
            "563",
            "]",
            " ()",
            "Kr",
            "aven",
            "'s",
            " First",
            " Hunt",
            " [#",
            "564",
            "–",
            "567",
            ";",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Extra",
            "!",
            " #",
            "1",
            " (",
            "story",
            " #",
            "2",
            ")]",
            " ()",
            "New",
            " Ways",
            " to",
            " Die",
            " [#",
            "568",
            "–",
            "573",
            ";",
            " Marvel",
            " Spotlight",
            ":",
            " Spider",
            "-Man",
            " –",
            " Brand",
            " New",
            " Day",
            "]",
            " ()",
            "Crime",
            " and",
            " Pun",
            "isher",
            " [#",
            "574",
            "–",
            "577",
            ";",
            " The",
            " Amazing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " algorithm",
            " in",
            " pseud",
            "ocode",
            " or",
            " pid",
            "gin",
            " code",
            ":",
            " Input",
            ":",
            " A",
            " list",
            " of",
            " numbers",
            " L",
            ".",
            " Output",
            ":",
            " The",
            " largest",
            " number",
            " in",
            " the",
            " list",
            " L",
            ".",
            " if",
            " L",
            ".size",
            " =",
            " ",
            "0",
            " return",
            " null",
            " largest",
            " âĨĲ",
            " L",
            "[",
            "0",
            "]",
            " for",
            " each",
            " item",
            " in",
            " L",
            ",",
            " do",
            "    ",
            " if",
            " item",
            " >",
            " largest",
            ",",
            " then",
            "        ",
            " largest",
            " âĨĲ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            "ans",
            ",",
            " ",
            "209",
            " sal",
            "am",
            "anders",
            ",",
            " and",
            " ",
            "21",
            " ca",
            "ec",
            "ilians",
            ".",
            " Generally",
            ",",
            " the",
            " k",
            "ary",
            "otypes",
            " of",
            " dip",
            "loid",
            " amphib",
            "ians",
            " are",
            " characterized",
            " by",
            " ",
            "20",
            "–",
            "26",
            " bi",
            "-",
            "armed",
            " chromosomes",
            ".",
            " Amph",
            "ib",
            "ians",
            " have",
            " also",
            " very",
            " large",
            " genomes",
            " compared",
            " to",
            " other",
            " taxa",
            " of",
            " verte",
            "brates",
            " and",
            " corresponding",
            " variation",
            " in",
            " genome",
            " size",
            " (",
            "C",
            "-value",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " System",
            ":",
            " computer",
            " simulations",
            " of",
            " collisions",
            " involving",
            " solid",
            " bodies",
            " show",
            " them",
            " destroying",
            " each",
            " other",
            " as",
            " often",
            " as",
            " merging",
            ",",
            " but",
            " coll",
            "iding",
            " rubble",
            " piles",
            " are",
            " more",
            " likely",
            " to",
            " merge",
            ".",
            " This",
            " means",
            " that",
            " the",
            " cores",
            " of",
            " the",
            " planets",
            " could",
            " have",
            " formed",
            " relatively",
            " quickly",
            ".",
            "Water",
            " ",
            "Scientists",
            " hypo",
            "thesize",
            " that",
            " some",
            " of",
            " the",
            " first",
            " water",
            " brought",
            " to",
            " Earth",
            " was",
            " delivered",
            " by",
            " asteroid",
            " impacts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "a",
            "ard",
            "v",
            "ark",
            "\"",
            " is",
            " Afrika",
            "ans",
            " (),",
            " comes",
            " from",
            " earlier",
            " Afrika",
            "ans",
            " ",
            " and",
            " means",
            " \"",
            "earth",
            " pig",
            "\"",
            " or",
            " \"",
            "ground",
            " pig",
            "\"",
            " (",
            "a",
            "arde",
            ":",
            " \"",
            "earth",
            "\",",
            " var",
            "k",
            ":",
            " \"",
            "pig",
            "\",",
            " or",
            " \"",
            "young",
            " pig",
            "\"/",
            "child",
            "),",
            " because",
            " of",
            " its",
            " bur",
            "rowing",
            " habits",
            ".",
            " The",
            " name",
            " O",
            "ry",
            "cter",
            "opus",
            " means",
            " \"",
            "bur"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.387,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Info",
            " on",
            " Newton",
            " H",
            "WR",
            " from",
            " Apple",
            "'s",
            " H",
            "WR",
            " Technical",
            " Lead",
            ":",
            " ",
            " Notes",
            " on",
            " the",
            " History",
            " of",
            " Pen",
            "-based",
            " Computing",
            ":",
            " ",
            " This",
            " links",
            " to",
            ":",
            "External",
            " links",
            "Additional",
            " resources",
            " and",
            " information",
            " Def",
            "ying",
            " Gravity",
            ":",
            " The",
            " Making",
            " of",
            " Newton",
            ",",
            " by",
            " K",
            "oun",
            "al",
            "akis",
            " &",
            " Men",
            "uez",
            " (",
            "Hard",
            "cover",
            ")",
            " Hardcover",
            ":",
            " ",
            "192",
            " pages"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.277,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.273,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            " years",
            " ",
            "female",
            ":",
            " ",
            "16",
            ".",
            "3",
            " years",
            " (",
            "201",
            "8",
            " est",
            ".)",
            "Population",
            " growth",
            " rate",
            "3",
            ".",
            "36",
            "%",
            " (",
            "202",
            "2",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "6",
            "th",
            "3",
            ".",
            "49",
            "%",
            " (",
            "201",
            "8",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "2",
            "nd",
            "The",
            " population",
            " is",
            " growing",
            " by",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " Definitions",
            " of",
            " Art",
            ",",
            " ",
            "199",
            "1",
            " Nina",
            " Fel",
            "sh",
            "in",
            ",",
            " ed",
            ".",
            " But",
            " is",
            " it",
            " Art",
            "?,",
            " ",
            "199",
            "5",
            " Catherine",
            " de",
            " Z",
            "eg",
            "her",
            " (",
            "ed",
            ".).",
            " Inside",
            " the",
            " Visible",
            ".",
            " MIT",
            " Press",
            ",",
            " ",
            "199",
            "6",
            " Evelyn",
            " H",
            "atcher",
            ",",
            " ed",
            ".",
            " Art",
            " as",
            " Culture",
            ":",
            " An",
            " Introduction",
            " to",
            " the",
            " Anthrop",
            "ology",
            " of",
            " Art",
            ",",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " Empire",
            " (",
            "Jul",
            "ian",
            " calendar",
            ").",
            "177",
            "6",
            " –",
            " American",
            " Revolutionary",
            " War",
            ":",
            " Members",
            " of",
            " the",
            " ",
            "1",
            "st",
            " Maryland",
            " Regiment",
            " repeatedly",
            " charged",
            " a",
            " numer",
            "ically",
            " superior",
            " British",
            " force",
            " during",
            " the",
            " Battle",
            " of",
            " Long",
            " Island",
            ",",
            " allowing",
            " General",
            " Washington",
            " and",
            " the",
            " rest",
            " of",
            " the",
            " American",
            " troops",
            " to",
            " escape",
            ".",
            "179",
            "1",
            " –",
            " French",
            " Revolution",
            ":",
            " Frederick",
            " William",
            " II",
            " of",
            " Pr",
            "ussia",
            " and",
            " Le"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.303,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.249,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "41",
            ".",
            " Academic",
            " Press",
            ",",
            " London",
            ".",
            " J",
            "udd",
            ",",
            " W",
            ".",
            " S",
            ".",
            " et",
            " al",
            ".",
            " (",
            "199",
            "9",
            ").",
            " Plant",
            " System",
            "atics",
            ":",
            " A",
            " Phy",
            "logen",
            "etic",
            " Approach",
            ".",
            " Sunderland",
            ",",
            " MA",
            ":",
            " S",
            "ina",
            "uer",
            " Associates",
            ",",
            " Inc",
            ".",
            "  ",
            " ",
            " Niet",
            "o",
            " Fel",
            "iner",
            ",",
            " Gonz",
            "alo",
            ";",
            " Jury",
            ",",
            " Stephen",
            " Leonard",
            " &",
            " Herr",
            "ero",
            " Niet",
            "o",
            ",",
            " Alberto"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " objects",
            ",",
            " even",
            " used",
            " plane",
            " tickets",
            " and",
            " food",
            "—which",
            " was",
            " sealed",
            " in",
            " plain",
            " cardboard",
            " boxes",
            " dubbed",
            " Time",
            " Caps",
            "ules",
            ".",
            " By",
            " the",
            " time",
            " of",
            " his",
            " death",
            ",",
            " the",
            " collection",
            " grew",
            " to",
            " include",
            " ",
            "600",
            ",",
            " individually",
            " dated",
            " \"",
            "caps",
            "ules",
            "\".",
            " The",
            " boxes",
            " are",
            " now",
            " housed",
            " at",
            " the",
            " Andy",
            " War",
            "hol",
            " Museum",
            ".",
            " Television",
            ":",
            " Andy",
            " War",
            "hol",
            " dreamed",
            " of",
            " a",
            " television",
            " special"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "ob",
            "Ãº",
            "in",
            ":",
            " D",
            "ug",
            "ort",
            ",",
            " Ach",
            "ill",
            " Island",
            " ",
            "183",
            "1",
            "–",
            "186",
            "1",
            ":",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " a",
            " Mission",
            "ary",
            " Community",
            ",",
            " ",
            "200",
            "1",
            "Pat",
            "ricia",
            " Byrne",
            ":",
            " The",
            " Ve",
            "iled",
            " Woman",
            " of",
            " Ach",
            "ill",
            " –",
            " Island",
            " Out",
            "rage",
            " &",
            " A",
            " Playboy",
            " Drama",
            ",",
            " ",
            "201",
            "2",
            "Mary",
            " J",
            ".",
            " Murphy",
            ":",
            " Ach",
            "ill"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.236,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            " The",
            " Criterion",
            " Collection",
            " Ak",
            "ira",
            " K",
            "uros",
            "awa",
            ":",
            " News",
            ",",
            " Information",
            " and",
            " Discussion",
            " S",
            "enses",
            " of",
            " Cinema",
            ":",
            " Great",
            " Directors",
            " Critical",
            " Database",
            " Ak",
            "ira",
            " K",
            "uros",
            "awa",
            " at",
            " Japanese",
            " celebrity",
            "'s",
            " grave",
            " guide",
            "  ",
            " Several",
            " trailers",
            " Anaheim",
            " University",
            " Ak",
            "ira",
            " K",
            "uros",
            "awa",
            " School",
            " of",
            " Film",
            " ",
            " ",
            "191",
            "0",
            " births",
            "199",
            "8",
            " deaths"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.214,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " her",
            " biography",
            " as",
            " a",
            " Team",
            " of",
            " R",
            "ivals",
            ".",
            "There",
            " were",
            " two",
            " measures",
            " passed",
            " to",
            " raise",
            " revenues",
            " for",
            " the",
            " Federal",
            " government",
            ":",
            " tariffs",
            " (",
            "a",
            " policy",
            " with",
            " long",
            " precedent",
            "),",
            " and",
            " a",
            " Federal",
            " income",
            " tax",
            ".",
            " In",
            " ",
            "186",
            "1",
            ",",
            " Lincoln",
            " signed",
            " the",
            " second",
            " and",
            " third",
            " Morr",
            "ill",
            " Tar",
            "iffs",
            ",",
            " following",
            " the",
            " first",
            " enacted",
            " by",
            " Buchanan",
            ".",
            " He",
            " also",
            " signed",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.167,
            -0.0,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " present",
            " major",
            " landing",
            " or",
            " E",
            "VA",
            " challenges",
            ".",
            " It",
            " lay",
            " about",
            " ",
            " southeast",
            " of",
            " the",
            " Survey",
            "or",
            "Âł",
            "5",
            " landing",
            " site",
            ",",
            " and",
            " ",
            " southwest",
            " of",
            " Ranger",
            "Âł",
            "8",
            "'s",
            " crash",
            " site",
            ".",
            "L",
            "unar",
            " descent",
            " ",
            "At",
            " ",
            "12",
            ":",
            "52",
            ":",
            "00",
            " UTC",
            " on",
            " July",
            " ",
            "20",
            ",",
            " Ald",
            "rin",
            " and",
            " Armstrong",
            " entered",
            " Eagle",
            ",",
            " and",
            " began",
            " the",
            " final",
            " preparations"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " ocean",
            " surfaces",
            " can",
            " drive",
            " weather",
            ".",
            "The",
            " response",
            " of",
            " the",
            " climate",
            " system",
            " to",
            " an",
            " initial",
            " forcing",
            " is",
            " modified",
            " by",
            " feedback",
            "s",
            ":",
            " increased",
            " by",
            " \"",
            "self",
            "-re",
            "in",
            "forcing",
            "\"",
            " or",
            " \"",
            "positive",
            "\"",
            " feedback",
            "s",
            " and",
            " reduced",
            " by",
            " \"",
            "bal",
            "ancing",
            "\"",
            " or",
            " \"",
            "negative",
            "\"",
            " feedback",
            "s",
            ".",
            " The",
            " main",
            " reinforcing",
            " feedback",
            "s",
            " are",
            " the",
            " water",
            "-v",
            "ap",
            "our",
            " feedback",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Comm",
            "union",
            " and",
            " two",
            " break",
            "away",
            " churches",
            " in",
            " North",
            " America",
            " and",
            " Brazil",
            " from",
            " the",
            " Global",
            " South",
            " Fellowship",
            " of",
            " Anglic",
            "an",
            " Churches",
            " (",
            "GS",
            "FA",
            ")",
            " declared",
            " a",
            " state",
            " of",
            " impaired",
            " communion",
            " with",
            " the",
            " Church",
            " of",
            " England",
            " and",
            " announced",
            " that",
            " they",
            " would",
            " no",
            " longer",
            " recognise",
            " the",
            " arch",
            "bishop",
            " of",
            " Canterbury",
            " as",
            " the",
            " \"",
            "first",
            " among",
            " equals",
            "\"",
            " among",
            " the",
            " bishops",
            " in",
            " the",
            " Anglic",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " UK",
            ",",
            " the",
            " major",
            " club",
            " series",
            " are",
            " the",
            " Mon",
            "op",
            "osto",
            " Racing",
            " Club",
            ",",
            " BR",
            "SC",
            "C",
            " F",
            "3",
            " (",
            "Former",
            "ly",
            " Club",
            "F",
            "3",
            ",",
            " formerly",
            " ARP",
            " F",
            "3",
            "),",
            " Formula",
            " V",
            "ee",
            " and",
            " Club",
            " Formula",
            " Ford",
            ".",
            " Each",
            " series",
            " cat",
            "ers",
            " to",
            " a",
            " section",
            " of",
            " the",
            " market",
            ",",
            " with",
            " some",
            " primarily",
            " providing",
            " low",
            "-cost",
            " racing",
            ",",
            " while",
            " others",
            " aim",
            " for",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ust",
            "ro",
            "asi",
            "atic",
            ".",
            " Bl",
            "ench",
            " (",
            "201",
            "7",
            ")",
            " finds",
            " widespread",
            " Aust",
            "ro",
            "asi",
            "atic",
            " roots",
            " for",
            " '",
            "river",
            ",",
            " valley",
            "',",
            " '",
            "boat",
            "',",
            " '",
            "fish",
            "',",
            " '",
            "cat",
            "fish",
            " sp",
            ".',",
            " '",
            "eel",
            "',",
            " '",
            "pr",
            "awn",
            "',",
            " '",
            "sh",
            "rimp",
            "'",
            " (",
            "Central",
            " Aust",
            "ro",
            "asi",
            "atic",
            "),",
            " '",
            "cr",
            "ab",
            "',",
            " '",
            "t",
            "ort",
            "oise",
            "',",
            " '"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " disc",
            " flower",
            ")",
            " is",
            " a",
            " rad",
            "ially",
            " symmetric",
            " individual",
            " flower",
            " in",
            " the",
            " head",
            ",",
            " which",
            " is",
            " ring",
            "ed",
            " by",
            " the",
            " ray",
            " flowers",
            " when",
            " both",
            " are",
            " present",
            ".",
            " In",
            " some",
            " species",
            ",",
            " ray",
            " flowers",
            " may",
            " be",
            " arranged",
            " around",
            " the",
            " disc",
            " in",
            " irregular",
            " symmetry",
            ",",
            " or",
            " with",
            " a",
            " weak",
            "ly",
            " bil",
            "ater",
            "ally",
            " symmetric",
            " arrangement",
            ".",
            "Vari",
            "ations",
            " ",
            "A",
            " radi",
            "ate",
            " head",
            " has",
            " disc"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "unik",
    "hev",
    "orte",
    "umph",
    "ÑĢÐ°ÑĤ"
  ],
  "bottom_logits": [
    " legacy",
    "iday",
    "tak",
    " Lehr",
    " aff"
  ],
  "act_min": -0.0,
  "act_max": 0.656
}