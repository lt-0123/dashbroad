{
  "index": 32658,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " a",
            " topic",
            " of",
            " academic",
            " interest",
            " in",
            " the",
            " last",
            " few",
            " years",
            ".\"",
            "Works",
            "Surv",
            "iving",
            " plays",
            "Most",
            " of",
            " these",
            " are",
            " traditionally",
            " referred",
            " to",
            " by",
            " abbrev",
            "iations",
            " of",
            " their",
            " Latin",
            " titles",
            ";",
            " Latin",
            " remains",
            " a",
            " customary",
            " language",
            " of",
            " scholarship",
            " in",
            " classical",
            " studies",
            ".",
            " The",
            " A",
            "char",
            "n",
            "ians",
            " (",
            " Ak",
            "har",
            "ne",
            "is",
            ";",
            " Att",
            "ic",
            " ;",
            " ),",
            " ",
            "425",
            " BC"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " employed",
            " if",
            " the",
            " only",
            " question",
            " is",
            " the",
            " existence",
            " of",
            " a",
            " proof",
            ".",
            " It",
            " is",
            " possible",
            ",",
            " however",
            ",",
            " that",
            " there",
            " is",
            " a",
            " shorter",
            " proof",
            " of",
            " a",
            " theorem",
            " from",
            " Z",
            "FC",
            " than",
            " from",
            " Z",
            "F",
            ".",
            "The",
            " axiom",
            " of",
            " choice",
            " is",
            " not",
            " the",
            " only",
            " significant",
            " statement",
            " which",
            " is",
            " independent",
            " of",
            " Z",
            "F",
            ".",
            " ",
            " For",
            " example",
            ",",
            " the",
            " generalized",
            " continuum",
            " hypothesis",
            " (",
            "G",
            "CH"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " associated",
            " with",
            " dental",
            " decay",
            ".",
            "See",
            " also",
            " ",
            " Phase",
            " change",
            " memory",
            "Notes",
            "References",
            "C",
            "ited",
            " sources",
            "External",
            " links",
            " Public",
            " Health",
            " Statement",
            " for",
            " Ant",
            "imony",
            " International",
            " Ant",
            "imony",
            " Association",
            " v",
            "zw",
            " (",
            "i",
            "2",
            "a",
            ")",
            " Chemistry",
            " in",
            " its",
            " element",
            " podcast",
            " (",
            "MP",
            "3",
            ")",
            " from",
            " the",
            " Royal",
            " Society",
            " of",
            " Chemistry",
            "'s",
            " Chemistry",
            " World",
            ":",
            " Ant",
            "imony"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " similarly",
            " to",
            " substituted",
            " al",
            "kan",
            "es",
            " –",
            " the",
            " cyc",
            "lo",
            "alk",
            "ane",
            " ring",
            " is",
            " stated",
            ",",
            " and",
            " the",
            " substit",
            "u",
            "ents",
            " are",
            " according",
            " to",
            " their",
            " position",
            " on",
            " the",
            " ring",
            ",",
            " with",
            " the",
            " numbering",
            " decided",
            " by",
            " the",
            " C",
            "ahn",
            "–",
            "Ing",
            "old",
            "–",
            "Pre",
            "log",
            " priority",
            " rules",
            ".",
            "Tr",
            "ivial",
            "/common",
            " names",
            "The",
            " trivial",
            " (",
            "non",
            "-system",
            "atic",
            ")",
            " name",
            " for",
            " al",
            "kan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            "o",
            " and",
            " Sil",
            "via",
            " Åł",
            "t",
            "ub",
            "ÅĪ",
            "ovÃ¡",
            " N",
            "ig",
            "rell",
            "i",
            " write",
            " that",
            " there",
            " are",
            " about",
            " ",
            "400",
            " languages",
            " in",
            " Afro",
            "asi",
            "atic",
            ";",
            " Eth",
            "n",
            "ologue",
            " lists",
            " ",
            "375",
            " languages",
            ".",
            " Many",
            " scholars",
            " estimate",
            " fewer",
            " languages",
            ";",
            " exact",
            " numbers",
            " vary",
            " depending",
            " on",
            " the",
            " definitions",
            " of",
            " \"",
            "language",
            "\"",
            " and",
            " \"",
            "dia",
            "lect",
            "\".",
            "Ber",
            "ber",
            "The",
            " Ber",
            "ber",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " there",
            " are",
            " criticized",
            " patents",
            " involving",
            " algorithms",
            ",",
            " especially",
            " data",
            " compression",
            " algorithms",
            ",",
            " such",
            " as",
            " Un",
            "is",
            "ys",
            "'s",
            " L",
            "ZW",
            " patent",
            ".",
            "Additionally",
            ",",
            " some",
            " cryptographic",
            " algorithms",
            " have",
            " export",
            " restrictions",
            " (",
            "see",
            " export",
            " of",
            " cryptography",
            ").",
            "History",
            ":",
            " Development",
            " of",
            " the",
            " notion",
            " of",
            " \"",
            "algorithm",
            "\"",
            "Anc",
            "ient",
            " Near",
            " East",
            " ",
            "The",
            " earliest",
            " evidence",
            " of",
            " algorithms",
            " is",
            " found",
            " in",
            " the",
            " Babylon",
            "ian",
            " mathematics"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "21",
            " languages",
            " of",
            " Burma",
            ",",
            " southern",
            " China",
            ",",
            " and",
            " Thailand",
            " Nuclear",
            " Mon",
            "–",
            "Kh",
            "mer",
            " languages",
            " Kh",
            "mer",
            "o",
            "-V",
            "iet",
            "ic",
            " languages",
            " (",
            "Eastern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " Viet",
            "o",
            "-K",
            "atu",
            "ic",
            " languages",
            " ?",
            " Viet",
            "ic",
            ":",
            " ",
            "10",
            " languages",
            " of",
            " Vietnam",
            " and",
            " Laos",
            ",",
            " including",
            " Mu",
            "ong",
            " and",
            " Vietnamese",
            ",",
            " which",
            " has",
            " the",
            " most",
            " speakers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " written",
            " by",
            " Michael",
            " Green",
            ".",
            "Other",
            " ",
            " Anat",
            "oly",
            " Rav",
            "ik",
            "ovich",
            ",",
            " Zag",
            "ad",
            "ka",
            " End",
            "kh",
            "au",
            "za",
            " (",
            "End",
            " House",
            " Mystery",
            ")",
            " (",
            "198",
            "9",
            ";",
            " based",
            " on",
            " \"",
            "Per",
            "il",
            " at",
            " End",
            " House",
            "\")",
            "Te",
            "levision",
            "David",
            " Such",
            "et",
            " ",
            "David",
            " Such",
            "et",
            " starred",
            " as",
            " P",
            "oi",
            "rot",
            " in",
            " the",
            " ITV",
            " series",
            " Ag",
            "atha",
            " Christie",
            "'s",
            " P",
            "oi"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " mathematic",
            "ians",
            ".",
            "India",
            "The",
            " Ab",
            "hid",
            "h",
            "arm",
            "ako",
            "ÅĽ",
            "abh",
            "Äģ",
            "á¹",
            "£",
            "ya",
            " of",
            " Vas",
            "ub",
            "and",
            "hu",
            " (",
            "316",
            "-",
            "396",
            "),",
            " a",
            " Sans",
            "krit",
            " work",
            " on",
            " Buddhist",
            " philosophy",
            ",",
            " says",
            " that",
            " the",
            " second",
            "-century",
            " CE",
            " philosopher",
            " Vas",
            "um",
            "itra",
            " said",
            " that",
            " \"",
            "placing",
            " a",
            " w",
            "ick",
            " (",
            "S",
            "ansk",
            "rit",
            " v",
            "art",
            "ik",
            "Äģ",
            ")",
            " on",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "ap",
            "ut",
            ":",
            " ",
            "7",
            " languages",
            "Core",
            " M",
            "unda",
            " languages",
            " Kh",
            "arian",
            "–",
            "Ju",
            "ang",
            ":",
            " ",
            "2",
            " languages",
            "North",
            " M",
            "unda",
            " languages",
            " K",
            "ork",
            "u",
            " K",
            "her",
            "w",
            "arian",
            ":",
            " ",
            "12",
            " languages",
            " K",
            "has",
            "i",
            "–",
            "Kh",
            "mu",
            "ic",
            " languages",
            " (",
            "Northern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " K",
            "has",
            "ian",
            ":",
            " ",
            "3",
            " languages",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            "etic",
            "ide",
            ".",
            "Methods",
            "Medical",
            "Medical",
            " abortions",
            " are",
            " those",
            " induced",
            " by",
            " abort",
            "if",
            "ac",
            "ient",
            " pharmaceutical",
            "s",
            ".",
            " Medical",
            " abortion",
            " became",
            " an",
            " alternative",
            " method",
            " of",
            " abortion",
            " with",
            " the",
            " availability",
            " of",
            " prost",
            "ag",
            "land",
            "in",
            " analog",
            "s",
            " in",
            " the",
            " ",
            "197",
            "0",
            "s",
            " and",
            " the",
            " ant",
            "ipro",
            "gest",
            "ogen",
            " m",
            "ife",
            "pr",
            "ist",
            "one",
            " (",
            "also",
            " known",
            " as",
            " RU",
            "-",
            "486",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " few",
            " other",
            " de",
            "ict",
            "ic",
            " and",
            " auxiliary",
            " items",
            ",",
            " whose",
            " sharing",
            " could",
            " be",
            " explained",
            " in",
            " other",
            " ways",
            ";",
            " not",
            " the",
            " kind",
            " of",
            " sharing",
            " expected",
            " in",
            " cases",
            " of",
            " genetic",
            " relationship",
            ".",
            "The",
            " Spr",
            "ach",
            "b",
            "und",
            " hypothesis",
            "Instead",
            " of",
            " a",
            " common",
            " genetic",
            " origin",
            ",",
            " Claus",
            "on",
            ",",
            " Do",
            "er",
            "fer",
            ",",
            " and",
            " Sh",
            "cher",
            "bak",
            " proposed",
            " (",
            "in",
            " ",
            "195",
            "6",
            "–",
            "196"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "In",
            " ",
            "192",
            "8",
            ",",
            " a",
            " partial",
            " formal",
            "ization",
            " of",
            " the",
            " modern",
            " concept",
            " of",
            " algorithms",
            " began",
            " with",
            " attempts",
            " to",
            " solve",
            " the",
            " Ents",
            "cheid",
            "ungs",
            "problem",
            " (",
            "decision",
            " problem",
            ")",
            " posed",
            " by",
            " David",
            " Hil",
            "bert",
            ".",
            " Later",
            " formal",
            "izations",
            " were",
            " framed",
            " as",
            " attempts",
            " to",
            " define",
            " \"",
            "effective",
            " calcul",
            "ability",
            "\"",
            " or",
            " \"",
            "effective",
            " method",
            "\".",
            " Those",
            " formal",
            "izations",
            " included",
            " the",
            " GÃ¶",
            "del",
            "–"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " was",
            " \"",
            "the",
            " first",
            " attempt",
            " at",
            " an",
            " axiom",
            "at",
            "ization",
            " of",
            " mathematics",
            " in",
            " a",
            " symbolic",
            " language",
            "\".",
            "But",
            " He",
            "ij",
            "eno",
            "ort",
            " gives",
            " Fre",
            "ge",
            " (",
            "187",
            "9",
            ")",
            " this",
            " k",
            "udos",
            ":",
            " Fre",
            "ge",
            "'s",
            " is",
            " \"",
            "perhaps",
            " the",
            " most",
            " important",
            " single",
            " work",
            " ever",
            " written",
            " in",
            " logic",
            ".",
            " ...",
            " in",
            " which",
            " we",
            " see",
            " a",
            " formula",
            " language",
            "',",
            " that",
            " is",
            " a",
            " ling",
            "ua"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " neuron",
            " as",
            " it",
            " extends",
            " to",
            " become",
            " the",
            " ax",
            "on",
            ".",
            " It",
            " preced",
            "es",
            " the",
            " initial",
            " segment",
            ".",
            " The",
            " received",
            " action",
            " potentials",
            " that",
            " are",
            " summed",
            " in",
            " the",
            " neuron",
            " are",
            " transmitted",
            " to",
            " the",
            " ax",
            "on",
            " hil",
            "lock",
            " for",
            " the",
            " generation",
            " of",
            " an",
            " action",
            " potential",
            " from",
            " the",
            " initial",
            " segment",
            ".",
            "Ax",
            "onal",
            " initial",
            " segment",
            "The",
            " ax",
            "onal",
            " initial",
            " segment",
            " (",
            "A",
            "IS",
            ")",
            " is",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " called",
            " \"",
            "ind",
            "uced",
            " miscar",
            "riage",
            "\".",
            " ",
            " Such",
            " methods",
            " are",
            " rarely",
            " used",
            " in",
            " countries",
            " where",
            " surgical",
            " abortion",
            " is",
            " legal",
            " and",
            " available",
            ".",
            "Safety",
            "The",
            " health",
            " risks",
            " of",
            " abortion",
            " depend",
            " principally",
            " upon",
            " whether",
            " the",
            " procedure",
            " is",
            " performed",
            " safely",
            " or",
            " uns",
            "af",
            "ely",
            ".",
            " The",
            " World",
            " Health",
            " Organization",
            " (",
            "WHO",
            ")",
            " defines",
            " unsafe",
            " abortions",
            " as",
            " those",
            " performed",
            " by",
            " un",
            "sk",
            "illed",
            " individuals"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " Similarly",
            ",",
            " although",
            " a",
            " subset",
            " of",
            " the",
            " real",
            " numbers",
            " that",
            " is",
            " not",
            " Le",
            "bes",
            "gue",
            " measurable",
            " can",
            " be",
            " proved",
            " to",
            " exist",
            " using",
            " the",
            " axiom",
            " of",
            " choice",
            ",",
            " it",
            " is",
            " consistent",
            " that",
            " no",
            " such",
            " set",
            " is",
            " defin",
            "able",
            ".",
            "The",
            " axiom",
            " of",
            " choice",
            " proves",
            " the",
            " existence",
            " of",
            " these",
            " int",
            "ang",
            "ibles",
            " (",
            "objects",
            " that",
            " are",
            " proved",
            " to",
            " exist",
            ",",
            " but",
            " which",
            " cannot",
            " be",
            " explicitly"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " use",
            " the",
            " expression",
            " \"",
            "comput",
            "able",
            " function",
            "\"",
            " to",
            " mean",
            " a",
            " function",
            " calcul",
            "able",
            " by",
            " a",
            " machine",
            ",",
            " and",
            " we",
            " let",
            " \"",
            "effect",
            "ively",
            " calcul",
            "able",
            "\"",
            " refer",
            " to",
            " the",
            " intuitive",
            " idea",
            " without",
            " particular",
            " identification",
            " with",
            " any",
            " one",
            " of",
            " these",
            " definitions",
            "\".",
            "J",
            ".",
            " B",
            ".",
            " Ros",
            "ser",
            " (",
            "193",
            "9",
            ")",
            " and",
            " S",
            ".",
            " C",
            ".",
            " Kle",
            "ene",
            " (",
            "194",
            "3",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " radi",
            "ated",
            " out",
            " from",
            " the",
            " central",
            " Mek",
            "ong",
            " river",
            " valley",
            " relatively",
            " quickly",
            ".",
            "Sub",
            "sequently",
            ",",
            " Sid",
            "well",
            " (",
            "201",
            "5",
            "a",
            ":",
            " ",
            "179",
            ")",
            " proposed",
            " that",
            " Nic",
            "ob",
            "are",
            "se",
            " sub",
            "groups",
            " with",
            " As",
            "lian",
            ",",
            " just",
            " as",
            " how",
            " K",
            "has",
            "ian",
            " and",
            " P",
            "ala",
            "ung",
            "ic",
            " subgroup",
            " with",
            " each",
            " other",
            ".",
            "A",
            " subsequent",
            " computational",
            " phy",
            "logen",
            "etic",
            " analysis",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " is",
            " used",
            " as",
            " a",
            " source",
            " of",
            " flavor",
            "ing",
            " for",
            " abs",
            "in",
            "the",
            ",",
            " a",
            " bitter",
            " classical",
            " liquor",
            " of",
            " European",
            " origin",
            ".",
            "References",
            "Further",
            " reading",
            " ",
            " W",
            ".",
            " S",
            ".",
            " J",
            "udd",
            ",",
            " C",
            ".",
            " S",
            ".",
            " Campbell",
            ",",
            " E",
            ".",
            " A",
            ".",
            " Kel",
            "logg",
            ",",
            " P",
            ".",
            " F",
            ".",
            " Stevens",
            ",",
            " M",
            ".",
            " J",
            ".",
            " Don",
            "ogh",
            "ue",
            " (",
            "200",
            "2"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " advocates",
            " U",
            ".S",
            ".",
            " policy",
            " and",
            " technical",
            " positions",
            " in",
            " international",
            " and",
            " regional",
            " standards",
            " organizations",
            ",",
            " and",
            " encourages",
            " the",
            " adoption",
            " of",
            " international",
            " standards",
            " as",
            " national",
            " standards",
            " where",
            " appropriate",
            ".",
            "The",
            " institute",
            " is",
            " the",
            " official",
            " U",
            ".S",
            ".",
            " representative",
            " to",
            " the",
            " two",
            " major",
            " international",
            " standards",
            " organizations",
            ",",
            " the",
            " International",
            " Organization",
            " for",
            " Standard",
            "ization",
            " (",
            "ISO",
            "),",
            " as",
            " a",
            " founding",
            " member",
            ",",
            " and",
            " the",
            " International",
            " Elect"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Dutch",
            " model",
            ".",
            " In",
            " Ar",
            "uba",
            ",",
            " legal",
            " jurisdiction",
            " lies",
            " with",
            " the",
            " G",
            "ere",
            "cht",
            " in",
            " E",
            "er",
            "ste",
            " A",
            "an",
            "leg",
            " (",
            "Court",
            " of",
            " First",
            " Instance",
            ")",
            " on",
            " Ar",
            "uba",
            ",",
            " the",
            " Geme",
            "ensch",
            "app",
            "elijk",
            " Hof",
            " van",
            " Just",
            "it",
            "ie",
            " van",
            " Ar",
            "uba",
            ",",
            " C",
            "ura",
            "Ã§",
            "ao",
            ",",
            " S",
            "int",
            " Ma",
            "arten",
            ",",
            " en",
            " van",
            " B",
            "ona",
            "ire",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " languages",
            " as",
            " W",
            "sp",
            "om",
            "n",
            "ienie",
            " W",
            "sz",
            "yst",
            "kich",
            " Wi",
            "ern",
            "ych",
            " Z",
            "mar",
            "ÅĤ",
            "ych",
            " or",
            " Z",
            "ad",
            "usz",
            "ki",
            " (",
            "Pol",
            "ish",
            "),",
            " V",
            "zp",
            "om",
            "ÃŃn",
            "ka",
            " na",
            " vÅ¡echny",
            " vÄĽ",
            "rn",
            "Ã©",
            " z",
            "es",
            "nul",
            "Ã©",
            ",",
            " Pam",
            "Ã¡tka",
            " z",
            "es",
            "nul",
            "Ã½ch",
            " or",
            " Du",
            "Å¡",
            "iÄįky",
            " (",
            "C",
            "zech",
            "),",
            " Pam",
            "iat",
            "ka",
            " z",
            "os",
            "nul",
            "Ã½ch"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " metast",
            "able",
            ",",
            " they",
            " also",
            " emit",
            " gamma",
            " rays",
            " with",
            " the",
            " discrete",
            " energies",
            " between",
            " ",
            "26",
            ".",
            "3",
            " and",
            " ",
            "158",
            ".",
            "5",
            "Âł",
            "ke",
            "V",
            ".",
            "Americ",
            "ium",
            "-",
            "242",
            " is",
            " a",
            " short",
            "-lived",
            " is",
            "otope",
            " with",
            " a",
            " half",
            "-life",
            " of",
            " ",
            "16",
            ".",
            "02",
            "Âłh",
            ".",
            " It",
            " mostly",
            " (",
            "82",
            ".",
            "7",
            "%)",
            " converts",
            " by",
            " Î²",
            "-de",
            "c",
            "ay",
            " to",
            " ",
            "242"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Newton",
            " device",
            " is",
            " reset",
            ".",
            "Hand",
            "writing",
            " recognition",
            "In",
            " initial",
            " versions",
            " (",
            "Newton",
            " OS",
            " ",
            "1",
            ".x",
            ")",
            " the",
            " handwriting",
            " recognition",
            " gave",
            " extremely",
            " mixed",
            " results",
            " for",
            " users",
            " and",
            " was",
            " sometimes",
            " inaccurate",
            ".",
            " The",
            " original",
            " handwriting",
            " recognition",
            " engine",
            " was",
            " called",
            " Call",
            "igraph",
            "er",
            ",",
            " and",
            " was",
            " licensed",
            " from",
            " a",
            " Russian",
            " company",
            " called",
            " Paragraph",
            " International",
            ".",
            " Call",
            "igraph",
            "er",
            "'s",
            " design",
            " was",
            " quite"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " around",
            " which",
            " the",
            " languages",
            " are",
            " spoken",
            ",",
            " the",
            " Red",
            " Sea",
            "—",
            "have",
            " also",
            " been",
            " proposed",
            ".",
            "Distribution",
            " and",
            " branches",
            "Sch",
            "olars",
            " generally",
            " consider",
            " Afro",
            "asi",
            "atic",
            " to",
            " have",
            " between",
            " five",
            " and",
            " eight",
            " branches",
            ".",
            " The",
            " five",
            " that",
            " are",
            " universally",
            " agreed",
            " upon",
            " are",
            " Ber",
            "ber",
            " (",
            "also",
            " called",
            " \"",
            "Lib",
            "y",
            "co",
            "-B",
            "er",
            "ber",
            "\"),",
            " Ch",
            "adic",
            ",",
            " Cush",
            "itic",
            ",",
            " Egyptian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " Boh",
            "r",
            "'s",
            " hypothesis",
            " directly",
            ",",
            " by",
            " seeing",
            " if",
            " spectral",
            " lines",
            " emitted",
            " from",
            " excited",
            " atoms",
            " fitted",
            " the",
            " Boh",
            "r",
            " theory",
            "'s",
            " post",
            "ulation",
            " that",
            " the",
            " frequency",
            " of",
            " the",
            " spectral",
            " lines",
            " be",
            " proportional",
            " to",
            " the",
            " square",
            " of",
            " Z",
            ".",
            "To",
            " do",
            " this",
            ",",
            " M",
            "ose",
            "ley",
            " measured",
            " the",
            " wavelengths",
            " of",
            " the",
            " inner",
            "most",
            " photon",
            " transitions",
            " (",
            "K",
            " and",
            " L",
            " lines",
            ")",
            " produced",
            " by",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " therm",
            "odynamic",
            " properties",
            " such",
            " as",
            " ion",
            "isation",
            " energy",
            " and",
            " electron",
            " affinity",
            ",",
            " but",
            " hydrogen",
            " cannot",
            " be",
            " t",
            "etr",
            "aval",
            "ent",
            ".",
            " Thus",
            " none",
            " of",
            " the",
            " three",
            " placements",
            " are",
            " entirely",
            " satisfactory",
            ",",
            " although",
            " group",
            " ",
            "1",
            " is",
            " the",
            " most",
            " common",
            " placement",
            " (",
            "if",
            " one",
            " is",
            " chosen",
            ")",
            " because",
            " of",
            " the",
            " electron",
            " configuration",
            " and",
            " the",
            " fact",
            " that",
            " the",
            " hy",
            "d",
            "ron",
            " is",
            " by",
            " far"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " ways",
            " to",
            " axiom",
            "at",
            "ize",
            " a",
            " given",
            " mathematical",
            " domain",
            ".",
            "Any",
            " axiom",
            " is",
            " a",
            " statement",
            " that",
            " serves",
            " as",
            " a",
            " starting",
            " point",
            " from",
            " which",
            " other",
            " statements",
            " are",
            " logically",
            " derived",
            ".",
            " Whether",
            " it",
            " is",
            " meaningful",
            " (",
            "and",
            ",",
            " if",
            " so",
            ",",
            " what",
            " it",
            " means",
            ")",
            " for",
            " an",
            " axiom",
            " to",
            " be",
            " \"",
            "true",
            "\"",
            " is",
            " a",
            " subject",
            " of",
            " debate",
            " in",
            " the",
            " philosophy",
            " of",
            " mathematics",
            ".",
            "Et"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " Sol",
            "vang",
            ",",
            " United",
            " States",
            " ,",
            " Romania",
            " ,",
            " Bulgaria",
            " Vil",
            "ni",
            "us",
            ",",
            " Lithuania",
            " ,",
            " Germany",
            "References",
            " Municipal",
            " statistics",
            ":",
            " Net",
            "B",
            "or",
            "ger",
            " Komm",
            "un",
            "ef",
            "ak",
            "ta",
            ",",
            " delivered",
            " from",
            " K",
            "MD",
            " a",
            ".k",
            ".a",
            ".",
            " Komm",
            "uned",
            "ata",
            " (",
            "M",
            "unicip",
            "al",
            " Data",
            ")",
            " Municipal",
            " merg",
            "ers",
            " and",
            " neighbors",
            ":",
            " En",
            "iro",
            " map"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " bil",
            "ayer",
            ".",
            " Ac",
            "ids",
            " that",
            " lose",
            " a",
            " proton",
            " at",
            " the",
            " intr",
            "acellular",
            " pH",
            " will",
            " exist",
            " in",
            " their",
            " soluble",
            ",",
            " charged",
            " form",
            " and",
            " are",
            " thus",
            " able",
            " to",
            " diffuse",
            " through",
            " the",
            " cy",
            "tos",
            "ol",
            " to",
            " their",
            " target",
            ".",
            " Ib",
            "upro",
            "fen",
            ",",
            " aspir",
            "in",
            " and",
            " pen",
            "ic",
            "illin",
            " are",
            " examples",
            " of",
            " drugs",
            " that",
            " are",
            " weak",
            " acids",
            ".",
            "Common",
            " acids",
            "Min",
            "eral",
            " acids",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Vol",
            ".",
            " ",
            "62",
            ".",
            " P",
            "erg",
            "amon",
            ",",
            " ",
            "201",
            "7",
            ".",
            " Ol",
            "uk",
            "b",
            "asi",
            ",",
            " Su",
            "ha",
            ".",
            " Azerbaijan",
            ":",
            " A",
            " Political",
            " History",
            ".",
            " I",
            ".B",
            ".",
            " T",
            "aur",
            "is",
            " (",
            "201",
            "1",
            ").",
            " Focus",
            " on",
            " post",
            "-S",
            "ov",
            "iet",
            " era",
            ".",
            "External",
            " links",
            "General",
            " information",
            " Azerbaijan",
            " International",
            " Hey",
            "dar",
            " Ali",
            "y",
            "ev",
            " Foundation",
            " ",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            "imal",
            " Principle",
            ",",
            " Z",
            "orn",
            "'s",
            " Lemma",
            " and",
            " formal",
            " proofs",
            " of",
            " their",
            " equivalence",
            " down",
            " to",
            " the",
            " finest",
            " detail",
            ".",
            "Con",
            "sequences",
            " of",
            " the",
            " A",
            "xi",
            "om",
            " of",
            " Choice",
            " ,",
            " based",
            " on",
            " the",
            " book",
            " by",
            " Paul",
            " Howard",
            " ",
            " and",
            " Jean",
            " Rubin",
            ".",
            ".",
            "<|begin_of_text|>",
            "At",
            "til",
            "a",
            " (",
            " ",
            " or",
            " ",
            " ;",
            " ),",
            " frequently",
            " called",
            " At",
            "til",
            "a",
            " the",
            " Hun",
            ",",
            " was",
            " the",
            " ruler"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " angle",
            " of",
            " sweep",
            ",",
            " and",
            " any",
            " variations",
            " along",
            " the",
            " span",
            " (",
            "including",
            " the",
            " important",
            " class",
            " of",
            " delta",
            " wings",
            ").",
            " Location",
            " of",
            " the",
            " horizontal",
            " stabil",
            "izer",
            ",",
            " if",
            " any",
            ".",
            " D",
            "ih",
            "edral",
            " angle",
            "Âł",
            "—",
            " positive",
            ",",
            " zero",
            ",",
            " or",
            " negative",
            " (",
            "anh",
            "edral",
            ").",
            "A",
            " variable",
            " geometry",
            " aircraft",
            " can",
            " change",
            " its",
            " wing",
            " configuration",
            " during",
            " flight",
            ".",
            "A",
            " flying",
            " wing",
            " has",
            " no"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            "l",
            ",",
            " for",
            " example",
            ",",
            " proceeds",
            " through",
            " an",
            " unstable",
            " ac",
            "yl",
            " nickel",
            " carb",
            "ony",
            "l",
            " complex",
            " which",
            " then",
            " undergo",
            "es",
            " elect",
            "roph",
            "il",
            "ic",
            " substitution",
            " to",
            " give",
            " the",
            " desired",
            " al",
            "dehyde",
            " (",
            "using",
            " H",
            "+",
            " as",
            " the",
            " elect",
            "roph",
            "ile",
            ")",
            " or",
            " ket",
            "one",
            " (",
            "using",
            " an",
            " alk",
            "yl",
            " hal",
            "ide",
            ")",
            " product",
            ".",
            "Li",
            "R",
            " \\",
            " +",
            " \\",
            " [",
            "Ni",
            "(C",
            "O"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " Tax",
            "onomy",
            " Browser",
            " ",
            "Ang",
            "ios",
            "perm",
            " orders",
            "Late",
            " C",
            "ret",
            "aceous",
            " plants",
            "Ext",
            "ant",
            " Cam",
            "pan",
            "ian",
            " first",
            " appearances",
            "es",
            ":",
            "Are",
            "cales",
            "<|begin_of_text|>",
            "H",
            "erc",
            "ule",
            " P",
            "oi",
            "rot",
            " (",
            ",",
            " )",
            " is",
            " a",
            " fictional",
            " Belgian",
            " detective",
            " created",
            " by",
            " British",
            " writer",
            " Ag",
            "atha",
            " Christie",
            ".",
            " P",
            "oi",
            "rot",
            " is",
            " one",
            " of",
            " Christie",
            "'s",
            " most",
            " famous",
            " and",
            " long"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " arranged",
            " before",
            " the",
            " letters",
            ",",
            " and",
            " the",
            " letter",
            " A",
            " was",
            " placed",
            " in",
            " position",
            " ",
            "41",
            "hex",
            " to",
            " match",
            " the",
            " draft",
            " of",
            " the",
            " corresponding",
            " British",
            " standard",
            ".",
            " The",
            " digits",
            " ",
            "0",
            "–",
            "9",
            " are",
            " prefixed",
            " with",
            " ",
            "011",
            ",",
            " but",
            " the",
            " remaining",
            " ",
            "4",
            " bits",
            " correspond",
            " to",
            " their",
            " respective",
            " values",
            " in",
            " binary",
            ",",
            " making",
            " conversion",
            " with",
            " binary",
            "-coded",
            " decimal",
            " straightforward",
            " (",
            "for",
            " example",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            0.34,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Te",
            "let",
            "ype",
            " Model",
            " ",
            "35",
            " as",
            " a",
            " seven",
            "-bit",
            " tele",
            "printer",
            " code",
            " promoted",
            " by",
            " Bell",
            " data",
            " services",
            ".",
            " Work",
            " on",
            " the",
            " ASCII",
            " standard",
            " began",
            " in",
            " May",
            " ",
            "196",
            "1",
            ",",
            " with",
            " the",
            " first",
            " meeting",
            " of",
            " the",
            " American",
            " Standards",
            " Association",
            "'s",
            " (",
            "ASA",
            ")",
            " (",
            "now",
            " the",
            " American",
            " National",
            " Standards",
            " Institute",
            " or",
            " ANSI",
            ")",
            " X",
            "3",
            ".",
            "2",
            " sub",
            "committee",
            ".",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            "ano",
            ",",
            " \"",
            "so",
            "-and",
            "-so",
            "\"),",
            " everyday",
            " terms",
            " (",
            "like",
            " Hind",
            "ust",
            "ani",
            " le",
            "kin",
            ",",
            " \"",
            "but",
            "\",",
            " or",
            " Spanish",
            " t",
            "aza",
            " and",
            " French",
            " t",
            "asse",
            ",",
            " meaning",
            " \"",
            "cup",
            "\"),",
            " and",
            " expressions",
            " (",
            "like",
            " Catalan",
            " a",
            " bet",
            "z",
            "ef",
            ",",
            " \"",
            "gal",
            "ore",
            ",",
            " in",
            " quantity",
            "\").",
            " Most",
            " Ber",
            "ber",
            " varieties",
            " (",
            "such",
            " as",
            " Kab",
            "yle",
            "),",
            " along",
            " with",
            " Sw"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Catalan",
            " writings",
            " in",
            " And",
            "orra",
            ",",
            " with",
            " cultural",
            " works",
            " such",
            " as",
            " the",
            " Book",
            " of",
            " Priv",
            "ileges",
            " (",
            "L",
            "lib",
            "re",
            " de",
            " Priv",
            "ile",
            "gis",
            " de",
            " ",
            "167",
            "4",
            "),",
            " Manual",
            " Digest",
            " (",
            "174",
            "8",
            ")",
            " by",
            " Anton",
            "i",
            " F",
            "iter",
            " i",
            " Ross",
            "ell",
            " or",
            " the",
            " Polit",
            "Ãł",
            " and",
            "orr",
            "Ãł",
            " (",
            "176",
            "3",
            ")",
            " by",
            " Anton",
            "i",
            " Pu",
            "ig",
            ".",
            "19",
            "th"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Han",
            " (",
            "111",
            " CE",
            ")",
            " several",
            " dozen",
            " Proto",
            "-T",
            "ur",
            "k",
            "ic",
            " ex",
            "ot",
            "isms",
            " in",
            " Chinese",
            " Han",
            " trans",
            "criptions",
            ".",
            " Lan",
            "hai",
            " Wei",
            " and",
            " H",
            "ui",
            " Li",
            " reconstruct",
            " the",
            " name",
            " of",
            " the",
            " Xi",
            "Åį",
            "ng",
            "n",
            "Ãº",
            " ruling",
            " house",
            " as",
            "PT",
            " *",
            "Al",
            "ay",
            "und",
            "luÄŁ",
            " /",
            "al",
            "aj",
            "unt",
            "Ë",
            "Ī",
            "lu",
            "Î³",
            "/",
            " '",
            "pie",
            "b",
            "ald",
            " horse"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " T",
            "oshi",
            "ro",
            " M",
            "if",
            "une",
            " in",
            " a",
            " starring",
            " role",
            ",",
            " cement",
            "ed",
            " the",
            " director",
            "'s",
            " reputation",
            " as",
            " one",
            " of",
            " the",
            " most",
            " important",
            " young",
            " filmmakers",
            " in",
            " Japan",
            ".",
            " The",
            " two",
            " men",
            " would",
            " go",
            " on",
            " to",
            " collaborate",
            " on",
            " another",
            " fifteen",
            " films",
            ".",
            "R",
            "ash",
            "omon",
            " (",
            "195",
            "0",
            "),",
            " which",
            " premiered",
            " in",
            " Tokyo",
            ",",
            " became",
            " the",
            " surprise",
            " winner",
            " of",
            " the",
            " Golden",
            " Lion",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.426,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "%",
            ");",
            " ",
            "388",
            ",",
            "210",
            " French",
            " (",
            "11",
            ".",
            "9",
            "%",
            ");",
            " ",
            "332",
            ",",
            "180",
            " Ukrainian",
            " (",
            "10",
            ".",
            "2",
            "%",
            ");",
            " ",
            "172",
            ",",
            "910",
            " Dutch",
            " (",
            "5",
            ".",
            "3",
            "%",
            ");",
            " ",
            "170",
            ",",
            "935",
            " Polish",
            " (",
            "5",
            ".",
            "2",
            "%",
            ");",
            " ",
            "169",
            ",",
            "355",
            " North",
            " American",
            " Indian",
            " (",
            "5",
            ".",
            "2",
            "%",
            ");",
            " ",
            "144",
            ",",
            "585"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.434,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Canada",
            ")",
            " Day",
            " of",
            " Hearts",
            " (",
            "Ha",
            "ar",
            "lem",
            " and",
            " Amsterdam",
            ",",
            " Netherlands",
            ")",
            " National",
            " Mour",
            "ning",
            " Day",
            " (",
            "Bang",
            "ladesh",
            ")",
            "3",
            "rd",
            " Friday",
            " ",
            " Hawaii",
            " Admission",
            " Day",
            " (",
            "H",
            "awaii",
            ",",
            " United",
            " States",
            ")",
            "Last",
            " Thursday",
            " ",
            " National",
            " Burger",
            " Day",
            " (",
            "United",
            " Kingdom",
            ")",
            "Last",
            " Sunday",
            " ",
            " Coal",
            " Miner",
            "'s",
            " Day",
            " (",
            "some",
            " former",
            " Soviet",
            " Union",
            " countries",
            ")",
            " National",
            " Grand"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " per",
            " year",
            " (",
            "E",
            "ston",
            "ia",
            ")",
            " in",
            " countries",
            " with",
            " complete",
            " statistics",
            " in",
            " ",
            "200",
            "8",
            ".",
            " The",
            " proportion",
            " of",
            " pregnancies",
            " that",
            " ended",
            " in",
            " induced",
            " abortion",
            " ranged",
            " from",
            " about",
            " ",
            "10",
            "%",
            " (",
            "Israel",
            ",",
            " the",
            " Netherlands",
            " and",
            " Switzerland",
            ")",
            " to",
            " ",
            "30",
            "%",
            " (",
            "E",
            "ston",
            "ia",
            ")",
            " in",
            " the",
            " same",
            " group",
            ",",
            " though",
            " it",
            " might",
            " be",
            " as",
            " high",
            " as",
            " ",
            "36"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " essential",
            " for",
            " data",
            " transmission",
            " were",
            " the",
            " start",
            " of",
            " message",
            " (",
            "S",
            "OM",
            "),",
            " end",
            " of",
            " address",
            " (",
            "EO",
            "A",
            "),",
            " end",
            " of",
            " message",
            " (",
            "E",
            "OM",
            "),",
            " end",
            " of",
            " transmission",
            " (",
            "E",
            "OT",
            "),",
            " \"",
            "who",
            " are",
            " you",
            "?\"",
            " (",
            "WR",
            "U",
            "),",
            " \"",
            "are",
            " you",
            "?\"",
            " (",
            "RU",
            "),",
            " a",
            " reserved",
            " device",
            " control",
            " (",
            "DC",
            "0",
            "),",
            " synchronous",
            " idle",
            " (",
            "SYNC",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.336,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " shr",
            "ub",
            "land",
            " of",
            " Turkish",
            " pine",
            " and",
            " ever",
            "green",
            " scl",
            "er",
            "oph",
            "yll",
            "ous",
            " trees",
            " and",
            " shr",
            "ubs",
            ",",
            " including",
            " Olive",
            " (",
            "O",
            "lea",
            " europ",
            "aea",
            "),",
            " Strawberry",
            " Tree",
            " (",
            "Ar",
            "but",
            "us",
            " un",
            "edo",
            "),",
            " Ar",
            "but",
            "us",
            " and",
            "r",
            "ach",
            "ne",
            ",",
            " K",
            "erm",
            "es",
            " Oak",
            " (",
            "Qu",
            "erc",
            "us",
            " c",
            "occ",
            "if",
            "era",
            "),",
            " and",
            " Bay",
            " Laurel",
            " (",
            "L",
            "aurus"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.277,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            0.174,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " black",
            "),",
            " MC",
            "48",
            " (",
            "dark",
            " grey",
            ",",
            " almost",
            " black",
            "),",
            " MC",
            "36",
            " (",
            "blue",
            "),",
            " MC",
            "24",
            " (",
            "steel",
            " blue",
            "),",
            " and",
            " MC",
            "8",
            " (",
            "bron",
            "ze",
            ")",
            " (",
            "M",
            " =",
            " K",
            ",",
            " R",
            "b",
            ",",
            " or",
            " Cs",
            ").",
            " These",
            " compounds",
            " are",
            " over",
            " ",
            "200",
            " times",
            " more",
            " electric",
            "ally",
            " conduct",
            "ive",
            " than",
            " pure",
            " graphite",
            ",",
            " suggesting",
            " that",
            " the",
            " val",
            "ence",
            " electron",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "prepared",
            " in",
            " a",
            " variety",
            " of",
            " ways",
            "),",
            " mac",
            "aron",
            "i",
            " pie",
            ",",
            " salads",
            ",",
            " and",
            " a",
            " local",
            " beverage",
            ".",
            " Dess",
            "ert",
            " options",
            " include",
            " ice",
            " cream",
            " and",
            " cake",
            ",",
            " apple",
            " pie",
            " (",
            "or",
            " mango",
            " or",
            " pineapple",
            " pie",
            " when",
            " those",
            " fruits",
            " are",
            " in",
            " season",
            "),",
            " gel",
            "atin",
            ",",
            " and",
            " cake",
            ".",
            " The",
            " soft",
            ",",
            " butter",
            "y",
            " loaf",
            " of",
            " bread",
            " known",
            " as",
            " Ant",
            "ig",
            "uan",
            " butter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.214,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.245,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " As",
            "(",
            "III",
            ")",
            " in",
            " seaw",
            "ater",
            " ranging",
            " from",
            " several",
            " months",
            " to",
            " a",
            " year",
            ".",
            " In",
            " other",
            " studies",
            ",",
            " As",
            "(V",
            ")/",
            "As",
            "(",
            "III",
            ")",
            " ratios",
            " were",
            " stable",
            " over",
            " periods",
            " of",
            " days",
            " or",
            " weeks",
            " during",
            " water",
            " sampling",
            " when",
            " no",
            " particular",
            " care",
            " was",
            " taken",
            " to",
            " prevent",
            " oxidation",
            ",",
            " again",
            " suggesting",
            " relatively",
            " slow",
            " oxidation",
            " rates",
            ".",
            " Cherry",
            " found",
            " from",
            " experimental",
            " studies",
            " that",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.197,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " body",
            ".",
            " They",
            " also",
            " believed",
            " that",
            " music",
            " was",
            " delegated",
            " to",
            " the",
            " same",
            " mathematical",
            " laws",
            " of",
            " harmony",
            " as",
            " the",
            " mechanics",
            " of",
            " the",
            " cosmos",
            ",",
            " evolving",
            " into",
            " an",
            " idea",
            " known",
            " as",
            " the",
            " music",
            " of",
            " the",
            " spheres",
            ".",
            "Apollo",
            " appears",
            " as",
            " the",
            " companion",
            " of",
            " the",
            " M",
            "uses",
            ",",
            " and",
            " as",
            " Mus",
            "aget",
            "es",
            " (\"",
            "leader",
            " of",
            " M",
            "uses",
            "\")",
            " he",
            " leads",
            " them",
            " in",
            " dance",
            ".",
            " They"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " odd",
            " atomic",
            " nuclei",
            " History",
            " of",
            " the",
            " periodic",
            " table",
            " List",
            " of",
            " elements",
            " by",
            " atomic",
            " number",
            " Mass",
            " number",
            " Ne",
            "utron",
            " number",
            " Ne",
            "utron",
            "–",
            "pro",
            "ton",
            " ratio",
            " Pr",
            "out",
            "'s",
            " hypothesis",
            "References",
            "Chem",
            "ical",
            " properties",
            "N",
            "uclear",
            " physics",
            "Atoms",
            "Dimension",
            "less",
            " numbers",
            " of",
            " chemistry",
            "Numbers",
            "<|begin_of_text|>",
            "An",
            "atomy",
            " ()",
            " is",
            " the",
            " branch"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " She",
            " did",
            " not",
            " blame",
            " the",
            " incest",
            "uous",
            " relationship",
            " on",
            " Byron",
            ",",
            " but",
            " instead",
            " blamed",
            " Augusta",
            " Leigh",
            ":",
            " \"",
            "I",
            " fear",
            " she",
            " is",
            " more",
            " inherently",
            " wicked",
            " than",
            " he",
            " ever",
            " was",
            ".\"",
            " In",
            " the",
            " ",
            "184",
            "0",
            "s",
            ",",
            " Ada",
            " flirt",
            "ed",
            " with",
            " scandals",
            ":",
            " firstly",
            ",",
            " from",
            " a",
            " relaxed",
            " approach",
            " to",
            " extra",
            "-mar",
            "ital",
            " relationships",
            " with",
            " men",
            ",",
            " leading",
            " to",
            " rumours",
            " of",
            " affairs",
            ";"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "National",
            " Aviation",
            " Hall",
            " of",
            " Fame",
            " in",
            "duct",
            "ees",
            "National",
            " Geographic",
            " Society",
            "Officers",
            " of",
            " the",
            " Legion",
            " of",
            " Honour",
            "People",
            " educated",
            " at",
            " the",
            " Royal",
            " High",
            " School",
            ",",
            " Edinburgh",
            "People",
            " from",
            " Bad",
            "deck",
            ",",
            " Nova",
            " Scotia",
            "People",
            " from",
            " Br",
            "ant",
            "ford",
            "Scientists",
            " from",
            " Edinburgh",
            "Scientists",
            " from",
            " Washington",
            ",",
            " D",
            ".C",
            ".",
            "Scott",
            "ish",
            " ag",
            "nost",
            "ics",
            "Scott"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "icides",
            ",",
            " which",
            " are",
            " more",
            " likely",
            " to",
            " cause",
            " ch",
            "olin",
            "ergic",
            " poisoning",
            ".",
            " Re",
            "pt",
            "ile",
            " exposure",
            " to",
            " an",
            " A",
            "Ch",
            "E",
            " inhib",
            "itory",
            " pesticide",
            " may",
            " result",
            " in",
            " disruption",
            " of",
            " neural",
            " function",
            " in",
            " rept",
            "iles",
            ".",
            " The",
            " buildup",
            " of",
            " these",
            " inhib",
            "itory",
            " effects",
            " on",
            " motor",
            " performance",
            ",",
            " such",
            " as",
            " food",
            " consumption",
            " and",
            " other",
            " activities",
            ".",
            "Cons",
            "ervation",
            " and",
            " protection",
            " strategies",
            " ",
            "The",
            " Amph"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    ".scalablytyped",
    "âĢĮØ§ÙĨØ¨Ø§Ø±",
    ".LookAndFeel",
    "outine",
    "-Javadoc"
  ],
  "bottom_logits": [
    " m",
    "n",
    ",",
    " yours",
    " "
  ],
  "act_min": -0.0,
  "act_max": 0.68
}