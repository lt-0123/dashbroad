{
  "index": 65751,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.228,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            0.119,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " involved",
            ".",
            "Rare",
            "ly",
            " parasites",
            " can",
            " cause",
            " abs",
            "cess",
            "es",
            " and",
            " this",
            " is",
            " more",
            " common",
            " in",
            " the",
            " developing",
            " world",
            ".",
            " Specific",
            " parasites",
            " known",
            " to",
            " do",
            " this",
            " include",
            " dr",
            "ac",
            "unc",
            "ul",
            "ias",
            "is",
            " and",
            " my",
            "ias",
            "is",
            ".",
            "Per",
            "ian",
            "al",
            " abs",
            "cess",
            "S",
            "urgery",
            " of",
            " the",
            " anal",
            " fist",
            "ula",
            " to",
            " drain",
            " an",
            " abs",
            "cess",
            " treats",
            " the",
            " fist",
            "ula",
            " and",
            " reduces",
            " likelihood"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "icus",
            ",",
            " a",
            " type",
            " of",
            " har",
            "vester",
            " term",
            "ite",
            " active",
            " in",
            " the",
            " afternoon",
            ",",
            " which",
            " explains",
            " some",
            " of",
            " their",
            " di",
            "urnal",
            " behavior",
            " in",
            " the",
            " winter",
            ".",
            " The",
            " eastern",
            " a",
            "ard",
            "wolf",
            ",",
            " during",
            " the",
            " rainy",
            " season",
            ",",
            " subs",
            "ists",
            " on",
            " ter",
            "mites",
            " from",
            " the",
            " genera",
            " Od",
            "ont",
            "ot",
            "erm",
            "es",
            " and",
            " Macro",
            "term",
            "es",
            ".",
            " They",
            " are",
            " also",
            " known",
            " to",
            " feed",
            " on",
            " other"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " until",
            " the",
            " ",
            "6",
            "th",
            " century",
            " CE",
            ",",
            " C",
            "app",
            "ad",
            "oc",
            "ian",
            " in",
            " the",
            " hom",
            "onymous",
            " region",
            ",",
            " Armenian",
            " in",
            " the",
            " east",
            ",",
            " and",
            " Kart",
            "vel",
            "ian",
            " languages",
            " in",
            " the",
            " northeast",
            ".",
            "An",
            "at",
            "olia",
            " is",
            " known",
            " as",
            " the",
            " birth",
            "place",
            " of",
            " mint",
            "ed",
            " coin",
            "age",
            " (",
            "as",
            " opposed",
            " to",
            " unm",
            "int",
            "ed",
            " coin",
            "age",
            ",",
            " which",
            " first",
            " appears",
            " in",
            " Mes"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " has",
            " been",
            " categor",
            "ised",
            " by",
            " the",
            " US",
            " government",
            " funded",
            " Freedom",
            " House",
            " as",
            " \"",
            "not",
            " free",
            "\"",
            " since",
            " it",
            " began",
            " publishing",
            " such",
            " ratings",
            " in",
            " ",
            "197",
            "2",
            ",",
            " with",
            " the",
            " exception",
            " of",
            " ",
            "198",
            "9",
            ",",
            " ",
            "199",
            "0",
            ",",
            " and",
            " ",
            "199",
            "1",
            ",",
            " when",
            " the",
            " country",
            " was",
            " labelled",
            " \"",
            "part",
            "ly",
            " free",
            ".\"",
            " In",
            " December",
            " ",
            "201",
            "6",
            ",",
            " the",
            " Euro"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " and",
            " other",
            " neurological",
            " disorders",
            ".",
            "Radi",
            "ation",
            " poisoning",
            " ",
            "At",
            "ax",
            "ia",
            " can",
            " be",
            " induced",
            " as",
            " a",
            " result",
            " of",
            " severe",
            " acute",
            " radiation",
            " poisoning",
            " with",
            " an",
            " absorbed",
            " dose",
            " of",
            " more",
            " than",
            " ",
            "30",
            " gr",
            "ays",
            ".",
            "V",
            "itamin",
            " B",
            "12",
            " deficiency",
            " ",
            "V",
            "itamin",
            " B",
            "12",
            " deficiency",
            " may",
            " cause",
            ",",
            " among",
            " several",
            " neurological",
            " abnormalities",
            ",",
            " overlapping",
            " cere",
            "bell",
            "ar",
            " and",
            " sensory",
            " at",
            "ax"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " has",
            " invested",
            " an",
            " estimated",
            " ",
            "100",
            " billion",
            " din",
            "ars",
            " towards",
            " developing",
            " research",
            " facilities",
            " and",
            " paying",
            " researchers",
            ".",
            " This",
            " development",
            " program",
            " is",
            " meant",
            " to",
            " advance",
            " alternative",
            " energy",
            " production",
            ",",
            " especially",
            " solar",
            " and",
            " wind",
            " power",
            ".",
            " Algeria",
            " is",
            " estimated",
            " to",
            " have",
            " the",
            " largest",
            " solar",
            " energy",
            " potential",
            " in",
            " the",
            " Mediterranean",
            ",",
            " so",
            " the",
            " government",
            " has",
            " funded",
            " the",
            " creation",
            " of",
            " a",
            " solar",
            " science",
            " park",
            " in",
            " Hass"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " on",
            " July",
            " ",
            "4",
            " and",
            " then",
            " secured",
            " V",
            "inc",
            "ennes",
            ",",
            " though",
            " V",
            "inc",
            "ennes",
            " was",
            " rec",
            "aptured",
            " by",
            " Quebec",
            " Governor",
            " Henry",
            " Hamilton",
            ".",
            " In",
            " early",
            " ",
            "177",
            "9",
            ",",
            " the",
            " Virgin",
            "ians",
            " counter",
            "-",
            "att",
            "acked",
            " in",
            " the",
            " siege",
            " of",
            " Fort",
            " V",
            "inc",
            "ennes",
            " and",
            " took",
            " Hamilton",
            " prisoner",
            ".",
            " Clark",
            " secured",
            " western",
            " British",
            " Quebec",
            " as",
            " the",
            " American",
            " Northwest",
            " Territory",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            ",",
            " Anglo",
            "-S",
            "axon",
            " England",
            ",",
            " Medieval",
            " Iceland",
            ",",
            " the",
            " American",
            " Old",
            " West",
            ",",
            " Gael",
            "ic",
            " Ireland",
            ",",
            " and",
            " merchant",
            " law",
            ",",
            " admir",
            "alty",
            " law",
            ",",
            " and",
            " early",
            " common",
            " law",
            ".",
            "An",
            "ar",
            "cho",
            "-capital",
            "ism",
            " is",
            " distinguished",
            " from",
            " min",
            "arch",
            "ism",
            ",",
            " which",
            " advocates",
            " a",
            " night",
            "-watch",
            "man",
            " state",
            " limited",
            " to",
            " protecting",
            " individuals",
            " from",
            " aggression",
            " and",
            " enforcing",
            " private",
            " property",
            ".",
            " Unlike"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " de",
            " Sant",
            " Este",
            "ve",
            ",",
            " Sant",
            " Joan",
            " de",
            " Cas",
            "elles",
            ",",
            " E",
            "sg",
            "l",
            "Ã©s",
            "ia",
            " de",
            " Sant",
            " M",
            "iqu",
            "el",
            " d",
            "'",
            "Eng",
            "ol",
            "asters",
            ",",
            " Sant",
            " Mart",
            "ÃŃ",
            " de",
            " la",
            " Cort",
            "in",
            "ada",
            " and",
            " the",
            " medieval",
            " bridges",
            " of",
            " Marg",
            "ined",
            "a",
            " and",
            " Esc",
            "alls",
            " among",
            " many",
            " others",
            ".",
            "The",
            " Catalan",
            " Py",
            "rene",
            "es",
            " were",
            " embry",
            "onic",
            " of",
            " the",
            " Catalan",
            " language"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            ".",
            " Neu",
            "ropsych",
            "ological",
            " symptoms",
            " may",
            " include",
            " sense",
            " loss",
            ",",
            " difficulty",
            " in",
            " proprio",
            "ception",
            ",",
            " poor",
            " balance",
            ",",
            " loss",
            " of",
            " sensation",
            " in",
            " the",
            " feet",
            ",",
            " changes",
            " in",
            " reflex",
            "es",
            ",",
            " dementia",
            ",",
            " and",
            " psychosis",
            ",",
            " can",
            " be",
            " reversible",
            " with",
            " treatment",
            ".",
            " Comp",
            "lications",
            " may",
            " include",
            " a",
            " neurological",
            " complex",
            " known",
            " as",
            " sub",
            "acute",
            " combined",
            " deg",
            "eneration",
            " of",
            " spinal",
            " cord",
            ",",
            " and",
            " other",
            " neurological"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.101,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " between",
            " ",
            "13",
            ",",
            "000",
            " and",
            " ",
            "11",
            ",",
            "000",
            " years",
            " ago",
            ".",
            " C",
            "attle",
            " were",
            " domestic",
            "ated",
            " from",
            " the",
            " wild",
            " a",
            "uro",
            "chs",
            " in",
            " the",
            " areas",
            " of",
            " modern",
            " Turkey",
            " and",
            " Pakistan",
            " some",
            " ",
            "10",
            ",",
            "500",
            " years",
            " ago",
            ".",
            " Pig",
            " production",
            " emerged",
            " in",
            " Euras",
            "ia",
            ",",
            " including",
            " Europe",
            ",",
            " East",
            " Asia",
            " and",
            " Southwest",
            " Asia",
            ",",
            " where",
            " wild",
            " bo",
            "ar",
            " were",
            " first"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.152,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "ically",
            " inert",
            " gas",
            ".",
            " Arg",
            "on",
            " is",
            " the",
            " cheapest",
            " alternative",
            " when",
            " nitrogen",
            " is",
            " not",
            " sufficiently",
            " inert",
            ".",
            " Arg",
            "on",
            " has",
            " low",
            " thermal",
            " conductivity",
            ".",
            " Arg",
            "on",
            " has",
            " electronic",
            " properties",
            " (",
            "ion",
            "ization",
            " and",
            "/or",
            " the",
            " emission",
            " spectrum",
            ")",
            " desirable",
            " for",
            " some",
            " applications",
            ".",
            "Other",
            " noble",
            " gases",
            " would",
            " be",
            " equally",
            " suitable",
            " for",
            " most",
            " of",
            " these",
            " applications",
            ",",
            " but",
            " arg",
            "on",
            " is",
            " by",
            " far",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.152,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "ically",
            " inert",
            " gas",
            ".",
            " Arg",
            "on",
            " is",
            " the",
            " cheapest",
            " alternative",
            " when",
            " nitrogen",
            " is",
            " not",
            " sufficiently",
            " inert",
            ".",
            " Arg",
            "on",
            " has",
            " low",
            " thermal",
            " conductivity",
            ".",
            " Arg",
            "on",
            " has",
            " electronic",
            " properties",
            " (",
            "ion",
            "ization",
            " and",
            "/or",
            " the",
            " emission",
            " spectrum",
            ")",
            " desirable",
            " for",
            " some",
            " applications",
            ".",
            "Other",
            " noble",
            " gases",
            " would",
            " be",
            " equally",
            " suitable",
            " for",
            " most",
            " of",
            " these",
            " applications",
            ",",
            " but",
            " arg",
            "on",
            " is",
            " by",
            " far",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            " Hor",
            "vat",
            ",",
            " Croatian",
            " football",
            "er",
            " and",
            " manager",
            " (",
            "b",
            ".",
            " ",
            "192",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " Richard",
            " Kings",
            "land",
            ",",
            " Australian",
            " captain",
            " and",
            " pilot",
            " (",
            "b",
            ".",
            " ",
            "191",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " G",
            "eli",
            "y",
            " Kor",
            "z",
            "hev",
            ",",
            " Russian",
            " painter",
            " (",
            "b",
            ".",
            " ",
            "192",
            "5",
            ")",
            " ",
            "201",
            "3",
            " –"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.041,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            ",",
            " where",
            " they",
            " are",
            " still",
            " one",
            " of",
            " the",
            " essential",
            " elements",
            " of",
            " the",
            " settlement",
            " but",
            " most",
            " arrived",
            " in",
            " If",
            "ri",
            "qi",
            "ya",
            " by",
            " the",
            " Gab",
            "es",
            " region",
            ",",
            " arriving",
            " ",
            "105",
            "1",
            ".",
            " The",
            " Z",
            "ir",
            "id",
            " ruler",
            " tried",
            " to",
            " stop",
            " this",
            " rising",
            " tide",
            ",",
            " but",
            " with",
            " each",
            " encounter",
            ",",
            " the",
            " last",
            " under",
            " the",
            " walls",
            " of",
            " K",
            "air",
            "ou",
            "an",
            ",",
            " his",
            " troops"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            "Journal",
            " of",
            " Algorithms",
            " (",
            "Else",
            "vier",
            ")",
            "References",
            "External",
            " links",
            "Computer",
            " science",
            " journals",
            "Open",
            " access",
            " journals",
            "MD",
            "PI",
            " academic",
            " journals",
            "English",
            "-language",
            " journals",
            "Ac",
            "ademic",
            " journals",
            " established",
            " in",
            " ",
            "200",
            "8",
            "Math",
            "ematics",
            " journals",
            "Monthly",
            " journals",
            "<|begin_of_text|>",
            "A",
            "zerbai",
            "jan",
            " (",
            ",",
            " ;",
            " ,",
            " ),",
            " officially",
            " the",
            " Republic",
            " of",
            " Azerbaijan",
            ",",
            " is",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ion",
            ",",
            " and",
            " if",
            " they",
            " are",
            " seen",
            " eating",
            " while",
            " h",
            "unched",
            " over",
            " a",
            " dead",
            " carc",
            "ass",
            ",",
            " they",
            " are",
            " actually",
            " eating",
            " larvae",
            " and",
            " beet",
            "les",
            ".",
            " Also",
            ",",
            " contrary",
            " to",
            " some",
            " sources",
            ",",
            " they",
            " do",
            " not",
            " like",
            " meat",
            ",",
            " unless",
            " it",
            " is",
            " finely",
            " ground",
            " or",
            " cooked",
            " for",
            " them",
            ".",
            " The",
            " adult",
            " a",
            "ard",
            "wolf",
            " was",
            " formerly",
            " assumed",
            " to",
            " for",
            "age",
            " in",
            " small",
            " groups"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ion",
            ",",
            " at",
            " the",
            " sight",
            " of",
            " his",
            " dead",
            " sons",
            ",",
            " either",
            " killed",
            " himself",
            " or",
            " was",
            " killed",
            " by",
            " Apollo",
            " after",
            " swearing",
            " revenge",
            ".",
            "A",
            " devastated",
            " Ni",
            "obe",
            " fled",
            " to",
            " Mount",
            " S",
            "ipy",
            "los",
            " in",
            " Asia",
            " Minor",
            " and",
            " turned",
            " into",
            " stone",
            " as",
            " she",
            " we",
            "pt",
            ".",
            " Her",
            " tears",
            " formed",
            " the",
            " river",
            " Ach",
            "el",
            "ous",
            ".",
            " Zeus",
            " had",
            " turned",
            " all",
            " the",
            " people",
            " of",
            " The",
            "bes",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ions",
            " during",
            " ",
            "186",
            "3",
            " came",
            " in",
            " the",
            " thousands",
            " and",
            " only",
            " increased",
            " after",
            " Freder",
            "icks",
            "burg",
            ",",
            " so",
            " Lincoln",
            " replaced",
            " Burn",
            "side",
            " with",
            " Joseph",
            " Hook",
            "er",
            ".",
            "In",
            " the",
            " ",
            "186",
            "2",
            " midterm",
            " elections",
            " the",
            " Republicans",
            " suffered",
            " severe",
            " losses",
            " due",
            " to",
            " rising",
            " inflation",
            ",",
            " high",
            " taxes",
            ",",
            " rumors",
            " of",
            " corruption",
            ",",
            " suspension",
            " of",
            " habe",
            "as",
            " corpus",
            ",",
            " military",
            " draft",
            " law",
            ",",
            " and",
            " fears"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            "Examples",
            " using",
            " the",
            " Dev",
            "an",
            "ag",
            "ari",
            " script",
            " K",
            " =",
            " /",
            "ka",
            "/",
            " =",
            " ",
            " Ki",
            " =",
            " /",
            "ki",
            "/",
            " =",
            " ",
            " K",
            "*",
            " =",
            " /",
            "k",
            "/",
            " =",
            " ",
            " (",
            "with",
            " a",
            " Hal",
            "ant",
            " ",
            " under",
            " the",
            " character",
            ")",
            " K",
            "*M",
            " =",
            " /",
            "k",
            "ma",
            "/",
            " =",
            " ",
            " Ä°",
            "K",
            " =",
            " /",
            "ika",
            "/",
            " =",
            " ",
            " Ä°",
            "K",
            "*",
            " =",
            " /"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ÃŃa",
            ",",
            " tur",
            "ismo",
            " y",
            " de",
            " via",
            "jes",
            " History",
            " of",
            " And",
            "orra",
            ":",
            " Primary",
            " Documents",
            " from",
            " Euro",
            "Docs",
            " A",
            " New",
            " Path",
            " for",
            " And",
            "orra",
            " –",
            " slideshow",
            " by",
            " The",
            " New",
            " York",
            " Times",
            " ",
            " ",
            " ",
            "127",
            "8",
            " establishments",
            " in",
            " Europe",
            "C",
            "atal",
            "an",
            " Countries",
            "Christian",
            " states",
            "Countries",
            " in",
            " Europe",
            "Di",
            "arch",
            "ies",
            "D",
            "uty",
            "-free",
            " zones",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            " in",
            " weak",
            " acidic",
            " solutions",
            ":",
            " Am",
            "^",
            "3",
            "+",
            " +",
            " ",
            "3",
            "F",
            "^-",
            " ->",
            " Am",
            "F",
            "3",
            "(v",
            ")",
            "The",
            " t",
            "etr",
            "aval",
            "ent",
            " americ",
            "ium",
            "(",
            "IV",
            ")",
            " fluoride",
            " (",
            "Am",
            "F",
            "4",
            ")",
            " is",
            " obtained",
            " by",
            " reacting",
            " solid",
            " americ",
            "ium",
            "(",
            "III",
            ")",
            " fluoride",
            " with",
            " molecular",
            " fluor",
            "ine",
            ":",
            " ",
            "2",
            "Am",
            "F",
            "3",
            " +",
            " F",
            "2",
            " ->",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            ".",
            " It",
            " was",
            " sometimes",
            " considered",
            " an",
            " alk",
            "ali",
            " metal",
            " in",
            " continental",
            " Europe",
            " (",
            "but",
            " not",
            " in",
            " England",
            ")",
            " in",
            " the",
            " years",
            " immediately",
            " following",
            " its",
            " discovery",
            ",",
            " and",
            " was",
            " placed",
            " just",
            " after",
            " ca",
            "esium",
            " as",
            " the",
            " sixth",
            " alk",
            "ali",
            " metal",
            " in",
            " Dmit",
            "ri",
            " Mend",
            "ele",
            "ev",
            "'s",
            " ",
            "186",
            "9",
            " periodic",
            " table",
            " and",
            " Julius",
            " Lo",
            "th",
            "ar",
            " Meyer",
            "'s",
            " ",
            "186",
            "8",
            " periodic"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            " are",
            " acids",
            " according",
            " to",
            " all",
            " three",
            " definitions",
            ".",
            " Although",
            " al",
            "coh",
            "ols",
            " and",
            " a",
            "min",
            "es",
            " can",
            " be",
            " Br",
            "Ã¸",
            "n",
            "sted",
            "–",
            "Low",
            "ry",
            " acids",
            ",",
            " they",
            " can",
            " also",
            " function",
            " as",
            " Lewis",
            " bases",
            " due",
            " to",
            " the",
            " lone",
            " pairs",
            " of",
            " electrons",
            " on",
            " their",
            " oxygen",
            " and",
            " nitrogen",
            " atoms",
            ".",
            "Arr",
            "hen",
            "ius",
            " acids",
            "In",
            " ",
            "188",
            "4",
            ",",
            " Sv",
            "ante",
            " Arr",
            "hen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            ",",
            " so",
            " that",
            " less",
            " lattice",
            " energy",
            " needs",
            " to",
            " be",
            " released",
            " for",
            " the",
            " salts",
            " to",
            " form",
            ".",
            " These",
            " are",
            " not",
            " the",
            " only",
            " phosph",
            "ides",
            " and",
            " arsen",
            "ides",
            " of",
            " the",
            " alk",
            "ali",
            " metals",
            ":",
            " for",
            " example",
            ",",
            " potassium",
            " has",
            " nine",
            " different",
            " known",
            " phosph",
            "ides",
            ",",
            " with",
            " formula",
            "e",
            " K",
            "3",
            "P",
            ",",
            " K",
            "4",
            "P",
            "3",
            ",",
            " K",
            "5",
            "P",
            "4",
            ",",
            " KP",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            " in",
            " weak",
            " acidic",
            " solutions",
            ":",
            " Am",
            "^",
            "3",
            "+",
            " +",
            " ",
            "3",
            "F",
            "^-",
            " ->",
            " Am",
            "F",
            "3",
            "(v",
            ")",
            "The",
            " t",
            "etr",
            "aval",
            "ent",
            " americ",
            "ium",
            "(",
            "IV",
            ")",
            " fluoride",
            " (",
            "Am",
            "F",
            "4",
            ")",
            " is",
            " obtained",
            " by",
            " reacting",
            " solid",
            " americ",
            "ium",
            "(",
            "III",
            ")",
            " fluoride",
            " with",
            " molecular",
            " fluor",
            "ine",
            ":",
            " ",
            "2",
            "Am",
            "F",
            "3",
            " +",
            " F",
            "2",
            " ->",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ions",
            " are",
            " acids",
            " according",
            " to",
            " all",
            " three",
            " definitions",
            ".",
            " Although",
            " al",
            "coh",
            "ols",
            " and",
            " a",
            "min",
            "es",
            " can",
            " be",
            " Br",
            "Ã¸",
            "n",
            "sted",
            "–",
            "Low",
            "ry",
            " acids",
            ",",
            " they",
            " can",
            " also",
            " function",
            " as",
            " Lewis",
            " bases",
            " due",
            " to",
            " the",
            " lone",
            " pairs",
            " of",
            " electrons",
            " on",
            " their",
            " oxygen",
            " and",
            " nitrogen",
            " atoms",
            ".",
            "Arr",
            "hen",
            "ius",
            " acids",
            "In",
            " ",
            "188",
            "4",
            ",",
            " Sv",
            "ante",
            " Arr",
            "hen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ene",
            ",",
            " in",
            " accordance",
            " with",
            " the",
            " ",
            "18",
            "-elect",
            "ron",
            " rule",
            ".",
            " This",
            " additional",
            " electron",
            " occupies",
            " an",
            " orbital",
            " that",
            " is",
            " antib",
            "ond",
            "ing",
            " with",
            " respect",
            " to",
            " the",
            " Co",
            "–",
            "C",
            " bonds",
            ".",
            " Consequently",
            ",",
            " many",
            " chemical",
            " reactions",
            " of",
            " Co",
            "(C",
            "5",
            "H",
            "5",
            ")",
            "2",
            " are",
            " characterized",
            " by",
            " its",
            " tendency",
            " to",
            " lose",
            " this",
            " \"",
            "extra",
            "\"",
            " electron",
            ",",
            " yielding",
            " a",
            " very",
            " stable",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " ch",
            "alc",
            "ogen",
            " atoms",
            " in",
            " question",
            ".",
            " For",
            " example",
            ",",
            " sodium",
            " can",
            " react",
            " with",
            " sulfur",
            " to",
            " form",
            " the",
            " sulf",
            "ide",
            " (",
            "Na",
            "2",
            "S",
            ")",
            " and",
            " various",
            " polys",
            "ulf",
            "ides",
            " with",
            " the",
            " formula",
            " Na",
            "2",
            "S",
            "x",
            " (",
            "x",
            " from",
            " ",
            "2",
            " to",
            " ",
            "6",
            "),",
            " containing",
            " the",
            " ",
            " ions",
            ".",
            " Due",
            " to",
            " the",
            " basic",
            "ity",
            " of",
            " the",
            " Se",
            "2",
            "âĪĴ",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ien",
            " Shan",
            " mountains",
            ".",
            "192",
            "2",
            " –",
            " Michael",
            " Collins",
            ",",
            " Commander",
            "-in",
            "-chief",
            " of",
            " the",
            " Irish",
            " Free",
            " State",
            " Army",
            ",",
            " is",
            " shot",
            " dead",
            " in",
            " an",
            " ambush",
            " during",
            " the",
            " Irish",
            " Civil",
            " War",
            ".",
            "193",
            "4",
            " –",
            " Bill",
            " Wood",
            "full",
            " of",
            " Australia",
            " becomes",
            " the",
            " only",
            " test",
            " cricket",
            " captain",
            " to",
            " twice",
            " regain",
            " The",
            " Ash",
            "es",
            ".",
            "194",
            "1",
            " –",
            " World",
            " War",
            " II",
            ":",
            " German",
            " troops",
            " begin"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " these",
            " are",
            " the",
            " same",
            " as",
            " the",
            " ASCII",
            " set",
            ".",
            "The",
            " Internet",
            " Assigned",
            " Numbers",
            " Authority",
            " (",
            "IAN",
            "A",
            ")",
            " prefers",
            " the",
            " name",
            " US",
            "-",
            "ASCII",
            " for",
            " this",
            " character",
            " encoding",
            ".",
            " ",
            "ASCII",
            " is",
            " one",
            " of",
            " the",
            " IEEE",
            " milestones",
            ".",
            "Overview",
            "ASCII",
            " was",
            " developed",
            " from",
            " tele",
            "graph",
            " code",
            ".",
            " Its",
            " first",
            " commercial",
            " use",
            " was",
            " in",
            " the",
            " Te",
            "let",
            "ype",
            " Model",
            " ",
            "33",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " these",
            " are",
            " the",
            " same",
            " as",
            " the",
            " ASCII",
            " set",
            ".",
            "The",
            " Internet",
            " Assigned",
            " Numbers",
            " Authority",
            " (",
            "IAN",
            "A",
            ")",
            " prefers",
            " the",
            " name",
            " US",
            "-",
            "ASCII",
            " for",
            " this",
            " character",
            " encoding",
            ".",
            " ",
            "ASCII",
            " is",
            " one",
            " of",
            " the",
            " IEEE",
            " milestones",
            ".",
            "Overview",
            "ASCII",
            " was",
            " developed",
            " from",
            " tele",
            "graph",
            " code",
            ".",
            " Its",
            " first",
            " commercial",
            " use",
            " was",
            " in",
            " the",
            " Te",
            "let",
            "ype",
            " Model",
            " ",
            "33",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "ionic",
            " bonding",
            " gives",
            " way",
            " to",
            " metallic",
            " bonding",
            " along",
            " the",
            " series",
            " Na",
            "Cl",
            ",",
            " Na",
            "2",
            "O",
            ",",
            " Na",
            "2",
            "S",
            ",",
            " Na",
            "3",
            "P",
            ",",
            " Na",
            "3",
            "As",
            ",",
            " Na",
            "3",
            "S",
            "b",
            ",",
            " Na",
            "3",
            "Bi",
            ",",
            " Na",
            ".",
            "Hy",
            "dro",
            "x",
            "ides",
            " ",
            "All",
            " the",
            " alk",
            "ali",
            " metals",
            " react",
            " vigorously",
            " or",
            " explos",
            "ively",
            " with",
            " cold",
            " water",
            ",",
            " producing",
            " an",
            " aque"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.357,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ienne",
            " valley",
            " in",
            " Sav",
            "oy",
            ",",
            " where",
            " the",
            " amount",
            " of",
            " snow",
            " during",
            " the",
            " cold",
            " months",
            " is",
            " important",
            ".",
            " The",
            " inclination",
            " of",
            " the",
            " roof",
            " cannot",
            " exceed",
            " ",
            "40",
            "%,",
            " allowing",
            " the",
            " snow",
            " to",
            " stay",
            " on",
            " top",
            ",",
            " thereby",
            " functioning",
            " as",
            " insulation",
            " from",
            " the",
            " cold",
            ".",
            " In",
            " the",
            " lower",
            " areas",
            " where",
            " the",
            " forests",
            " are",
            " widespread",
            ",",
            " wooden",
            " tiles",
            " are",
            " traditionally",
            " used",
            ".",
            " Common",
            "ly",
            " made"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.206,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " form",
            " ",
            "ionic",
            " alk",
            "ali",
            " metal",
            " hy",
            "d",
            "rides",
            ",",
            " where",
            " the",
            " hy",
            "d",
            "ride",
            " an",
            "ion",
            " acts",
            " as",
            " a",
            " pseud",
            "oh",
            "al",
            "ide",
            ":",
            " these",
            " are",
            " often",
            " used",
            " as",
            " reducing",
            " agents",
            ",",
            " producing",
            " hy",
            "d",
            "rides",
            ",",
            " complex",
            " metal",
            " hy",
            "d",
            "rides",
            ",",
            " or",
            " hydrogen",
            " gas",
            ".",
            " Other",
            " pseud",
            "oh",
            "al",
            "ides",
            " are",
            " also",
            " known",
            ",",
            " notably",
            " the",
            " cyan",
            "ides",
            ".",
            " These"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " all",
            " the",
            " energy",
            " released",
            " from",
            " the",
            " formation",
            " of",
            " an",
            " alk",
            "ali",
            " metal",
            " nit",
            "ride",
            " is",
            " from",
            " the",
            " lattice",
            " energy",
            " of",
            " the",
            " alk",
            "ali",
            " metal",
            " nit",
            "ride",
            ".",
            " The",
            " lattice",
            " energy",
            " is",
            " maxim",
            "ised",
            " with",
            " small",
            ",",
            " highly",
            " charged",
            " ions",
            ";",
            " the",
            " alk",
            "ali",
            " metals",
            " do",
            " not",
            " form",
            " highly",
            " charged",
            " ions",
            ",",
            " only",
            " forming",
            " ions",
            " with",
            " a",
            " charge",
            " of",
            " +",
            "1",
            ",",
            " so"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.318,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " Group",
            " A",
            "ard",
            "wolf",
            " pages",
            " on",
            " hy",
            "a",
            "en",
            "idae",
            ".org",
            " Cam",
            " footage",
            " from",
            " the",
            " Nam",
            "ib",
            " desert",
            " https",
            "://",
            "m",
            ".youtube",
            ".com",
            "/watch",
            "?v",
            "=l",
            "Rev",
            "q",
            "S",
            "6",
            "Px",
            "gg",
            " ",
            " ",
            "M",
            "amm",
            "als",
            " described",
            " in",
            " ",
            "178",
            "3",
            "C",
            "arn",
            "ivor",
            "ans",
            " of",
            " Africa",
            "Hy",
            "enas",
            "M",
            "amm",
            "als",
            " of",
            " Southern",
            " Africa",
            "Fa",
            "una"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " A",
            "arhus",
            " with",
            " historic",
            " racing",
            " cars",
            ",",
            " all",
            " attracting",
            " thousands",
            " of",
            " people",
            ".",
            " Mars",
            "el",
            "is",
            "borg",
            " Deer",
            " Park",
            " (",
            "M",
            "ars",
            "el",
            "is",
            "borg",
            " Dy",
            "reh",
            "ave",
            ")",
            " in",
            " Mars",
            "el",
            "is",
            "borg",
            " Forest",
            "s",
            ",",
            " comprises",
            " ",
            " of",
            " fenced",
            " woodland",
            " past",
            "ures",
            " with",
            " free",
            "-ro",
            "aming",
            " s",
            "ika",
            " and",
            " ro",
            "e",
            " deer",
            ".",
            " Below",
            " the",
            " Mo",
            "es",
            "g",
            "Ã¥",
            "rd",
            " Museum"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.24,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " stemmed",
            " directly",
            " from",
            " their",
            " environment",
            ".",
            " Living",
            " and",
            " working",
            " close",
            " to",
            " the",
            " Nile",
            " brought",
            " hazards",
            " from",
            " malaria",
            " and",
            " debilitating",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            " parasites",
            ",",
            " which",
            " caused",
            " liver",
            " and",
            " intestinal",
            " damage",
            ".",
            " Dangerous",
            " wildlife",
            " such",
            " as",
            " cro",
            "cod",
            "iles",
            " and",
            " hip",
            "pos",
            " were",
            " also",
            " a",
            " common",
            " threat",
            ".",
            " The",
            " lifelong",
            " lab",
            "ors",
            " of",
            " farming",
            " and",
            " building",
            " put",
            " stress",
            " on",
            " the",
            " spine"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.069,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " longer",
            " processing",
            " time",
            " which",
            " is",
            " particularly",
            " advantageous",
            " for",
            " fine",
            " aggregates",
            " (",
            "1",
            ").",
            "Ad",
            "hesion",
            " problems",
            " are",
            " reported",
            " for",
            " an",
            "ionic",
            " em",
            "uls",
            "ions",
            " in",
            " contact",
            " with",
            " quartz",
            "-rich",
            " aggregates",
            ".",
            " They",
            " are",
            " substituted",
            " by",
            " c",
            "ation",
            "ic",
            " em",
            "uls",
            "ions",
            " achieving",
            " better",
            " ad",
            "hesion",
            ".",
            " The",
            " extensive",
            " range",
            " of",
            " bit",
            "umen",
            " em",
            "uls",
            "ions",
            " is",
            " covered",
            " insufficient",
            "ly",
            " by",
            " standard",
            "ization",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.171,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " trick",
            "ed",
            " E",
            "ile",
            "ith",
            "y",
            "ia",
            ",",
            " the",
            " goddess",
            " of",
            " childbirth",
            ",",
            " to",
            " stay",
            " on",
            " Olympus",
            ",",
            " due",
            " to",
            " which",
            " Let",
            "o",
            " was",
            " unable",
            " to",
            " give",
            " birth",
            ".",
            " The",
            " goddess",
            "es",
            " then",
            " convinced",
            " Iris",
            " to",
            " go",
            " bring",
            " E",
            "ile",
            "ith",
            "y",
            "ia",
            " by",
            " offering",
            " her",
            " a",
            " necklace",
            " of",
            " amber",
            " ",
            "9",
            " yards",
            " (",
            "8",
            ".",
            "2",
            " m",
            ")",
            " long",
            ".",
            " Iris",
            " did"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " most",
            " common",
            " form",
            " of",
            " trans",
            "missible",
            " s",
            "pong",
            "iform",
            " en",
            "ceph",
            "al",
            "opath",
            "ies",
            " caused",
            " by",
            " pr",
            "ions",
            ".",
            " E",
            "pon",
            "ym",
            " introduced",
            " by",
            " Wal",
            "ther",
            " Spi",
            "elm",
            "eyer",
            " in",
            " ",
            "192",
            "2",
            ".",
            "B",
            "ibli",
            "ography",
            " Die",
            " extr",
            "apy",
            "ram",
            "idal",
            "en",
            " Er",
            "kr",
            "ank",
            "ungen",
            ".",
            " In",
            ":",
            " Mon",
            "ograph",
            "ien",
            " aus",
            " dem",
            " Ges",
            "amt",
            "geb",
            "iete",
            " der",
            " Neuro",
            "log"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " metals",
            " increase",
            " going",
            " down",
            " the",
            " group",
            ".",
            " This",
            " is",
            " the",
            " result",
            " of",
            " a",
            " combination",
            " of",
            " two",
            " factors",
            ":",
            " the",
            " first",
            " ion",
            "isation",
            " energies",
            " and",
            " atom",
            "isation",
            " energies",
            " of",
            " the",
            " alk",
            "ali",
            " metals",
            ".",
            " Because",
            " the",
            " first",
            " ion",
            "isation",
            " energy",
            " of",
            " the",
            " alk",
            "ali",
            " metals",
            " decreases",
            " down",
            " the",
            " group",
            ",",
            " it",
            " is",
            " easier",
            " for",
            " the",
            " outer",
            "most",
            " electron",
            " to",
            " be",
            " removed",
            " from",
            " the",
            " atom"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.138,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " thrown",
            ",",
            " or",
            " ch",
            "ucked",
            " from",
            " off",
            " camera",
            " or",
            " w",
            "igg",
            "led",
            " around",
            " to",
            " simulate",
            " talking",
            " by",
            " unseen",
            " hands",
            ".",
            " The",
            " magic",
            " lantern",
            " used",
            " mechanical",
            " slides",
            " to",
            " project",
            " moving",
            " images",
            ",",
            " probably",
            " since",
            " Christ",
            "ia",
            "an",
            " H",
            "uy",
            "gens",
            " invented",
            " this",
            " early",
            " image",
            " projector",
            " in",
            " ",
            "165",
            "9",
            ".",
            "Other",
            " ",
            " Hy",
            "d",
            "rote",
            "chn",
            "ics",
            ":",
            " a",
            " technique",
            " that",
            " includes",
            " lights",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " indigenous",
            " Greeks",
            ",",
            " Il",
            "ly",
            "rians",
            " and",
            " Th",
            "rac",
            "ians",
            " in",
            " the",
            " Balk",
            "ans",
            ";",
            " thus",
            ",",
            " the",
            " Il",
            "ly",
            "rians",
            " were",
            " mentioned",
            " for",
            " the",
            " last",
            " time",
            " in",
            " historical",
            " records",
            " in",
            " the",
            " ",
            "7",
            "th",
            " century",
            ".",
            "In",
            " the",
            " ",
            "11",
            "th",
            " century",
            ",",
            " the",
            " Great",
            " Sch",
            "ism",
            " formal",
            "ised",
            " the",
            " break",
            " of",
            " communion",
            " between",
            " the",
            " Eastern",
            " Orthodox",
            " and",
            " Western",
            " Catholic",
            " Church"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " journalist",
            ",",
            " from",
            " ",
            "186",
            "7",
            ",",
            " he",
            " wrote",
            " many",
            " articles",
            " and",
            " notices",
            ".",
            " He",
            " became",
            " known",
            " with",
            " the",
            " novel",
            " ",
            " (",
            "188",
            "1",
            ").",
            " Its",
            " protagonist",
            ",",
            " skeptical",
            " old",
            " scholar",
            " Sylv",
            "ester",
            " Bon",
            "nard",
            ",",
            " embodied",
            " France",
            "'s",
            " own",
            " personality",
            ".",
            " The",
            " novel",
            " was",
            " praised",
            " for",
            " its",
            " elegant",
            " prose",
            " and",
            " won",
            " him",
            " a",
            " prize",
            " from",
            " the",
            " Acad",
            "Ã©m",
            "ie",
            " FranÃ§",
            "aise"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " young",
            " Jewish",
            " girl",
            " Anne",
            " Frank",
            ",",
            " who",
            " died",
            " in",
            " the",
            " Bergen",
            "-B",
            "elsen",
            " concentration",
            " camp",
            ".",
            " At",
            " the",
            " end",
            " of",
            " the",
            " Second",
            " World",
            " War",
            ",",
            " communication",
            " with",
            " the",
            " rest",
            " of",
            " the",
            " country",
            " broke",
            " down",
            ",",
            " and",
            " food",
            " and",
            " fuel",
            " became",
            " scarce",
            ".",
            " Many",
            " citizens",
            " traveled",
            " to",
            " the",
            " countryside",
            " to",
            " for",
            "age",
            ".",
            " Dogs",
            ",",
            " cats",
            ",",
            " raw",
            " sugar",
            " be",
            "ets",
            ",",
            " and",
            " tul"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "absolute",
            " right",
            "\"",
            " as",
            " their",
            " own",
            " body",
            ",",
            " and",
            " retaining",
            " those",
            " rights",
            " forever",
            ",",
            " regardless",
            " of",
            " whether",
            " the",
            " resource",
            " is",
            " still",
            " being",
            " used",
            " by",
            " them",
            ".",
            " According",
            " to",
            " Roth",
            "bard",
            ",",
            " property",
            " can",
            " only",
            " come",
            " about",
            " through",
            " labor",
            ",",
            " therefore",
            " original",
            " appropriation",
            " of",
            " land",
            " is",
            " not",
            " legitimate",
            " by",
            " merely",
            " claiming",
            " it",
            " or",
            " building",
            " a",
            " fence",
            " around",
            " it",
            "—it",
            " is",
            " only",
            " by",
            " using",
            " land"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " area",
            " was",
            " settled",
            " in",
            " the",
            " Bronze",
            " Age",
            " by",
            " the",
            " Be",
            "aker",
            " culture",
            ",",
            " who",
            " arrived",
            " from",
            " the",
            " south",
            " around",
            " ",
            "200",
            "0",
            "–",
            "180",
            "0",
            " BC",
            ".",
            " Stone",
            " circles",
            " and",
            " c",
            "air",
            "ns",
            " were",
            " constructed",
            " predominantly",
            " in",
            " this",
            " era",
            ".",
            " In",
            " the",
            " Iron",
            " Age",
            ",",
            " hill",
            " forts",
            " were",
            " built",
            ".",
            " Around",
            " the",
            " ",
            "1",
            "st",
            " century",
            " AD",
            ",",
            " the",
            " Ta",
            "ex"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "869",
    "gae",
    "hoot",
    "_PUSHDATA",
    ".Networking"
  ],
  "bottom_logits": [
    "strand",
    "Ð°ÑĢÐ°",
    "_Renderer",
    " fair",
    " Bis"
  ],
  "act_min": -0.0,
  "act_max": 0.594
}