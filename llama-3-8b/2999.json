{
  "index": 2999,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " origin",
            ".",
            "H",
            "omer",
            " and",
            " Por",
            "phy",
            "ry",
            " wrote",
            " that",
            " Apollo",
            " had",
            " a",
            " hawk",
            " as",
            " his",
            " messenger",
            ".",
            " In",
            " many",
            " myths",
            " Apollo",
            " is",
            " transformed",
            " into",
            " a",
            " hawk",
            ".",
            " In",
            " addition",
            ",",
            " Claud",
            "ius",
            " A",
            "elian",
            "us",
            " wrote",
            " that",
            " in",
            " Ancient",
            " Egypt",
            " people",
            " believed",
            " that",
            " h",
            "awks",
            " were",
            " sacred",
            " to",
            " the",
            " god",
            " and",
            " that",
            " according",
            " to",
            " the",
            " ministers",
            " of",
            " Apollo",
            " in",
            " Egypt",
            " there",
            " were"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "ople",
            " with",
            " a",
            " Turkish",
            " escort",
            ",",
            " carrying",
            " a",
            " banner",
            " above",
            " the",
            " disgr",
            "aced",
            " emperor",
            " that",
            " read",
            ":",
            " \"",
            "There",
            " is",
            " no",
            " god",
            " but",
            " Allah",
            " and",
            " Muhammad",
            " is",
            " his",
            " messenger",
            "\".",
            "The",
            " reason",
            " Al",
            "p",
            " Ar",
            "sl",
            "an",
            " spared",
            " Roman",
            "os",
            " was",
            " likely",
            " to",
            " avoid",
            " a",
            " two",
            "-front",
            " war",
            ".",
            " The",
            " Fat",
            "im",
            "ids",
            " were",
            " launching",
            " devastating",
            " raids",
            " on",
            " the",
            " Sel",
            "j",
            "uk",
            " domains"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "ople",
            " with",
            " a",
            " Turkish",
            " escort",
            ",",
            " carrying",
            " a",
            " banner",
            " above",
            " the",
            " disgr",
            "aced",
            " emperor",
            " that",
            " read",
            ":",
            " \"",
            "There",
            " is",
            " no",
            " god",
            " but",
            " Allah",
            " and",
            " Muhammad",
            " is",
            " his",
            " messenger",
            "\".",
            "The",
            " reason",
            " Al",
            "p",
            " Ar",
            "sl",
            "an",
            " spared",
            " Roman",
            "os",
            " was",
            " likely",
            " to",
            " avoid",
            " a",
            " two",
            "-front",
            " war",
            ".",
            " The",
            " Fat",
            "im",
            "ids",
            " were",
            " launching",
            " devastating",
            " raids",
            " on",
            " the",
            " Sel",
            "j",
            "uk",
            " domains"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " origin",
            ".",
            "H",
            "omer",
            " and",
            " Por",
            "phy",
            "ry",
            " wrote",
            " that",
            " Apollo",
            " had",
            " a",
            " hawk",
            " as",
            " his",
            " messenger",
            ".",
            " In",
            " many",
            " myths",
            " Apollo",
            " is",
            " transformed",
            " into",
            " a",
            " hawk",
            ".",
            " In",
            " addition",
            ",",
            " Claud",
            "ius",
            " A",
            "elian",
            "us",
            " wrote",
            " that",
            " in",
            " Ancient",
            " Egypt",
            " people",
            " believed",
            " that",
            " h",
            "awks",
            " were",
            " sacred",
            " to",
            " the",
            " god",
            " and",
            " that",
            " according",
            " to",
            " the",
            " ministers",
            " of",
            " Apollo",
            " in",
            " Egypt",
            " there",
            " were"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Press",
            ",",
            " ",
            "201",
            "8",
            ",",
            " p",
            ".",
            "Âł",
            "721",
            "–",
            "742",
            ".",
            " ",
            "201",
            "8",
            ".",
            " .",
            " Shannon",
            " Gr",
            "imes",
            ",",
            " Bec",
            "oming",
            " Gold",
            ":",
            " Z",
            "os",
            "imos",
            " of",
            " Pan",
            "opol",
            "is",
            " and",
            " the",
            " Al",
            "chemical",
            " Arts",
            " in",
            " Roman",
            " Egypt",
            ",",
            " Auckland",
            ",",
            " Rub",
            "edo",
            " Press",
            ",",
            " ",
            "201",
            "8",
            ",",
            " ",
            " Paul",
            " T",
            ".",
            " Key",
            "ser",
            ",",
            " \"",
            " Gre",
            "co",
            "-R"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Press",
            ",",
            " ",
            "201",
            "8",
            ",",
            " p",
            ".",
            "Âł",
            "721",
            "–",
            "742",
            ".",
            " ",
            "201",
            "8",
            ".",
            " .",
            " Shannon",
            " Gr",
            "imes",
            ",",
            " Bec",
            "oming",
            " Gold",
            ":",
            " Z",
            "os",
            "imos",
            " of",
            " Pan",
            "opol",
            "is",
            " and",
            " the",
            " Al",
            "chemical",
            " Arts",
            " in",
            " Roman",
            " Egypt",
            ",",
            " Auckland",
            ",",
            " Rub",
            "edo",
            " Press",
            ",",
            " ",
            "201",
            "8",
            ",",
            " ",
            " Paul",
            " T",
            ".",
            " Key",
            "ser",
            ",",
            " \"",
            " Gre",
            "co",
            "-R"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "ones",
            ",",
            " chemical",
            " messenger",
            " materials",
            ",",
            " on",
            " which",
            " insects",
            " depend",
            " for",
            " communication",
            ".",
            " In",
            " some",
            " species",
            ",",
            " e",
            ".g",
            ".",
            " the",
            " support",
            " beetle",
            " X",
            "yl",
            "otre",
            "ch",
            "us",
            " colon",
            "us",
            ",",
            " pent",
            "acos",
            "ane",
            " (",
            "C",
            "25",
            "H",
            "52",
            "),",
            " ",
            "3",
            "-m",
            "ethyl",
            "p",
            "enta",
            "icos",
            "ane",
            " (",
            "C",
            "26",
            "H",
            "54",
            ")",
            " and",
            " ",
            "9",
            "-m",
            "ethyl",
            "p",
            "enta",
            "icos",
            "ane"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " Cup",
            ".",
            " At",
            " the",
            " end",
            " of",
            " the",
            " ",
            "200",
            "1",
            " season",
            ",",
            " she",
            " was",
            " ranked",
            " No",
            ".",
            " ",
            "74",
            " in",
            " singles",
            " and",
            " No",
            ".",
            " ",
            "26",
            " in",
            " doubles",
            ".",
            "K",
            "ourn",
            "ik",
            "ova",
            " regained",
            " some",
            " success",
            " in",
            " ",
            "200",
            "2",
            ".",
            " She",
            " reached",
            " the",
            " semi",
            "-finals",
            " of",
            " Auckland",
            ",",
            " Tokyo",
            ",",
            " Ac",
            "ap",
            "ul",
            "co",
            " and",
            " San",
            " Diego",
            ",",
            " and",
            " the",
            " final",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " Cup",
            ".",
            " At",
            " the",
            " end",
            " of",
            " the",
            " ",
            "200",
            "1",
            " season",
            ",",
            " she",
            " was",
            " ranked",
            " No",
            ".",
            " ",
            "74",
            " in",
            " singles",
            " and",
            " No",
            ".",
            " ",
            "26",
            " in",
            " doubles",
            ".",
            "K",
            "ourn",
            "ik",
            "ova",
            " regained",
            " some",
            " success",
            " in",
            " ",
            "200",
            "2",
            ".",
            " She",
            " reached",
            " the",
            " semi",
            "-finals",
            " of",
            " Auckland",
            ",",
            " Tokyo",
            ",",
            " Ac",
            "ap",
            "ul",
            "co",
            " and",
            " San",
            " Diego",
            ",",
            " and",
            " the",
            " final",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            "'s",
            " own",
            " stage",
            " version",
            ",",
            " was",
            " performed",
            " by",
            " the",
            " Si",
            "Lo",
            " Theatre",
            " in",
            " Auckland",
            ",",
            " New",
            " Zealand",
            " in",
            " early",
            " ",
            "200",
            "7",
            ".",
            "In",
            " ",
            "202",
            "1",
            ",",
            " the",
            " International",
            " Anthony",
            " Burgess",
            " Foundation",
            " premiered",
            " a",
            " webpage",
            " catalog",
            "ing",
            " various",
            " productions",
            " of",
            " A",
            " Clock",
            "work",
            " Orange",
            " from",
            " around",
            " the",
            " world",
            ".",
            "Release",
            " details",
            " ",
            "196",
            "2",
            ",",
            " UK",
            ",",
            " William",
            " Hein",
            "emann",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " ",
            " ",
            "188",
            "3",
            "  ",
            " –",
            " D",
            "ally",
            " Messenger",
            ",",
            " Australian",
            " rugby",
            " player",
            ",",
            " cr",
            "ick",
            "eter",
            ",",
            " and",
            " sailor",
            " (",
            "d",
            ".",
            " ",
            "195",
            "9",
            ")",
            "188",
            "4",
            " –",
            " Ten",
            "by",
            " Davies",
            ",",
            " Welsh",
            " runner",
            " (",
            "d",
            ".",
            " ",
            "193",
            "2",
            ")",
            " ",
            " ",
            "188",
            "4",
            "  ",
            " –",
            " Otto",
            " Meyer",
            "hof",
            ",",
            " German",
            " physician",
            " and",
            " bio",
            "chemist",
            ",",
            " Nobel",
            " Prize",
            " laure"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " ",
            " ",
            "188",
            "3",
            "  ",
            " –",
            " D",
            "ally",
            " Messenger",
            ",",
            " Australian",
            " rugby",
            " player",
            ",",
            " cr",
            "ick",
            "eter",
            ",",
            " and",
            " sailor",
            " (",
            "d",
            ".",
            " ",
            "195",
            "9",
            ")",
            "188",
            "4",
            " –",
            " Ten",
            "by",
            " Davies",
            ",",
            " Welsh",
            " runner",
            " (",
            "d",
            ".",
            " ",
            "193",
            "2",
            ")",
            " ",
            " ",
            "188",
            "4",
            "  ",
            " –",
            " Otto",
            " Meyer",
            "hof",
            ",",
            " German",
            " physician",
            " and",
            " bio",
            "chemist",
            ",",
            " Nobel",
            " Prize",
            " laure"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " Grand",
            " Canyon",
            ".",
            "A",
            "udi",
            " e",
            "-tr",
            "on",
            "The",
            " next",
            " phase",
            " of",
            " technology",
            " Audi",
            " is",
            " developing",
            " is",
            " the",
            " e",
            "-tr",
            "on",
            " electric",
            " drive",
            " power",
            "train",
            " system",
            ".",
            " They",
            " have",
            " shown",
            " several",
            " concept",
            " cars",
            " ,",
            " each",
            " with",
            " different",
            " levels",
            " of",
            " size",
            " and",
            " performance",
            ".",
            " The",
            " original",
            " e",
            "-tr",
            "on",
            " concept",
            " shown",
            " at",
            " the",
            " ",
            "200",
            "9",
            " Frankfurt",
            " motor",
            " show",
            " is",
            " based"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " Grand",
            " Canyon",
            ".",
            "A",
            "udi",
            " e",
            "-tr",
            "on",
            "The",
            " next",
            " phase",
            " of",
            " technology",
            " Audi",
            " is",
            " developing",
            " is",
            " the",
            " e",
            "-tr",
            "on",
            " electric",
            " drive",
            " power",
            "train",
            " system",
            ".",
            " They",
            " have",
            " shown",
            " several",
            " concept",
            " cars",
            " ,",
            " each",
            " with",
            " different",
            " levels",
            " of",
            " size",
            " and",
            " performance",
            ".",
            " The",
            " original",
            " e",
            "-tr",
            "on",
            " concept",
            " shown",
            " at",
            " the",
            " ",
            "200",
            "9",
            " Frankfurt",
            " motor",
            " show",
            " is",
            " based"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " location",
            " survey",
            " of",
            " Europe",
            "'s",
            " leading",
            " companies",
            " carried",
            " out",
            " by",
            " global",
            " real",
            " estate",
            " consultant",
            " C",
            "ushman",
            " &",
            " Wake",
            "field",
            " –",
            " Amsterdam",
            " is",
            " one",
            " of",
            " the",
            " top",
            " European",
            " cities",
            " in",
            " which",
            " to",
            " locate",
            " an",
            " international",
            " business",
            ",",
            " ranking",
            " fifth",
            " in",
            " the",
            " survey",
            ".",
            " with",
            " the",
            " survey",
            " determining",
            " London",
            ",",
            " Paris",
            ",",
            " Frankfurt",
            " and",
            " Barcelona",
            " as",
            " the",
            " four",
            " European",
            " cities",
            " surpass",
            "ing",
            " Amsterdam",
            " in",
            " this"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " minority",
            " of",
            " Frankfurt",
            " views",
            " Advanced",
            " Chemistry",
            "'s",
            " appeal",
            " to",
            " the",
            " German",
            " image",
            " as",
            " a",
            " \"",
            "symbol",
            "ic",
            " betrayal",
            " of",
            " the",
            " right",
            " of",
            " ethnic",
            " minorities",
            " to",
            " '",
            "roots",
            "'",
            " or",
            " to",
            " any",
            " expression",
            " of",
            " cultural",
            " heritage",
            ".\"",
            " In",
            " this",
            " sense",
            ",",
            " their",
            " rap",
            " represents",
            " a",
            " complex",
            " social",
            " discourse",
            " internal",
            " to",
            " the",
            " German",
            " sounds",
            "cape",
            " in",
            " which",
            " they",
            " attempt",
            " to",
            " negotiate",
            " immigrant",
            " assim",
            "ilation",
            " into"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "Upon",
            " his",
            " arrival",
            " in",
            " Frankfurt",
            ",",
            " he",
            " experienced",
            " a",
            " period",
            " of",
            " depression",
            " and",
            " declining",
            " health",
            ".",
            " He",
            " renewed",
            " his",
            " correspondence",
            " with",
            " his",
            " mother",
            ",",
            " and",
            " she",
            " seemed",
            " concerned",
            " that",
            " he",
            " might",
            " commit",
            " suicide",
            " like",
            " his",
            " father",
            ".",
            " By",
            " now",
            " Joh",
            "anna",
            " and",
            " Ade",
            "le",
            " were",
            " living",
            " very",
            " modest",
            "ly",
            ".",
            " Joh",
            "anna",
            "'s",
            " writing",
            " did",
            " not",
            " bring",
            " her",
            " much",
            " income",
            ",",
            " and",
            " her"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "Companies",
            " formerly",
            " listed",
            " on",
            " the",
            " Frankfurt",
            " Stock",
            " Exchange",
            "Vehicle",
            " manufacturing",
            " companies",
            " established",
            " in",
            " ",
            "190",
            "9",
            "Vehicle",
            " manufacturing",
            " companies",
            " dis",
            "establish",
            "ed",
            " in",
            " ",
            "193",
            "9",
            "Vehicle",
            " manufacturing",
            " companies",
            " established",
            " in",
            " ",
            "196",
            "5",
            "Re",
            "-established",
            " companies",
            "German",
            " brands",
            "Lux",
            "ury",
            " motor",
            " vehicle",
            " manufacturers",
            "Companies",
            " based",
            " in",
            " Sax",
            "ony",
            "Sports",
            " car",
            " manufacturers",
            "V",
            "olk"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " New",
            " York",
            ":",
            " New",
            " York",
            " University",
            " Press",
            ".",
            " .",
            "I",
            "sh",
            "ig",
            "uro",
            ",",
            " Hiro",
            "shi",
            ".",
            " \"",
            "Android",
            " science",
            ".\"",
            " Cognitive",
            " Science",
            " Society",
            ".",
            " ",
            "200",
            "5",
            ".",
            "Gl",
            "aser",
            ",",
            " Hor",
            "st",
            " Albert",
            " and",
            " Ross",
            "bach",
            ",",
            " Sab",
            "ine",
            ":",
            " The",
            " Artificial",
            " Human",
            ",",
            " Frankfurt",
            "/M",
            ".,",
            " Bern",
            ",",
            " New",
            " York",
            " ",
            "201",
            "1",
            " \"",
            "The",
            " Artificial",
            " Human",
            "\"",
            "Tech",
            "Cast"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " New",
            " York",
            ":",
            " New",
            " York",
            " University",
            " Press",
            ".",
            " .",
            "I",
            "sh",
            "ig",
            "uro",
            ",",
            " Hiro",
            "shi",
            ".",
            " \"",
            "Android",
            " science",
            ".\"",
            " Cognitive",
            " Science",
            " Society",
            ".",
            " ",
            "200",
            "5",
            ".",
            "Gl",
            "aser",
            ",",
            " Hor",
            "st",
            " Albert",
            " and",
            " Ross",
            "bach",
            ",",
            " Sab",
            "ine",
            ":",
            " The",
            " Artificial",
            " Human",
            ",",
            " Frankfurt",
            "/M",
            ".,",
            " Bern",
            ",",
            " New",
            " York",
            " ",
            "201",
            "1",
            " \"",
            "The",
            " Artificial",
            " Human",
            "\"",
            "Tech",
            "Cast"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "Upon",
            " his",
            " arrival",
            " in",
            " Frankfurt",
            ",",
            " he",
            " experienced",
            " a",
            " period",
            " of",
            " depression",
            " and",
            " declining",
            " health",
            ".",
            " He",
            " renewed",
            " his",
            " correspondence",
            " with",
            " his",
            " mother",
            ",",
            " and",
            " she",
            " seemed",
            " concerned",
            " that",
            " he",
            " might",
            " commit",
            " suicide",
            " like",
            " his",
            " father",
            ".",
            " By",
            " now",
            " Joh",
            "anna",
            " and",
            " Ade",
            "le",
            " were",
            " living",
            " very",
            " modest",
            "ly",
            ".",
            " Joh",
            "anna",
            "'s",
            " writing",
            " did",
            " not",
            " bring",
            " her",
            " much",
            " income",
            ",",
            " and",
            " her"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " two",
            " particles",
            " interact",
            " in",
            " such",
            " a",
            " way",
            " that",
            " the",
            " wave",
            "function",
            " describing",
            " them",
            " is",
            " ent",
            "angled",
            ".",
            " Then",
            ",",
            " no",
            " matter",
            " how",
            " far",
            " the",
            " two",
            " particles",
            " were",
            " separated",
            ",",
            " a",
            " precise",
            " position",
            " measurement",
            " on",
            " one",
            " particle",
            " would",
            " imply",
            " the",
            " ability",
            " to",
            " predict",
            ",",
            " perfectly",
            ",",
            " the",
            " result",
            " of",
            " measuring",
            " the",
            " position",
            " of",
            " the",
            " other",
            " particle",
            ".",
            " Likewise",
            ",",
            " a",
            " precise",
            " momentum",
            " measurement"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " way",
            " on",
            " her",
            " second",
            " son",
            ".",
            "Sch",
            "openh",
            "auer",
            " claimed",
            " that",
            ",",
            " in",
            " his",
            " last",
            " year",
            " in",
            " Berlin",
            ",",
            " he",
            " had",
            " a",
            " proph",
            "etic",
            " dream",
            " that",
            " urged",
            " him",
            " to",
            " escape",
            " from",
            " the",
            " city",
            ".",
            " As",
            " he",
            " arrived",
            " in",
            " his",
            " new",
            " home",
            " in",
            " Frankfurt",
            ",",
            " he",
            " supposedly",
            " had",
            " another",
            " supernatural",
            " experience",
            ",",
            " an",
            " appar",
            "ition",
            " of",
            " his",
            " dead",
            " father",
            " and",
            " his",
            " mother",
            ",",
            " who"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " literary",
            " estate",
            ".",
            " Frauen",
            "st",
            "Ã¤",
            "dt",
            " also",
            " became",
            " the",
            " editor",
            " of",
            " the",
            " first",
            " collected",
            " works",
            " of",
            " Sch",
            "openh",
            "auer",
            ".",
            "In",
            " ",
            "184",
            "8",
            ",",
            " Sch",
            "openh",
            "auer",
            " witnessed",
            " violent",
            " uphe",
            "aval",
            " in",
            " Frankfurt",
            " after",
            " General",
            " Hans",
            " Adolf",
            " Erd",
            "mann",
            " von",
            " Au",
            "ers",
            "wald",
            " and",
            " Prince",
            " Felix",
            " L",
            "ich",
            "now",
            "sky",
            " were",
            " murdered",
            ".",
            " He",
            " became",
            " worried",
            " for",
            " his",
            " own",
            " safety",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            "anza",
            " is",
            " arrested",
            " for",
            " her",
            "esy",
            ".",
            "160",
            "1",
            "–",
            "190",
            "0",
            "161",
            "4",
            " –",
            " F",
            "ett",
            "mil",
            "ch",
            " U",
            "prising",
            ":",
            " Jews",
            " are",
            " expelled",
            " from",
            " Frankfurt",
            ",",
            " Holy",
            " Roman",
            " Empire",
            ",",
            " following",
            " the",
            " plunder",
            "ing",
            " of",
            " the",
            " Jud",
            "eng",
            "asse",
            ".",
            "163",
            "9",
            " –",
            " Mad",
            "ras",
            " (",
            "now",
            " Chennai",
            "),",
            " India",
            ",",
            " is",
            " founded",
            " by",
            " the",
            " British",
            " East",
            " India",
            " Company",
            " on"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " ent",
            "ang",
            "lement",
            ",",
            " measuring",
            " one",
            " object",
            " would",
            " lead",
            " to",
            " an",
            " instantaneous",
            " change",
            " of",
            " the",
            " wave",
            "function",
            " describing",
            " the",
            " other",
            " object",
            ",",
            " no",
            " matter",
            " how",
            " far",
            " away",
            " it",
            " is",
            ".",
            " Moreover",
            ",",
            " the",
            " choice",
            " of",
            " which",
            " measurement",
            " to",
            " perform",
            " upon",
            " the",
            " first",
            " object",
            " would",
            " affect",
            " what",
            " wave",
            "function",
            " could",
            " result",
            " for",
            " the",
            " second",
            " object",
            ".",
            " Einstein",
            " reasoned",
            " that",
            " no",
            " influence",
            " could",
            " propagate",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            "anza",
            " is",
            " arrested",
            " for",
            " her",
            "esy",
            ".",
            "160",
            "1",
            "–",
            "190",
            "0",
            "161",
            "4",
            " –",
            " F",
            "ett",
            "mil",
            "ch",
            " U",
            "prising",
            ":",
            " Jews",
            " are",
            " expelled",
            " from",
            " Frankfurt",
            ",",
            " Holy",
            " Roman",
            " Empire",
            ",",
            " following",
            " the",
            " plunder",
            "ing",
            " of",
            " the",
            " Jud",
            "eng",
            "asse",
            ".",
            "163",
            "9",
            " –",
            " Mad",
            "ras",
            " (",
            "now",
            " Chennai",
            "),",
            " India",
            ",",
            " is",
            " founded",
            " by",
            " the",
            " British",
            " East",
            " India",
            " Company",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " claim",
            " that",
            " it",
            " was",
            " his",
            " first",
            " chapel",
            ".",
            " As",
            " his",
            " fame",
            " increased",
            ",",
            " copies",
            " of",
            " paintings",
            " and",
            " photographs",
            " of",
            " him",
            " were",
            " being",
            " sold",
            " and",
            " admir",
            "ers",
            " were",
            " visiting",
            " the",
            " places",
            " where",
            " he",
            " had",
            " lived",
            " and",
            " written",
            " his",
            " works",
            ".",
            " People",
            " visited",
            " Frankfurt",
            "'s",
            " Engl",
            "ischer",
            " Hof",
            " to",
            " observe",
            " him",
            " dining",
            ".",
            " Admir",
            "ers",
            " gave",
            " him",
            " gifts",
            " and",
            " asked",
            " for",
            " aut",
            "ographs",
            ".",
            " He"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " claim",
            " that",
            " it",
            " was",
            " his",
            " first",
            " chapel",
            ".",
            " As",
            " his",
            " fame",
            " increased",
            ",",
            " copies",
            " of",
            " paintings",
            " and",
            " photographs",
            " of",
            " him",
            " were",
            " being",
            " sold",
            " and",
            " admir",
            "ers",
            " were",
            " visiting",
            " the",
            " places",
            " where",
            " he",
            " had",
            " lived",
            " and",
            " written",
            " his",
            " works",
            ".",
            " People",
            " visited",
            " Frankfurt",
            "'s",
            " Engl",
            "ischer",
            " Hof",
            " to",
            " observe",
            " him",
            " dining",
            ".",
            " Admir",
            "ers",
            " gave",
            " him",
            " gifts",
            " and",
            " asked",
            " for",
            " aut",
            "ographs",
            ".",
            " He"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " popularity",
            " was",
            " w",
            "aning",
            ".",
            " Their",
            " correspondence",
            " remained",
            " reserved",
            ",",
            " and",
            " Arthur",
            " seemed",
            " und",
            "ist",
            "urbed",
            " by",
            " her",
            " death",
            " in",
            " ",
            "183",
            "8",
            ".",
            " His",
            " relationship",
            " with",
            " his",
            " sister",
            " grew",
            " closer",
            " and",
            " he",
            " correspond",
            "ed",
            " with",
            " her",
            " until",
            " she",
            " died",
            " in",
            " ",
            "184",
            "9",
            ".",
            "In",
            " July",
            " ",
            "183",
            "2",
            ",",
            " Sch",
            "openh",
            "auer",
            " left",
            " Frankfurt",
            " for",
            " Mann",
            "heim",
            " but",
            " returned",
            " in",
            " July"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "local",
            "\",",
            " which",
            " is",
            " to",
            " say",
            " that",
            " somehow",
            " the",
            " two",
            " particles",
            " are",
            " able",
            " to",
            " interact",
            " instant",
            "aneously",
            " no",
            " matter",
            " how",
            " widely",
            " they",
            " ever",
            " become",
            " separated",
            ".",
            " Bell",
            " argued",
            " that",
            " because",
            " an",
            " explanation",
            " of",
            " quantum",
            " phenomena",
            " in",
            " terms",
            " of",
            " hidden",
            " variables",
            " would",
            " require",
            " non",
            "local",
            "ity",
            ",",
            " the",
            " E",
            "PR",
            " paradox",
            " \"",
            "is",
            " resolved",
            " in",
            " the",
            " way",
            " which",
            " Einstein",
            " would",
            " have",
            " liked",
            " least",
            "\"."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " characters",
            ".",
            " This",
            " change",
            " allows",
            " for",
            " mixed",
            "-script",
            " writing",
            ",",
            " where",
            " one",
            " syll",
            "able",
            " always",
            " takes",
            " up",
            " one",
            " type",
            " space",
            " no",
            " matter",
            " how",
            " many",
            " letters",
            " get",
            " stacked",
            " into",
            " building",
            " that",
            " one",
            " sound",
            "-block",
            ".",
            "Zh",
            "uy",
            "in",
            "Zh",
            "uy",
            "in",
            ",",
            " sometimes",
            " referred",
            " to",
            " as",
            " B",
            "op",
            "omo",
            "fo",
            ",",
            " is",
            " a",
            " semi",
            "-s",
            "yll",
            "ab",
            "ary",
            ".",
            " It",
            " trans",
            "cribes",
            " Mandarin"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " writers",
            "19",
            "th",
            "-century",
            " philosophers",
            "Ab",
            "ol",
            "ition",
            "ists",
            "Animal",
            " rights",
            " scholars",
            "Anti",
            "-n",
            "atal",
            "ists",
            "A",
            "ph",
            "or",
            "ists",
            "A",
            "the",
            "ist",
            " philosophers",
            "Bur",
            "ials",
            " at",
            " Frankfurt",
            " Main",
            " Cemetery",
            "Critical",
            " theorists",
            "German",
            " critics",
            " of",
            " Christianity",
            "Crit",
            "ics",
            " of",
            " Judaism",
            "Crit",
            "ics",
            " of",
            " religions",
            "Ep",
            "istem",
            "ologists",
            "German",
            " atheists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "Music",
            " ",
            "War",
            "hol",
            " strongly",
            " influenced",
            " the",
            " new",
            " wave",
            "/p",
            "unk",
            " rock",
            " band",
            " De",
            "vo",
            ",",
            " as",
            " well",
            " as",
            " David",
            " Bowie",
            ".",
            " Bowie",
            " recorded",
            " a",
            " song",
            " called",
            " \"",
            "Andy",
            " War",
            "hol",
            "\"",
            " for",
            " his",
            " ",
            "197",
            "1",
            " album",
            " H",
            "unky",
            " D",
            "ory",
            ".",
            " Lou",
            " Reed",
            " wrote",
            " the",
            " song",
            " \"",
            "Andy",
            "'s",
            " Chest",
            "\",",
            " about",
            " Valerie",
            " Sol",
            "anas",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ",",
            " was",
            " renamed",
            " Cedar",
            " Crest",
            " Elementary",
            ".",
            " Johnston",
            " Middle",
            " School",
            " in",
            " Houston",
            ",",
            " Texas",
            ",",
            " was",
            " also",
            " renamed",
            " Meyer",
            "land",
            " Middle",
            " School",
            ".",
            " Three",
            " other",
            " elementary",
            " schools",
            " named",
            " for",
            " Confederate",
            " veterans",
            " were",
            " renamed",
            " simultaneously",
            ".",
            "See",
            " also",
            " Albert",
            " Sidney",
            " Johnston",
            " High",
            " School",
            ",",
            " a",
            " def",
            "unct",
            " public",
            " high",
            " school",
            " in",
            " Austin",
            ",",
            " Texas",
            " Statue",
            " of",
            " Albert",
            " Sidney"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " legends",
            " which",
            " are",
            " nevertheless",
            " pretty",
            " childish",
            ".",
            " No",
            " interpretation",
            " no",
            " matter",
            " how",
            " subtle",
            " can",
            " (",
            "for",
            " me",
            ")",
            " change",
            " this",
            ".",
            " ...",
            " For",
            " me",
            " the",
            " Jewish",
            " religion",
            " like",
            " all",
            " other",
            " religions",
            " is",
            " an",
            " incarnation",
            " of",
            " the",
            " most",
            " childish",
            " superst",
            "itions",
            ".",
            " And",
            " the",
            " Jewish",
            " people",
            " to",
            " whom",
            " I",
            " gladly",
            " belong",
            " and",
            " with",
            " whose",
            " mentality",
            " I",
            " have",
            " a",
            " deep",
            " affinity",
            " have",
            " no",
            " different",
            " quality"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " complained",
            " that",
            " he",
            " still",
            " felt",
            " isolated",
            " due",
            " to",
            " his",
            " not",
            " very",
            " social",
            " nature",
            " and",
            " the",
            " fact",
            " that",
            " many",
            " of",
            " his",
            " good",
            " friends",
            " had",
            " already",
            " died",
            " from",
            " old",
            " age",
            ".",
            "He",
            " remained",
            " healthy",
            " in",
            " his",
            " own",
            " old",
            " age",
            ",",
            " which",
            " he",
            " attributed",
            " to",
            " regular",
            " walks",
            " no",
            " matter",
            " the",
            " weather",
            " and",
            " always",
            " getting",
            " enough",
            " sleep",
            ".",
            " He",
            " had",
            " a",
            " great",
            " appetite",
            " and",
            " could",
            " read",
            " without"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "0",
            " T",
            "DI",
            ",",
            " Audi",
            " A",
            "3",
            " Sport",
            "back",
            " ",
            "2",
            ".",
            "0",
            " T",
            "DI",
            " with",
            " S",
            " tr",
            "onic",
            " transmission",
            ")",
            " travelling",
            " across",
            " the",
            " American",
            " continent",
            " from",
            " New",
            " York",
            " to",
            " Los",
            " Angeles",
            ",",
            " passing",
            " major",
            " cities",
            " like",
            " Chicago",
            ",",
            " Dallas",
            " and",
            " Las",
            " Vegas",
            " during",
            " the",
            " ",
            "13",
            " daily",
            " stages",
            ",",
            " as",
            " well",
            " as",
            " natural",
            " wonders",
            " including",
            " the",
            " Rocky",
            " Mountains",
            ",",
            " Death",
            " Valley"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " Bran",
            "iff",
            "'s",
            " chairman",
            " and",
            " president",
            " Harding",
            " Lawrence",
            ",",
            " was",
            " representing",
            " the",
            " Dallas",
            "-based",
            " carrier",
            " at",
            " that",
            " time",
            ".",
            " Lois",
            " succeeded",
            " Wells",
            " Rich",
            " Greene",
            " Agency",
            " on",
            " December",
            " ",
            "1",
            ",",
            " ",
            "196",
            "8",
            ".",
            " The",
            " rights",
            " to",
            " War",
            "hol",
            "'s",
            " films",
            " for",
            " Bran",
            "iff",
            " and",
            " his",
            " signed",
            " contracts",
            " are",
            " owned",
            " by",
            " a",
            " private",
            " trust",
            " and",
            " are",
            " administered",
            " by",
            " Bran",
            "iff",
            " Airways",
            " Foundation"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            " Dallas",
            ",",
            " Low",
            "nd",
            "es",
            ",",
            " Mare",
            "ngo",
            " and",
            " Perry",
            ".\"",
            "In",
            " ",
            "197",
            "2",
            ",",
            " for",
            " the",
            " first",
            " time",
            " since",
            " ",
            "190",
            "1",
            ",",
            " the",
            " legislature",
            " completed",
            " the",
            " congressional",
            " red",
            "istrict",
            "ing",
            " based",
            " on",
            " the",
            " dec",
            "ennial",
            " census",
            ".",
            " This",
            " benefited",
            " the",
            " urban",
            " areas",
            " that",
            " had",
            " developed",
            ",",
            " as",
            " well",
            " as",
            " all",
            " in",
            " the",
            " population",
            " who",
            " had",
            " been",
            " under",
            "represented",
            " for"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Tw",
            "entieth",
            " Century",
            " Motor",
            " Company",
            ".",
            "Dr",
            ".",
            " Bl",
            "od",
            "gett",
            " is",
            " the",
            " scientist",
            " who",
            " pulls",
            " the",
            " lever",
            " to",
            " demonstrate",
            " Project",
            " X",
            ".",
            "Or",
            "ren",
            " Boyle",
            " is",
            " the",
            " head",
            " of",
            " Associated",
            " Steel",
            ",",
            " ant",
            "ith",
            "esis",
            " of",
            " Hank",
            " Re",
            "arden",
            " and",
            " a",
            " friend",
            " of",
            " James",
            " Tag",
            "gart",
            ".",
            " He",
            " is",
            " an",
            " investor",
            " in",
            " the",
            " San",
            " Sebast",
            "i",
            "Ã¡n",
            " Mines",
            ".",
            " He"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "Battle",
            " of",
            " the",
            " Cow",
            "shed",
            "\"),",
            " Snow",
            "ball",
            " announces",
            " his",
            " plans",
            " to",
            " modern",
            "ise",
            " the",
            " farm",
            " by",
            " building",
            " a",
            " wind",
            "mill",
            ".",
            " Napoleon",
            " disputes",
            " this",
            " idea",
            ",",
            " and",
            " matters",
            " come",
            " to",
            " a",
            " head",
            ",",
            " which",
            " cul",
            "min",
            "ates",
            " in",
            " Napoleon",
            "'s",
            " dogs",
            " chasing",
            " Snow",
            "ball",
            " away",
            " and",
            " Napoleon",
            " effectively",
            " declaring",
            " himself",
            " supreme",
            " commander",
            ".",
            "N",
            "ap",
            "oleon",
            " en",
            "acts",
            " changes",
            " to",
            " the",
            " governance"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " Congress",
            " began",
            " the",
            " signing",
            " process",
            " by",
            " examining",
            " their",
            " copy",
            " of",
            " the",
            " Articles",
            " on",
            " June",
            " ",
            "27",
            ",",
            " ",
            "177",
            "8",
            ".",
            " They",
            " ordered",
            " a",
            " final",
            " copy",
            " prepared",
            " (",
            "the",
            " one",
            " in",
            " the",
            " National",
            " Archives",
            "),",
            " and",
            " that",
            " delegates",
            " should",
            " inform",
            " the",
            " secretary",
            " of",
            " their",
            " authority",
            " for",
            " rat",
            "ification",
            ".",
            "On",
            " July",
            " ",
            "9",
            ",",
            " ",
            "177",
            "8",
            ",",
            " the",
            " prepared",
            " copy",
            " was",
            " ready"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " held",
            " at",
            " Harvard",
            " University",
            ",",
            " one",
            " at",
            " the",
            " University",
            " of",
            " Oklahoma",
            ",",
            " and",
            " one",
            " at",
            " the",
            " United",
            " States",
            " Air",
            " Force",
            " Academy",
            ".",
            " On",
            " ",
            "20",
            " July",
            " ",
            "201",
            "8",
            ",",
            " the",
            " sixth",
            " copy",
            " was",
            " sold",
            " at",
            " auction",
            " to",
            " an",
            " anonymous",
            " buyer",
            " for",
            " Â£",
            "95",
            ",",
            "000",
            ".",
            " A",
            " digital",
            " fac",
            "sim",
            "ile",
            " of",
            " one",
            " of",
            " the",
            " copies",
            " in",
            " the",
            " Harvard",
            " University",
            " Library"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.197,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.175,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "onic",
            " matter",
            " are",
            " concentrated",
            " inside",
            " stars",
            ",",
            " where",
            " conditions",
            " are",
            " unfavorable",
            " for",
            " atomic",
            " matter",
            ".",
            " The",
            " total",
            " b",
            "ary",
            "onic",
            " mass",
            " is",
            " about",
            " ",
            "10",
            "%",
            " of",
            " the",
            " mass",
            " of",
            " the",
            " galaxy",
            ";",
            " the",
            " remainder",
            " of",
            " the",
            " mass",
            " is",
            " an",
            " unknown",
            " dark",
            " matter",
            ".",
            " High",
            " temperature",
            " inside",
            " stars",
            " makes",
            " most",
            " \"",
            "atoms",
            "\"",
            " fully",
            " ion",
            "ized",
            ",",
            " that",
            " is",
            ",",
            " separates",
            " all",
            " electrons"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " vessels",
            " of",
            " war",
            " of",
            " the",
            " United",
            " States",
            " in",
            " British",
            " ports",
            ",",
            " harb",
            "ors",
            ",",
            " and",
            " waters",
            ",",
            " are",
            " now",
            " to",
            " be",
            " considered",
            " as",
            " at",
            " an",
            " end",
            "\".",
            " Nonetheless",
            ",",
            " the",
            " final",
            " Confederate",
            " surrender",
            " was",
            " in",
            " Liverpool",
            ",",
            " England",
            " where",
            " James",
            " I",
            "red",
            "ell",
            " W",
            "add",
            "ell",
            ",",
            " the",
            " captain",
            " of",
            " the",
            " CSS",
            " Shen",
            "ando",
            "ah",
            ",",
            " surrendered",
            " the",
            " cruiser",
            " to",
            " British",
            " authorities",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " ",
            " In",
            " criminal",
            " matters",
            ",",
            " however",
            ",",
            " the",
            " state",
            " or",
            " prosecution",
            " generally",
            " has",
            " no",
            " appeal",
            " \"",
            "as",
            " of",
            " right",
            "\".",
            " ",
            " And",
            " due",
            " to",
            " the",
            " double",
            " jeopardy",
            " principle",
            ",",
            " the",
            " state",
            " or",
            " prosecution",
            " may",
            " never",
            " appeal",
            " a",
            " jury",
            " or",
            " bench",
            " verdict",
            " of",
            " acqu",
            "ittal",
            ".",
            " ",
            " But",
            " in",
            " some",
            " jurisdictions",
            ",",
            " the",
            " state",
            " or",
            " prosecution",
            " may",
            " appeal",
            " \"",
            "as",
            " of",
            " right",
            "\""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " belt",
            " have",
            " been",
            " folded",
            " and",
            " fractured",
            " in",
            " such",
            " a",
            " manner",
            " that",
            " erosion",
            " produced",
            " the",
            " characteristic",
            " steep",
            " vertical",
            " peaks",
            " of",
            " the",
            " Swiss",
            " Alps",
            " that",
            " rise",
            " seemingly",
            " straight",
            " out",
            " of",
            " the",
            " fore",
            "land",
            " areas",
            ".",
            " Peaks",
            " such",
            " as",
            " Mont",
            " Blanc",
            ",",
            " the",
            " Matter",
            "horn",
            ",",
            " and",
            " high",
            " peaks",
            " in",
            " the",
            " Penn",
            "ine",
            " Alps",
            ",",
            " the",
            " Brian",
            "Ã§",
            "onn",
            "ais",
            ",",
            " and",
            " Ho",
            "he",
            " Tau",
            "ern"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.182,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.165,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "duction",
            ".",
            "The",
            " my",
            "el",
            "inated",
            " ax",
            "ons",
            " from",
            " the",
            " cortical",
            " neurons",
            " form",
            " the",
            " bulk",
            " of",
            " the",
            " neural",
            " tissue",
            " called",
            " white",
            " matter",
            " in",
            " the",
            " brain",
            ".",
            " The",
            " my",
            "elin",
            " gives",
            " the",
            " white",
            " appearance",
            " to",
            " the",
            " tissue",
            " in",
            " contrast",
            " to",
            " the",
            " grey",
            " matter",
            " of",
            " the",
            " cerebral",
            " cortex",
            " which",
            " contains",
            " the",
            " neuronal",
            " cell",
            " bodies",
            ".",
            " A",
            " similar",
            " arrangement",
            " is",
            " seen",
            " in",
            " the",
            " cere",
            "bell",
            "um"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.163,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.168,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "65",
            " years",
            " or",
            " older",
            ".",
            "Population",
            " by",
            " Sex",
            " and",
            " Age",
            " Group",
            " (",
            "C",
            "ensus",
            " ",
            "16",
            ".V",
            ".",
            "201",
            "4",
            "):",
            "Population",
            " Estimates",
            " by",
            " Sex",
            " and",
            " Age",
            " Group",
            " (",
            "01",
            ".V",
            "II",
            ".",
            "202",
            "0",
            ")",
            " (",
            "Post",
            "-c",
            "ens",
            "al",
            " estimates",
            ".)",
            ":",
            "V",
            "ital",
            " statistics",
            "Registration",
            " of",
            " vital",
            " events",
            " in",
            " Angola",
            " is",
            " not",
            " complete",
            ".",
            " The",
            " website",
            " Our",
            " World"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " \"",
            "sun",
            " disk",
            "\"",
            " is",
            " made",
            " up",
            " of",
            " smaller",
            ",",
            " rad",
            "ially",
            " symmetric",
            ",",
            " individual",
            " flowers",
            " called",
            " disc",
            " flowers",
            " or",
            " disc",
            " flo",
            "rets",
            ".",
            " The",
            " word",
            " aster",
            " means",
            " \"",
            "star",
            "\"",
            " in",
            " Greek",
            ",",
            " referring",
            " to",
            " the",
            " appearance",
            " of",
            " most",
            " family",
            " members",
            " as",
            " a",
            " \"",
            "cele",
            "stial",
            " body",
            " with",
            " rays",
            "\".",
            "The",
            " capit",
            "ulum",
            ",",
            " which",
            " often",
            " appears",
            " to",
            " be",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "b",
            ".",
            " ",
            "192",
            "4",
            ")",
            " ",
            " ",
            "200",
            "9",
            "  ",
            " –",
            " L",
            "Ã¡s",
            "zl",
            "Ã³",
            " T",
            "is",
            "za",
            ",",
            " Hungarian",
            "-American",
            " physicist",
            " and",
            " academic",
            " (",
            "b",
            ".",
            " ",
            "190",
            "7",
            ")",
            " ",
            " ",
            "200",
            "9",
            "  ",
            " –",
            " Sal",
            "ih",
            " Ne",
            "ft",
            "Ã§i",
            ",",
            " Turkish",
            " economist",
            " and",
            " author",
            " (",
            "b",
            ".",
            " ",
            "194",
            "7",
            ")",
            "201",
            "0",
            " –",
            " Jack",
            " Her",
            "er",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "200",
            "9",
            ",",
            " NASA",
            " streamed",
            " the",
            " original",
            " mission",
            " audio",
            " on",
            " its",
            " website",
            " in",
            " real",
            " time",
            " ",
            "40",
            " years",
            " to",
            " the",
            " minute",
            " after",
            " the",
            " events",
            " occurred",
            ".",
            " It",
            " is",
            " in",
            " the",
            " process",
            " of",
            " restoring",
            " the",
            " video",
            " footage",
            " and",
            " has",
            " released",
            " a",
            " preview",
            " of",
            " key",
            " moments",
            ".",
            " In",
            " July",
            " ",
            "201",
            "0",
            ",",
            " air",
            "-to",
            "-ground",
            " voice",
            " recordings",
            " and",
            " film",
            " footage",
            " shot",
            " in",
            " Mission",
            " Control"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " can",
            " be",
            " found",
            " in",
            " the",
            " Lesser",
            " Ant",
            "illes",
            " chain",
            " of",
            " islands",
            " in",
            " the",
            " Caribbean",
            ".",
            " It",
            " is",
            " a",
            " second",
            " home",
            " for",
            " many",
            " of",
            " the",
            " popular",
            " music",
            " genres",
            " that",
            " are",
            " popular",
            " throughout",
            " the",
            " Caribbean",
            ",",
            " including",
            " cal",
            "yp",
            "so",
            ",",
            " soc",
            "a",
            ",",
            " ste",
            "eld",
            "rum",
            ",",
            " zou",
            "k",
            ",",
            " and",
            " reg",
            "gae",
            ",",
            " and",
            " it",
            " has",
            " produced"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Russia",
            " now",
            " actively",
            " tried",
            " to",
            " gain",
            " possession",
            " of",
            " the",
            " Caucas",
            "us",
            " region",
            " which",
            " was",
            ",",
            " for",
            " the",
            " most",
            " part",
            ",",
            " in",
            " the",
            " hands",
            " of",
            " Iran",
            ".",
            " In",
            " ",
            "180",
            "4",
            ",",
            " the",
            " Russians",
            " invaded",
            " and",
            " sacked",
            " the",
            " Iranian",
            " town",
            " of",
            " Gan",
            "ja",
            ",",
            " sparking",
            " the",
            " Russo",
            "-P",
            "ers",
            "ian",
            " War",
            " of",
            " ",
            "180",
            "4",
            "–",
            "181",
            "3",
            ".",
            " The",
            " milit",
            "arily",
            " superior"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "iaux",
    "uet",
    "avaÅŁ",
    "çĽĳåĲ¬é¡µéĿ¢",
    "ACHE"
  ],
  "bottom_logits": [
    " Ple",
    " ",
    " or",
    " gen",
    " bel"
  ],
  "act_min": -0.0,
  "act_max": 0.703
}