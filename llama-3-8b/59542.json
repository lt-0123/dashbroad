{
  "index": 59542,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " AN",
            "O",
            "VA",
            " procedure",
            " after",
            " encountering",
            " an",
            " interaction",
            ".",
            " Inter",
            "actions",
            " comp",
            "licate",
            " the",
            " interpretation",
            " of",
            " experimental",
            " data",
            ".",
            " Neither",
            " the",
            " calculations",
            " of",
            " significance",
            " nor",
            " the",
            " estimated",
            " treatment",
            " effects",
            " can",
            " be",
            " taken",
            " at",
            " face",
            " value",
            ".",
            " \"",
            "A",
            " significant",
            " interaction",
            " will",
            " often",
            " mask",
            " the",
            " significance",
            " of",
            " main",
            " effects",
            ".\"",
            " Graph",
            "ical",
            " methods",
            " are",
            " recommended",
            " to",
            " enhance",
            " understanding",
            ".",
            " Regression",
            " is",
            " often",
            " useful",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " AN",
            "O",
            "VA",
            " is",
            " often",
            " followed",
            " by",
            " additional",
            " tests",
            ".",
            " This",
            " can",
            " be",
            " done",
            " in",
            " order",
            " to",
            " assess",
            " which",
            " groups",
            " are",
            " different",
            " from",
            " which",
            " other",
            " groups",
            " or",
            " to",
            " test",
            " various",
            " other",
            " focused",
            " hypotheses",
            ".",
            " Follow",
            "-up",
            " tests",
            " are",
            " often",
            " distinguished",
            " in",
            " terms",
            " of",
            " whether",
            " they",
            " are",
            " \"",
            "pl",
            "anned",
            "\"",
            " (",
            "a",
            " prior",
            "i",
            ")",
            " or",
            " \"",
            "post",
            " hoc",
            ".\"",
            " Planned",
            " tests",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " alternative",
            " hypothesis",
            " is",
            " true",
            ".",
            "Effect",
            " size",
            "Several",
            " standardized",
            " measures",
            " of",
            " effect",
            " have",
            " been",
            " proposed",
            " for",
            " AN",
            "O",
            "VA",
            " to",
            " summarize",
            " the",
            " strength",
            " of",
            " the",
            " association",
            " between",
            " a",
            " predictor",
            "(s",
            ")",
            " and",
            " the",
            " dependent",
            " variable",
            " or",
            " the",
            " overall",
            " standardized",
            " difference",
            " of",
            " the",
            " complete",
            " model",
            ".",
            " Standard",
            "ized",
            " effect",
            "-size",
            " estimates",
            " facilitate",
            " comparison",
            " of",
            " findings",
            " across",
            " studies",
            " and",
            " disciplines",
            ".",
            " However",
            ",",
            " while"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "loyd",
            " from",
            " Forbidden",
            "\"The",
            " Animal",
            "\"",
            "\"The",
            " Animal",
            "\"",
            " (",
            "Dist",
            "urbed",
            " song",
            "),",
            " ",
            "201",
            "0",
            "\"The",
            " Animal",
            "\",",
            " by",
            " Steve",
            " V",
            "ai",
            " from",
            " Passion",
            " and",
            " Warfare",
            "Other",
            " uses",
            " AN",
            "IMAL",
            " (",
            "computer",
            " worm",
            "),",
            " an",
            " early",
            " self",
            "-re",
            "p",
            "lic",
            "ating",
            " computer",
            " program",
            " AN",
            "IMAL",
            " (",
            "image",
            " processing",
            "),",
            " an",
            " interactive",
            " software",
            " environment",
            " for",
            " image",
            " processing"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.131,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " a",
            " major",
            " advantage",
            " of",
            " multiple",
            " factor",
            " AN",
            "O",
            "VA",
            ".",
            " Testing",
            " one",
            " factor",
            " at",
            " a",
            " time",
            " hides",
            " interactions",
            ",",
            " but",
            " produces",
            " apparently",
            " inconsistent",
            " experimental",
            " results",
            ".",
            "Ca",
            "ution",
            " is",
            " advised",
            " when",
            " encountering",
            " interactions",
            ";",
            " Test",
            " interaction",
            " terms",
            " first",
            " and",
            " expand",
            " the",
            " analysis",
            " beyond",
            " AN",
            "O",
            "VA",
            " if",
            " interactions",
            " are",
            " found",
            ".",
            " Text",
            "s",
            " vary",
            " in",
            " their",
            " recommendations",
            " regarding",
            " the",
            " continuation",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " A",
            " relatively",
            " complete",
            " discussion",
            " of",
            " the",
            " analysis",
            " (",
            "models",
            ",",
            " data",
            " summaries",
            ",",
            " AN",
            "O",
            "VA",
            " table",
            ")",
            " of",
            " the",
            " completely",
            " randomized",
            " experiment",
            " is",
            " available",
            ".",
            "There",
            " are",
            " some",
            " alternatives",
            " to",
            " conventional",
            " one",
            "-way",
            " analysis",
            " of",
            " variance",
            ",",
            " e",
            ".g",
            ".:",
            " Welch",
            "'s",
            " heter",
            "os",
            "ced",
            "astic",
            " F",
            " test",
            ",",
            " Welch",
            "'s",
            " heter",
            "os",
            "ced",
            "astic",
            " F",
            " test",
            " with",
            " trimmed",
            " means",
            " and",
            " Wins"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.59,
            0.095,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " nan",
            "of",
            "abric",
            "ation",
            " technique",
            "B",
            "iology",
            " and",
            " healthcare",
            " Atlas",
            " (",
            "an",
            "atomy",
            "),",
            " part",
            " of",
            " the",
            " cervical",
            " spine",
            " Atlas",
            " personality",
            ",",
            " the",
            " personality",
            " of",
            " someone",
            " whose",
            " childhood",
            " was",
            " characterized",
            " by",
            " excessive",
            " responsibilities",
            " Brain",
            " atlas",
            ",",
            " a",
            " neuro",
            "an",
            "atom",
            "ical",
            " map",
            " of",
            " the",
            " brain",
            " of",
            " a",
            " human",
            " or",
            " other",
            " animal",
            "Anim",
            "als"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "ity",
            ",",
            " as",
            " an",
            " assumption",
            " for",
            " the",
            " normal",
            "-model",
            " analysis",
            " and",
            " as",
            " a",
            " consequence",
            " of",
            " random",
            "ization",
            " and",
            " add",
            "itivity",
            " for",
            " the",
            " random",
            "ization",
            "-based",
            " analysis",
            ".",
            "However",
            ",",
            " studies",
            " of",
            " processes",
            " that",
            " change",
            " var",
            "iances",
            " rather",
            " than",
            " means",
            " (",
            "called",
            " dispersion",
            " effects",
            ")",
            " have",
            " been",
            " successfully",
            " conducted",
            " using",
            " AN",
            "O",
            "VA",
            ".",
            " There",
            " are",
            " no",
            " necessary",
            " assumptions",
            " for",
            " AN",
            "O",
            "VA",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " variance",
            " (",
            "ANO",
            "VA",
            ")",
            " is",
            " a",
            " collection",
            " of",
            " statistical",
            " models",
            " and",
            " their",
            " associated",
            " estimation",
            " procedures",
            " (",
            "such",
            " as",
            " the",
            " \"",
            "variation",
            "\"",
            " among",
            " and",
            " between",
            " groups",
            ")",
            " used",
            " to",
            " analyze",
            " the",
            " differences",
            " among",
            " means",
            ".",
            " AN",
            "O",
            "VA",
            " was",
            " developed",
            " by",
            " the",
            " statistic",
            "ian",
            " Ronald",
            " Fisher",
            ".",
            " AN",
            "O",
            "VA",
            " is",
            " based",
            " on",
            " the",
            " law",
            " of",
            " total",
            " variance",
            ",",
            " where",
            " the",
            " observed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.4,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "RE",
            "OB",
            "\"",
            " or",
            " \"",
            "RE",
            "OB",
            "s",
            "\"",
            "the",
            " residue",
            " of",
            " recycled",
            " automotive",
            " engine",
            " oil",
            " collected",
            " from",
            " the",
            " bottoms",
            " of",
            " re",
            "-ref",
            "ining",
            " vacuum",
            " dist",
            "illation",
            " towers",
            ",",
            " in",
            " the",
            " manufacture",
            " of",
            " asphalt",
            ".",
            " RE",
            "OB",
            " contains",
            " various",
            " elements",
            " and",
            " compounds",
            " found",
            " in",
            " recycled",
            " engine",
            " oil",
            ":",
            " additives",
            " to",
            " the",
            " original",
            " oil",
            " and",
            " materials",
            " accumulating",
            " from",
            " its",
            " circulation",
            " in",
            " the",
            " engine",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " European",
            " Union",
            "'s",
            " European",
            " Ne",
            "ighbour",
            "hood",
            " Policy",
            " (",
            "EN",
            "P",
            ")",
            " which",
            " aims",
            " at",
            " bringing",
            " the",
            " EU",
            " and",
            " its",
            " neighbours",
            " closer",
            ".",
            "Giving",
            " incentives",
            " and",
            " rewarding",
            " best",
            " performers",
            ",",
            " as",
            " well",
            " as",
            " offering",
            " funds",
            " in",
            " a",
            " faster",
            " and",
            " more",
            " flexible",
            " manner",
            ",",
            " are",
            " the",
            " two",
            " main",
            " principles",
            " underlying",
            " the",
            " European",
            " Ne",
            "ighbour",
            "hood",
            " Instrument",
            " (",
            "EN",
            "I",
            ")",
            " that",
            " came",
            " into",
            " force"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " European",
            " Union",
            "'s",
            " European",
            " Ne",
            "ighbour",
            "hood",
            " Policy",
            " (",
            "EN",
            "P",
            ")",
            " which",
            " aims",
            " at",
            " bringing",
            " the",
            " EU",
            " and",
            " its",
            " neighbours",
            " closer",
            ".",
            "Giving",
            " incentives",
            " and",
            " rewarding",
            " best",
            " performers",
            ",",
            " as",
            " well",
            " as",
            " offering",
            " funds",
            " in",
            " a",
            " faster",
            " and",
            " more",
            " flexible",
            " manner",
            ",",
            " are",
            " the",
            " two",
            " main",
            " principles",
            " underlying",
            " the",
            " European",
            " Ne",
            "ighbour",
            "hood",
            " Instrument",
            " (",
            "EN",
            "I",
            ")",
            " that",
            " came",
            " into",
            " force"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " variance",
            " (",
            "ANO",
            "VA",
            ")",
            " is",
            " a",
            " collection",
            " of",
            " statistical",
            " models",
            " and",
            " their",
            " associated",
            " estimation",
            " procedures",
            " (",
            "such",
            " as",
            " the",
            " \"",
            "variation",
            "\"",
            " among",
            " and",
            " between",
            " groups",
            ")",
            " used",
            " to",
            " analyze",
            " the",
            " differences",
            " among",
            " means",
            ".",
            " AN",
            "O",
            "VA",
            " was",
            " developed",
            " by",
            " the",
            " statistic",
            "ian",
            " Ronald",
            " Fisher",
            ".",
            " AN",
            "O",
            "VA",
            " is",
            " based",
            " on",
            " the",
            " law",
            " of",
            " total",
            " variance",
            ",",
            " where",
            " the",
            " observed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " was",
            " theor",
            "ized",
            " to",
            " have",
            " been",
            " the",
            " regional",
            " tongue",
            " for",
            " many",
            " centuries",
            ".",
            " AN",
            "A",
            ",",
            " despite",
            " its",
            " name",
            ",",
            " was",
            " considered",
            " a",
            " very",
            " distinct",
            " language",
            ",",
            " and",
            " mutually",
            " unint",
            "ellig",
            "ible",
            ",",
            " from",
            " \"",
            "Ar",
            "abic",
            "\".",
            " Scholars",
            " named",
            " its",
            " variant",
            " dialect",
            "s",
            " after",
            " the",
            " towns",
            " where",
            " the",
            " ins",
            "criptions",
            " were",
            " discovered",
            " (",
            "D",
            "adan",
            "itic",
            ",",
            " Tay",
            "man",
            "itic",
            ",",
            " His"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "-h",
            "ypo",
            "thesis",
            " that",
            " all",
            " treatments",
            " have",
            " exactly",
            " the",
            " same",
            " effect",
            ")",
            " is",
            " recommended",
            " as",
            " a",
            " practical",
            " test",
            ",",
            " because",
            " of",
            " its",
            " robust",
            "ness",
            " against",
            " many",
            " alternative",
            " distributions",
            ".",
            "Extended",
            " algorithm",
            "ANO",
            "VA",
            " consists",
            " of",
            " separ",
            "able",
            " parts",
            ";",
            " partition",
            "ing",
            " sources",
            " of",
            " variance",
            " and",
            " hypothesis",
            " testing",
            " can",
            " be",
            " used",
            " individually",
            ".",
            " AN",
            "O",
            "VA",
            " is",
            " used",
            " to",
            " support",
            " other",
            " statistical",
            " tools"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " variance",
            " in",
            " a",
            " particular",
            " variable",
            " is",
            " partition",
            "ed",
            " into",
            " components",
            " attributable",
            " to",
            " different",
            " sources",
            " of",
            " variation",
            ".",
            " In",
            " its",
            " simplest",
            " form",
            ",",
            " AN",
            "O",
            "VA",
            " provides",
            " a",
            " statistical",
            " test",
            " of",
            " whether",
            " two",
            " or",
            " more",
            " population",
            " means",
            " are",
            " equal",
            ",",
            " and",
            " therefore",
            " general",
            "izes",
            " the",
            " t",
            "-test",
            " beyond",
            " two",
            " means",
            ".",
            " In",
            " other",
            " words",
            ",",
            " the",
            " AN",
            "O",
            "VA",
            " is",
            " used",
            " to",
            " test",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " called",
            " the",
            " mean",
            " square",
            " (",
            "MS",
            ")",
            " and",
            " the",
            " squared",
            " terms",
            " are",
            " deviations",
            " from",
            " the",
            " sample",
            " mean",
            ".",
            " AN",
            "O",
            "VA",
            " estimates",
            " ",
            "3",
            " sample",
            " var",
            "iances",
            ":",
            " a",
            " total",
            " variance",
            " based",
            " on",
            " all",
            " the",
            " observation",
            " deviations",
            " from",
            " the",
            " grand",
            " mean",
            ",",
            " an",
            " error",
            " variance",
            " based",
            " on",
            " all",
            " the",
            " observation",
            " deviations",
            " from",
            " their",
            " appropriate",
            " treatment",
            " means",
            ",",
            " and",
            " a",
            " treatment",
            " variance",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "ification",
            " by",
            " J",
            ".",
            " Marshall",
            " U",
            "nger",
            ".",
            " (",
            "199",
            "4",
            ")",
            " ",
            "Ag",
            "gl",
            "utin",
            "ative",
            " languages",
            "Central",
            " Asia",
            "Prop",
            "osed",
            " language",
            " families",
            "<|begin_of_text|>",
            "A",
            "ust",
            "rian",
            " German",
            " (),",
            " Austrian",
            " Standard",
            " German",
            " (",
            "AS",
            "G",
            "),",
            " Standard",
            " Austrian",
            " German",
            " (),",
            " Austrian",
            " High",
            " German",
            " (),",
            " or",
            " simply",
            " just",
            " Austrian",
            " (),",
            " is",
            " an",
            " official",
            " and",
            " standard",
            " variety",
            " of",
            " Standard",
            " High",
            " German",
            " written"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.551,
            0.076,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            0.112,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " AT",
            "LAS",
            " (",
            "simulation",
            ")",
            " (",
            "Army",
            " Tactical",
            " Level",
            " Advanced",
            " Simulation",
            "),",
            " a",
            " Thai",
            " military",
            " system",
            " Atlas",
            " (",
            "storm",
            "),",
            " which",
            " hit",
            " the",
            " Mid",
            "western",
            " United",
            " States",
            " in",
            " October",
            " ",
            "201",
            "3",
            ",",
            " named",
            " by",
            " The",
            " Weather",
            " Channel",
            " Agr",
            "up",
            "acin",
            " de",
            " Tr",
            "abaj",
            "adores",
            " Latino",
            "american",
            "os",
            " Sind",
            "ical",
            "istas",
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " ",
            "195",
            "0",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " from",
            " RF",
            "Z",
            " to",
            " Florian",
            "opol",
            "is",
            " Fr",
            "act",
            "ure",
            " Zone",
            " (",
            "FF",
            "Z",
            ",",
            " north",
            " of",
            " Wal",
            "vis",
            " Ridge",
            " and",
            " Rio",
            " Grande",
            " Rise",
            ");",
            " Southern",
            " segment",
            ",",
            " from",
            " FF",
            "Z",
            " to",
            " the",
            " Ag",
            "ul",
            "has",
            "-F",
            "alk",
            "land",
            " Fr",
            "act",
            "ure",
            " Zone",
            " (",
            "AFF",
            "Z",
            ");",
            " and",
            " Falk",
            "land",
            " segment",
            ",",
            " south",
            " of",
            " AFF",
            "Z",
            ".",
            "In",
            " the",
            " southern",
            " segment",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " skill",
            " of",
            " employing",
            " assist",
            "ive",
            " technology",
            " (",
            "AT",
            ")",
            " in",
            " the",
            " improvement",
            " and",
            " maintenance",
            " of",
            " optimal",
            ",",
            " functional",
            " participation",
            " in",
            " occupations",
            ".",
            " The",
            " application",
            " of",
            " AT",
            " enables",
            " an",
            " individual",
            " to",
            " adapt",
            " aspects",
            " of",
            " the",
            " environment",
            ",",
            " that",
            " may",
            " otherwise",
            " be",
            " challenging",
            ",",
            " to",
            " the",
            " user",
            " in",
            " order",
            " to",
            " optimize",
            " functional",
            " participation",
            " in",
            " those",
            " occupations",
            ".",
            " As",
            " a",
            " result",
            ",",
            " occupational",
            " therapists",
            " may",
            " educate"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            0.167,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "im",
            "ult",
            "aneous",
            " component",
            " analysis",
            "Analysis",
            " of",
            " covariance",
            " (",
            "AN",
            "CO",
            "VA",
            ")",
            "Analysis",
            " of",
            " molecular",
            " variance",
            " (",
            "AM",
            "O",
            "VA",
            ")",
            "Analysis",
            " of",
            " rhyth",
            "mic",
            " variance",
            " (",
            "AN",
            "OR",
            "VA",
            ")",
            "Expected",
            " mean",
            " squares",
            "Expl",
            "ained",
            " variation",
            "Linear",
            " trend",
            " estimation",
            "Mixed",
            "-design",
            " analysis",
            " of",
            " variance",
            "Mult",
            "ivariate",
            " analysis",
            " of",
            " covariance",
            " (",
            "MAN",
            "CO",
            "VA",
            ")",
            "Per",
            "mut"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            0.167,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "im",
            "ult",
            "aneous",
            " component",
            " analysis",
            "Analysis",
            " of",
            " covariance",
            " (",
            "AN",
            "CO",
            "VA",
            ")",
            "Analysis",
            " of",
            " molecular",
            " variance",
            " (",
            "AM",
            "O",
            "VA",
            ")",
            "Analysis",
            " of",
            " rhyth",
            "mic",
            " variance",
            " (",
            "AN",
            "OR",
            "VA",
            ")",
            "Expected",
            " mean",
            " squares",
            "Expl",
            "ained",
            " variation",
            "Linear",
            " trend",
            " estimation",
            "Mixed",
            "-design",
            " analysis",
            " of",
            " variance",
            "Mult",
            "ivariate",
            " analysis",
            " of",
            " covariance",
            " (",
            "MAN",
            "CO",
            "VA",
            ")",
            "Per",
            "mut"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " Telescope",
            " (",
            "AT",
            "LAST",
            ")",
            " ",
            " Alt",
            "as",
            " (",
            "dis",
            "amb",
            "ig",
            "uation",
            ")",
            " Atl",
            "ant",
            " (",
            "dis",
            "amb",
            "ig",
            "uation",
            ")",
            " ()",
            "<|begin_of_text|>",
            "M",
            "outh",
            "wash",
            ",",
            " mouth",
            " rinse",
            ",",
            " oral",
            " rinse",
            ",",
            " or",
            " mouth",
            " bath",
            " is",
            " a",
            " liquid",
            " which",
            " is",
            " held",
            " in",
            " the",
            " mouth",
            " pass",
            "ively",
            " or",
            " sw",
            "ir",
            "led",
            " around",
            " the",
            " mouth",
            " by",
            " contraction",
            " of",
            " the",
            " per",
            "ior",
            "al",
            " muscles"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            0.12,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            0.106,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " Atlas",
            ",",
            " a",
            " computer",
            " used",
            " at",
            " the",
            " Lawrence",
            " Liver",
            "more",
            " National",
            " Laboratory",
            " in",
            " ",
            "200",
            "6",
            " Abb",
            "rev",
            "iated",
            " Test",
            " Language",
            " for",
            " All",
            " Systems",
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " computer",
            " language",
            " for",
            " equipment",
            " testing",
            " Advanced",
            " Technology",
            " Leisure",
            " Application",
            " Simulator",
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " hydraulic",
            " motion",
            " simulator",
            " used",
            " in",
            " theme",
            " parks",
            " ASP",
            ".NET",
            " AJAX",
            " (",
            "formerly",
            " \"",
            "Atlas",
            "\"),",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            0.109,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            0.088,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Ple",
            "i",
            "ades",
            " Advanced",
            " Top",
            "ographic",
            " Laser",
            " Al",
            "tim",
            "eter",
            " System",
            " (",
            "AT",
            "LAS",
            "),",
            " a",
            " space",
            "-based",
            " lid",
            "ar",
            " instrument",
            " on",
            " IC",
            "ES",
            "at",
            "-",
            "2",
            " Aster",
            "oid",
            " Ter",
            "restrial",
            "-",
            "impact",
            " Last",
            " Alert",
            " System",
            " (",
            "AT",
            "LAS",
            ")",
            "Math",
            "ematics",
            " Atlas",
            " (",
            "top",
            "ology",
            "),",
            " a",
            " set",
            " of",
            " charts",
            " A",
            " set",
            " of",
            " charts",
            " which",
            " covers",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "38",
            ",",
            " COM",
            "T",
            ",",
            " DR",
            "D",
            "4",
            ",",
            " DR",
            "D",
            "5",
            ",",
            " I",
            "GF",
            "2",
            ",",
            " and",
            " G",
            "AB",
            "RB",
            "2",
            " are",
            " candidate",
            " genes",
            " for",
            " influencing",
            " altru",
            "istic",
            " behavior",
            ".",
            "Digital",
            " altru",
            "ism",
            "Digital",
            " altru",
            "ism",
            " is",
            " the",
            " notion",
            " that",
            " some",
            " are",
            " willing",
            " to",
            " freely",
            " share",
            " information",
            " based",
            " on",
            " the",
            " principle",
            " of",
            " recipro",
            "city",
            " and",
            " in",
            " the",
            " belief",
            " that",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " then",
            " every",
            " ",
            "6",
            "",
            "hours",
            " for",
            " the",
            " second",
            " day",
            ",",
            " and",
            " finally",
            " every",
            " ",
            "8",
            "",
            "hours",
            " for",
            " ",
            "8",
            " additional",
            " days",
            ".",
            " However",
            " the",
            " USA",
            "'s",
            " Agency",
            " for",
            " Toxic",
            " Sub",
            "stances",
            " and",
            " Disease",
            " Registry",
            " (",
            "AT",
            "SD",
            "R",
            ")",
            " states",
            " that",
            " the",
            " long",
            "-term",
            " effects",
            " of",
            " arsen",
            "ic",
            " exposure",
            " cannot",
            " be",
            " predicted",
            ".",
            " Blood",
            ",",
            " urine",
            ",",
            " hair",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            ".",
            "External",
            " links",
            " Americ",
            "ium",
            " at",
            " The",
            " Period",
            "ic",
            " Table",
            " of",
            " Videos",
            " (",
            "University",
            " of",
            " Nottingham",
            ")",
            " AT",
            "SD",
            "R",
            " ",
            " Public",
            " Health",
            " Statement",
            ":",
            " Americ",
            "ium",
            " World",
            " Nuclear",
            " Association",
            " ",
            " Smoke",
            " Det",
            "ectors",
            " and",
            " Americ",
            "ium",
            " ",
            " ",
            "Chem",
            "ical",
            " elements",
            "Chem",
            "ical",
            " elements",
            " with",
            " double",
            " hex",
            "agonal",
            " close",
            "-packed",
            " structure",
            "Act",
            "in",
            "ides",
            "C",
            "arc"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.555,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            0.223,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.171,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "/",
            "AS",
            "ME",
            " Y",
            "14",
            ".",
            "1",
            ").",
            "See",
            " also",
            " Accred",
            "ited",
            " Crane",
            " Operator",
            " Certification",
            " ANSI",
            " ASC",
            " X",
            "9",
            " ANSI",
            " ASC",
            " X",
            "12",
            " ANSI",
            " C",
            " Institute",
            " of",
            " Environmental",
            " Sciences",
            " and",
            " Technology",
            " (",
            "I",
            "EST",
            ")",
            " Institute",
            " of",
            " Nuclear",
            " Materials",
            " Management",
            " (",
            "IN",
            "MM",
            ")",
            " ISO",
            " (",
            "to",
            " which",
            " ANSI",
            " is",
            " the",
            " official",
            " US",
            " representative",
            ")",
            " National",
            " Information",
            " Standards"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " other",
            " care",
            ";",
            " or",
            " they",
            " can",
            " be",
            " advanced",
            " speech",
            " generating",
            " devices",
            ",",
            " based",
            " on",
            " speech",
            " synthesis",
            ",",
            " that",
            " are",
            " capable",
            " of",
            " storing",
            " hundreds",
            " of",
            " phrases",
            " and",
            " words",
            ".",
            "C",
            "ognitive",
            " impair",
            "ments",
            "Ass",
            "ist",
            "ive",
            " Technology",
            " for",
            " C",
            "ognition",
            " (",
            "AT",
            "C",
            ")",
            " is",
            " the",
            " use",
            " of",
            " technology",
            " (",
            "usually",
            " high",
            " tech",
            ")",
            " to",
            " augment",
            " and",
            " assist",
            " cognitive",
            " processes",
            " such",
            " as",
            " attention"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " and",
            " sustainable",
            " agriculture",
            " movements",
            ".",
            " One",
            " of",
            " the",
            " major",
            " forces",
            " behind",
            " this",
            " movement",
            " has",
            " been",
            " the",
            " European",
            " Union",
            ",",
            " which",
            " first",
            " certified",
            " organic",
            " food",
            " in",
            " ",
            "199",
            "1",
            " and",
            " began",
            " reform",
            " of",
            " its",
            " Common",
            " Agricultural",
            " Policy",
            " (",
            "CAP",
            ")",
            " in",
            " ",
            "200",
            "5",
            " to",
            " phase",
            " out",
            " commodity",
            "-linked",
            " farm",
            " subsidies",
            ",",
            " also",
            " known",
            " as",
            " dec",
            "ou",
            "pling",
            ".",
            " The",
            " growth",
            " of",
            " organic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " its",
            " full",
            " gener",
            "ality",
            ",",
            " but",
            " the",
            " F",
            "-test",
            " used",
            " for",
            " AN",
            "O",
            "VA",
            " hypothesis",
            " testing",
            " has",
            " assumptions",
            " and",
            " practical",
            " ",
            "limitations",
            " which",
            " are",
            " of",
            " continuing",
            " interest",
            ".",
            "Pro",
            "blems",
            " which",
            " do",
            " not",
            " satisfy",
            " the",
            " assumptions",
            " of",
            " AN",
            "O",
            "VA",
            " can",
            " often",
            " be",
            " transformed",
            " to",
            " satisfy",
            " the",
            " assumptions",
            ".",
            " ",
            "The",
            " property",
            " of",
            " unit",
            "-t",
            "reatment",
            " add",
            "itivity",
            " is",
            " not",
            " invariant",
            " under",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Alaska",
            ",",
            " and",
            " today",
            " over",
            " ",
            "1",
            ".",
            "4",
            " million",
            " people",
            " visit",
            " the",
            " state",
            " each",
            " year",
            ".",
            "With",
            " tourism",
            " more",
            " vital",
            " to",
            " the",
            " economy",
            ",",
            " environmental",
            "ism",
            " also",
            " rose",
            " in",
            " importance",
            ".",
            " The",
            " Alaska",
            " National",
            " Interest",
            " Lands",
            " Conservation",
            " Act",
            " (",
            "AN",
            "IL",
            "CA",
            ")",
            " of",
            " ",
            "198",
            "0",
            " added",
            " ",
            "53",
            ".",
            "7",
            " million",
            " acres",
            " (",
            "217",
            ",",
            "000",
            "",
            "km",
            "2"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " alle",
            "lic",
            " polym",
            "orphism",
            " of",
            " the",
            " cate",
            "ch",
            "ol",
            "-O",
            "-m",
            "ethyl",
            "transfer",
            "ase",
            " (",
            "COM",
            "T",
            ")",
            " gene",
            ";",
            " increased",
            " ad",
            "enos",
            "ine",
            " receptor",
            " function",
            ";",
            " increased",
            " cortisol",
            ".",
            "In",
            " the",
            " central",
            " nervous",
            " system",
            " (",
            "C",
            "NS",
            "),",
            " the",
            " major",
            " medi",
            "ators",
            " of",
            " the",
            " symptoms",
            " of",
            " anxiety",
            " disorders",
            " appear",
            " to",
            " be",
            " n",
            "ore",
            "pine",
            "ph",
            "rine",
            ",",
            " serotonin",
            ",",
            " dopamine",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "ISS",
            ")",
            " Jared",
            " Isaac",
            "man",
            " (",
            "American",
            "):",
            " ",
            "15",
            "",
            "18",
            " September",
            " ",
            "202",
            "1",
            " (",
            "Free",
            " F",
            "lier",
            ")",
            " Yus",
            "aku",
            " Mae",
            "z",
            "awa",
            " (",
            "Japanese",
            "):",
            " ",
            "8",
            " ",
            " ",
            "24",
            " December",
            " ",
            "202",
            "1",
            " (",
            "ISS",
            ")",
            "Training",
            "The",
            " first",
            " NASA",
            " astronauts",
            " were",
            " selected",
            " for",
            " training",
            " in",
            " ",
            "195",
            "9",
            ".",
            " Early",
            " in",
            " the",
            " space",
            " program",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            "em",
            "is",
            "pherical",
            " reflect",
            "ance",
            " and",
            " bi",
            "-h",
            "em",
            "is",
            "pherical",
            " reflect",
            "ance",
            " (",
            "e",
            ".g",
            ".,",
            ").",
            " These",
            " calculations",
            " are",
            " based",
            " on",
            " the",
            " bid",
            "irectional",
            " reflect",
            "ance",
            " distribution",
            " function",
            " (",
            "BR",
            "DF",
            "),",
            " which",
            " describes",
            " how",
            " the",
            " reflect",
            "ance",
            " of",
            " a",
            " given",
            " surface",
            " depends",
            " on",
            " the",
            " view",
            " angle",
            " of",
            " the",
            " observer",
            " and",
            " the",
            " solar",
            " angle",
            ".",
            " B",
            "DR",
            "F",
            " can",
            " facilitate",
            " translations"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "step",
            "-by",
            "-step",
            " procedure",
            "\"",
            " in",
            " English",
            ".",
            "In",
            " ",
            "184",
            "2",
            ",",
            " in",
            " the",
            " Dictionary",
            " of",
            " Science",
            ",",
            " Literature",
            " and",
            " Art",
            ",",
            " it",
            " says",
            ":",
            "AL",
            "GORITHM",
            ",",
            " signifies",
            " the",
            " art",
            " of",
            " computing",
            " in",
            " reference",
            " to",
            " some",
            " particular",
            " subject",
            ",",
            " or",
            " in",
            " some",
            " particular",
            " way",
            ";",
            " as",
            " the",
            " algorithm",
            " of",
            " numbers",
            ";",
            " the",
            " algorithm",
            " of",
            " the",
            " differential",
            " calculus",
            ".",
            "Machine",
            " usage"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.244,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " R",
            "1",
            " in",
            " ",
            "201",
            "0",
            ".",
            " In",
            " ",
            "200",
            "6",
            ",",
            " Kok",
            "oro",
            " developed",
            " a",
            " new",
            " DER",
            " ",
            "2",
            " android",
            ".",
            " The",
            " height",
            " of",
            " the",
            " human",
            " body",
            " part",
            " of",
            " DER",
            "2",
            " is",
            " ",
            "165",
            "",
            "cm",
            ".",
            " There",
            " are",
            " ",
            "47",
            " mobile",
            " points",
            ".",
            " DER",
            "2",
            " can",
            " not",
            " only",
            " change",
            " its",
            " expression",
            " but",
            " also",
            " move",
            " its",
            " hands",
            " and",
            " feet",
            " and",
            " twist",
            " its"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            "rophy",
            " and",
            " fasc",
            "ic",
            "ulations",
            ",",
            " and",
            " upper",
            " motor",
            " neuron",
            " (",
            "UM",
            "N",
            ")",
            " findings",
            " include",
            " hyper",
            "ref",
            "lex",
            "ia",
            ",",
            " sp",
            "astic",
            "ity",
            ",",
            " muscle",
            " sp",
            "asm",
            ",",
            " and",
            " abnormal",
            " reflex",
            "es",
            ".",
            "Pure",
            " upper",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just",
            " U",
            "MN",
            " findings",
            ",",
            " include",
            " P",
            "LS",
            ".",
            "Pure",
            " lower",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.35,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.03,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " END",
            ",",
            " STOP",
            ".",
            "An",
            " elegant",
            " program",
            " for",
            " Eu",
            "clid",
            "'s",
            " algorithm",
            " ",
            " The",
            " flow",
            "chart",
            " of",
            " \"",
            "E",
            "legant",
            "\"",
            " can",
            " be",
            " found",
            " at",
            " the",
            " top",
            " of",
            " this",
            " article",
            ".",
            " In",
            " the",
            " (",
            "un",
            "structured",
            ")",
            " Basic",
            " language",
            ",",
            " the",
            " steps",
            " are",
            " numbered",
            ",",
            " and",
            " the",
            " instruction",
            " LET",
            " []",
            " =",
            " []",
            " is",
            " the",
            " assignment",
            " instruction",
            " symbol",
            "ized",
            " by",
            " ",
            ".",
            "5"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.127,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " REM",
            " Eu",
            "clid",
            "'s",
            " algorithm",
            " for",
            " greatest",
            " common",
            " divisor",
            "6",
            " PRINT",
            " \"",
            "Type",
            " two",
            " integers",
            " greater",
            " than",
            " ",
            "0",
            "\"",
            "10",
            " INPUT",
            " A",
            ",B",
            "20",
            " IF",
            " B",
            "=",
            "0",
            " THEN",
            " G",
            "OTO",
            " ",
            "80",
            "30",
            " IF",
            " A",
            " >",
            " B",
            " THEN",
            " G",
            "OTO",
            " ",
            "60",
            "40",
            " LET",
            " B",
            "=B",
            "-A",
            "50",
            " G",
            "OTO",
            " ",
            "20",
            "60",
            " LET",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.248,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " an",
            " attempt",
            " to",
            " draw",
            " foreign",
            " investment",
            ".",
            " By",
            " May",
            " ",
            "199",
            "1",
            " it",
            " reached",
            " a",
            " peace",
            " agreement",
            " with",
            " UNIT",
            "A",
            ",",
            " the",
            " B",
            "ices",
            "se",
            " Acc",
            "ords",
            ",",
            " which",
            " scheduled",
            " new",
            " general",
            " elections",
            " for",
            " September",
            " ",
            "199",
            "2",
            ".",
            " When",
            " the",
            " MPL",
            "A",
            " secured",
            " a",
            " major",
            " electoral",
            " victory",
            ",",
            " UNIT",
            "A",
            " objected",
            " to",
            " the",
            " results",
            " of",
            " both",
            " the",
            " presidential",
            " and",
            " legislative",
            " vote"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " alumni",
            "Ac",
            "ademic",
            " staff",
            " of",
            " ETH",
            " Zurich",
            "European",
            " democratic",
            " social",
            "ists",
            "German",
            " ag",
            "nost",
            "ics",
            "German",
            " Ash",
            "ken",
            "azi",
            " Jews",
            "German",
            " em",
            "igrants",
            " to",
            " Switzerland",
            "German",
            " human",
            "ists",
            "19",
            "th",
            "-century",
            " German",
            " Jews",
            "German",
            " Nobel",
            " laure",
            "ates",
            "German",
            " rel",
            "ativity",
            " theorists",
            "In",
            "stitute",
            " for",
            " Advanced",
            " Study",
            " faculty",
            "J",
            "ewish",
            " ag",
            "nost",
            "ics"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            0.228,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " Information",
            " k",
            "ios",
            "ks",
            " etc",
            ".",
            " The",
            " concept",
            " is",
            " encompass",
            "ed",
            " by",
            " the",
            " C",
            "EN",
            " EN",
            " ",
            "133",
            "2",
            "-",
            "4",
            " Identification",
            " Card",
            " Systems",
            " ",
            " Man",
            "-M",
            "achine",
            " Interface",
            ".",
            " This",
            " development",
            " of",
            " this",
            " standard",
            " has",
            " been",
            " supported",
            " in",
            " Europe",
            " by",
            " SN",
            "API",
            " and",
            " has",
            " been",
            " successfully",
            " incorporated",
            " into",
            " the",
            " L",
            "asse",
            "o",
            " specifications",
            ",",
            " but",
            " with",
            " limited",
            " success",
            " due",
            " to",
            " the",
            " lack"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " the",
            " UK",
            ",",
            " increasing",
            " growing",
            " speculation",
            " that",
            " the",
            " group",
            " were",
            " simply",
            " a",
            " Euro",
            "vision",
            " one",
            "-hit",
            " wonder",
            ".",
            "Post",
            "-E",
            "urovision",
            " ",
            "In",
            " November",
            " ",
            "197",
            "4",
            ",",
            " AB",
            "BA",
            " embarked",
            " on",
            " their",
            " first",
            " European",
            " tour",
            ",",
            " playing",
            " dates",
            " in",
            " Denmark",
            ",",
            " West",
            " Germany",
            " and",
            " Austria",
            ".",
            " It",
            " was",
            " not",
            " as",
            " successful",
            " as",
            " the",
            " band",
            " had",
            " hoped",
            ",",
            " since",
            " most",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.206,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " searches",
            " are",
            " expected",
            " to",
            " detect",
            " more",
            " objects",
            " because",
            " there",
            " is",
            " no",
            " atmosphere",
            " to",
            " interfere",
            " and",
            " because",
            " they",
            " can",
            " observe",
            " larger",
            " portions",
            " of",
            " the",
            " sky",
            ".",
            " NE",
            "OW",
            "ISE",
            " observed",
            " more",
            " than",
            " ",
            "100",
            ",",
            "000",
            " asteroids",
            " of",
            " the",
            " main",
            " belt",
            ",",
            " Sp",
            "itzer",
            " Space",
            " Telescope",
            " observed",
            " more",
            " than",
            " ",
            "700",
            " near",
            "-E",
            "arth",
            " asteroids",
            ".",
            " These",
            " observations",
            " determined",
            " rough",
            " sizes",
            " of",
            " the",
            " majority"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.172,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " the",
            " molecule",
            " is",
            " classified",
            " separately",
            " as",
            " a",
            " phen",
            "ol",
            " and",
            " is",
            " named",
            " using",
            " the",
            " I",
            "UP",
            "AC",
            " rules",
            " for",
            " naming",
            " phen",
            "ols",
            ".",
            " Phen",
            "ols",
            " have",
            " distinct",
            " properties",
            " and",
            " are",
            " not",
            " classified",
            " as",
            " al",
            "coh",
            "ols",
            ".",
            "Common",
            " names",
            "In",
            " other",
            " less",
            " formal",
            " contexts",
            ",",
            " an",
            " alcohol",
            " is",
            " often",
            " called",
            " with",
            " the",
            " name",
            " of",
            " the",
            " corresponding",
            " alk",
            "yl",
            " group",
            " followed",
            " by"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.138,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.163,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " meanwhile",
            ",",
            " started",
            " song",
            "writing",
            " sessions",
            " for",
            " the",
            " musical",
            " Chess",
            ".",
            " In",
            " interviews",
            " at",
            " the",
            " time",
            ",",
            " Bj",
            "r",
            "n",
            " and",
            " Benny",
            " denied",
            " the",
            " split",
            " of",
            " AB",
            "BA",
            " (\"",
            "Who",
            " are",
            " we",
            " without",
            " our",
            " ladies",
            "?",
            " Initial",
            "s",
            " of",
            " Br",
            "igit",
            "te",
            " Bard",
            "ot",
            "?",
            "\"),",
            " and",
            " Ly",
            "ng",
            "stad",
            " and",
            " F",
            "l",
            "ts",
            "k",
            "og",
            " kept",
            " claiming",
            " in",
            " interviews",
            " that",
            " AB",
            "BA"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " project",
            " had",
            " been",
            " delayed",
            ".",
            " Five",
            " out",
            " of",
            " the",
            " eight",
            " original",
            " songs",
            " written",
            " by",
            " Benny",
            " for",
            " the",
            " new",
            " album",
            " had",
            " been",
            " recorded",
            " by",
            " the",
            " two",
            " female",
            " members",
            ",",
            " and",
            " the",
            " release",
            " of",
            " a",
            " new",
            " ",
            "15",
            "",
            "million",
            " music",
            " video",
            " with",
            " new",
            " unseen",
            " technology",
            " was",
            " under",
            " consideration",
            ".",
            " In",
            " May",
            " ",
            "202",
            "0",
            ",",
            " it",
            " was",
            " announced",
            " that",
            " AB",
            "BA",
            "'s",
            " entire",
            " studio"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " the",
            " British",
            " Army",
            " in",
            " January",
            " ",
            "191",
            "6",
            ",",
            " for",
            " the",
            " Great",
            " War",
            ";",
            " however",
            ",",
            " he",
            " was",
            " rejected",
            " on",
            " health",
            " grounds",
            ",",
            " being",
            " half",
            "-blind",
            " in",
            " one",
            " eye",
            ".",
            " His",
            " eyes",
            "ight",
            " later",
            " partly",
            " recovered",
            ".",
            " He",
            " edited",
            " Oxford",
            " Poetry",
            " in",
            " ",
            "191",
            "6",
            ",",
            " and",
            " in",
            " June",
            " of",
            " that",
            " year",
            " graduated",
            " BA",
            " with",
            " first",
            " class",
            " hon",
            "ours",
            ".",
            " His",
            " brother"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " he",
            " was",
            " expected",
            " to",
            " call",
            " on",
            " him",
            " in",
            " the",
            " morning",
            " for",
            " a",
            " walk",
            " or",
            " chat",
            ",",
            " to",
            " provide",
            " consultation",
            " on",
            " architectural",
            " matters",
            ",",
            " and",
            " to",
            " discuss",
            " Hitler",
            "'s",
            " ideas",
            ".",
            " Most",
            " days",
            " he",
            " was",
            " invited",
            " to",
            " dinner",
            ".",
            "In",
            " the",
            " English",
            " version",
            " of",
            " his",
            " memoir",
            "s",
            ",",
            " Spe",
            "er",
            " says",
            " that",
            " his",
            " political",
            " commitment",
            " merely",
            " consisted",
            " of",
            " paying",
            " his",
            " \"",
            "monthly",
            " dues"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " yet",
            " as",
            " of",
            " ",
            "201",
            "9",
            ",",
            " and",
            " they",
            " may",
            " or",
            " may",
            " not",
            " be",
            " able",
            " to",
            " exist",
            ".",
            " In",
            " periods",
            " ",
            "8",
            " and",
            " above",
            " of",
            " the",
            " periodic",
            " table",
            ",",
            " relativ",
            "istic",
            " and",
            " shell",
            "-",
            "structure",
            " effects",
            " become",
            " so",
            " strong",
            " that",
            " extrapol",
            "ations",
            " from",
            " lighter",
            " congen",
            "ers",
            " become",
            " completely",
            " inaccurate",
            ".",
            " In",
            " addition",
            ",",
            " the",
            " relativ",
            "istic",
            " and",
            " shell",
            "-",
            "structure",
            " effects",
            " (",
            "which"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " but",
            " the",
            " origins",
            " of",
            " the",
            " strategy",
            " can",
            " be",
            " traced",
            " back",
            " to",
            " the",
            " ",
            "196",
            "0",
            "s",
            " with",
            " the",
            " proliferation",
            " of",
            " anime",
            ",",
            " with",
            " its",
            " inter",
            "connection",
            " of",
            " media",
            " and",
            " commodity",
            " goods",
            ".",
            "A",
            " number",
            " of",
            " anime",
            " and",
            " manga",
            " media",
            " franchises",
            " such",
            " as",
            " Demon",
            " Slayer",
            ":",
            " Kim",
            "ets",
            "u",
            " no",
            " Ya",
            "iba",
            ",",
            " Dragon",
            " Ball",
            " and",
            " Gundam",
            " have",
            " gained",
            " considerable",
            " global",
            " popularity",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " pig",
            "ments",
            ".",
            " In",
            " As",
            "4",
            "S",
            "10",
            ",",
            " arsen",
            "ic",
            " has",
            " a",
            " formal",
            " oxidation",
            " state",
            " of",
            " +",
            "2",
            " in",
            " As",
            "4",
            "S",
            "4",
            " which",
            " features",
            " As",
            "-",
            "As",
            " bonds",
            " so",
            " that",
            " the",
            " total",
            " co",
            "val",
            "ency",
            " of",
            " As",
            " is",
            " still",
            " ",
            "3",
            ".",
            " Both",
            " or",
            "p",
            "iment",
            " and",
            " real",
            "gar",
            ",",
            " as",
            " well",
            " as",
            " As",
            "4",
            "S",
            "3",
            ",",
            " have",
            " selenium",
            " analog"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "Enlarge",
    "iments",
    "iment",
    "quette",
    " gratuiti"
  ],
  "bottom_logits": [
    " Singer",
    "itchens",
    " Tod",
    "skin",
    " Heritage"
  ],
  "act_min": -0.0,
  "act_max": 0.613
}