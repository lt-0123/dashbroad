{
  "index": 44597,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ij",
            "eno",
            "ort",
            ",",
            " ",
            "200",
            "2",
            ".",
            " From",
            " Fre",
            "ge",
            " to",
            " GÃ¶",
            "del",
            ":",
            " A",
            " Source",
            " Book",
            " in",
            " Mathematical",
            " Logic",
            ",",
            " ",
            "187",
            "9",
            "–",
            "193",
            "1",
            ".",
            " New",
            " edition",
            ".",
            " Harvard",
            " University",
            " Press",
            ".",
            " ",
            "190",
            "4",
            ".",
            " \"",
            "Proof",
            " that",
            " every",
            " set",
            " can",
            " be",
            " well",
            "-",
            "ordered",
            ",\"",
            " ",
            "139",
            "-",
            "41",
            ".",
            "190",
            "8",
            ".",
            " \"",
            "Invest",
            "ig",
            "ations"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " k",
            "x",
            ")",
            " *",
            " (",
            "iy",
            " -",
            " ky",
            ")",
            "   ",
            " return",
            " (",
            "Ex",
            "y",
            " -",
            " Ex",
            " *",
            " Ey",
            " /",
            " n",
            ")",
            " /",
            " n",
            "Two",
            "-pass",
            "The",
            " two",
            "-pass",
            " algorithm",
            " first",
            " computes",
            " the",
            " sample",
            " means",
            ",",
            " and",
            " then",
            " the",
            " covariance",
            ":",
            "The",
            " two",
            "-pass",
            " algorithm",
            " may",
            " be",
            " written",
            " as",
            ":",
            "def",
            " two",
            "_pass",
            "_cov",
            "ariance",
            "(data",
            "1",
            ",",
            " data",
            "2",
            "):",
            "   "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " these",
            " are",
            " the",
            " same",
            " as",
            " the",
            " ASCII",
            " set",
            ".",
            "The",
            " Internet",
            " Assigned",
            " Numbers",
            " Authority",
            " (",
            "IAN",
            "A",
            ")",
            " prefers",
            " the",
            " name",
            " US",
            "-",
            "ASCII",
            " for",
            " this",
            " character",
            " encoding",
            ".",
            " ",
            "ASCII",
            " is",
            " one",
            " of",
            " the",
            " IEEE",
            " milestones",
            ".",
            "Overview",
            "ASCII",
            " was",
            " developed",
            " from",
            " tele",
            "graph",
            " code",
            ".",
            " Its",
            " first",
            " commercial",
            " use",
            " was",
            " in",
            " the",
            " Te",
            "let",
            "ype",
            " Model",
            " ",
            "33",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.66,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.068,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ih",
            "ai",
            " Sto",
            "ica",
            ",",
            " Romanian",
            " football",
            "er",
            " and",
            " manager",
            "196",
            "6",
            " –",
            " N",
            "ils",
            "-O",
            "lav",
            " Joh",
            "ansen",
            ",",
            " Norwegian",
            " guitarist",
            " and",
            " singer",
            " ",
            " ",
            " ",
            "196",
            "6",
            "  ",
            " –",
            " Lorenzo",
            " White",
            ",",
            " American",
            " football",
            " player",
            "196",
            "7",
            " –",
            " Sarah",
            " Crack",
            "nell",
            ",",
            " English",
            " singer",
            "-song",
            "writer",
            " ",
            "196",
            "8",
            " –",
            " Alicia",
            " Copp",
            "ola",
            ",",
            " American",
            " actress",
            " ",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " small",
            " bodies",
            " orbit",
            "ing",
            " the",
            " Sun",
            " were",
            " classified",
            " as",
            " com",
            "ets",
            ",",
            " asteroids",
            ",",
            " or",
            " meteor",
            "oids",
            ",",
            " with",
            " anything",
            " smaller",
            " than",
            " one",
            " meter",
            " across",
            " being",
            " called",
            " a",
            " meteor",
            "oid",
            ".",
            " The",
            " term",
            " asteroid",
            " never",
            " had",
            " a",
            " formal",
            " definition",
            ",",
            " with",
            " the",
            " broader",
            " term",
            " small",
            " Solar",
            " System",
            " bodies",
            " being",
            " preferred",
            " by",
            " the",
            " International",
            " Astr",
            "onom",
            "ical",
            " Union",
            " (",
            "IA",
            "U",
            ").",
            " As",
            " no"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " ranking",
            " it",
            " ",
            "127",
            "th",
            " globally",
            " out",
            " of",
            " ",
            "172",
            " countries",
            ".",
            "Important",
            " Bird",
            " Area",
            "The",
            " whole",
            " country",
            " has",
            " been",
            " recognised",
            " as",
            " a",
            " single",
            " Important",
            " Bird",
            " Area",
            " (",
            "IB",
            "A",
            ")",
            " by",
            " Bird",
            "Life",
            " International",
            ",",
            " because",
            " it",
            " is",
            " important",
            " for",
            " forest",
            " and",
            " mountain",
            " birds",
            " and",
            " supports",
            " populations",
            " of",
            " red",
            "-b",
            "illed",
            " ch",
            "ough",
            "s",
            ",",
            " cit",
            "ril",
            " fin",
            "ches",
            " and",
            " rock"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iam",
            ",",
            " after",
            " seven",
            " days",
            "'",
            " quarantine",
            ",",
            " was",
            " healed",
            ".",
            " Aaron",
            " once",
            " again",
            " escaped",
            " any",
            " re",
            "tribution",
            ".",
            "According",
            " to",
            " Numbers",
            " ",
            "16",
            "–",
            "17",
            ",",
            " a",
            " Lev",
            "ite",
            " named",
            " Kor",
            "ah",
            " led",
            " many",
            " in",
            " challenging",
            " Aaron",
            "'s",
            " exclusive",
            " claim",
            " to",
            " the",
            " priesthood",
            ".",
            " When",
            " the",
            " rebels",
            " were",
            " punished",
            " by",
            " being",
            " swallowed",
            " up",
            " by",
            " the",
            " earth",
            ",",
            " Ele",
            "azar",
            ",",
            " the",
            " son"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iam",
            " segment",
            " was",
            " introduced",
            ",",
            " honoring",
            " those",
            " who",
            " had",
            " made",
            " a",
            " significant",
            " contribution",
            " to",
            " cinema",
            " who",
            " had",
            " died",
            " in",
            " the",
            " preceding",
            " ",
            "12",
            " months",
            ",",
            " a",
            " selection",
            " compiled",
            " by",
            " a",
            " small",
            " committee",
            " of",
            " Academy",
            " members",
            ".",
            " This",
            " segment",
            " has",
            " drawn",
            " criticism",
            " over",
            " the",
            " years",
            " for",
            " the",
            " omission",
            " of",
            " some",
            " names",
            ".",
            " Crit",
            "icism",
            " was",
            " also",
            " lev",
            "ied",
            " for",
            " many",
            " years",
            " regarding",
            " another",
            " aspect"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iet",
            " bias",
            " was",
            " unacceptable",
            ",",
            " and",
            " the",
            " choice",
            " of",
            " pigs",
            " as",
            " the",
            " dominant",
            " class",
            " was",
            " thought",
            " to",
            " be",
            " especially",
            " offensive",
            ".",
            " It",
            " may",
            " reasonably",
            " be",
            " assumed",
            " that",
            " the",
            " \"",
            "important",
            " official",
            "\"",
            " was",
            " a",
            " man",
            " named",
            " Peter",
            " Sm",
            "ol",
            "lett",
            ",",
            " who",
            " was",
            " later",
            " un",
            "masked",
            " as",
            " a",
            " Soviet",
            " agent",
            ".",
            " Orwell",
            " was",
            " suspicious",
            " of",
            " Sm",
            "ol",
            "lett",
            "/",
            "Sm",
            "ol",
            "ka",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iar",
            "id",
            " climates",
            " due",
            " to",
            " the",
            " presence",
            " of",
            " a",
            " large",
            " body",
            " of",
            " water",
            ".",
            " This",
            " is",
            " most",
            " predominant",
            " in",
            " the",
            " west",
            " and",
            " east",
            " co",
            "asts",
            " of",
            " the",
            " Ae",
            "ge",
            "an",
            ",",
            " and",
            " within",
            " the",
            " Ae",
            "ge",
            "an",
            " islands",
            ".",
            " In",
            " the",
            " north",
            " of",
            " the",
            " Ae",
            "ge",
            "an",
            " Sea",
            ",",
            " the",
            " climate",
            " is",
            " instead",
            " classified",
            " as",
            " Cold",
            " semi",
            "-ar",
            "id",
            " (",
            "BS",
            "k",
            "),"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.198,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            0.387,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " FM",
            " (",
            "Frequency",
            " Mod",
            "ulation",
            ")",
            " transmission",
            ",",
            " IR",
            " (",
            "Inf",
            "ra",
            " Red",
            ")",
            " transmission",
            ",",
            " IL",
            " (",
            "Ind",
            "uction",
            " Loop",
            ")",
            " transmission",
            ",",
            " or",
            " other",
            " transmission",
            " methods",
            ".",
            " The",
            " person",
            " who",
            " is",
            " listening",
            " may",
            " use",
            " an",
            " FM",
            "/",
            "IR",
            "/",
            "IL",
            " Receiver",
            " to",
            " tune",
            " into",
            " the",
            " signal",
            " and",
            " listen",
            " at",
            " his",
            "/her",
            " preferred",
            " volume",
            ".",
            "Am",
            "pl",
            "ified",
            " telephone",
            " equipment",
            " ",
            "This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            ".",
            " Neu",
            "ropsych",
            "ological",
            " symptoms",
            " may",
            " include",
            " sense",
            " loss",
            ",",
            " difficulty",
            " in",
            " proprio",
            "ception",
            ",",
            " poor",
            " balance",
            ",",
            " loss",
            " of",
            " sensation",
            " in",
            " the",
            " feet",
            ",",
            " changes",
            " in",
            " reflex",
            "es",
            ",",
            " dementia",
            ",",
            " and",
            " psychosis",
            ",",
            " can",
            " be",
            " reversible",
            " with",
            " treatment",
            ".",
            " Comp",
            "lications",
            " may",
            " include",
            " a",
            " neurological",
            " complex",
            " known",
            " as",
            " sub",
            "acute",
            " combined",
            " deg",
            "eneration",
            " of",
            " spinal",
            " cord",
            ",",
            " and",
            " other",
            " neurological"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.069,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            ",",
            " Anglo",
            "-S",
            "axon",
            " England",
            ",",
            " Medieval",
            " Iceland",
            ",",
            " the",
            " American",
            " Old",
            " West",
            ",",
            " Gael",
            "ic",
            " Ireland",
            ",",
            " and",
            " merchant",
            " law",
            ",",
            " admir",
            "alty",
            " law",
            ",",
            " and",
            " early",
            " common",
            " law",
            ".",
            "An",
            "ar",
            "cho",
            "-capital",
            "ism",
            " is",
            " distinguished",
            " from",
            " min",
            "arch",
            "ism",
            ",",
            " which",
            " advocates",
            " a",
            " night",
            "-watch",
            "man",
            " state",
            " limited",
            " to",
            " protecting",
            " individuals",
            " from",
            " aggression",
            " and",
            " enforcing",
            " private",
            " property",
            ".",
            " Unlike"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.093,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " between",
            " ",
            "13",
            ",",
            "000",
            " and",
            " ",
            "11",
            ",",
            "000",
            " years",
            " ago",
            ".",
            " C",
            "attle",
            " were",
            " domestic",
            "ated",
            " from",
            " the",
            " wild",
            " a",
            "uro",
            "chs",
            " in",
            " the",
            " areas",
            " of",
            " modern",
            " Turkey",
            " and",
            " Pakistan",
            " some",
            " ",
            "10",
            ",",
            "500",
            " years",
            " ago",
            ".",
            " Pig",
            " production",
            " emerged",
            " in",
            " Euras",
            "ia",
            ",",
            " including",
            " Europe",
            ",",
            " East",
            " Asia",
            " and",
            " Southwest",
            " Asia",
            ",",
            " where",
            " wild",
            " bo",
            "ar",
            " were",
            " first"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " de",
            " Sant",
            " Este",
            "ve",
            ",",
            " Sant",
            " Joan",
            " de",
            " Cas",
            "elles",
            ",",
            " E",
            "sg",
            "l",
            "Ã©s",
            "ia",
            " de",
            " Sant",
            " M",
            "iqu",
            "el",
            " d",
            "'",
            "Eng",
            "ol",
            "asters",
            ",",
            " Sant",
            " Mart",
            "ÃŃ",
            " de",
            " la",
            " Cort",
            "in",
            "ada",
            " and",
            " the",
            " medieval",
            " bridges",
            " of",
            " Marg",
            "ined",
            "a",
            " and",
            " Esc",
            "alls",
            " among",
            " many",
            " others",
            ".",
            "The",
            " Catalan",
            " Py",
            "rene",
            "es",
            " were",
            " embry",
            "onic",
            " of",
            " the",
            " Catalan",
            " language"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " has",
            " been",
            " categor",
            "ised",
            " by",
            " the",
            " US",
            " government",
            " funded",
            " Freedom",
            " House",
            " as",
            " \"",
            "not",
            " free",
            "\"",
            " since",
            " it",
            " began",
            " publishing",
            " such",
            " ratings",
            " in",
            " ",
            "197",
            "2",
            ",",
            " with",
            " the",
            " exception",
            " of",
            " ",
            "198",
            "9",
            ",",
            " ",
            "199",
            "0",
            ",",
            " and",
            " ",
            "199",
            "1",
            ",",
            " when",
            " the",
            " country",
            " was",
            " labelled",
            " \"",
            "part",
            "ly",
            " free",
            ".\"",
            " In",
            " December",
            " ",
            "201",
            "6",
            ",",
            " the",
            " Euro"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " has",
            " invested",
            " an",
            " estimated",
            " ",
            "100",
            " billion",
            " din",
            "ars",
            " towards",
            " developing",
            " research",
            " facilities",
            " and",
            " paying",
            " researchers",
            ".",
            " This",
            " development",
            " program",
            " is",
            " meant",
            " to",
            " advance",
            " alternative",
            " energy",
            " production",
            ",",
            " especially",
            " solar",
            " and",
            " wind",
            " power",
            ".",
            " Algeria",
            " is",
            " estimated",
            " to",
            " have",
            " the",
            " largest",
            " solar",
            " energy",
            " potential",
            " in",
            " the",
            " Mediterranean",
            ",",
            " so",
            " the",
            " government",
            " has",
            " funded",
            " the",
            " creation",
            " of",
            " a",
            " solar",
            " science",
            " park",
            " in",
            " Hass"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " and",
            " other",
            " neurological",
            " disorders",
            ".",
            "Radi",
            "ation",
            " poisoning",
            " ",
            "At",
            "ax",
            "ia",
            " can",
            " be",
            " induced",
            " as",
            " a",
            " result",
            " of",
            " severe",
            " acute",
            " radiation",
            " poisoning",
            " with",
            " an",
            " absorbed",
            " dose",
            " of",
            " more",
            " than",
            " ",
            "30",
            " gr",
            "ays",
            ".",
            "V",
            "itamin",
            " B",
            "12",
            " deficiency",
            " ",
            "V",
            "itamin",
            " B",
            "12",
            " deficiency",
            " may",
            " cause",
            ",",
            " among",
            " several",
            " neurological",
            " abnormalities",
            ",",
            " overlapping",
            " cere",
            "bell",
            "ar",
            " and",
            " sensory",
            " at",
            "ax"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.041,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " until",
            " the",
            " ",
            "6",
            "th",
            " century",
            " CE",
            ",",
            " C",
            "app",
            "ad",
            "oc",
            "ian",
            " in",
            " the",
            " hom",
            "onymous",
            " region",
            ",",
            " Armenian",
            " in",
            " the",
            " east",
            ",",
            " and",
            " Kart",
            "vel",
            "ian",
            " languages",
            " in",
            " the",
            " northeast",
            ".",
            "An",
            "at",
            "olia",
            " is",
            " known",
            " as",
            " the",
            " birth",
            "place",
            " of",
            " mint",
            "ed",
            " coin",
            "age",
            " (",
            "as",
            " opposed",
            " to",
            " unm",
            "int",
            "ed",
            " coin",
            "age",
            ",",
            " which",
            " first",
            " appears",
            " in",
            " Mes"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            0.08,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ia",
            " on",
            " July",
            " ",
            "4",
            " and",
            " then",
            " secured",
            " V",
            "inc",
            "ennes",
            ",",
            " though",
            " V",
            "inc",
            "ennes",
            " was",
            " rec",
            "aptured",
            " by",
            " Quebec",
            " Governor",
            " Henry",
            " Hamilton",
            ".",
            " In",
            " early",
            " ",
            "177",
            "9",
            ",",
            " the",
            " Virgin",
            "ians",
            " counter",
            "-",
            "att",
            "acked",
            " in",
            " the",
            " siege",
            " of",
            " Fort",
            " V",
            "inc",
            "ennes",
            " and",
            " took",
            " Hamilton",
            " prisoner",
            ".",
            " Clark",
            " secured",
            " western",
            " British",
            " Quebec",
            " as",
            " the",
            " American",
            " Northwest",
            " Territory",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "rote",
            "chn",
            "ical",
            " Commission",
            " (",
            "IE",
            "C",
            "),",
            " via",
            " the",
            " U",
            ".S",
            ".",
            " National",
            " Committee",
            " (",
            "US",
            "NC",
            ").",
            " ANSI",
            " participates",
            " in",
            " almost",
            " the",
            " entire",
            " technical",
            " program",
            " of",
            " both",
            " the",
            " ISO",
            " and",
            " the",
            " I",
            "EC",
            ",",
            " and",
            " admin",
            "isters",
            " many",
            " key",
            " committees",
            " and",
            " sub",
            "groups",
            ".",
            " In",
            " many",
            " instances",
            ",",
            " U",
            ".S",
            ".",
            " standards",
            " are",
            " taken",
            " forward",
            " to",
            " ISO",
            " and",
            " I",
            "EC",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "rote",
            "chn",
            "ical",
            " Commission",
            " (",
            "IE",
            "C",
            "),",
            " via",
            " the",
            " U",
            ".S",
            ".",
            " National",
            " Committee",
            " (",
            "US",
            "NC",
            ").",
            " ANSI",
            " participates",
            " in",
            " almost",
            " the",
            " entire",
            " technical",
            " program",
            " of",
            " both",
            " the",
            " ISO",
            " and",
            " the",
            " I",
            "EC",
            ",",
            " and",
            " admin",
            "isters",
            " many",
            " key",
            " committees",
            " and",
            " sub",
            "groups",
            ".",
            " In",
            " many",
            " instances",
            ",",
            " U",
            ".S",
            ".",
            " standards",
            " are",
            " taken",
            " forward",
            " to",
            " ISO",
            " and",
            " I",
            "EC",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ib",
            "yl",
            ",",
            " and",
            " where",
            " some",
            " of",
            " the",
            " oldest",
            " or",
            "acular",
            " shr",
            "ines",
            " originated",
            ".",
            " Om",
            "ens",
            ",",
            " symbols",
            ",",
            " pur",
            "ifications",
            ",",
            " and",
            " ex",
            "orc",
            "isms",
            " appear",
            " in",
            " old",
            " Ass",
            "yro",
            "-B",
            "ab",
            "ylon",
            "ian",
            " texts",
            ".",
            " These",
            " rituals",
            " were",
            " spread",
            " into",
            " the",
            " empire",
            " of",
            " the",
            " H",
            "itt",
            "ites",
            ",",
            " and",
            " from",
            " there",
            " into",
            " Greece",
            ".",
            "H",
            "omer",
            " pictures",
            " Apollo",
            " on",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ib",
            "ai",
            " surged",
            " in",
            " the",
            " tw",
            "elfth",
            " century",
            " and",
            " remained",
            " popular",
            " in",
            " the",
            " street",
            " theater",
            " until",
            " the",
            " ",
            "193",
            "0",
            "s",
            ".",
            " P",
            "upp",
            "ets",
            " of",
            " the",
            " bun",
            "r",
            "aku",
            " theater",
            " and",
            " u",
            "ki",
            "yo",
            "-e",
            " prints",
            " are",
            " considered",
            " ancestors",
            " of",
            " characters",
            " of",
            " most",
            " Japanese",
            " animations",
            ".",
            " Finally",
            ",",
            " mang",
            "as",
            " were",
            " a",
            " heavy",
            " inspiration",
            " for",
            " anime",
            ".",
            " Cartoon",
            "ists",
            " K",
            "itz",
            "awa"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            0.14,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.224,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ib",
            "ama",
            ",",
            " Al",
            "ib",
            "am",
            "ou",
            ",",
            " Al",
            "ab",
            "amu",
            ",",
            " and",
            " All",
            "ib",
            "am",
            "ou",
            ".",
            " The",
            " use",
            " of",
            " state",
            " names",
            " derived",
            " from",
            " Native",
            " American",
            " languages",
            " is",
            " common",
            " in",
            " the",
            " U",
            ".S",
            ".;",
            " an",
            " estimated",
            " ",
            "26",
            " states",
            " have",
            " names",
            " of",
            " Native",
            " American",
            " origin",
            ".",
            "Sources",
            " disagree",
            " on",
            " the",
            " word",
            "'s",
            " meaning",
            ".",
            " Some",
            " scholars",
            " suggest",
            " the",
            " word",
            " comes",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.061,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ib",
            "ian",
            " Specialist",
            " Group",
            " of",
            " the",
            " I",
            "UC",
            "N",
            " is",
            " spear",
            "heading",
            " efforts",
            " to",
            " implement",
            " a",
            " comprehensive",
            " global",
            " strategy",
            " for",
            " amphib",
            "ian",
            " conservation",
            ".",
            " Amph",
            "ib",
            "ian",
            " Ark",
            " is",
            " an",
            " organization",
            " that",
            " was",
            " formed",
            " to",
            " implement",
            " the",
            " ex",
            "-s",
            "itu",
            " conservation",
            " recommendations",
            " of",
            " this",
            " plan",
            ",",
            " and",
            " they",
            " have",
            " been",
            " working",
            " with",
            " zo",
            "os",
            " and",
            " aqu",
            "aria",
            " around",
            " the",
            " world",
            ",",
            " encouraging",
            " them"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ib",
            "ians",
            " often",
            " eat",
            " the",
            " sl",
            "ough",
            "ed",
            " skin",
            ".",
            " Ca",
            "ec",
            "ilians",
            " are",
            " unique",
            " among",
            " amphib",
            "ians",
            " in",
            " having",
            " mineral",
            "ized",
            " der",
            "mal",
            " scales",
            " embedded",
            " in",
            " the",
            " der",
            "mis",
            " between",
            " the",
            " fur",
            "rows",
            " in",
            " the",
            " skin",
            ".",
            " The",
            " similarity",
            " of",
            " these",
            " to",
            " the",
            " scales",
            " of",
            " b",
            "ony",
            " fish",
            " is",
            " largely",
            " superficial",
            ".",
            " L",
            "izards",
            " and",
            " some",
            " frogs",
            " have",
            " somewhat",
            " similar",
            " oste",
            "oder"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "IV",
            ")",
            " oxide",
            " is",
            " the",
            " main",
            " form",
            " of",
            " solid",
            " americ",
            "ium",
            " which",
            " is",
            " used",
            " in",
            " nearly",
            " all",
            " its",
            " applications",
            ".",
            " As",
            " most",
            " other",
            " act",
            "in",
            "ide",
            " d",
            "iox",
            "ides",
            ",",
            " it",
            " is",
            " a",
            " black",
            " solid",
            " with",
            " a",
            " cubic",
            " (",
            "flu",
            "or",
            "ite",
            ")",
            " crystal",
            " structure",
            ".",
            "The",
            " ox",
            "al",
            "ate",
            " of",
            " americ",
            "ium",
            "(",
            "III",
            "),",
            " vacuum",
            " dried",
            " at",
            " room",
            " temperature",
            ",",
            " has"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "Autom",
            "obile",
            " (",
            "F",
            "IA",
            ")",
            " Grand",
            " American",
            " Road",
            " Racing",
            " Association",
            " International",
            " Conference",
            " of",
            " Sports",
            " Car",
            " Clubs",
            " (",
            "IC",
            "SC",
            "C",
            ")",
            " International",
            " Hot",
            " Rod",
            " Association",
            " (",
            "IH",
            "RA",
            ")",
            " International",
            " Motor",
            " Sports",
            " Association",
            " (",
            "IM",
            "SA",
            ")",
            " National",
            " Auto",
            " Sport",
            " Association",
            " National",
            " Association",
            " for",
            " Stock",
            " Car",
            " Auto",
            " Racing",
            " (",
            "N",
            "ASC",
            "AR",
            ")",
            " National",
            " Hot",
            " Rod",
            " Association",
            " (",
            "NH",
            "RA"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.609,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ius",
            " acid",
            " because",
            " it",
            " acts",
            " as",
            " a",
            " source",
            " of",
            " H",
            "3",
            "O",
            "+",
            " when",
            " dissolved",
            " in",
            " water",
            ",",
            " and",
            " it",
            " acts",
            " as",
            " a",
            " Br",
            "Ã¸",
            "n",
            "sted",
            " acid",
            " by",
            " donating",
            " a",
            " proton",
            " to",
            " water",
            ".",
            " In",
            " the",
            " second",
            " example",
            " CH",
            "3",
            "CO",
            "OH",
            " undergo",
            "es",
            " the",
            " same",
            " transformation",
            ",",
            " in",
            " this",
            " case",
            " donating",
            " a",
            " proton",
            " to",
            " ammonia",
            " (",
            "NH",
            "3",
            "),",
            " but",
            " does"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "8",
            ")",
            " became",
            " affiliated",
            " with",
            " the",
            " U",
            ".S",
            ".",
            " National",
            " Committee",
            " of",
            " the",
            " International",
            " Elect",
            "rote",
            "chn",
            "ical",
            " Commission",
            " (",
            "IE",
            "C",
            "),",
            " which",
            " had",
            " been",
            " formed",
            " in",
            " ",
            "190",
            "4",
            " to",
            " develop",
            " electrical",
            " and",
            " electronics",
            " standards",
            ".",
            "Members",
            "ANS",
            "I",
            "'s",
            " members",
            " are",
            " government",
            " agencies",
            ",",
            " organizations",
            ",",
            " academic",
            " and",
            " international",
            " bodies",
            ",",
            " and",
            " individuals",
            ".",
            " In",
            " total",
            ",",
            " the",
            " Institute"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.119,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ik",
            "ine",
            " River",
            " Garn",
            "et",
            " Fest",
            " in",
            " Wr",
            "ang",
            "ell",
            ".",
            " The",
            " St",
            "ik",
            "ine",
            " River",
            " attracts",
            " the",
            " largest",
            " spring",
            "time",
            " concentration",
            " of",
            " American",
            " bald",
            " eag",
            "les",
            " in",
            " the",
            " world",
            ".",
            "The",
            " Alaska",
            " Native",
            " Heritage",
            " Center",
            " celebrates",
            " the",
            " rich",
            " heritage",
            " of",
            " Alaska",
            "'s",
            " ",
            "11",
            " cultural",
            " groups",
            ".",
            " Their",
            " purpose",
            " is",
            " to",
            " encourage",
            " cross",
            "-cultural",
            " exchanges",
            " among",
            " all",
            " people",
            " and",
            " enhance",
            " self",
            "-esteem"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " program",
            " is",
            " '",
            "e",
            "legant",
            "—",
            "such",
            " a",
            " proof",
            " would",
            " solve",
            " the",
            " Hal",
            "ting",
            " problem",
            " (",
            "ib",
            "id",
            ").",
            "Algorithm",
            " versus",
            " function",
            " comput",
            "able",
            " by",
            " an",
            " algorithm",
            ":",
            " For",
            " a",
            " given",
            " function",
            " multiple",
            " algorithms",
            " may",
            " exist",
            ".",
            " This",
            " is",
            " true",
            ",",
            " even",
            " without",
            " expanding",
            " the",
            " available",
            " instruction",
            " set",
            " available",
            " to",
            " the",
            " programmer",
            ".",
            " Rogers",
            " observes",
            " that",
            " \"",
            "It",
            " is",
            " ...",
            " important",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "ards",
            ",",
            " and",
            " base",
            " things",
            " sire",
            " base",
            "\"",
            " (",
            "IV",
            ",",
            " ",
            "2",
            ")",
            " to",
            " reinforce",
            " his",
            " her",
            "edit",
            "arian",
            " argument",
            ".",
            "Mech",
            "an",
            "istically",
            ",",
            " Sch",
            "openh",
            "auer",
            " believed",
            " that",
            " a",
            " person",
            " inherits",
            " his",
            " intellect",
            " through",
            " his",
            " mother",
            ",",
            " and",
            " personal",
            " character",
            " through",
            " the",
            " father",
            ".",
            " This",
            " belief",
            " in",
            " her",
            "it",
            "ability",
            " of",
            " traits",
            " informed",
            " Sch",
            "openh",
            "auer",
            "'s",
            " view",
            " of",
            " love"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.135,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iate",
            " from",
            " the",
            " clear",
            " trends",
            " going",
            " from",
            " lithium",
            " to",
            " ca",
            "esium",
            ",",
            " such",
            " as",
            " the",
            " first",
            " ion",
            "isation",
            " energy",
            ",",
            " electron",
            " affinity",
            ",",
            " and",
            " an",
            "ion",
            " polar",
            "is",
            "ability",
            ",",
            " though",
            " due",
            " to",
            " the",
            " pa",
            "uc",
            "ity",
            " of",
            " known",
            " data",
            " about",
            " franc",
            "ium",
            " many",
            " sources",
            " give",
            " extrapol",
            "ated",
            " values",
            ",",
            " ignoring",
            " that",
            " relativ",
            "istic",
            " effects",
            " make",
            " the",
            " trend",
            " from",
            " lithium",
            " to",
            " ca",
            "esium"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            0.064,
            0.079,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " status",
            ".",
            " This",
            " bill",
            " gave",
            " them",
            " symbolic",
            " recognition",
            " as",
            " official",
            " languages",
            ",",
            " though",
            " they",
            " have",
            " not",
            " been",
            " adopted",
            " for",
            " official",
            " use",
            " within",
            " the",
            " government",
            ".",
            " The",
            " ",
            "20",
            " languages",
            " that",
            " were",
            " included",
            " in",
            " the",
            " bill",
            " are",
            ":",
            " In",
            "up",
            "ia",
            "q",
            " Siber",
            "ian",
            " Yup",
            "ik",
            " Central",
            " Al",
            "askan",
            " Yup",
            "'",
            "ik",
            " Al",
            "ut",
            "ii",
            "q",
            " Un",
            "ang",
            "ax"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.115,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "is",
            "ÅĤaw",
            " W",
            "Ã³",
            "jt",
            "ow",
            "icz",
            " says",
            " that",
            " although",
            " an",
            "ar",
            "cho",
            "-capital",
            "ists",
            " are",
            " against",
            " centralized",
            " states",
            ",",
            " they",
            " hold",
            " that",
            " all",
            " people",
            " would",
            " naturally",
            " share",
            " and",
            " agree",
            " to",
            " a",
            " specific",
            " moral",
            " theory",
            " based",
            " on",
            " the",
            " non",
            "-ag",
            "gression",
            " principle",
            ".",
            " While",
            " the",
            " Friedman",
            "ian",
            " formulation",
            " of",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " is",
            " robust",
            " to",
            " the",
            " presence",
            " of",
            " violence",
            " and",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            ",",
            " where",
            " they",
            " are",
            " still",
            " one",
            " of",
            " the",
            " essential",
            " elements",
            " of",
            " the",
            " settlement",
            " but",
            " most",
            " arrived",
            " in",
            " If",
            "ri",
            "qi",
            "ya",
            " by",
            " the",
            " Gab",
            "es",
            " region",
            ",",
            " arriving",
            " ",
            "105",
            "1",
            ".",
            " The",
            " Z",
            "ir",
            "id",
            " ruler",
            " tried",
            " to",
            " stop",
            " this",
            " rising",
            " tide",
            ",",
            " but",
            " with",
            " each",
            " encounter",
            ",",
            " the",
            " last",
            " under",
            " the",
            " walls",
            " of",
            " K",
            "air",
            "ou",
            "an",
            ",",
            " his",
            " troops"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            " Hor",
            "vat",
            ",",
            " Croatian",
            " football",
            "er",
            " and",
            " manager",
            " (",
            "b",
            ".",
            " ",
            "192",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " Richard",
            " Kings",
            "land",
            ",",
            " Australian",
            " captain",
            " and",
            " pilot",
            " (",
            "b",
            ".",
            " ",
            "191",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " G",
            "eli",
            "y",
            " Kor",
            "z",
            "hev",
            ",",
            " Russian",
            " painter",
            " (",
            "b",
            ".",
            " ",
            "192",
            "5",
            ")",
            " ",
            "201",
            "3",
            " –"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            " Hor",
            "vat",
            ",",
            " Croatian",
            " football",
            "er",
            " and",
            " manager",
            " (",
            "b",
            ".",
            " ",
            "192",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " Richard",
            " Kings",
            "land",
            ",",
            " Australian",
            " captain",
            " and",
            " pilot",
            " (",
            "b",
            ".",
            " ",
            "191",
            "6",
            ")",
            " ",
            " ",
            "201",
            "2",
            "  ",
            " –",
            " G",
            "eli",
            "y",
            " Kor",
            "z",
            "hev",
            ",",
            " Russian",
            " painter",
            " (",
            "b",
            ".",
            " ",
            "192",
            "5",
            ")",
            " ",
            "201",
            "3",
            " –"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            "add",
            "itivity",
            " or",
            " the",
            " triangle",
            " inequality",
            "|",
            "}",
            "The",
            " norm",
            " of",
            " a",
            " vector",
            " is",
            " also",
            " called",
            " its",
            " length",
            " or",
            " magnitude",
            ".",
            "In",
            " the",
            " case",
            " of",
            " Eu",
            "clidean",
            " space",
            " ,",
            " the",
            " function",
            " defined",
            " by",
            "is",
            " a",
            " norm",
            " called",
            " the",
            " Eu",
            "clidean",
            " norm",
            ".",
            " When",
            " the",
            " real",
            " numbers",
            " ",
            " are",
            " considered",
            " as",
            " the",
            " one",
            "-dimensional",
            " vector",
            " space",
            " ,",
            " the",
            " absolute",
            " value",
            " is",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.357,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " next",
            ",",
            " equivalent",
            " to",
            " the",
            " rotation",
            " of",
            " one",
            " CH",
            "3",
            " group",
            " by",
            " ",
            "120",
            "Â°",
            " relative",
            " to",
            " the",
            " other",
            ",",
            " is",
            " of",
            " the",
            " order",
            " of",
            " ",
            "10",
            "âĪĴ",
            "11",
            "Âł",
            "seconds",
            ".",
            "The",
            " case",
            " of",
            " higher",
            " al",
            "kan",
            "es",
            " is",
            " more",
            " complex",
            " but",
            " based",
            " on",
            " similar",
            " principles",
            ",",
            " with",
            " the",
            " ant",
            "iper",
            "ipl",
            "an",
            "ar",
            " con",
            "formation",
            " always",
            " being",
            " the",
            " most",
            " favored"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.338,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ide",
            " \"",
            "loses",
            "\"",
            " a",
            " pair",
            " of",
            " val",
            "ence",
            " electrons",
            " because",
            " the",
            " electrons",
            " shared",
            " in",
            " the",
            " B",
            "—",
            "F",
            " bond",
            " are",
            " located",
            " in",
            " the",
            " region",
            " of",
            " space",
            " between",
            " the",
            " two",
            " atomic",
            " nuclei",
            " and",
            " are",
            " therefore",
            " more",
            " distant",
            " from",
            " the",
            " fluoride",
            " nucleus",
            " than",
            " they",
            " are",
            " in",
            " the",
            " lone",
            " fluoride",
            " ion",
            ".",
            " BF",
            "3",
            " is",
            " a",
            " Lewis",
            " acid",
            " because",
            " it",
            " accepts",
            " the",
            " electron",
            " pair",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " classify",
            " these",
            " elements",
            " in",
            " group",
            " IB",
            " and",
            " remove",
            " them",
            " from",
            " group",
            " VIII",
            " for",
            " the",
            " resulting",
            " symmetry",
            ":",
            " this",
            " was",
            " the",
            " predominant",
            " classification",
            " until",
            " the",
            " rise",
            " of",
            " the",
            " modern",
            " medium",
            "-long",
            " ",
            "18",
            "-column",
            " periodic",
            " table",
            ",",
            " which",
            " separated",
            " the",
            " alk",
            "ali",
            " metals",
            " and",
            " group",
            " ",
            "11",
            " metals",
            ".",
            "The",
            " coin",
            "age",
            " metals",
            " were",
            " traditionally",
            " regarded",
            " as",
            " a",
            " subdivision",
            " of",
            " the",
            " alk",
            "ali"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.166,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "im",
            "mediately",
            " dangerous",
            " to",
            " life",
            " and",
            " health",
            ")",
            " value",
            " for",
            " arsen",
            "ic",
            " metal",
            " and",
            " in",
            "organic",
            " arsen",
            "ic",
            " compounds",
            " is",
            " ",
            "5",
            "Âł",
            "mg",
            "/m",
            "3",
            " (",
            "5",
            "Âł",
            "pp",
            "b",
            ").",
            " The",
            " Occupational",
            " Safety",
            " and",
            " Health",
            " Administration",
            " has",
            " set",
            " the",
            " permissible",
            " exposure",
            " limit",
            " (",
            "PE",
            "L",
            ")",
            " to",
            " a",
            " time",
            "-weight",
            "ed",
            " average",
            " (",
            "T",
            "WA",
            ")",
            " of",
            " ",
            "0",
            ".",
            "01"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " Germany",
            " to",
            " England",
            ",",
            " Einstein",
            " wrote",
            ",",
            " \"...",
            "Âł",
            "I",
            " must",
            " confess",
            " that",
            " the",
            " degree",
            " of",
            " their",
            " brutality",
            " and",
            " coward",
            "ice",
            " came",
            " as",
            " something",
            " of",
            " a",
            " surprise",
            ".\"",
            " After",
            " moving",
            " to",
            " the",
            " US",
            ",",
            " he",
            " described",
            " the",
            " book",
            " burn",
            "ings",
            " as",
            " a",
            " \"",
            "sp",
            "ont",
            "aneous",
            " emotional",
            " out",
            "burst",
            "\"",
            " by",
            " those",
            " who",
            " \"",
            "sh",
            "un",
            " popular",
            " enlightenment",
            "\",",
            " and",
            " \"",
            "more",
            " than"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.029,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.245,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " grammar",
            "In",
            " the",
            " Att",
            "ic",
            "–",
            "Ionic",
            " dialect",
            " of",
            " Ancient",
            " Greek",
            ",",
            " long",
            " alpha",
            " ",
            " front",
            "ed",
            " to",
            " ",
            " (",
            "eta",
            ").",
            " In",
            " Ionic",
            ",",
            " the",
            " shift",
            " took",
            " place",
            " in",
            " all",
            " positions",
            ".",
            " In",
            " Att",
            "ic",
            ",",
            " the",
            " shift",
            " did",
            " not",
            " take",
            " place",
            " after",
            " epsilon",
            ",",
            " iota",
            ",",
            " and",
            " rho",
            " (;",
            " ).",
            " In",
            " Dor",
            "ic",
            " and",
            " Ae",
            "olic",
            ",",
            " long",
            " alpha",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " south",
            " and",
            " west",
            ",",
            " while",
            " mongoose",
            " and",
            " che",
            "et",
            "ah",
            "s",
            " exist",
            " in",
            " the",
            " semi",
            "-des",
            "ert",
            " south",
            ".",
            " M",
            "arm",
            "ots",
            " and",
            " ib",
            "ex",
            " also",
            " live",
            " in",
            " the",
            " high",
            " mountains",
            " of",
            " Afghanistan",
            ",",
            " and",
            " p",
            "he",
            "as",
            "ants",
            " exist",
            " in",
            " some",
            " parts",
            " of",
            " the",
            " country",
            ".",
            " The",
            " Afghan",
            " h",
            "ound",
            " is",
            " a",
            " native",
            " breed",
            " of",
            " dog",
            " known",
            " for",
            " its",
            " fast",
            " speed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.155,
            -0.0,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "change",
            " of",
            " scale",
            "\",",
            " so",
            " statist",
            "icians",
            " often",
            " use",
            " transformations",
            " to",
            " achieve",
            " unit",
            "-t",
            "reatment",
            " add",
            "itivity",
            ".",
            " If",
            " the",
            " response",
            " variable",
            " is",
            " expected",
            " to",
            " follow",
            " a",
            " param",
            "etric",
            " family",
            " of",
            " probability",
            " distributions",
            ",",
            " then",
            " the",
            " statistic",
            "ian",
            " may",
            " specify",
            " (",
            "in",
            " the",
            " protocol",
            " for",
            " the",
            " experiment",
            " or",
            " observational",
            " study",
            ")",
            " that",
            " the",
            " responses",
            " be",
            " transformed",
            " to",
            " stabilize",
            " the",
            " variance",
            ".",
            " Also"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " occult",
            " revival",
            " including",
            " El",
            "ip",
            "has",
            " Levi",
            ",",
            " Arthur",
            " Edward",
            " Wa",
            "ite",
            ",",
            " and",
            " Rud",
            "olf",
            " Ste",
            "iner",
            ".",
            " Hitch",
            "cock",
            ",",
            " in",
            " his",
            " Remarks",
            " Upon",
            " Al",
            "ch",
            "ym",
            "ists",
            " (",
            "185",
            "5",
            ")",
            " attempted",
            " to",
            " make",
            " a",
            " case",
            " for",
            " his",
            " spiritual",
            " interpretation",
            " with",
            " his",
            " claim",
            " that",
            " the",
            " al",
            "chem",
            "ists",
            " wrote",
            " about",
            " a",
            " spiritual",
            " discipline",
            " under",
            " a",
            " material",
            "istic",
            " guise",
            " in",
            " order"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " characters",
            " to",
            " several",
            " code",
            " points",
            " reserved",
            " for",
            " \"",
            "national",
            " use",
            "\".",
            " However",
            ",",
            " the",
            " four",
            " years",
            " that",
            " elapsed",
            " between",
            " the",
            " publication",
            " of",
            " ASCII",
            "-",
            "196",
            "3",
            " and",
            " ISO",
            "'s",
            " first",
            " acceptance",
            " of",
            " an",
            " international",
            " recommendation",
            " during",
            " ",
            "196",
            "7",
            " caused",
            " ASCII",
            "'s",
            " choices",
            " for",
            " the",
            " national",
            " use",
            " characters",
            " to",
            " seem",
            " to",
            " be",
            " de",
            " facto",
            " standards",
            " for",
            " the",
            " world",
            ",",
            " causing",
            " confusion",
            " and",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " has",
            " been",
            " used",
            " since",
            " antiqu",
            "ity",
            ".",
            " Lith",
            "ium",
            " finds",
            " use",
            " as",
            " a",
            " psychiatric",
            " medication",
            " and",
            " as",
            " an",
            " an",
            "ode",
            " in",
            " lithium",
            " batteries",
            ".",
            " Sodium",
            ",",
            " potassium",
            " and",
            " lithium",
            " are",
            " essential",
            " elements",
            ",",
            " having",
            " major",
            " biological",
            " roles",
            " as",
            " electroly",
            "tes",
            ",",
            " and",
            " although",
            " the",
            " other",
            " alk",
            "ali",
            " metals",
            " are",
            " not",
            " essential",
            ",",
            " they",
            " also",
            " have",
            " various",
            " effects",
            " on",
            " the",
            " body",
            ",",
            " both",
            " beneficial"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " it",
            " ultimately",
            " proved",
            " unstable",
            " for",
            " use",
            " in",
            " oil",
            " painting",
            ",",
            " especially",
            " when",
            " mixed",
            " with",
            " the",
            " most",
            " common",
            " dil",
            "u",
            "ents",
            ",",
            " such",
            " as",
            " lin",
            "seed",
            " oil",
            ",",
            " var",
            "nish",
            " and",
            " tur",
            "p",
            "entine",
            ".",
            " Unless",
            " thoroughly",
            " diluted",
            ",",
            " bit",
            "umen",
            " never",
            " fully",
            " solid",
            "ifies",
            " and",
            " will",
            " in",
            " time",
            " corrupt",
            " the",
            " other",
            " pig",
            "ments",
            " with",
            " which",
            " it",
            " comes",
            " into",
            " contact",
            ".",
            " The",
            " use"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " record",
            " below",
            " and",
            " the",
            " submitted",
            " brief",
            "s",
            ".",
            "In",
            " an",
            " advers",
            "arial",
            " system",
            ",",
            " appellate",
            " courts",
            " do",
            " not",
            " have",
            " the",
            " power",
            " to",
            " review",
            " lower",
            " court",
            " decisions",
            " unless",
            " a",
            " party",
            " appeals",
            " it",
            ".",
            " Therefore",
            ",",
            " if",
            " a",
            " lower",
            " court",
            " has",
            " ruled",
            " in",
            " an",
            " improper",
            " manner",
            ",",
            " or",
            " against",
            " legal",
            " precedent",
            ",",
            " that",
            " judgment",
            " will",
            " stand",
            " if",
            " not",
            " appealed",
            " –",
            " even",
            " if",
            " it",
            " might"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "emachine",
    "orte",
    "antee",
    "izz",
    "allet"
  ],
  "bottom_logits": [
    "à¸±à¸ģà¸Ķ",
    " ============================================================================\n",
    "DDS",
    "ADDE",
    "leared"
  ],
  "act_min": -0.0,
  "act_max": 0.719
}