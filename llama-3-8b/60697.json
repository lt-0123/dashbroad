{
  "index": 60697,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " an",
            " analog",
            " signal",
            " by",
            " a",
            " trans",
            "ducer",
            ".",
            " For",
            " example",
            ",",
            " sound",
            " striking",
            " the",
            " di",
            "aphrag",
            "m",
            " of",
            " a",
            " microphone",
            " induces",
            " corresponding",
            " fluctuations",
            " in",
            " the",
            " current",
            " produced",
            " by",
            " a",
            " coil",
            " in",
            " an",
            " electromagnetic",
            " microphone",
            " or",
            " the",
            " voltage",
            " produced",
            " by",
            " a",
            " cond",
            "enser",
            " microphone",
            ".",
            " The",
            " voltage",
            " or",
            " the",
            " current",
            " is",
            " said",
            " to",
            " be",
            " an",
            " analog",
            " of",
            " the",
            " sound",
            ".",
            "Noise",
            "An",
            " analog"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " digital",
            " form",
            " introduces",
            " a",
            " low",
            "-level",
            " quant",
            "ization",
            " noise",
            " into",
            " the",
            " signal",
            " due",
            " to",
            " finite",
            " resolution",
            " of",
            " digital",
            " systems",
            ".",
            " Once",
            " in",
            " digital",
            " form",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " transmitted",
            ",",
            " stored",
            ",",
            " and",
            " processed",
            " without",
            " introducing",
            " additional",
            " noise",
            " or",
            " distortion",
            " using",
            " error",
            " detection",
            " and",
            " correction",
            ".",
            "Noise",
            " accumulation",
            " in",
            " analog",
            " systems",
            " can",
            " be",
            " minimized",
            " by",
            " electromagnetic",
            " shielding",
            ",",
            " balanced",
            " lines",
            ",",
            " low"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " Gra",
            "ffiti",
            " ab",
            "atement",
            ",",
            " a",
            " joint",
            " effort",
            " between",
            " groups",
            " to",
            " eliminate",
            " graffiti",
            " Marg",
            "inal",
            " ab",
            "atement",
            " cost",
            ",",
            " the",
            " marginal",
            " cost",
            " of",
            " reducing",
            " pollution",
            " Noise",
            " ab",
            "atement",
            ",",
            " strategies",
            " to",
            " reduce",
            " noise",
            " pollution",
            " or",
            " its",
            " impact",
            " N",
            "uis",
            "ance",
            " ab",
            "atement",
            ",",
            " regulatory",
            " compliance",
            " methodology",
            " Tax",
            " ab",
            "atement",
            ",",
            " temporary",
            " reduction",
            " or",
            " elimination",
            " of",
            " a",
            " tax",
            "See"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " Gra",
            "ffiti",
            " ab",
            "atement",
            ",",
            " a",
            " joint",
            " effort",
            " between",
            " groups",
            " to",
            " eliminate",
            " graffiti",
            " Marg",
            "inal",
            " ab",
            "atement",
            " cost",
            ",",
            " the",
            " marginal",
            " cost",
            " of",
            " reducing",
            " pollution",
            " Noise",
            " ab",
            "atement",
            ",",
            " strategies",
            " to",
            " reduce",
            " noise",
            " pollution",
            " or",
            " its",
            " impact",
            " N",
            "uis",
            "ance",
            " ab",
            "atement",
            ",",
            " regulatory",
            " compliance",
            " methodology",
            " Tax",
            " ab",
            "atement",
            ",",
            " temporary",
            " reduction",
            " or",
            " elimination",
            " of",
            " a",
            " tax",
            "See"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " digital",
            " form",
            " introduces",
            " a",
            " low",
            "-level",
            " quant",
            "ization",
            " noise",
            " into",
            " the",
            " signal",
            " due",
            " to",
            " finite",
            " resolution",
            " of",
            " digital",
            " systems",
            ".",
            " Once",
            " in",
            " digital",
            " form",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " transmitted",
            ",",
            " stored",
            ",",
            " and",
            " processed",
            " without",
            " introducing",
            " additional",
            " noise",
            " or",
            " distortion",
            " using",
            " error",
            " detection",
            " and",
            " correction",
            ".",
            "Noise",
            " accumulation",
            " in",
            " analog",
            " systems",
            " can",
            " be",
            " minimized",
            " by",
            " electromagnetic",
            " shielding",
            ",",
            " balanced",
            " lines",
            ",",
            " low"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " digital",
            " form",
            " introduces",
            " a",
            " low",
            "-level",
            " quant",
            "ization",
            " noise",
            " into",
            " the",
            " signal",
            " due",
            " to",
            " finite",
            " resolution",
            " of",
            " digital",
            " systems",
            ".",
            " Once",
            " in",
            " digital",
            " form",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " transmitted",
            ",",
            " stored",
            ",",
            " and",
            " processed",
            " without",
            " introducing",
            " additional",
            " noise",
            " or",
            " distortion",
            " using",
            " error",
            " detection",
            " and",
            " correction",
            ".",
            "Noise",
            " accumulation",
            " in",
            " analog",
            " systems",
            " can",
            " be",
            " minimized",
            " by",
            " electromagnetic",
            " shielding",
            ",",
            " balanced",
            " lines",
            ",",
            " low"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " digital",
            " form",
            " introduces",
            " a",
            " low",
            "-level",
            " quant",
            "ization",
            " noise",
            " into",
            " the",
            " signal",
            " due",
            " to",
            " finite",
            " resolution",
            " of",
            " digital",
            " systems",
            ".",
            " Once",
            " in",
            " digital",
            " form",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " transmitted",
            ",",
            " stored",
            ",",
            " and",
            " processed",
            " without",
            " introducing",
            " additional",
            " noise",
            " or",
            " distortion",
            " using",
            " error",
            " detection",
            " and",
            " correction",
            ".",
            "Noise",
            " accumulation",
            " in",
            " analog",
            " systems",
            " can",
            " be",
            " minimized",
            " by",
            " electromagnetic",
            " shielding",
            ",",
            " balanced",
            " lines",
            ",",
            " low"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " Gra",
            "ffiti",
            " ab",
            "atement",
            ",",
            " a",
            " joint",
            " effort",
            " between",
            " groups",
            " to",
            " eliminate",
            " graffiti",
            " Marg",
            "inal",
            " ab",
            "atement",
            " cost",
            ",",
            " the",
            " marginal",
            " cost",
            " of",
            " reducing",
            " pollution",
            " Noise",
            " ab",
            "atement",
            ",",
            " strategies",
            " to",
            " reduce",
            " noise",
            " pollution",
            " or",
            " its",
            " impact",
            " N",
            "uis",
            "ance",
            " ab",
            "atement",
            ",",
            " regulatory",
            " compliance",
            " methodology",
            " Tax",
            " ab",
            "atement",
            ",",
            " temporary",
            " reduction",
            " or",
            " elimination",
            " of",
            " a",
            " tax",
            "See"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " digital",
            " form",
            " introduces",
            " a",
            " low",
            "-level",
            " quant",
            "ization",
            " noise",
            " into",
            " the",
            " signal",
            " due",
            " to",
            " finite",
            " resolution",
            " of",
            " digital",
            " systems",
            ".",
            " Once",
            " in",
            " digital",
            " form",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " transmitted",
            ",",
            " stored",
            ",",
            " and",
            " processed",
            " without",
            " introducing",
            " additional",
            " noise",
            " or",
            " distortion",
            " using",
            " error",
            " detection",
            " and",
            " correction",
            ".",
            "Noise",
            " accumulation",
            " in",
            " analog",
            " systems",
            " can",
            " be",
            " minimized",
            " by",
            " electromagnetic",
            " shielding",
            ",",
            " balanced",
            " lines",
            ",",
            " low"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " =",
            " noise",
            " or",
            " error",
            " associated",
            " with",
            " the",
            " particular",
            " ij",
            " data",
            " value",
            "That",
            " is",
            ",",
            " we",
            " envision",
            " an",
            " additive",
            " model",
            " that",
            " says",
            " every",
            " data",
            " point",
            " can",
            " be",
            " represented",
            " by",
            " sum",
            "ming",
            " three",
            " quantities",
            ":",
            " the",
            " true",
            " mean",
            ",",
            " averaged",
            " over",
            " all",
            " factor",
            " levels",
            " being",
            " investigated",
            ",",
            " plus",
            " an",
            " incremental",
            " component",
            " associated",
            " with",
            " the",
            " particular",
            " column",
            " (",
            "factor",
            " level",
            "),",
            " plus",
            " a",
            " final",
            " component"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " =",
            " noise",
            " or",
            " error",
            " associated",
            " with",
            " the",
            " particular",
            " ij",
            " data",
            " value",
            "That",
            " is",
            ",",
            " we",
            " envision",
            " an",
            " additive",
            " model",
            " that",
            " says",
            " every",
            " data",
            " point",
            " can",
            " be",
            " represented",
            " by",
            " sum",
            "ming",
            " three",
            " quantities",
            ":",
            " the",
            " true",
            " mean",
            ",",
            " averaged",
            " over",
            " all",
            " factor",
            " levels",
            " being",
            " investigated",
            ",",
            " plus",
            " an",
            " incremental",
            " component",
            " associated",
            " with",
            " the",
            " particular",
            " column",
            " (",
            "factor",
            " level",
            "),",
            " plus",
            " a",
            " final",
            " component"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "ibly",
            " de",
            "grading",
            " the",
            " SN",
            "R",
            ",",
            " until",
            " in",
            " extreme",
            " cases",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " overwhelmed",
            ".",
            " Noise",
            " can",
            " show",
            " up",
            " as",
            " hiss",
            " and",
            " inter",
            "mod",
            "ulation",
            " distortion",
            " in",
            " audio",
            " signals",
            ",",
            " or",
            " snow",
            " in",
            " video",
            " signals",
            ".",
            " Generation",
            " loss",
            " is",
            " irreversible",
            " as",
            " there",
            " is",
            " no",
            " reliable",
            " method",
            " to",
            " distinguish",
            " the",
            " noise",
            " from",
            " the",
            " signal",
            ".",
            "Con",
            "verting",
            " an",
            " analog",
            " signal",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "ibly",
            " de",
            "grading",
            " the",
            " SN",
            "R",
            ",",
            " until",
            " in",
            " extreme",
            " cases",
            ",",
            " the",
            " signal",
            " can",
            " be",
            " overwhelmed",
            ".",
            " Noise",
            " can",
            " show",
            " up",
            " as",
            " hiss",
            " and",
            " inter",
            "mod",
            "ulation",
            " distortion",
            " in",
            " audio",
            " signals",
            ",",
            " or",
            " snow",
            " in",
            " video",
            " signals",
            ".",
            " Generation",
            " loss",
            " is",
            " irreversible",
            " as",
            " there",
            " is",
            " no",
            " reliable",
            " method",
            " to",
            " distinguish",
            " the",
            " noise",
            " from",
            " the",
            " signal",
            ".",
            "Con",
            "verting",
            " an",
            " analog",
            " signal",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " analyzed",
            " to",
            " confirm",
            " hom",
            "os",
            "ced",
            "astic",
            "ity",
            " and",
            " gross",
            " normal",
            "ity",
            ".",
            " Res",
            "idual",
            "s",
            " should",
            " have",
            " the",
            " appearance",
            " of",
            " (",
            "zero",
            " mean",
            " normal",
            " distribution",
            ")",
            " noise",
            " when",
            " plotted",
            " as",
            " a",
            " function",
            " of",
            " anything",
            " including",
            " time",
            " and",
            " ",
            "mode",
            "led",
            " data",
            " values",
            ".",
            " Trends",
            " hint",
            " at",
            " interactions",
            " among",
            " factors",
            " or",
            " among",
            " observations",
            ".",
            "Follow",
            "-up",
            " tests",
            "A",
            " statistically",
            " significant",
            " effect",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " primary",
            " producers",
            " in",
            " the",
            " context",
            " of",
            " male",
            "-out",
            "-m",
            "igration",
            ".",
            "Safety",
            " ",
            "A",
            "gricult",
            "ure",
            ",",
            " specifically",
            " farming",
            ",",
            " remains",
            " a",
            " hazardous",
            " industry",
            ",",
            " and",
            " farmers",
            " worldwide",
            " remain",
            " at",
            " high",
            " risk",
            " of",
            " work",
            "-related",
            " injuries",
            ",",
            " lung",
            " disease",
            ",",
            " noise",
            "-induced",
            " hearing",
            " loss",
            ",",
            " skin",
            " diseases",
            ",",
            " as",
            " well",
            " as",
            " certain",
            " cancers",
            " related",
            " to",
            " chemical",
            " use",
            " and",
            " prolonged",
            " sun",
            " exposure",
            ".",
            " On"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " faced",
            ".",
            " ",
            " This",
            " change",
            " from",
            " simple",
            " imitation",
            " to",
            " political",
            " commentary",
            " was",
            " the",
            " start",
            " of",
            " German",
            " identification",
            " with",
            " rap",
            ".",
            " The",
            " sound",
            " of",
            " “",
            "F",
            "rem",
            "d",
            " im",
            " eigenen",
            " Land",
            "”",
            " was",
            " influenced",
            " by",
            " the",
            " '",
            "wall",
            " of",
            " noise",
            "'",
            " created",
            " by",
            " Public",
            " Enemy",
            "'s",
            " producers",
            ",",
            " The",
            " Bomb",
            " Squad",
            ".",
            "After",
            " the",
            " reun",
            "ification",
            " of",
            " Germany",
            ",",
            " an",
            " abundance",
            " of",
            " anti",
            "-imm"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " Georgia",
            " (",
            "U",
            ".S",
            ".",
            " state",
            "),",
            " and",
            " Mississippi",
            ",",
            " United",
            " States",
            ")",
            "Last",
            " Wednesday",
            " ",
            " International",
            " Noise",
            " Awareness",
            " Day",
            "Fixed",
            " ",
            " April",
            " ",
            "1",
            " April",
            " F",
            "ools",
            "'",
            " Day",
            " Arbor",
            " Day",
            " (",
            "T",
            "anz",
            "ania",
            ")",
            " Civil",
            " Service",
            " Day",
            " (",
            "Th",
            "ailand",
            ")",
            " Cyprus",
            " National",
            " Day",
            " (",
            "Cy",
            "prus",
            ")",
            " Ed",
            "ible",
            " Book",
            " Day",
            " F",
            "ossil",
            " F",
            "ools"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "3",
            "-D",
            " animation",
            ",",
            " created",
            " by",
            " Walt",
            " Disney",
            " Imagine",
            "ering",
            " for",
            " shows",
            " and",
            " attractions",
            " at",
            " Disney",
            " theme",
            " parks",
            " move",
            " and",
            " make",
            " noise",
            " (",
            "gener",
            "ally",
            " a",
            " recorded",
            " speech",
            " or",
            " song",
            ").",
            " They",
            " are",
            " fixed",
            " to",
            " whatever",
            " supports",
            " them",
            ".",
            " They",
            " can",
            " sit",
            " and",
            " stand",
            ",",
            " and",
            " they",
            " cannot",
            " walk",
            ".",
            " An",
            " Audio",
            "-",
            "Anim",
            "atron",
            " is",
            " different",
            " from",
            " an",
            " android",
            "-type",
            " robot"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " signal",
            " is",
            " subject",
            " to",
            " electronic",
            " noise",
            " and",
            " distortion",
            " introduced",
            " by",
            " communication",
            " channels",
            ",",
            " recording",
            " and",
            " signal",
            " processing",
            " operations",
            ",",
            " which",
            " can",
            " progressively",
            " degrade",
            " the",
            " signal",
            "-to",
            "-no",
            "ise",
            " ratio",
            " (",
            "SN",
            "R",
            ").",
            " As",
            " the",
            " signal",
            " is",
            " transmitted",
            ",",
            " copied",
            ",",
            " or",
            " processed",
            ",",
            " the",
            " unavoidable",
            " noise",
            " introduced",
            " in",
            " the",
            " signal",
            " path",
            " will",
            " accumulate",
            " as",
            " a",
            " generation",
            " loss",
            ",",
            " progressively",
            " and",
            " irre",
            "vers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " signal",
            " is",
            " subject",
            " to",
            " electronic",
            " noise",
            " and",
            " distortion",
            " introduced",
            " by",
            " communication",
            " channels",
            ",",
            " recording",
            " and",
            " signal",
            " processing",
            " operations",
            ",",
            " which",
            " can",
            " progressively",
            " degrade",
            " the",
            " signal",
            "-to",
            "-no",
            "ise",
            " ratio",
            " (",
            "SN",
            "R",
            ").",
            " As",
            " the",
            " signal",
            " is",
            " transmitted",
            ",",
            " copied",
            ",",
            " or",
            " processed",
            ",",
            " the",
            " unavoidable",
            " noise",
            " introduced",
            " in",
            " the",
            " signal",
            " path",
            " will",
            " accumulate",
            " as",
            " a",
            " generation",
            " loss",
            ",",
            " progressively",
            " and",
            " irre",
            "vers"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " War",
            "hol",
            " \"",
            "w",
            "rote",
            "\"",
            " several",
            " books",
            " that",
            " were",
            " commercially",
            " published",
            ":",
            " a",
            ",",
            " A",
            " Novel",
            " (",
            "196",
            "8",
            ",",
            " )",
            " is",
            " a",
            " literal",
            " transcription",
            "—",
            "cont",
            "aining",
            " spelling",
            " errors",
            " and",
            " phon",
            "etically",
            " written",
            " background",
            " noise",
            " and",
            " mum",
            "bling",
            "—",
            "of",
            " audio",
            " recordings",
            " of",
            " O",
            "nd",
            "ine",
            " and",
            " several",
            " of",
            " Andy",
            " War",
            "hol",
            "'s",
            " friends",
            " hanging",
            " out",
            " at",
            " the",
            " Factory",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.492,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " prior",
            " to",
            " thought",
            ".",
            " Sch",
            "openh",
            "auer",
            " felt",
            " this",
            " was",
            " similar",
            " to",
            " notions",
            " of",
            " pur",
            "u",
            "á¹",
            "£",
            "Äģ",
            "r",
            "tha",
            " or",
            " goals",
            " of",
            " life",
            " in",
            " Ved",
            "Äģ",
            "nt",
            "a",
            " Hindu",
            "ism",
            ".",
            "In",
            " Sch",
            "openh",
            "auer",
            "'s",
            " philosophy",
            ",",
            " denial",
            " of",
            " the",
            " will",
            " is",
            " attained",
            " by",
            ":",
            " personal",
            " experience",
            " of",
            " an",
            " extremely",
            " great",
            " suffering",
            " that",
            " leads",
            " to",
            " loss",
            " of",
            " the",
            " will"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " great",
            " effect",
            " on",
            " artistic",
            " photography",
            ".",
            " War",
            "hol",
            " was",
            " an",
            " accomplished",
            " photographer",
            ",",
            " and",
            " took",
            " an",
            " enormous",
            " number",
            " of",
            " photographs",
            " of",
            " Factory",
            " visitors",
            ",",
            " friends",
            ",",
            " acquired",
            " by",
            " Stanford",
            " University",
            ".",
            " Music",
            ":",
            " In",
            " ",
            "196",
            "3",
            ",",
            " War",
            "hol",
            " founded",
            " The",
            " Dr",
            "uds",
            ",",
            " a",
            " short",
            "-lived",
            " avant",
            "-g",
            "arde",
            " noise",
            " music",
            " band",
            " that",
            " featured",
            " prominent",
            " members",
            " of",
            " the",
            " New",
            " York"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            0.426,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.41
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " Other",
            " early",
            " writings",
            " are",
            " found",
            " in",
            " the",
            " J",
            "aina",
            " medical",
            " treat",
            "ise",
            " K",
            "aly",
            "Äģ",
            "á¹",
            "ĩ",
            "ak",
            "Äģ",
            "rak",
            "am",
            " of",
            " U",
            "gr",
            "Äģ",
            "d",
            "ity",
            "a",
            ",",
            " written",
            " in",
            " South",
            " India",
            " in",
            " the",
            " early",
            " ",
            "9",
            "th",
            " century",
            ".",
            "Two",
            " famous",
            " early",
            " Indian",
            " al",
            "chemical",
            " authors",
            " were",
            " N",
            "Äģ",
            "g",
            "Äģ",
            "r",
            "j",
            "una",
            " Sidd",
            "ha",
            " and",
            " N",
            "ity",
            "an",
            "Äģ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            0.426,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.41
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " Other",
            " early",
            " writings",
            " are",
            " found",
            " in",
            " the",
            " J",
            "aina",
            " medical",
            " treat",
            "ise",
            " K",
            "aly",
            "Äģ",
            "á¹",
            "ĩ",
            "ak",
            "Äģ",
            "rak",
            "am",
            " of",
            " U",
            "gr",
            "Äģ",
            "d",
            "ity",
            "a",
            ",",
            " written",
            " in",
            " South",
            " India",
            " in",
            " the",
            " early",
            " ",
            "9",
            "th",
            " century",
            ".",
            "Two",
            " famous",
            " early",
            " Indian",
            " al",
            "chemical",
            " authors",
            " were",
            " N",
            "Äģ",
            "g",
            "Äģ",
            "r",
            "j",
            "una",
            " Sidd",
            "ha",
            " and",
            " N",
            "ity",
            "an",
            "Äģ"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " certain",
            " final",
            " sequences",
            " containing",
            " :",
            " E",
            "vid",
            "ently",
            ",",
            " final",
            " ",
            " became",
            " ",
            " as",
            " in",
            " the",
            " Classical",
            " language",
            ",",
            " but",
            " final",
            " ",
            " became",
            " a",
            " different",
            " sound",
            ",",
            " possibly",
            " ",
            " (",
            "rather",
            " than",
            " again",
            " ",
            " in",
            " the",
            " Classical",
            " language",
            ").",
            " This",
            " is",
            " the",
            " apparent",
            " source",
            " of",
            " the",
            " al",
            "if",
            " ma",
            "q",
            "á¹",
            "£",
            "Å«",
            "rah",
            " '",
            "restricted",
            " al",
            "if",
            "'",
            " where",
            " a",
            " final"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.137,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " tradition",
            " was",
            " traditional",
            "ist",
            " and",
            " corrective",
            " in",
            " nature",
            "—",
            "holding",
            " that",
            " linguistic",
            " correctness",
            " and",
            " elo",
            "quence",
            " derive",
            " from",
            " Qur",
            "Ê",
            "¾",
            "Äģ",
            "nic",
            " usage",
            ",",
            " ,",
            " and",
            " Bed",
            "ou",
            "in",
            " speech",
            "—",
            "position",
            "ing",
            " itself",
            " against",
            " la",
            "á",
            "¸",
            "¥",
            "n",
            " al",
            "-",
            "Ê",
            "¿",
            "Äģ",
            "mma",
            " (),",
            " the",
            " sole",
            "c",
            "ism",
            " it",
            " viewed",
            " as",
            " defective",
            ".",
            "Western",
            " le",
            "xic",
            "ography",
            " of",
            " Arabic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Indian",
            " mathematics",
            " (",
            "around",
            " ",
            "800",
            " BC",
            " and",
            " later",
            ";",
            " e",
            ".g",
            ".",
            " Sh",
            "ul",
            "ba",
            " S",
            "ut",
            "ras",
            ",",
            " Kerala",
            " School",
            ",",
            " and",
            " Br",
            "Äģ",
            "h",
            "mas",
            "ph",
            "u",
            "á¹",
            "Ń",
            "as",
            "idd",
            "h",
            "Äģ",
            "nt",
            "a",
            "),",
            " The",
            " If",
            "a",
            " Oracle",
            " (",
            "around",
            " ",
            "500",
            " BC",
            "),",
            " Greek",
            " mathematics",
            " (",
            "around",
            " ",
            "240",
            " BC",
            ",",
            " e",
            ".g",
            ".",
            " sieve",
            " of",
            " Er"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " images",
            " than",
            " the",
            " previous",
            " radioactive",
            " agent",
            ",",
            " iod",
            "ine",
            "-",
            "124",
            ",",
            " because",
            " the",
            " body",
            " tends",
            " to",
            " transport",
            " iod",
            "ine",
            " to",
            " the",
            " thyroid",
            " gland",
            " producing",
            " signal",
            " noise",
            ".",
            " Nan",
            "op",
            "articles",
            " of",
            " arsen",
            "ic",
            " have",
            " shown",
            " ability",
            " to",
            " kill",
            " cancer",
            " cells",
            " with",
            " lesser",
            " cyt",
            "otoxic",
            "ity",
            " than",
            " other",
            " arsen",
            "ic",
            " formulations",
            ".",
            "In",
            " sub",
            "to",
            "xic",
            " doses",
            ",",
            " soluble",
            " arsen",
            "ic",
            " compounds",
            " act"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " body",
            ".",
            " The",
            " \"",
            "air",
            " serv",
            "os",
            "ystem",
            "\"",
            " which",
            " Kok",
            "oro",
            " developed",
            " originally",
            " is",
            " used",
            " for",
            " the",
            " act",
            "uator",
            ".",
            " As",
            " a",
            " result",
            " of",
            " having",
            " an",
            " act",
            "uator",
            " controlled",
            " precisely",
            " with",
            " air",
            " pressure",
            " via",
            " a",
            " serv",
            "os",
            "ystem",
            ",",
            " the",
            " movement",
            " is",
            " very",
            " fluid",
            " and",
            " there",
            " is",
            " very",
            " little",
            " noise",
            ".",
            " DER",
            "2",
            " realized",
            " a",
            " sl",
            "immer",
            " body",
            " than",
            " that",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " as",
            " among",
            " Romance",
            " languages",
            ",",
            " retention",
            " (",
            "or",
            " change",
            " of",
            " meaning",
            ")",
            " of",
            " different",
            " classical",
            " forms",
            ".",
            " Thus",
            " Iraqi",
            " aku",
            ",",
            " Lev",
            "antine",
            " f",
            "Ä«",
            "h",
            " and",
            " North",
            " African",
            " kay",
            "ÉĻ",
            "n",
            " all",
            " mean",
            " '",
            "there",
            " is",
            "',",
            " and",
            " all",
            " come",
            " from",
            " Classical",
            " Arabic",
            " forms",
            " (",
            "y",
            "ak",
            "Å«",
            "n",
            ",",
            " f",
            "Ä«",
            "hi",
            ",",
            " k",
            "Äģ",
            "'in",
            " respectively",
            "),",
            " but",
            " now"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Chapter",
            " ",
            "13",
            " of",
            " HIM",
            "L",
            ",",
            " Various",
            " works",
            " on",
            " r",
            "asa",
            "ÅĽ",
            "Äģ",
            "stra",
            " and",
            " rat",
            "na",
            "ÅĽ",
            "Äģ",
            "stra",
            " (",
            "or",
            " Various",
            " works",
            " on",
            " al",
            "chemy",
            " and",
            " gems",
            ")",
            " gives",
            " brief",
            " details",
            " of",
            " a",
            " further",
            " ",
            "655",
            " (",
            "six",
            " hundred",
            " and",
            " fifty",
            "-five",
            ")",
            " treat",
            "ises",
            ".",
            " In",
            " some",
            " cases",
            " Me",
            "ulen",
            "b",
            "eld",
            " gives",
            " notes",
            " on",
            " the",
            " contents",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            0.414,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "tha",
            " Sidd",
            "ha",
            ".",
            " N",
            "Äģ",
            "g",
            "Äģ",
            "r",
            "j",
            "una",
            " Sidd",
            "ha",
            " was",
            " a",
            " Buddhist",
            " monk",
            ".",
            " His",
            " book",
            ",",
            " Ras",
            "end",
            "ram",
            "ang",
            "alam",
            ",",
            " is",
            " an",
            " example",
            " of",
            " Indian",
            " al",
            "chemy",
            " and",
            " medicine",
            ".",
            " N",
            "ity",
            "an",
            "Äģ",
            "tha",
            " Sidd",
            "ha",
            " wrote",
            " Ras",
            "arat",
            "n",
            "Äģ",
            "kara",
            ",",
            " also",
            " a",
            " highly",
            " influential",
            " work",
            ".",
            " In",
            " Sans",
            "krit",
            ",",
            " r",
            "asa"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            "usions",
            " may",
            " be",
            " related",
            " with",
            " the",
            " use",
            " of",
            " atomic",
            " percent",
            " as",
            " measure",
            " of",
            " concentration",
            " of",
            " a",
            " dop",
            "ant",
            ",",
            " or",
            " resolution",
            " of",
            " an",
            " imaging",
            " system",
            ",",
            " as",
            " measure",
            " of",
            " the",
            " size",
            " of",
            " the",
            " smallest",
            " detail",
            " which",
            " still",
            " can",
            " be",
            " resolved",
            " at",
            " the",
            " background",
            " of",
            " statistical",
            " noise",
            ".",
            " See",
            " also",
            " Accuracy",
            " and",
            " precision",
            " and",
            " its",
            " talk",
            ".",
            "The",
            " Berry",
            " paradox",
            " arises",
            " as",
            " a",
            " result"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " live",
            ";",
            " or",
            " knowledge",
            " of",
            " the",
            " essential",
            " nature",
            " of",
            " life",
            " in",
            " the",
            " world",
            " through",
            " observation",
            " of",
            " the",
            " suffering",
            " of",
            " other",
            " people",
            ".",
            "B",
            "udd",
            "hist",
            " nir",
            "v",
            "Äģ",
            "á¹",
            "ĩ",
            "a",
            " is",
            " not",
            " equivalent",
            " to",
            " the",
            " condition",
            " that",
            " Sch",
            "openh",
            "auer",
            " described",
            " as",
            " denial",
            " of",
            " the",
            " will",
            ".",
            " Nir",
            "v",
            "Äģ",
            "á¹",
            "ĩ",
            "a",
            " is",
            " not",
            " the",
            " extingu",
            "ishing",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.432,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "rah",
            "edral",
            " [",
            "Li",
            "(H",
            "2",
            "O",
            ")",
            "4",
            "]+",
            ":",
            " while",
            " sol",
            "vation",
            " numbers",
            " of",
            " ",
            "3",
            " to",
            " ",
            "6",
            " have",
            " been",
            " found",
            " for",
            " lithium",
            " aqu",
            "a",
            " ions",
            ",",
            " sol",
            "vation",
            " numbers",
            " less",
            " than",
            " ",
            "4",
            " may",
            " be",
            " the",
            " result",
            " of",
            " the",
            " formation",
            " of",
            " contact",
            " ion",
            " pairs",
            ",",
            " and",
            " the",
            " higher",
            " sol",
            "vation",
            " numbers",
            " may",
            " be",
            " interpreted",
            " in",
            " terms",
            " of",
            " water",
            " molecules"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            0.432,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " There",
            " are",
            " certain",
            " phrases",
            " in",
            " praise",
            " of",
            " God",
            " that",
            " are",
            " favored",
            " by",
            " Muslims",
            ",",
            " including",
            " \"\"",
            " (",
            "Gl",
            "ory",
            " be",
            " to",
            " God",
            "),",
            " \"\"",
            " (",
            "P",
            "raise",
            " be",
            " to",
            " God",
            "),",
            " \"\"",
            " (",
            "There",
            " is",
            " no",
            " deity",
            " but",
            " God",
            ")",
            " or",
            " sometimes",
            " \"",
            "l",
            "Äģ",
            " il",
            "Äģ",
            "ha",
            " ill",
            "Äģ",
            " int",
            "a",
            "/",
            " hu",
            "wa",
            "\"",
            " (",
            "There",
            " is",
            " no",
            " deity",
            " but",
            " You"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " boiling",
            " wine",
            ",",
            " which",
            " increases",
            " the",
            " wine",
            "'s",
            " relative",
            " volatility",
            ",",
            " the",
            " fl",
            "amm",
            "ability",
            " of",
            " the",
            " resulting",
            " vap",
            "ors",
            " may",
            " be",
            " enhanced",
            ".",
            " The",
            " dist",
            "illation",
            " of",
            " wine",
            " is",
            " att",
            "ested",
            " in",
            " Arabic",
            " works",
            " attributed",
            " to",
            " al",
            "-",
            "Kind",
            "Ä«",
            " (",
            "–",
            "873",
            " CE",
            ")",
            " and",
            " to",
            " al",
            "-F",
            "Äģ",
            "r",
            "Äģ",
            "b",
            "Ä«",
            " (",
            "–",
            "950",
            "),",
            " and",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            0.406,
            -0.0,
            -0.0,
            -0.0,
            0.182,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ĳ",
            " g",
            "Äģ",
            "z",
            "art",
            "Äģ",
            ".",
            "l",
            "Äģ",
            "z",
            "award",
            " (",
            "ÙĦØ§",
            "Ø²",
            "ÙĪØ±Ø¯",
            ")",
            " is",
            " taken",
            " from",
            " Persian",
            " ÙĦØ§",
            "Úĺ",
            "ÙĪØ±Ø¯",
            " l",
            "Äģ",
            "j",
            "vard",
            ",",
            " the",
            " name",
            " of",
            " a",
            " blue",
            " stone",
            ",",
            " l",
            "apis",
            " laz",
            "uli",
            ".",
            " This",
            " word",
            " was",
            " borrowed",
            " in",
            " several",
            " European",
            " languages",
            " to",
            " mean",
            " (",
            "light",
            ")",
            " blue",
            " –",
            " azure",
            " in",
            " English",
            ",",
            " az",
            "ur",
            " in",
            " French"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.414,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Äģ",
            "\"",
            " (",
            "in",
            " which",
            " it",
            " means",
            " \"",
            "state",
            "/c",
            "ity",
            "\").",
            "j",
            "az",
            "Ä«",
            "rah",
            " (",
            "Ø¬Ø²",
            "ÙĬØ±Ø©",
            "),",
            " as",
            " in",
            " the",
            " well",
            "-known",
            " form",
            " Ø§ÙĦØ¬Ø²",
            "ÙĬØ±Ø©",
            " \"",
            "Al",
            "-J",
            "azeera",
            "\",",
            " means",
            " \"",
            "is",
            "land",
            "\"",
            " and",
            " has",
            " its",
            " origin",
            " in",
            " the",
            " Sy",
            "ri",
            "ac",
            " ",
            "Ü",
            "ĵ",
            "Ü",
            "µ",
            "Ü",
            "Ļ",
            "Ü",
            "²",
            "Ü",
            "ª",
            "Ü",
            "¬",
            "Ü",
            "µ",
            "Ü"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " their",
            " atoms",
            ".",
            "In",
            " ",
            "187",
            "4",
            ",",
            " Jacob",
            "us",
            " Hen",
            "ric",
            "us",
            " van",
            " '",
            "t",
            " Hoff",
            " proposed",
            " that",
            " the",
            " carbon",
            " atom",
            " bonds",
            " to",
            " other",
            " atoms",
            " in",
            " a",
            " tet",
            "rah",
            "edral",
            " arrangement",
            ".",
            " Working",
            " from",
            " this",
            ",",
            " he",
            " explained",
            " the",
            " structures",
            " of",
            " organic",
            " molecules",
            " in",
            " such",
            " a",
            " way",
            " that",
            " he",
            " could",
            " predict",
            " how",
            " many",
            " is",
            "omers",
            " a",
            " compound",
            " could",
            " have",
            ".",
            " Consider",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.326,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "12",
            ",",
            " although",
            " oct",
            "ah",
            "edral",
            " hex",
            "aco",
            "ordination",
            " is",
            " its",
            " preferred",
            " mode",
            ".",
            " In",
            " aque",
            "ous",
            " solution",
            ",",
            " the",
            " alk",
            "ali",
            " metal",
            " ions",
            " exist",
            " as",
            " oct",
            "ah",
            "edral",
            " hex",
            "ahy",
            "dr",
            "ate",
            " complexes",
            " ([",
            "M",
            "(H",
            "2",
            "O",
            ")",
            "6",
            ")]",
            "+",
            "),",
            " with",
            " the",
            " exception",
            " of",
            " the",
            " lithium",
            " ion",
            ",",
            " which",
            " due",
            " to",
            " its",
            " small",
            " size",
            " forms",
            " tet",
            "rah",
            "edral"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " nuts",
            " and",
            " bolts",
            " is",
            " ANSI",
            "/",
            "AS",
            "ME",
            " B",
            "1",
            ".",
            "1",
            " which",
            " was",
            " defined",
            " in",
            " ",
            "193",
            "5",
            ",",
            " ",
            "194",
            "9",
            ",",
            " ",
            "198",
            "9",
            ",",
            " and",
            " ",
            "200",
            "3",
            ".",
            " The",
            " ANSI",
            "-",
            "NS",
            "F",
            " International",
            " standards",
            " used",
            " for",
            " commercial",
            " kitchens",
            ",",
            " such",
            " as",
            " restaurants",
            ",",
            " caf",
            "eter",
            "ias",
            ",",
            " del",
            "is",
            ",",
            " etc",
            ".",
            " The",
            " ANSI",
            "/AP",
            "SP"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "ph",
            "odel",
            "i",
            " (",
            "As",
            "ph",
            "odel",
            "aceae",
            "),",
            " Narc",
            "iss",
            "i",
            " (",
            "A",
            "mary",
            "ll",
            "id",
            "aceae",
            ")",
            " and",
            " I",
            "rides",
            " (",
            "I",
            "rid",
            "aceae",
            "),",
            " the",
            " remainder",
            " are",
            " now",
            " allocated",
            " to",
            " other",
            " orders",
            ".",
            " J",
            "uss",
            "ieu",
            "'s",
            " As",
            "par",
            "agi",
            " soon",
            " came",
            " to",
            " be",
            " referred",
            " to",
            " as",
            " As",
            "par",
            "ag",
            "ac",
            "Ã©es",
            " in",
            " the",
            " French",
            " literature",
            " (",
            "Latin",
            ":",
            " As"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            " often",
            " seem",
            "A",
            " bright",
            " chap",
            " and",
            " not",
            " awkward",
            ",",
            "None",
            " comes",
            " close",
            " to",
            " Am",
            "yn",
            "ias",
            ",",
            "Son",
            " of",
            " Sel",
            "los",
            " of",
            " the",
            " Big",
            "wig",
            "Cl",
            "an",
            ",",
            " a",
            " man",
            " I",
            " once",
            " saw",
            "D",
            "ine",
            " with",
            " rich",
            " Le",
            "og",
            "orus",
            ".",
            "Now",
            " as",
            " poor",
            " as",
            " Ant",
            "iph",
            "on",
            ",",
            "He",
            " lives",
            " on",
            " apples",
            " and",
            " p",
            "ome",
            "gran",
            "ates"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.207,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "rom",
            "antis",
            ",)",
            " from",
            " ,",
            " \"",
            "phys",
            "ician",
            "\",",
            " and",
            " ,",
            " \"",
            "pro",
            "phet",
            "\",",
            " referring",
            " to",
            " his",
            " role",
            " as",
            " a",
            " god",
            " both",
            " of",
            " healing",
            " and",
            " of",
            " prophecy",
            "Les",
            "chen",
            "or",
            "ius",
            " (",
            " ;",
            " ,",
            " Les",
            "kh",
            "Äĵ",
            "nor",
            "ios",
            "),",
            " from",
            " ,",
            " \"",
            "con",
            "vers",
            "er",
            "\"",
            "Lo",
            "x",
            "ias",
            " (",
            " ;",
            " ,",
            " L",
            "ox",
            "ias",
            "),",
            " from",
            " ,",
            " \"",
            "to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " the",
            " human",
            " community",
            " that",
            " wor",
            "ships",
            " it",
            ".",
            "Urban",
            " ",
            "Urban",
            " anthropology",
            " is",
            " concerned",
            " with",
            " issues",
            " of",
            " urban",
            "ization",
            ",",
            " poverty",
            ",",
            " and",
            " neoliberal",
            "ism",
            ".",
            " U",
            "lf",
            " H",
            "anner",
            "z",
            " quotes",
            " a",
            " ",
            "196",
            "0",
            "s",
            " remark",
            " that",
            " traditional",
            " anthrop",
            "ologists",
            " were",
            " \"",
            "a",
            " notoriously",
            " ag",
            "or",
            "aph",
            "obic",
            " lot",
            ",",
            " anti",
            "-",
            "urban",
            " by",
            " definition",
            "\".",
            " Various",
            " social",
            " processes",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.215,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.152,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "199",
            "5",
            ")",
            "190",
            "9",
            " –",
            " Robert",
            " Edison",
            " Fulton",
            " Jr",
            ".,",
            " American",
            " inventor",
            " and",
            " adventurer",
            " (",
            "d",
            ".",
            " ",
            "200",
            "4",
            ")",
            "191",
            "0",
            " –",
            " S",
            "ulo",
            " B",
            "Ã¤",
            "rl",
            "und",
            ",",
            " Finnish",
            " shot",
            " put",
            "ter",
            " (",
            "d",
            ".",
            " ",
            "198",
            "6",
            ")",
            " ",
            " ",
            "191",
            "0",
            "  ",
            " –",
            " Miguel",
            " Naj",
            "dorf",
            ",",
            " Polish",
            "-",
            "Arg",
            "entin",
            "ian",
            " chess",
            " player",
            " and",
            " theoret"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.215,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " but",
            " there",
            " is",
            " little",
            " danger",
            " in",
            " eating",
            " fish",
            " because",
            " this",
            " arsen",
            "ic",
            " compound",
            " is",
            " nearly",
            " non",
            "-to",
            "xic",
            ".",
            "Environmental",
            " issues",
            "Ex",
            "posure",
            " ",
            "N",
            "aturally",
            " occurring",
            " sources",
            " of",
            " human",
            " exposure",
            " include",
            " volcanic",
            " ash",
            ",",
            " weather",
            "ing",
            " of",
            " minerals",
            " and",
            " ores",
            ",",
            " and",
            " mineral",
            "ized",
            " groundwater",
            ".",
            " Ar",
            "sen",
            "ic",
            " is",
            " also",
            " found",
            " in",
            " food",
            ",",
            " water",
            ",",
            " soil",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " inst",
            "igation",
            " of",
            " Charles",
            ",",
            " Count",
            " of",
            " Val",
            "ois",
            ".",
            "149",
            "2",
            " –",
            " Spain",
            " gives",
            " Christopher",
            " Columbus",
            " his",
            " commission",
            " of",
            " exploration",
            ".",
            " He",
            " is",
            " named",
            " adm",
            "iral",
            " of",
            " the",
            " ocean",
            " sea",
            ",",
            " v",
            "icer",
            "oy",
            " and",
            " governor",
            " of",
            " any",
            " territory",
            " he",
            " discovers",
            ".",
            " ",
            "151",
            "3",
            " –",
            " Edmund",
            " de",
            " la",
            " Pole",
            ",",
            " York",
            "ist",
            " pret",
            "ender",
            " to",
            " the",
            " English",
            " throne",
            ",",
            " is"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " company",
            "'s",
            " first",
            " product",
            " was",
            " the",
            " Apple",
            " I",
            ",",
            " a",
            " computer",
            " designed",
            " and",
            " hand",
            "-built",
            " entirely",
            " by",
            " W",
            "oz",
            "ni",
            "ak",
            ".",
            " To",
            " finance",
            " its",
            " creation",
            ",",
            " Jobs",
            " sold",
            " his",
            " Volkswagen",
            " Bus",
            ",",
            " and",
            " W",
            "oz",
            "ni",
            "ak",
            " sold",
            " his",
            " HP",
            "-",
            "65",
            " calculator",
            ".",
            " W",
            "oz",
            "ni",
            "ak",
            " debuted",
            " the",
            " first",
            " prototype",
            " Apple",
            " I",
            " at",
            " the",
            " Home",
            "brew",
            " Computer",
            " Club"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " eliminated",
            ".",
            "During",
            " the",
            " Nap",
            "ole",
            "onic",
            " Wars",
            " in",
            " the",
            " late",
            " ",
            "18",
            "th",
            " century",
            " and",
            " early",
            " ",
            "19",
            "th",
            " century",
            ",",
            " Napoleon",
            " annex",
            "ed",
            " territory",
            " formerly",
            " controlled",
            " by",
            " the",
            " House",
            " of",
            " H",
            "abs",
            "burg",
            ",",
            " and",
            " the",
            " House",
            " of",
            " Sav",
            "oy",
            ".",
            " In",
            " ",
            "179",
            "8",
            ",",
            " the",
            " Hel",
            "v",
            "etic",
            " Republic",
            " was",
            " established",
            ",",
            " two",
            " years",
            " later",
            " an",
            " army",
            " across"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ian",
            " anatomy",
            " ",
            "Am",
            "ph",
            "ib",
            "ians",
            " are",
            " a",
            " class",
            " of",
            " animals",
            " comprising",
            " frogs",
            ",",
            " sal",
            "am",
            "anders",
            " and",
            " ca",
            "ec",
            "ilians",
            ".",
            " They",
            " are",
            " tet",
            "rap",
            "ods",
            ",",
            " but",
            " the",
            " ca",
            "ec",
            "ilians",
            " and",
            " a",
            " few",
            " species",
            " of",
            " sal",
            "am",
            "ander",
            " have",
            " either",
            " no",
            " limbs",
            " or",
            " their",
            " limbs",
            " are",
            " much",
            " reduced",
            " in",
            " size",
            ".",
            " Their",
            " main",
            " bones",
            " are",
            " hollow",
            " and",
            " lightweight",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ab",
            "stinence",
            " (",
            "also",
            " known",
            " as",
            " the",
            " \"",
            "Final",
            " Count",
            "\").",
            " Z",
            "os",
            "imos",
            " explains",
            " that",
            " the",
            " ancient",
            " practice",
            " of",
            " \"",
            "t",
            "inct",
            "ures",
            "\"",
            " (",
            "the",
            " technical",
            " Greek",
            " name",
            " for",
            " the",
            " al",
            "chemical",
            " arts",
            ")",
            " had",
            " been",
            " taken",
            " over",
            " by",
            " certain",
            " \"",
            "dem",
            "ons",
            "\"",
            " who",
            " taught",
            " the",
            " art",
            " only",
            " to",
            " those",
            " who",
            " offered",
            " them",
            " sacrifices",
            ".",
            " Since",
            " Z",
            "os",
            "imos",
            " also"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " operated",
            " successfully",
            " under",
            " the",
            " H",
            "afs",
            "ids",
            ",",
            " moved",
            " their",
            " base",
            " of",
            " operations",
            " to",
            " Alg",
            "iers",
            ".",
            " They",
            " succeeded",
            " in",
            " conqu",
            "ering",
            " J",
            "ij",
            "el",
            " and",
            " Alg",
            "iers",
            " from",
            " the",
            " Span",
            "iards",
            " with",
            " help",
            " from",
            " the",
            " locals",
            " who",
            " saw",
            " them",
            " as",
            " liber",
            "ators",
            " from",
            " the",
            " Christians",
            ",",
            " but",
            " the",
            " brothers",
            " eventually",
            " assass",
            "inated",
            " the",
            " local",
            " noble",
            " Sal",
            "im",
            " al",
            "-T",
            "umi",
            " and",
            " took"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    " âĹĦ",
    "achuset",
    "Ã±ana",
    "erdale",
    "(EXPR"
  ],
  "bottom_logits": [
    " ",
    " Ins",
    " Wool",
    " scarcely",
    " commerce"
  ],
  "act_min": -0.0,
  "act_max": 0.602
}