{
  "index": 69564,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " algorithm",
            " in",
            " pseud",
            "ocode",
            " or",
            " pid",
            "gin",
            " code",
            ":",
            " Input",
            ":",
            " A",
            " list",
            " of",
            " numbers",
            " L",
            ".",
            " Output",
            ":",
            " The",
            " largest",
            " number",
            " in",
            " the",
            " list",
            " L",
            ".",
            " if",
            " L",
            ".size",
            " =",
            " ",
            "0",
            " return",
            " null",
            " largest",
            " âĨĲ",
            " L",
            "[",
            "0",
            "]",
            " for",
            " each",
            " item",
            " in",
            " L",
            ",",
            " do",
            "    ",
            " if",
            " item",
            " >",
            " largest",
            ",",
            " then",
            "        ",
            " largest",
            " âĨĲ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "d",
            "iffer",
            "ently",
            ")",
            " in",
            " paperback",
            " in",
            " ",
            "196",
            "3",
            ")",
            " Destination",
            ":",
            " Universe",
            "!",
            " (",
            "195",
            "2",
            ")",
            " The",
            " Tw",
            "isted",
            " Men",
            " (",
            "196",
            "4",
            ")",
            " Monsters",
            " (",
            "196",
            "5",
            ")",
            " (",
            "later",
            " as",
            " SF",
            " Monsters",
            " (",
            "196",
            "7",
            "))",
            " ab",
            "ridged",
            " as",
            " The",
            " Bl",
            "al",
            " (",
            "197",
            "6",
            ")",
            " A",
            " Van",
            " Vog",
            "t",
            " Omn",
            "ibus",
            " (",
            "196",
            "7",
            "),",
            " omn",
            "ibus"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            ":",
            " Alexandria",
            " (",
            "Egypt",
            "),",
            " Alexandre",
            "ia",
            " (",
            "G",
            "reece",
            "),",
            " Is",
            "k",
            "ender",
            "un",
            " (",
            "Turkey",
            "),",
            " Is",
            "k",
            "and",
            "ari",
            "ya",
            " (",
            "Iraq",
            ")",
            " and",
            " K",
            "and",
            "ah",
            "ar",
            " (",
            "Af",
            "ghan",
            "istan",
            ").",
            "F",
            "unding",
            " of",
            " temples",
            "In",
            " ",
            "334",
            " BC",
            ",",
            " Alexander",
            " the",
            " Great",
            " donated",
            " funds",
            " for",
            " the",
            " completion",
            " of",
            " the",
            " new",
            " temple",
            " of",
            " Athena",
            " Pol",
            "ias"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " batteries",
            " installed",
            "|",
            "with",
            " batteries",
            " installed",
            "|",
            "with",
            " batteries",
            " installed",
            "|",
            "|",
            "|",
            "?",
            "|",
            "|",
            "with",
            " batteries",
            " installed",
            "|",
            "|",
            " colspan",
            "=\"",
            "2",
            "\"",
            " |",
            "|",
            "?",
            "|",
            "?",
            "|",
            "?",
            "|",
            "?",
            "|",
            "?",
            "|}",
            "*",
            " ",
            " V",
            "aries",
            " with",
            " installed",
            " OS",
            "Notes",
            ":",
            " The",
            " e",
            "Mate",
            " ",
            "300",
            " actually",
            " has",
            " ROM",
            " chips",
            " silk",
            " screened",
            " with"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.4,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " social",
            "ite",
            " Honey",
            " Berlin",
            ".",
            " The",
            " play",
            " featured",
            " Jay",
            "ne",
            " County",
            " as",
            " \"",
            "V",
            "ul",
            "va",
            "\"",
            " and",
            " Cherry",
            " Vanilla",
            " as",
            " \"",
            "A",
            "manda",
            " Pork",
            "\".",
            " In",
            " ",
            "197",
            "4",
            ",",
            " Andy",
            " War",
            "hol",
            " also",
            " produced",
            " the",
            " stage",
            " musical",
            " Man",
            " on",
            " the",
            " Moon",
            ",",
            " which",
            " was",
            " written",
            " by",
            " John",
            " Phillips",
            " of",
            " the",
            " M",
            "amas",
            " and",
            " the",
            " Pap",
            "as",
            ".",
            " Photography",
            ":",
            " To"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.181,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Sh",
            "attered",
            " Web",
            " [#",
            "56",
            "–",
            "60",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "13",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " [#",
            "61",
            "–",
            "65",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " #",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "14",
            ":",
            " Ch",
            "ameleon",
            " Conspiracy",
            " [#",
            "66",
            "–",
            "69",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Ch",
            "ameleon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "21",
            " languages",
            " of",
            " Burma",
            ",",
            " southern",
            " China",
            ",",
            " and",
            " Thailand",
            " Nuclear",
            " Mon",
            "–",
            "Kh",
            "mer",
            " languages",
            " Kh",
            "mer",
            "o",
            "-V",
            "iet",
            "ic",
            " languages",
            " (",
            "Eastern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " Viet",
            "o",
            "-K",
            "atu",
            "ic",
            " languages",
            " ?",
            " Viet",
            "ic",
            ":",
            " ",
            "10",
            " languages",
            " of",
            " Vietnam",
            " and",
            " Laos",
            ",",
            " including",
            " Mu",
            "ong",
            " and",
            " Vietnamese",
            ",",
            " which",
            " has",
            " the",
            " most",
            " speakers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " UTC",
            ",",
            " GPS",
            ",",
            " L",
            "OR",
            "AN",
            " and",
            " T",
            "AI",
            "Time",
            " scales",
            "<|begin_of_text|>",
            "Al",
            "tru",
            "ism",
            " is",
            " the",
            " principle",
            " and",
            " practice",
            " of",
            " concern",
            " for",
            " the",
            " well",
            "-being",
            " and",
            "/or",
            " happiness",
            " of",
            " other",
            " humans",
            " or",
            " animals",
            ".",
            " While",
            " objects",
            " of",
            " altru",
            "istic",
            " concern",
            " vary",
            ",",
            " it",
            " is",
            " an",
            " important",
            " moral",
            " value",
            " in",
            " many",
            " cultures",
            " and",
            " religions",
            ".",
            " It",
            " may",
            " be",
            " considered",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Become",
            " Human",
            " also",
            " explores",
            " how",
            " android",
            "s",
            " are",
            " treated",
            " as",
            " second",
            " class",
            " citizens",
            " in",
            " a",
            " near",
            " future",
            " society",
            ".",
            "Female",
            " android",
            "s",
            ",",
            " or",
            " \"",
            "g",
            "yn",
            "oids",
            "\",",
            " are",
            " often",
            " seen",
            " in",
            " science",
            " fiction",
            ",",
            " and",
            " can",
            " be",
            " viewed",
            " as",
            " a",
            " continuation",
            " of",
            " the",
            " long",
            " tradition",
            " of",
            " men",
            " attempting",
            " to",
            " create",
            " the",
            " stere",
            "otypical",
            " \"",
            "perfect",
            " woman",
            "\".",
            " Examples",
            " include",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "00",
            " UTC",
            " on",
            " July",
            " ",
            "28",
            ".",
            " Columbia",
            " was",
            " taken",
            " to",
            " Ford",
            " Island",
            " for",
            " de",
            "activation",
            ",",
            " and",
            " its",
            " py",
            "rote",
            "chn",
            "ics",
            " made",
            " safe",
            ".",
            " It",
            " was",
            " then",
            " taken",
            " to",
            " Hick",
            "ham",
            " Air",
            " Force",
            " Base",
            ",",
            " from",
            " whence",
            " it",
            " was",
            " flown",
            " to",
            " Houston",
            " in",
            " a",
            " Douglas",
            " C",
            "-",
            "133",
            " C",
            "arg",
            "om",
            "aster",
            ",",
            " reaching",
            " the",
            " Lunar",
            " Re",
            "ceiving",
            " Laboratory",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.137,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.076,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "57",
            "Âł",
            "minutes",
            ".",
            " Wimbledon",
            ":",
            " The",
            " Record",
            " Break",
            "ers",
            " (",
            "200",
            "5",
            ")",
            " St",
            "arring",
            ":",
            " Andre",
            " Ag",
            "assi",
            ",",
            " Boris",
            " Becker",
            ";",
            " Standing",
            " Room",
            " Only",
            ",",
            " DVD",
            " Release",
            " Date",
            ":",
            " August",
            " ",
            "16",
            ",",
            " ",
            "200",
            "5",
            ",",
            " Run",
            " Time",
            ":",
            " ",
            "52",
            "Âł",
            "minutes",
            ",",
            " .",
            "Video",
            " games",
            " Andre",
            " Ag",
            "assi",
            " Tennis",
            " for",
            " the",
            " Super",
            " Nintendo",
            " Entertainment"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " they",
            " concentrated",
            " on",
            " looking",
            " for",
            " the",
            " philosophers",
            "'",
            " stone",
            ".",
            " Bernard",
            " Tre",
            "vis",
            "an",
            " and",
            " George",
            " Rip",
            "ley",
            " made",
            " similar",
            " contributions",
            ".",
            " Their",
            " crypt",
            "ic",
            " all",
            "usions",
            " and",
            " symbolism",
            " led",
            " to",
            " wide",
            " variations",
            " in",
            " interpretation",
            " of",
            " the",
            " art",
            ".",
            "A",
            " common",
            " idea",
            " in",
            " European",
            " al",
            "chemy",
            " in",
            " the",
            " medieval",
            " era",
            " was",
            " a",
            " metaph",
            "ysical",
            " \"",
            "Hom",
            "eric",
            " chain",
            " of",
            " wise",
            " men",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.18,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.235,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "34",
            ")",
            " or",
            ",",
            " \"",
            "have",
            " taken",
            " place",
            "\"",
            " (",
            "Luke",
            " ",
            "21",
            ":",
            "32",
            ").",
            " Similarly",
            ",",
            " in",
            " ",
            "1",
            "st",
            " Peter",
            " ",
            "1",
            ":",
            "20",
            ",",
            " \"",
            "Christ",
            ",",
            " who",
            " ver",
            "ily",
            " was",
            " fore",
            "ord",
            "ained",
            " before",
            " the",
            " foundation",
            " of",
            " the",
            " world",
            " but",
            " was",
            " manifest",
            " in",
            " these",
            " last",
            " times",
            " for",
            " you",
            "\",",
            " as",
            " well",
            " as",
            " \"",
            "But",
            " the",
            " end",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " '",
            "To",
            " a",
            " gas",
            " chamber",
            "—",
            "go",
            "!.",
            "Rand",
            "'s",
            " non",
            "fiction",
            " received",
            " far",
            " fewer",
            " reviews",
            " than",
            " her",
            " novels",
            ".",
            " The",
            " ten",
            "or",
            " of",
            " the",
            " criticism",
            " for",
            " her",
            " first",
            " non",
            "fiction",
            " book",
            ",",
            " For",
            " the",
            " New",
            " Intellectual",
            ",",
            " was",
            " similar",
            " to",
            " that",
            " for",
            " Atlas",
            " Shr",
            "ugged",
            ".",
            " Phil",
            "osopher",
            " Sidney",
            " Hook",
            " liken",
            "ed",
            " her",
            " certainty",
            " to",
            " \"",
            "the",
            " way",
            " philosophy",
            " is",
            " written"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.226,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Civil",
            " War",
            " [#",
            "532",
            "–",
            "538",
            "]",
            " ()",
            "Vol",
            ".",
            " ",
            "12",
            ":",
            " Back",
            " in",
            " Black",
            " [#",
            "539",
            "–",
            "543",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "17",
            "–",
            "23",
            ",",
            " Annual",
            " #",
            "1",
            "]",
            " ()",
            "Spider",
            "-Man",
            ":",
            " One",
            " More",
            " Day",
            " [#",
            "544",
            "–",
            "545",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "24",
            ";",
            " The",
            " Sens",
            "ational",
            " Spider",
            "-Man",
            " #",
            "41",
            ";",
            " Marvel"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Mol",
            "oday",
            "a",
            " G",
            "vard",
            "ia",
            ".",
            " (",
            "also",
            " a",
            " ",
            "199",
            "2",
            " Simon",
            " &",
            " Sch",
            "uster",
            " edition",
            ")",
            "References",
            "Further",
            " reading",
            " Fine",
            ",",
            " Rue",
            "ben",
            " (",
            "198",
            "3",
            ").",
            " The",
            " World",
            "'s",
            " Great",
            " Chess",
            " Games",
            ".",
            " Dover",
            ".",
            " .",
            " Hur",
            "st",
            ",",
            " Sarah",
            " (",
            "200",
            "2",
            ").",
            " Curse",
            " of",
            " K",
            "irs",
            "an",
            ":",
            " Adventures",
            " in",
            " the",
            " Chess",
            " Under",
            "world"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "2",
            " languages",
            ",",
            " the",
            " Mon",
            " language",
            " of",
            " Burma",
            " and",
            " the",
            " Ny",
            "ah",
            "kur",
            " language",
            " of",
            " Thailand",
            ".",
            "Sid",
            "well",
            " (",
            "200",
            "9",
            "–",
            "201",
            "5",
            ")",
            " ",
            "Paul",
            " Sid",
            "well",
            " (",
            "200",
            "9",
            "),",
            " in",
            " a",
            " le",
            "xic",
            "ostat",
            "istical",
            " comparison",
            " of",
            " ",
            "36",
            " languages",
            " which",
            " are",
            " well",
            " known",
            " enough",
            " to",
            " exclude",
            " loan",
            "words",
            ",",
            " finds",
            " little",
            " evidence",
            " for",
            " internal",
            " branching"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " glob",
            "ular",
            " clusters",
            " Mess",
            "ier",
            "Âł",
            "2",
            ",",
            " Mess",
            "ier",
            "Âł",
            "72",
            ",",
            " and",
            " the",
            " aster",
            "ism",
            " Mess",
            "ier",
            "Âł",
            "73",
            ".",
            " While",
            " M",
            "73",
            " was",
            " originally",
            " catalog",
            "ued",
            " as",
            " a",
            " sp",
            "ars",
            "ely",
            " populated",
            " open",
            " cluster",
            ",",
            " modern",
            " analysis",
            " indicates",
            " the",
            " ",
            "6",
            " main",
            " stars",
            " are",
            " not",
            " close",
            " enough",
            " together",
            " to",
            " fit",
            " this",
            " definition",
            ",",
            " re",
            "class",
            "ifying",
            " M",
            "73"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.123,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Post",
            "card",
            " Book",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " (",
            "200",
            "2",
            ").",
            " ",
            " H",
            "re",
            "ben",
            "i",
            "ak",
            ",",
            " Michael",
            ".",
            " Action",
            " Writing",
            ":",
            " Jack",
            " Ker",
            "ou",
            "ac",
            "'s",
            " Wild",
            " Form",
            ",",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " UP",
            ",",
            " ",
            "200",
            "6",
            ".",
            " Kash",
            "ner",
            ",",
            " Sam",
            ",",
            " When",
            " I",
            " Was",
            " Cool",
            ",",
            " My",
            " Life",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " (",
            "1",
            ")",
            " Korean",
            " did",
            " not",
            " belong",
            " with",
            " the",
            " other",
            " three",
            " gene",
            "alog",
            "ically",
            ",",
            " but",
            " had",
            " been",
            " influenced",
            " by",
            " an",
            " Alta",
            "ic",
            " substr",
            "atum",
            ";",
            " (",
            "2",
            ")",
            " Korean",
            " was",
            " related",
            " to",
            " the",
            " other",
            " three",
            " at",
            " the",
            " same",
            " level",
            " they",
            " were",
            " related",
            " to",
            " each",
            " other",
            ";",
            " (",
            "3",
            ")",
            " Korean",
            " had",
            " split",
            " off",
            " from",
            " the",
            " other",
            " three",
            " before",
            " they",
            " underwent",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "191",
            "9",
            "–",
            "193",
            "9",
            "F",
            "amous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            "While",
            " still",
            " at",
            " Hen",
            "ley",
            "'s",
            ",",
            " he",
            " read",
            " in",
            " a",
            " trade",
            " paper",
            " that",
            " Famous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            ",",
            " the",
            " production",
            " arm",
            " of",
            " Paramount",
            " Pictures",
            ",",
            " was",
            " opening",
            " a",
            " studio",
            " in",
            " London",
            ".",
            " They",
            " were",
            " planning",
            " to",
            " film",
            " The",
            " Sor",
            "rows",
            " of",
            " Satan",
            " by",
            " Marie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " two",
            " lengths",
            " must",
            " not",
            " be",
            " prime",
            " to",
            " one",
            " another",
            ".",
            " Eu",
            "clid",
            " stip",
            "ulated",
            " this",
            " so",
            " that",
            " he",
            " could",
            " construct",
            " a",
            " re",
            "duct",
            "io",
            " ad",
            " absurd",
            "um",
            " proof",
            " that",
            " the",
            " two",
            " numbers",
            "'",
            " common",
            " measure",
            " is",
            " in",
            " fact",
            " the",
            " greatest",
            ".",
            " While",
            " Nic",
            "om",
            "ach",
            "us",
            "'",
            " algorithm",
            " is",
            " the",
            " same",
            " as",
            " Eu",
            "clid",
            "'s",
            ",",
            " when",
            " the",
            " numbers",
            " are",
            " prime"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " character",
            " who",
            " first",
            " questions",
            " whether",
            " it",
            " is",
            " moral",
            " to",
            " turn",
            " a",
            " violent",
            " person",
            " into",
            " a",
            " behavioural",
            " autom",
            "aton",
            " who",
            " can",
            " make",
            " no",
            " choice",
            " in",
            " such",
            " matters",
            ".",
            " This",
            " is",
            " the",
            " only",
            " character",
            " who",
            " is",
            " truly",
            " concerned",
            " about",
            " Alex",
            "'s",
            " welfare",
            ";",
            " he",
            " is",
            " not",
            " taken",
            " seriously",
            " by",
            " Alex",
            ",",
            " though",
            ".",
            " He",
            " is",
            " nicknamed",
            " by",
            " Alex",
            " \"",
            "pr",
            "ison",
            " char",
            "lie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.14,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.184,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.218,
            -0.0,
            -0.0,
            -0.0,
            0.24
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            " Birth",
            " of",
            " the",
            " Newton",
            ":",
            " ",
            " The",
            " Newton",
            " Hall",
            " of",
            " Fame",
            ":",
            " People",
            " behind",
            " the",
            " Newton",
            ":",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Why",
            " did",
            " Apple",
            " kill",
            " the",
            " Newton",
            "?:",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Newton",
            " Notes",
            " column",
            " archive",
            ":",
            " ",
            " A",
            ".I",
            ".",
            " Magazine",
            " article",
            " by",
            " Ya",
            "eger",
            " on",
            " Newton",
            " H",
            "WR",
            " design",
            ",",
            " algorithms",
            ",",
            " &",
            " quality",
            ":",
            " ",
            " Associated",
            " slides",
            ":"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " potassium",
            "-",
            "40",
            ",",
            " van",
            "adium",
            "-",
            "50",
            ",",
            " lan",
            "than",
            "um",
            "-",
            "138",
            ",",
            " and",
            " lut",
            "et",
            "ium",
            "-",
            "176",
            ".",
            " Most",
            " odd",
            "-",
            "odd",
            " nuclei",
            " are",
            " highly",
            " unstable",
            " with",
            " respect",
            " to",
            " beta",
            " decay",
            ",",
            " because",
            " the",
            " decay",
            " products",
            " are",
            " even",
            "-even",
            ",",
            " and",
            " are",
            " therefore",
            " more",
            " strongly",
            " bound",
            ",",
            " due",
            " to",
            " nuclear",
            " pairing",
            " effects",
            ".",
            "Mass",
            " ",
            "The",
            " large",
            " majority"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " \"",
            "If",
            " you",
            " see",
            " something",
            " horrible",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ",",
            " and",
            " if",
            " you",
            " see",
            " something",
            " beautiful",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ".\"",
            "After",
            " returning",
            " to",
            " the",
            " United",
            " States",
            ",",
            " a",
            " chance",
            " encounter",
            " on",
            " a",
            " New",
            " York",
            " City",
            " street",
            " with",
            " Ch",
            "Ã¶",
            "gy",
            "am",
            " Trung",
            "pa",
            " Rin",
            "po",
            "che",
            " (",
            "they",
            " both",
            " tried",
            " to",
            " catch",
            " the",
            " same",
            " cab",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " annual",
            " production",
            " of",
            " aluminium",
            " exceeded",
            " ",
            "50",
            ",",
            "000",
            ",",
            "000",
            " metric",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            "The",
            " real",
            " price",
            " for",
            " aluminium",
            " declined",
            " from",
            " $",
            "14",
            ",",
            "000",
            " per",
            " metric",
            " ton",
            " in",
            " ",
            "190",
            "0",
            " to",
            " $",
            "2",
            ",",
            "340",
            " in",
            " ",
            "194",
            "8",
            " (",
            "in",
            " ",
            "199",
            "8",
            " United",
            " States",
            " dollars",
            ").",
            " Extraction",
            " and",
            " processing",
            " costs",
            " were",
            " lowered",
            " over"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.138,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " S",
            "ins",
            " of",
            " Norman",
            " Os",
            "born",
            " #",
            "1",
            ",",
            " FC",
            "BD",
            " ",
            "202",
            "0",
            ":",
            " Spider",
            "-Man",
            "/V",
            "en",
            "om",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "11",
            ":",
            " Last",
            " Rem",
            "ains",
            " [#",
            "50",
            "–",
            "55",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            ":",
            " Last",
            " Rem",
            "ains",
            " Companion",
            " [#",
            "50",
            ".",
            "1",
            "–",
            "54",
            ".",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "12"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.181,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "32",
            " UTC",
            ",",
            " and",
            " it",
            " was",
            " the",
            " fifth",
            " crew",
            "ed",
            " mission",
            " of",
            " NASA",
            "'s",
            " Apollo",
            " program",
            ".",
            " The",
            " Apollo",
            " spacecraft",
            " had",
            " three",
            " parts",
            ":",
            " a",
            " command",
            " module",
            " (",
            "CM",
            ")",
            " with",
            " a",
            " cabin",
            " for",
            " the",
            " three",
            " astronauts",
            ",",
            " the",
            " only",
            " part",
            " that",
            " returned",
            " to",
            " Earth",
            ";",
            " a",
            " service",
            " module",
            " (",
            "SM",
            "),",
            " which",
            " supported",
            " the",
            " command",
            " module",
            " with",
            " propulsion",
            ",",
            " electrical",
            " power"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " they",
            " concentrated",
            " on",
            " looking",
            " for",
            " the",
            " philosophers",
            "'",
            " stone",
            ".",
            " Bernard",
            " Tre",
            "vis",
            "an",
            " and",
            " George",
            " Rip",
            "ley",
            " made",
            " similar",
            " contributions",
            ".",
            " Their",
            " crypt",
            "ic",
            " all",
            "usions",
            " and",
            " symbolism",
            " led",
            " to",
            " wide",
            " variations",
            " in",
            " interpretation",
            " of",
            " the",
            " art",
            ".",
            "A",
            " common",
            " idea",
            " in",
            " European",
            " al",
            "chemy",
            " in",
            " the",
            " medieval",
            " era",
            " was",
            " a",
            " metaph",
            "ysical",
            " \"",
            "Hom",
            "eric",
            " chain",
            " of",
            " wise",
            " men",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.113,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " Battle",
            " of",
            " Fort",
            " Pillow",
            ":",
            " Confederate",
            " forces",
            " kill",
            " most",
            " of",
            " the",
            " African",
            " American",
            " soldiers",
            " that",
            " surrendered",
            " at",
            " Fort",
            " Pillow",
            ",",
            " Tennessee",
            ".",
            "186",
            "5",
            " –",
            " American",
            " Civil",
            " War",
            ":",
            " Mobile",
            ",",
            " Alabama",
            ",",
            " falls",
            " to",
            " the",
            " Union",
            " Army",
            ".",
            "187",
            "7",
            " –",
            " The",
            " United",
            " Kingdom",
            " annex",
            "es",
            " the",
            " Trans",
            "va",
            "al",
            ".",
            "190",
            "0",
            " –",
            " One",
            " day",
            " after",
            " its",
            " enactment",
            " by"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " a",
            " concentrated",
            " action",
            ",",
            " happening",
            " in",
            " one",
            " place",
            ",",
            " within",
            " the",
            " span",
            " of",
            " a",
            " single",
            " day",
            ".",
            "Several",
            " of",
            " T",
            "ark",
            "ovsky",
            "'s",
            " films",
            " have",
            " color",
            " or",
            " black",
            "-and",
            "-white",
            " sequences",
            ".",
            " This",
            " first",
            " occurs",
            " in",
            " the",
            " otherwise",
            " mon",
            "ochrome",
            " Andre",
            "i",
            " Rub",
            "lev",
            ",",
            " which",
            " features",
            " a",
            " color",
            " ep",
            "ilogue",
            " of",
            " Rub",
            "lev",
            "'s",
            " authentic",
            " religious",
            " icon",
            " paintings",
            ".",
            " All",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.249,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Against",
            " the",
            " State",
            " Hans",
            "-H",
            "ermann",
            " Hop",
            "pe",
            ",",
            " An",
            "ar",
            "cho",
            "-C",
            "ap",
            "ital",
            "ism",
            ":",
            " An",
            " An",
            "notated",
            " Bibli",
            "ography",
            " A",
            " Theory",
            " of",
            " Social",
            "ism",
            " and",
            " Capital",
            "ism",
            " Democracy",
            ":",
            " The",
            " God",
            " That",
            " Failed",
            " The",
            " Economics",
            " and",
            " Ethics",
            " of",
            " Private",
            " Property",
            " Linda",
            " and",
            " Morris",
            " T",
            "anne",
            "hill",
            ",",
            " The",
            " Market",
            " for",
            " Liberty",
            " Michael",
            " Hu"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.128,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.332,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " age",
            "total",
            ":",
            " ",
            "15",
            ".",
            "9",
            " years",
            ".",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "225",
            "th",
            "male",
            ":",
            " ",
            "15",
            ".",
            "4",
            " years",
            " ",
            "female",
            ":",
            " ",
            "16",
            ".",
            "4",
            " years",
            " (",
            "202",
            "0",
            " est",
            ".)",
            "total",
            ":",
            " ",
            "15",
            ".",
            "9",
            " years",
            ".",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " ",
            "224",
            "th",
            "male",
            ":",
            " ",
            "15",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " oxygen",
            " for",
            " every",
            " ",
            "140",
            "Âł",
            "g",
            " of",
            " nitrogen",
            ",",
            " and",
            " in",
            " nitrogen",
            " dioxide",
            " there",
            " is",
            " ",
            "320",
            "Âł",
            "g",
            " of",
            " oxygen",
            " for",
            " every",
            " ",
            "140",
            "Âł",
            "g",
            " of",
            " nitrogen",
            ".",
            " ",
            "80",
            ",",
            " ",
            "160",
            ",",
            " and",
            " ",
            "320",
            " form",
            " a",
            " ratio",
            " of",
            " ",
            "1",
            ":",
            "2",
            ":",
            "4",
            ".",
            " The",
            " respective",
            " formulas",
            " for",
            " these",
            " ox",
            "ides",
            " are",
            " N",
            "2",
            "O",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.33,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.236,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " Title",
            " Design",
            ":",
            " rejected",
            " in",
            " ",
            "199",
            "9",
            "Special",
            " categories",
            " ",
            "The",
            " Special",
            " Academy",
            " Awards",
            " are",
            " voted",
            " on",
            " by",
            " special",
            " committees",
            ",",
            " rather",
            " than",
            " by",
            " the",
            " Academy",
            " membership",
            " as",
            " a",
            " whole",
            ".",
            " They",
            " are",
            " not",
            " always",
            " presented",
            " on",
            " an",
            " annual",
            " basis",
            ".",
            "Current",
            " special",
            " categories",
            " ",
            " Academy",
            " Hon",
            "orary",
            " Award",
            ":",
            " since",
            " ",
            "192",
            "9",
            " Academy",
            " Scientific",
            " and",
            " Technical",
            " Award",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "ness",
            ":",
            " different",
            " voices",
            " singing",
            " the",
            " same",
            " music",
            " together",
            ".",
            "Sch",
            "we",
            "itzer",
            " also",
            " studied",
            " piano",
            " under",
            " Is",
            "idor",
            " Philipp",
            ",",
            " head",
            " of",
            " the",
            " piano",
            " department",
            " at",
            " the",
            " Paris",
            " Conserv",
            "atory",
            ".",
            "In",
            " ",
            "190",
            "5",
            ",",
            " W",
            "idor",
            " and",
            " Schwe",
            "itzer",
            " were",
            " among",
            " the",
            " six",
            " musicians",
            " who",
            " founded",
            " the",
            " Paris",
            " Bach",
            " Society",
            ",",
            " a",
            " choir",
            " dedicated",
            " to",
            " performing",
            " J",
            ".",
            " S",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " algorithms",
            " List",
            " of",
            " algorithm",
            " general",
            " topics",
            " Regulation",
            " of",
            " algorithms",
            " Theory",
            " of",
            " computation",
            " Comput",
            "ability",
            " theory",
            " Computational",
            " complexity",
            " theory",
            " Computational",
            " mathematics",
            "Notes",
            "B",
            "ibli",
            "ography",
            " ",
            " ",
            " Bell",
            ",",
            " C",
            ".",
            " Gordon",
            " and",
            " New",
            "ell",
            ",",
            " Allen",
            " (",
            "197",
            "1",
            "),",
            " Computer",
            " Structures",
            ":",
            " Read",
            "ings",
            " and",
            " Examples",
            ",",
            " McG",
            "raw",
            "–",
            "H",
            "ill",
            " Book"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Indian",
            " War",
            ".",
            " The",
            " thirteen",
            " colonial",
            " state",
            " legisl",
            "atures",
            " independently",
            " funded",
            " and",
            " controlled",
            " their",
            " local",
            " militias",
            ".",
            " In",
            " the",
            " American",
            " Revolution",
            ",",
            " they",
            " trained",
            " and",
            " provided",
            " Continental",
            " Line",
            " reg",
            "iments",
            " to",
            " the",
            " regular",
            " army",
            ",",
            " each",
            " with",
            " their",
            " own",
            " state",
            " officer",
            " corps",
            ".",
            " Mot",
            "ivation",
            " was",
            " also",
            " a",
            " major",
            " asset",
            ":",
            " each",
            " colonial",
            " capital",
            " had",
            " its",
            " own",
            " newspapers",
            " and",
            " printers",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.117,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " Languages",
            ":",
            " Studies",
            " in",
            " Honour",
            " of",
            " Roy",
            " Andrew",
            " Miller",
            " on",
            " His",
            " ",
            "75",
            "th",
            " Birthday",
            ",",
            " edited",
            " by",
            " Karl",
            " H",
            ".",
            " Meng",
            "es",
            " and",
            " N",
            "elly",
            " Na",
            "umann",
            ",",
            " ",
            "1",
            "–",
            "13",
            ".",
            " W",
            "ies",
            "bad",
            "en",
            ":",
            " Otto",
            " Harr",
            "ass",
            "owitz",
            ".",
            " (",
            "Also",
            ":",
            " HTML",
            " version",
            ".)",
            "J",
            "oh",
            "anson",
            ",",
            " Lars",
            ".",
            " ",
            "199",
            "9",
            ".",
            " \"",
            "At",
            "tract"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.227,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "41",
            ".",
            " Academic",
            " Press",
            ",",
            " London",
            ".",
            " J",
            "udd",
            ",",
            " W",
            ".",
            " S",
            ".",
            " et",
            " al",
            ".",
            " (",
            "199",
            "9",
            ").",
            " Plant",
            " System",
            "atics",
            ":",
            " A",
            " Phy",
            "logen",
            "etic",
            " Approach",
            ".",
            " Sunderland",
            ",",
            " MA",
            ":",
            " S",
            "ina",
            "uer",
            " Associates",
            ",",
            " Inc",
            ".",
            "  ",
            " ",
            " Niet",
            "o",
            " Fel",
            "iner",
            ",",
            " Gonz",
            "alo",
            ";",
            " Jury",
            ",",
            " Stephen",
            " Leonard",
            " &",
            " Herr",
            "ero",
            " Niet",
            "o",
            ",",
            " Alberto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.218,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " people",
            " in",
            " a",
            " war",
            " with",
            " the",
            " king",
            " of",
            " Ar",
            "ad",
            ",",
            " in",
            " consequence",
            " of",
            " which",
            " the",
            " Israel",
            "ites",
            " fled",
            ",",
            " marching",
            " seven",
            " stations",
            " backward",
            " to",
            " Mos",
            "era",
            ",",
            " where",
            " they",
            " performed",
            " the",
            " rites",
            " of",
            " mourning",
            " for",
            " Aaron",
            ";",
            " where",
            "fore",
            " it",
            " is",
            " said",
            ":",
            " \"",
            "There",
            " [",
            "at",
            " Mos",
            "era",
            "]",
            " died",
            " Aaron",
            ".\"",
            "The",
            " r",
            "abb",
            "is",
            " particularly",
            " praise",
            " the",
            " brother"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "primary",
            "\"",
            " position",
            " in",
            " social",
            " hierarchy",
            ",",
            " examples",
            " being",
            " the",
            " concept",
            " of",
            " dominant",
            " \"",
            "alpha",
            "\"",
            " members",
            " in",
            " groups",
            " of",
            " animals",
            ".",
            "Computer",
            " enc",
            "odings",
            " Greek",
            " alpha",
            " /",
            " C",
            "optic",
            " al",
            "fa",
            "For",
            " acc",
            "ented",
            " Greek",
            " characters",
            ",",
            " see",
            " Greek",
            " di",
            "ac",
            "rit",
            "ics",
            ":",
            " Computer",
            " encoding",
            ".",
            " Latin",
            " /",
            " IPA",
            " alpha",
            " Mathematical",
            " /",
            " Technical",
            " alpha",
            "References",
            "Greek"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.169,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "58",
            "th",
            " Academy",
            " Awards",
            ".",
            " Both",
            " the",
            " producer",
            " and",
            " K",
            "uros",
            "awa",
            " himself",
            " attributed",
            " the",
            " failure",
            " to",
            " even",
            " submit",
            " ",
            " for",
            " competition",
            " to",
            " a",
            " misunderstanding",
            ":",
            " because",
            " of",
            " the",
            " academy",
            "'s",
            " arcane",
            " rules",
            ",",
            " no",
            " one",
            " was",
            " sure",
            " whether",
            " ",
            " qualified",
            " as",
            " a",
            " Japanese",
            " film",
            ",",
            " a",
            " French",
            " film",
            " (",
            "due",
            " to",
            " its",
            " financing",
            "),",
            " or",
            " both",
            ",",
            " so",
            " it",
            " was",
            " not"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.163,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "male",
            " ",
            "466",
            ",",
            "085",
            " /",
            "female",
            " ",
            "540",
            ",",
            "452",
            ")",
            "65",
            " years",
            " and",
            " over",
            ":",
            " ",
            "2",
            ".",
            "32",
            "%",
            " (",
            "male",
            " ",
            "296",
            ",",
            "411",
            " /",
            "female",
            " ",
            "408",
            ",",
            "648",
            ")",
            " (",
            "201",
            "8",
            " est",
            ".)",
            "Total",
            " fertility",
            " rate",
            "5",
            ".",
            "83",
            " children",
            " born",
            "/w",
            "oman",
            " (",
            "202",
            "2",
            " est",
            ".)",
            " Country",
            " comparison",
            " to",
            " the",
            " world",
            ":",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.162,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " negotiations",
            " that",
            " failed",
            ".",
            " However",
            ",",
            " President",
            " Lincoln",
            " was",
            " determined",
            " to",
            " hold",
            " all",
            " remaining",
            " Union",
            "-",
            "occupied",
            " forts",
            " in",
            " the",
            " Confeder",
            "acy",
            ":",
            " Fort",
            " Monroe",
            " in",
            " Virginia",
            ",",
            " Fort",
            " Pick",
            "ens",
            ",",
            " Fort",
            " Jefferson",
            " and",
            " Fort",
            " Taylor",
            " in",
            " Florida",
            ",",
            " and",
            " Fort",
            " Sum",
            "ter",
            " in",
            " South",
            " Carolina",
            ".",
            "Battle",
            " of",
            " Fort",
            " Sum",
            "ter",
            "The",
            " American",
            " Civil",
            " War",
            " began",
            " on",
            " April",
            " ",
            "12"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.141,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " D",
            " minor",
            ".",
            " Ch",
            "or",
            "ale",
            " Prel",
            "udes",
            ":",
            " ",
            " (",
            "P",
            "eters",
            " Vol",
            " ",
            "7",
            ",",
            " ",
            "49",
            " (",
            "Le",
            "ipzig",
            " ",
            "4",
            "));",
            " ",
            " (",
            "Vol",
            " ",
            "5",
            ",",
            " ",
            "45",
            ");",
            " ",
            " (",
            "Vol",
            " ",
            "7",
            ",",
            " ",
            "48",
            " (",
            "Le",
            "ipzig",
            " ",
            "6",
            "));",
            " ",
            " (",
            "Vol",
            " ",
            "5",
            ",",
            " ",
            "8",
            ");",
            " ",
            " (",
            "Vol",
            " ",
            "5",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.1,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " two",
            " autobi",
            "ographical",
            " books",
            ",",
            " Inside",
            " the",
            " Third",
            " Reich",
            " and",
            " Sp",
            "and",
            "au",
            ":",
            " The",
            " Secret",
            " Di",
            "aries",
            ".",
            " Spe",
            "er",
            "'s",
            " books",
            " were",
            " a",
            " success",
            ";",
            " the",
            " public",
            " was",
            " fascinated",
            " by",
            " an",
            " inside",
            " view",
            " of",
            " the",
            " Third",
            " Reich",
            ".",
            " Spe",
            "er",
            " died",
            " of",
            " a",
            " stroke",
            " in",
            " ",
            "198",
            "1",
            ".",
            " Little",
            " remains",
            " of",
            " his",
            " personal",
            " architectural",
            " work",
            ".",
            "Through",
            " his",
            " autobi",
            "ographies"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "193",
            "4",
            ",",
            " they",
            " bought",
            " Winter",
            "brook",
            " House",
            " in",
            " Winter",
            "brook",
            ",",
            " a",
            " ham",
            "let",
            " near",
            " Wall",
            "ing",
            "ford",
            ".",
            " This",
            " was",
            " their",
            " main",
            " residence",
            " for",
            " the",
            " rest",
            " of",
            " their",
            " lives",
            " and",
            " the",
            " place",
            " where",
            " Christie",
            " did",
            " much",
            " of",
            " her",
            " writing",
            ".",
            " This",
            " house",
            " also",
            " bears",
            " a",
            " blue",
            " plaque",
            ".",
            " Christie",
            " led",
            " a",
            " quiet",
            " life",
            " despite",
            " being",
            " known",
            " in",
            " Wall",
            "ing",
            "ford",
            ";"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Aristotle",
            " considers",
            " types",
            " of",
            " movement",
            ").",
            " In",
            " contrast",
            " to",
            " earlier",
            " philosophers",
            ",",
            " but",
            " in",
            " accordance",
            " with",
            " the",
            " Egyptians",
            ",",
            " he",
            " placed",
            " the",
            " rational",
            " soul",
            " in",
            " the",
            " heart",
            ",",
            " rather",
            " than",
            " the",
            " brain",
            ".",
            " Not",
            "able",
            " is",
            " Aristotle",
            "'s",
            " division",
            " of",
            " sensation",
            " and",
            " thought",
            ",",
            " which",
            " generally",
            " differed",
            " from",
            " the",
            " concepts",
            " of",
            " previous",
            " philosophers",
            ",",
            " with",
            " the",
            " exception",
            " of",
            " Al",
            "c",
            "ma",
            "eon",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " man",
            " himself",
            ",",
            " as",
            " P",
            "oi",
            "rot",
            " knew",
            " that",
            " his",
            " friend",
            " was",
            " not",
            " a",
            " murderer",
            " and",
            " refused",
            " to",
            " let",
            " a",
            " man",
            " capable",
            " of",
            " manipulating",
            " Hastings",
            " in",
            " such",
            " a",
            " manner",
            " go",
            " on",
            ".",
            "Mrs",
            " Ari",
            "ad",
            "ne",
            " Oliver",
            " ",
            "Detect",
            "ive",
            " novelist",
            " Ari",
            "ad",
            "ne",
            " Oliver",
            " is",
            " Ag",
            "atha",
            " Christie",
            "'s",
            " humorous",
            " self",
            "-car",
            "ic",
            "ature",
            ".",
            " Like",
            " Christie",
            ",",
            " she",
            " is",
            " not"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ()",
            " are",
            " independent",
            " and",
            "Random",
            "ization",
            "-based",
            " analysis",
            "In",
            " a",
            " randomized",
            " controlled",
            " experiment",
            ",",
            " the",
            " treatments",
            " are",
            " randomly",
            " assigned",
            " to",
            " experimental",
            " units",
            ",",
            " following",
            " the",
            " experimental",
            " protocol",
            ".",
            " This",
            " random",
            "ization",
            " is",
            " objective",
            " and",
            " declared",
            " before",
            " the",
            " experiment",
            " is",
            " carried",
            " out",
            ".",
            " The",
            " objective",
            " random",
            "-",
            "assignment",
            " is",
            " used",
            " to",
            " test",
            " the",
            " significance",
            " of",
            " the",
            " null",
            " hypothesis",
            ",",
            " following",
            " the",
            " ideas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "win",
            "'s",
            " ",
            "194",
            "0",
            " book",
            " Hop",
            "ous",
            "ia",
            " or",
            " The",
            " Sexual",
            " and",
            " Economic",
            " Foundations",
            " of",
            " a",
            " New",
            " Society",
            ".",
            "On",
            " ",
            "21",
            " October",
            " ",
            "194",
            "9",
            ",",
            " H",
            "ux",
            "ley",
            " wrote",
            " to",
            " George",
            " Orwell",
            ",",
            " author",
            " of",
            " Nin",
            "ete",
            "en",
            " Eight",
            "y",
            "-F",
            "our",
            ",",
            " congrat",
            "ulating",
            " him",
            " on",
            " \"",
            "how",
            " fine",
            " and",
            " how",
            " profoundly",
            " important",
            " the",
            " book",
            " is",
            "\".",
            " In",
            " his"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "ignon",
    "edback",
    "Enlarge",
    "±Ð¾ÑĤ",
    "CREEN"
  ],
  "bottom_logits": [
    "isia",
    " Kits",
    "izzas",
    "seau",
    " isolation"
  ],
  "act_min": -0.0,
  "act_max": 0.469
}