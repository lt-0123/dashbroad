{
  "index": 46563,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            1.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " capital",
            ",",
            " where",
            " a",
            " high",
            " official",
            " ruled",
            " from",
            " the",
            " city",
            "'s",
            " Pra",
            "et",
            "or",
            "ium",
            ",",
            " a",
            " large",
            " administrative",
            " palace",
            " or",
            " office",
            ".",
            " During",
            " the",
            " ",
            "3",
            "rd",
            " century",
            ",",
            " life",
            " in",
            " A",
            "ncy",
            "ra",
            ",",
            " as",
            " in",
            " other",
            " Anat",
            "olian",
            " towns",
            ",",
            " seems",
            " to",
            " have",
            " become",
            " somewhat",
            " militar",
            "ized",
            " in",
            " response",
            " to",
            " the",
            " inv",
            "asions",
            " and",
            " instability",
            " of",
            " the",
            " town",
            ".",
            "By"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.148,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " capital",
            " of",
            " Per",
            "se",
            "pol",
            "is",
            " via",
            " the",
            " Persian",
            " Royal",
            " Road",
            ".",
            " Alexander",
            " himself",
            " took",
            " selected",
            " troops",
            " on",
            " the",
            " direct",
            " route",
            " to",
            " the",
            " city",
            ".",
            " He",
            " then",
            " stormed",
            " the",
            " pass",
            " of",
            " the",
            " Persian",
            " Gates",
            " (",
            "in",
            " the",
            " modern",
            " Zag",
            "ros",
            " Mountains",
            ")",
            " which",
            " had",
            " been",
            " blocked",
            " by",
            " a",
            " Persian",
            " army",
            " under",
            " Ari",
            "obar",
            "z",
            "anes",
            " and",
            " then",
            " hurried",
            " to",
            " Per",
            "se",
            "pol",
            "is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.938,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " future",
            " investments",
            ".",
            "Op",
            "portunity",
            " cost",
            " is",
            " a",
            " key",
            " concept",
            " in",
            " mainstream",
            " economics",
            " and",
            " has",
            " been",
            " described",
            " as",
            " expressing",
            " \"",
            "the",
            " basic",
            " relationship",
            " between",
            " scarcity",
            " and",
            " choice",
            "\".",
            " The",
            " notion",
            " of",
            " opportunity",
            " cost",
            " plays",
            " a",
            " crucial",
            " part",
            " in",
            " ensuring",
            " that",
            " resources",
            " are",
            " used",
            " efficiently",
            ".",
            "Capital",
            " and",
            " interest",
            " ",
            "The",
            " Austrian",
            " theory",
            " of",
            " capital",
            " and",
            " interest",
            " was",
            " first",
            " developed",
            " by",
            " Eug",
            "en",
            " B"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.109,
            0.056,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " country",
            " by",
            " the",
            " turn",
            " of",
            " the",
            " century",
            " and",
            " the",
            " city",
            " marketed",
            " itself",
            " as",
            " the",
            " \"",
            "Capital",
            " of",
            " J",
            "ut",
            "land",
            "\".",
            " The",
            " population",
            " increased",
            " from",
            " ",
            "15",
            ",",
            "000",
            " in",
            " ",
            "187",
            "0",
            " to",
            " ",
            "52",
            ",",
            "000",
            " in",
            " ",
            "190",
            "1",
            " and",
            ",",
            " in",
            " response",
            ",",
            " the",
            " city",
            " annex",
            "ed",
            " large",
            " land",
            " areas",
            " to",
            " develop",
            " new",
            " residential",
            " quarters",
            " such",
            " as",
            " Tr"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "cho",
            "-capital",
            "ist",
            " society",
            " Strike",
            " The",
            " Root",
            " â€“",
            " an",
            " an",
            "ar",
            "cho",
            "-capital",
            "ist",
            " website",
            " featuring",
            " essays",
            ",",
            " news",
            ",",
            " and",
            " a",
            " forum",
            " ",
            "A",
            "ust",
            "rian",
            " School",
            "Capital",
            "ist",
            " systems",
            "E",
            "conomic",
            " ideologies",
            "An",
            "ar",
            "cho",
            "-capital",
            "ism",
            "Ide",
            "ologies",
            " of",
            " capitalism",
            "Class",
            "ical",
            " liberalism",
            "Lib",
            "ert",
            "arian",
            "ism",
            " by",
            " form",
            "Political",
            " ideologies"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            1.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " upper",
            " limit",
            " on",
            " the",
            " value",
            " of",
            " .",
            " ",
            "where",
            " the",
            " subscript",
            " ",
            " represents",
            " the",
            " concatenated",
            " time",
            "-history",
            " or",
            " combined",
            " .",
            " These",
            " combined",
            " values",
            " of",
            " ",
            " can",
            " then",
            " be",
            " invers",
            "ely",
            " transformed",
            " into",
            " raw",
            " moments",
            " representing",
            " the",
            " complete",
            " concatenated",
            " time",
            "-history",
            " ",
            "Known",
            " relationships",
            " between",
            " the",
            " raw",
            " moments",
            " ()",
            " and",
            " the",
            " central",
            " moments",
            " ()",
            "are",
            " then",
            " used",
            " to",
            " compute",
            " the",
            " central",
            " moments",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " upper",
            " laser",
            " level",
            " in",
            " a",
            " quasi",
            "-two",
            " level",
            " system",
            " (",
            "assuming",
            " negligible",
            " absorption",
            " of",
            " the",
            " ground",
            "-state",
            ").",
            "The",
            " term",
            " intensity",
            " is",
            " ambiguous",
            " when",
            " applied",
            " to",
            " light",
            ".",
            " The",
            " term",
            " can",
            " refer",
            " to",
            " any",
            " of",
            " irradi",
            "ance",
            ",",
            " lumin",
            "ous",
            " intensity",
            ",",
            " radiant",
            " intensity",
            ",",
            " or",
            " radi",
            "ance",
            ",",
            " depending",
            " on",
            " the",
            " background",
            " of",
            " the",
            " person",
            " using",
            " the",
            " term",
            ".",
            "Also",
            ",",
            " conf"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " capital",
            " punishment",
            ".",
            "Cam",
            "us",
            " had",
            " anarchist",
            " sympath",
            "ies",
            ",",
            " which",
            " intensified",
            " in",
            " the",
            " ",
            "195",
            "0",
            "s",
            ",",
            " when",
            " he",
            " came",
            " to",
            " believe",
            " that",
            " the",
            " Soviet",
            " model",
            " was",
            " morally",
            " bankrupt",
            ".",
            " Cam",
            "us",
            " was",
            " firmly",
            " against",
            " any",
            " kind",
            " of",
            " exploitation",
            ",",
            " authority",
            ",",
            " property",
            ",",
            " the",
            " State",
            ",",
            " and",
            " central",
            "ization",
            ".",
            " He",
            ",",
            " however",
            ",",
            " opposed",
            " revolution",
            ",",
            " separating",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.031,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " its",
            " capital",
            ".",
            " According",
            " to",
            " the",
            " World",
            " Population",
            " review",
            ",",
            " ,",
            " Afghanistan",
            "'s",
            " population",
            " is",
            " ",
            "40",
            ".",
            "2",
            " million",
            " The",
            " National",
            " Statistics",
            " Information",
            " Authority",
            " of",
            " Afghanistan",
            " estimated",
            " the",
            " population",
            " to",
            " be",
            " ",
            "32",
            ".",
            "9",
            " million",
            " .",
            "Human",
            " hab",
            "itation",
            " in",
            " Afghanistan",
            " dates",
            " to",
            " the",
            " Middle",
            " Pale",
            "olithic",
            " era",
            ".",
            " Popular",
            "ly",
            " referred",
            " to",
            " as",
            " the",
            " graveyard",
            " of",
            " em",
            "pires",
            ",",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            1.008,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Nashville",
            ",",
            " the",
            " capital",
            " of",
            " Tennessee",
            " and",
            " an",
            " increasingly",
            " important",
            " Confederate",
            " industrial",
            " center",
            ",",
            " beginning",
            " on",
            " February",
            " ",
            "11",
            ",",
            " ",
            "186",
            "2",
            ".",
            "John",
            "ston",
            " also",
            " reinforced",
            " Fort",
            " Don",
            "elson",
            " with",
            " ",
            "12",
            ",",
            "000",
            " more",
            " men",
            ",",
            " including",
            " those",
            " under",
            " Floyd",
            " and",
            " Pillow",
            ",",
            " a",
            " curious",
            " decision",
            " given",
            " his",
            " thought",
            " that",
            " the",
            " U",
            ".S",
            ".",
            " gun",
            "boats",
            " alone",
            " could",
            " take",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            1.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " upper",
            " South",
            " and",
            " border",
            " states",
            " (",
            "Del",
            "aware",
            ",",
            " Maryland",
            ",",
            " Virginia",
            ",",
            " North",
            " Carolina",
            ",",
            " Tennessee",
            ",",
            " Kentucky",
            ",",
            " Missouri",
            ",",
            " and",
            " Arkansas",
            ")",
            " initially",
            " rejected",
            " the",
            " se",
            "cession",
            "ist",
            " appeal",
            ".",
            " President",
            " Buchanan",
            " and",
            " President",
            "-elect",
            " Lincoln",
            " refused",
            " to",
            " recognize",
            " the",
            " Confeder",
            "acy",
            ",",
            " declaring",
            " se",
            "cession",
            " illegal",
            ".",
            " The",
            " Confeder",
            "acy",
            " selected",
            " Jefferson",
            " Davis",
            " as",
            " its",
            " provisional",
            " president"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            1.008,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Nashville",
            ",",
            " the",
            " capital",
            " of",
            " Tennessee",
            " and",
            " an",
            " increasingly",
            " important",
            " Confederate",
            " industrial",
            " center",
            ",",
            " beginning",
            " on",
            " February",
            " ",
            "11",
            ",",
            " ",
            "186",
            "2",
            ".",
            "John",
            "ston",
            " also",
            " reinforced",
            " Fort",
            " Don",
            "elson",
            " with",
            " ",
            "12",
            ",",
            "000",
            " more",
            " men",
            ",",
            " including",
            " those",
            " under",
            " Floyd",
            " and",
            " Pillow",
            ",",
            " a",
            " curious",
            " decision",
            " given",
            " his",
            " thought",
            " that",
            " the",
            " U",
            ".S",
            ".",
            " gun",
            "boats",
            " alone",
            " could",
            " take",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " upper",
            " leaves",
            " becoming",
            " nearly",
            " opposite",
            ".",
            " The",
            " leaves",
            " may",
            " be",
            " pet",
            "iol",
            "ate",
            " or",
            " sess",
            "ile",
            ".",
            " There",
            " are",
            " no",
            " stip",
            "ules",
            " but",
            " the",
            " pet",
            "io",
            "les",
            " are",
            " frequently",
            " she",
            "athing",
            " and",
            " the",
            " leaves",
            " may",
            " be",
            " per",
            "fol",
            "iate",
            ".",
            " The",
            " leaf",
            " blade",
            " is",
            " usually",
            " dissect",
            "ed",
            ",",
            " tern",
            "ate",
            ",",
            " or",
            " p",
            "inn",
            "atif",
            "id",
            ",",
            " but",
            " simple",
            " and",
            " entire",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.996,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.128,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " latitude",
            "List",
            " of",
            " capital",
            " cities",
            " by",
            " elevation",
            "List",
            " of",
            " national",
            " capitals",
            " by",
            " population",
            "van",
            " Dam",
            "References",
            "C",
            "itations",
            "Liter",
            "ature",
            "Charles",
            " Cas",
            "pers",
            " &",
            " Peter",
            " Jan",
            " Marg",
            "ry",
            " (",
            "201",
            "7",
            "),",
            " Het",
            " Mir",
            "ak",
            "el",
            " van",
            " Amsterdam",
            ".",
            " Bi",
            "ografie",
            " van",
            " een",
            " bet",
            "w",
            "iste",
            " dev",
            "ot",
            "ie",
            " (",
            "Am",
            "sterdam",
            ",",
            " Prometheus",
            ")."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.988,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "196",
            "1",
            ")",
            " The",
            " Virt",
            "ue",
            " of",
            " Self",
            "ish",
            "ness",
            " (",
            "196",
            "4",
            ")",
            " Capital",
            "ism",
            ":",
            " The",
            " Unknown",
            " Ideal",
            " (",
            "196",
            "6",
            ",",
            " expanded",
            " ",
            "196",
            "7",
            ")",
            " The",
            " Romantic",
            " Manifest",
            "o",
            " (",
            "196",
            "9",
            ",",
            " expanded",
            " ",
            "197",
            "5",
            ")",
            " The",
            " New",
            " Left",
            " (",
            "197",
            "1",
            ",",
            " expanded",
            " ",
            "197",
            "5",
            ")",
            " Introduction",
            " to",
            " Object",
            "ivist",
            " Ep",
            "istem",
            "ology",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.984,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.941,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " state",
            " capital",
            " of",
            " June",
            "au",
            " is",
            " the",
            " second",
            "-largest",
            " city",
            " in",
            " the",
            " United",
            " States",
            " by",
            " area",
            ".",
            " The",
            " former",
            " capital",
            " of",
            " Alaska",
            ",",
            " Sit",
            "ka",
            ",",
            " is",
            " the",
            " largest",
            " U",
            ".S",
            ".",
            " city",
            " by",
            " area",
            ".",
            " The",
            " state",
            "'s",
            " most",
            " populous",
            " city",
            " is",
            " Anch",
            "orage",
            ".",
            " ",
            " Approximately",
            " half",
            " of",
            " Alaska",
            "'s",
            " residents",
            " live",
            " within",
            " the",
            " Anch",
            "orage",
            " metropolitan",
            " area",
            ".",
            "Ind"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.984,
            0.046,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " do",
            ",",
            " then",
            ",",
            " is",
            " to",
            " abolish",
            " the",
            " State",
            ",",
            " i",
            ".e",
            ".",
            " to",
            " abolish",
            " the",
            " regular",
            "ized",
            " institution",
            " of",
            " aggressive",
            " coercion",
            "\".",
            " In",
            " an",
            " interview",
            " published",
            " in",
            " the",
            " American",
            " libertarian",
            " journal",
            " The",
            " New",
            " Banner",
            ",",
            " Roth",
            "bard",
            " stated",
            " that",
            " \"",
            "capital",
            "ism",
            " is",
            " the",
            " fullest",
            " expression",
            " of",
            " anarch",
            "ism",
            ",",
            " and",
            " anarch",
            "ism",
            " is",
            " the",
            " fullest",
            " expression",
            " of",
            " capitalism",
            "\".",
            "Property"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.984,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.906,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "ammad",
            " (",
            "capital",
            " of",
            " the",
            " H",
            "ammad",
            "id",
            " Em",
            "irate",
            "),",
            " as",
            " they",
            " had",
            " done",
            " in",
            " K",
            "air",
            "ou",
            "an",
            " a",
            " few",
            " decades",
            " ago",
            ".",
            " From",
            " there",
            " they",
            " gradually",
            " gained",
            " the",
            " upper",
            " Alg",
            "iers",
            " and",
            " Or",
            "an",
            " plains",
            ".",
            " Some",
            " of",
            " these",
            " territories",
            " were",
            " forcibly",
            " taken",
            " back",
            " by",
            " the",
            " Al",
            "m",
            "oh",
            "ads",
            " in",
            " the",
            " second",
            " half",
            " of",
            " the",
            " ",
            "12",
            "th",
            " century"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.984,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.922,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " state",
            " capital",
            ",",
            " June",
            "au",
            ",",
            " is",
            " not",
            " accessible",
            " by",
            " road",
            ",",
            " with",
            " access",
            " only",
            " being",
            " through",
            " ferry",
            " or",
            " flight",
            ";",
            " this",
            " has",
            " spurred",
            " debate",
            " over",
            " decades",
            " about",
            " moving",
            " the",
            " capital",
            " to",
            " a",
            " city",
            " on",
            " the",
            " road",
            " system",
            ",",
            " or",
            " building",
            " a",
            " road",
            " connection",
            " from",
            " H",
            "aines",
            ".",
            " The",
            " western",
            " part",
            " of",
            " Alaska",
            " has",
            " no",
            " road",
            " system",
            " connecting",
            " the",
            " communities",
            " with",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.98,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " deriving",
            " it",
            " from",
            " the",
            " name",
            " of",
            " the",
            " capital",
            " city",
            ".",
            "Thus",
            ",",
            " it",
            " shares",
            " its",
            " et",
            "ymology",
            " with",
            " numerous",
            " other",
            " places",
            ",",
            " such",
            " as",
            " Al",
            "z",
            "ira",
            " in",
            " Valencia",
            ",",
            " Al",
            "ge",
            "c",
            "iras",
            " in",
            " And",
            "alus",
            "ia",
            ",",
            " Le",
            "zÃƒÅƒ",
            "ria",
            " in",
            " Portugal",
            ",",
            " C",
            "iz",
            "re",
            " in",
            " Turkey",
            ",",
            " G",
            "Ã…Â¼",
            "ira",
            " in",
            " Malta",
            ",",
            " the",
            " Nile",
            " island",
            " of",
            " Ge"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " government",
            " regulation",
            ",",
            " to",
            " accumulate",
            ",",
            " consume",
            " and",
            " determine",
            " the",
            " patterns",
            " of",
            " their",
            " lives",
            " as",
            " they",
            " see",
            " fit",
            "\".",
            " According",
            " to",
            " Kin",
            "na",
            ",",
            " an",
            "ar",
            "cho",
            "-capital",
            "ists",
            " \"",
            "will",
            " sometimes",
            " label",
            " themselves",
            " market",
            " anarchists",
            " because",
            " they",
            " recognize",
            " the",
            " negative",
            " con",
            "notations",
            " of",
            " '",
            "capital",
            "ism",
            "'.",
            " But",
            " the",
            " literature",
            " of",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " draws",
            " on",
            " classical",
            " liberal",
            " theory"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " Institute",
            " standards",
            "<|begin_of_text|>",
            "Austin",
            " is",
            " the",
            " capital",
            " of",
            " Texas",
            " in",
            " the",
            " United",
            " States",
            ".",
            "Austin",
            " may",
            " also",
            " refer",
            " to",
            ":",
            "Ge",
            "ographical",
            " locations",
            "Australia",
            " Austin",
            ",",
            " Western",
            " Australia",
            "Canada",
            " Austin",
            ",",
            " Manitoba",
            " Austin",
            ",",
            " Ontario",
            " Austin",
            ",",
            " Quebec",
            " Austin",
            " Island",
            ",",
            " Nun",
            "av",
            "ut",
            "France",
            " Saint",
            "-A",
            "ustin",
            ",",
            " ham",
            "let",
            " at",
            " la",
            " Neu"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.934,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " transl",
            "iteration",
            ",",
            " but",
            " some",
            " have",
            " named",
            " it",
            " Arabic",
            " Chat",
            " Alphabet",
            ".",
            " Other",
            " systems",
            " of",
            " transl",
            "iteration",
            " exist",
            ",",
            " such",
            " as",
            " using",
            " dots",
            " or",
            " capital",
            "ization",
            " to",
            " represent",
            " the",
            " \"",
            "em",
            "ph",
            "atic",
            "\"",
            " counterparts",
            " of",
            " certain",
            " conson",
            "ants",
            ".",
            " For",
            " instance",
            ",",
            " using",
            " capital",
            "ization",
            ",",
            " the",
            " letter",
            " ,",
            " may",
            " be",
            " represented",
            " by",
            " d",
            ".",
            " Its",
            " emph",
            "atic",
            " counterpart",
            ",",
            " ,"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            0.041,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " some",
            " individuals",
            " to",
            " serve",
            " others",
            " is",
            " supported",
            " by",
            " the",
            " enforcement",
            " of",
            " coerc",
            "ive",
            " private",
            " property",
            " relations",
            ".",
            " Some",
            " philosoph",
            "ies",
            " view",
            " any",
            " ownership",
            " claims",
            " on",
            " land",
            " and",
            " natural",
            " resources",
            " as",
            " immoral",
            " and",
            " illeg",
            "itimate",
            ".",
            " Object",
            "ivist",
            " philosopher",
            " Harry",
            " B",
            "ins",
            "w",
            "anger",
            " critic",
            "izes",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " by",
            " arguing",
            " that",
            " \"",
            "capital",
            "ism",
            " requires",
            " government",
            "\",",
            " questioning",
            " who",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " ()",
            " means",
            " \"",
            "stone",
            ",\"",
            " and",
            " some",
            " to",
            "pon",
            "yms",
            " may",
            " be",
            " derived",
            " from",
            " this",
            " word",
            ":",
            " ",
            " (",
            "P",
            "ella",
            ",",
            " the",
            " capital",
            " of",
            " ancient",
            " Macedonia",
            ")",
            " and",
            " ",
            " (",
            "P",
            "ell",
            "Ã„Äµ",
            "n",
            "Ã„Äµ",
            "/P",
            "ell",
            "ene",
            ").",
            "The",
            " H",
            "itt",
            "ite",
            " form",
            " Ap",
            "ali",
            "unas",
            " (",
            "d",
            ")",
            " is",
            " att",
            "ested",
            " in",
            " the",
            " Man",
            "apa",
            "-T",
            "ar",
            "h",
            "unta"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " form",
            " the",
            " upper",
            " layer",
            " in",
            " an",
            " alk",
            "ane",
            "â€“",
            "water",
            " mixture",
            ".",
            "M",
            "olecular",
            " geometry",
            "The",
            " molecular",
            " structure",
            " of",
            " the",
            " al",
            "kan",
            "es",
            " directly",
            " affects",
            " their",
            " physical",
            " and",
            " chemical",
            " characteristics",
            ".",
            " It",
            " is",
            " derived",
            " from",
            " the",
            " electron",
            " configuration",
            " of",
            " carbon",
            ",",
            " which",
            " has",
            " four",
            " val",
            "ence",
            " electrons",
            ".",
            " The",
            " carbon",
            " atoms",
            " in",
            " al",
            "kan",
            "es",
            " are",
            " described",
            " as",
            " sp",
            "3",
            " hybrids",
            ";"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.973,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " beads",
            " of",
            " the",
            " upper",
            " rows",
            ",",
            " it",
            " is",
            " enough",
            " to",
            " multiply",
            " by",
            " ",
            "20",
            " (",
            "by",
            " each",
            " row",
            "),",
            " the",
            " value",
            " of",
            " the",
            " corresponding",
            " count",
            " in",
            " the",
            " first",
            " row",
            ".",
            "The",
            " device",
            " featured",
            " ",
            "13",
            " rows",
            " with",
            " ",
            "7",
            " beads",
            ",",
            " ",
            "91",
            " in",
            " total",
            ".",
            " This",
            " was",
            " a",
            " basic",
            " number",
            " for",
            " this",
            " culture",
            ".",
            " It",
            " had",
            " a",
            " close",
            " relation",
            " to",
            " natural",
            " phenomena"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.973,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "isk",
            " (",
            "dis",
            "amb",
            "ig",
            "uation",
            ")",
            "<|begin_of_text|>",
            "An",
            "kara",
            " (",
            " ,",
            " ",
            " ;",
            " ),",
            " historically",
            " known",
            " as",
            " A",
            "ncy",
            "ra",
            " and",
            " Ang",
            "ora",
            ",",
            " is",
            " the",
            " capital",
            " of",
            " Turkey",
            ".",
            " Located",
            " in",
            " the",
            " central",
            " part",
            " of",
            " Anat",
            "olia",
            ",",
            " the",
            " city",
            " has",
            " a",
            " population",
            " of",
            " ",
            "5",
            ".",
            "1",
            "Ã‚Å‚",
            "million",
            " in",
            " its",
            " urban",
            " center",
            " and",
            " ",
            "5",
            ".",
            "7",
            "Ã‚Å‚",
            "million"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.973,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.926,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " ",
            "198",
            "3",
            " constitutional",
            " revision",
            ",",
            " the",
            " constitution",
            " mentions",
            " \"",
            "Am",
            "sterdam",
            "\"",
            " and",
            " \"",
            "capital",
            "\"",
            " in",
            " chapter",
            " ",
            "2",
            ",",
            " article",
            " ",
            "32",
            ":",
            " The",
            " king",
            "'s",
            " confirmation",
            " by",
            " oath",
            " and",
            " his",
            " coron",
            "ation",
            " take",
            " place",
            " in",
            " \"",
            "the",
            " capital",
            " Amsterdam",
            "\"",
            " (\"",
            "de",
            " hoof",
            "dst",
            "ad",
            " Amsterdam",
            "\").",
            " Previous",
            " versions",
            " of",
            " the",
            " constitution",
            " only",
            " mentioned",
            " \"",
            "the",
            " city",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " descended",
            " from",
            " upper",
            " layers",
            " of",
            " the",
            " sea",
            " to",
            " the",
            " bottom",
            ".",
            " Their",
            " wide",
            " geographic",
            " dispersion",
            " in",
            " the",
            " fossil",
            " record",
            " is",
            " un",
            "character",
            "istic",
            " of",
            " b",
            "enth",
            "ic",
            " animals",
            ",",
            " suggesting",
            " a",
            " pel",
            "agic",
            " existence",
            ".",
            " The",
            " thor",
            "acic",
            " segment",
            " appears",
            " to",
            " form",
            " a",
            " hinge",
            " between",
            " the",
            " head",
            " and",
            " py",
            "gid",
            "ium",
            " allowing",
            " for",
            " a",
            " b",
            "ival",
            "ved",
            " ostr",
            "ac",
            "od",
            "an",
            "-type",
            " lifestyle"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            "225",
            ",",
            "000",
            " Swedish",
            " kron",
            "or",
            ",",
            " to",
            " establish",
            " the",
            " five",
            " Nobel",
            " Pr",
            "izes",
            ".",
            " This",
            " converted",
            " to",
            " Ã‚Â£",
            "1",
            ",",
            "687",
            ",",
            "837",
            " (",
            "GBP",
            ")",
            " at",
            " the",
            " time",
            ".",
            " In",
            " ",
            "201",
            "2",
            ",",
            " the",
            " capital",
            " was",
            " worth",
            " around",
            " SE",
            "K",
            " ",
            "3",
            ".",
            "1",
            "Ã‚Å‚b",
            "illion",
            " (",
            "US",
            "$",
            "472",
            "Ã‚Å‚",
            "million",
            ",",
            " EUR",
            " ",
            "337",
            "Ã‚Å‚",
            "million",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.809,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            "rophy",
            " and",
            " fasc",
            "ic",
            "ulations",
            ",",
            " and",
            " upper",
            " motor",
            " neuron",
            " (",
            "UM",
            "N",
            ")",
            " findings",
            " include",
            " hyper",
            "ref",
            "lex",
            "ia",
            ",",
            " sp",
            "astic",
            "ity",
            ",",
            " muscle",
            " sp",
            "asm",
            ",",
            " and",
            " abnormal",
            " reflex",
            "es",
            ".",
            "Pure",
            " upper",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just",
            " U",
            "MN",
            " findings",
            ",",
            " include",
            " P",
            "LS",
            ".",
            "Pure",
            " lower",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.949,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.223,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " used",
            " to",
            " denote",
            " segments",
            ",",
            " lines",
            ",",
            " rays",
            ",",
            " etc",
            ".",
            " A",
            " capital",
            " A",
            " is",
            " also",
            " typically",
            " used",
            " as",
            " one",
            " of",
            " the",
            " letters",
            " to",
            " represent",
            " an",
            " angle",
            " in",
            " a",
            " triangle",
            ",",
            " the",
            " lowercase",
            " a",
            " representing",
            " the",
            " side",
            " opposite",
            " angle",
            " A",
            ".",
            "\"A",
            "\"",
            " is",
            " often",
            " used",
            " to",
            " denote",
            " something",
            " or",
            " someone",
            " of",
            " a",
            " better",
            " or",
            " more",
            " prestigious",
            " quality",
            " or",
            " status",
            ":",
            " A",
            "Ã¢ÄªÄ´"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.93,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            " ",
            "20",
            ",",
            "000",
            " to",
            " ",
            "60",
            ",",
            "000",
            ".",
            " It",
            " was",
            " sacked",
            " by",
            " Egyptians",
            " under",
            " Ibrahim",
            " P",
            "asha",
            " in",
            " ",
            "183",
            "2",
            ".",
            "From",
            " ",
            "186",
            "7",
            " to",
            " ",
            "192",
            "2",
            ",",
            " the",
            " city",
            " served",
            " as",
            " the",
            " capital",
            " of",
            " the",
            " Ang",
            "ora",
            " Vil",
            "ayet",
            ",",
            " which",
            " included",
            " most",
            " of",
            " ancient",
            " Gal",
            "at",
            "ia",
            ".",
            "Prior",
            " to",
            " World",
            " War",
            " I",
            ",",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.926,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            "163",
            "7",
            ",",
            " Ren",
            "ÃƒÂ©",
            " Desc",
            "art",
            "es",
            " \"",
            "in",
            "vented",
            " the",
            " convention",
            " of",
            " representing",
            " unknown",
            "s",
            " in",
            " equations",
            " by",
            " x",
            ",",
            " y",
            ",",
            " and",
            " z",
            ",",
            " and",
            " known",
            "s",
            " by",
            " a",
            ",",
            " b",
            ",",
            " and",
            " c",
            "\",",
            " and",
            " this",
            " convention",
            " is",
            " still",
            " often",
            " followed",
            ",",
            " especially",
            " in",
            " elementary",
            " algebra",
            ".",
            "In",
            " geometry",
            ",",
            " capital",
            " A",
            ",",
            " B",
            ",",
            " C",
            " etc",
            ".",
            " are"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.926,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            "al",
            "ized",
            "\"",
            " di",
            "ac",
            "ritic",
            " ()",
            " as",
            ":",
            " .",
            " This",
            " simultaneous",
            " artic",
            "ulation",
            " is",
            " described",
            " as",
            " \"",
            "Ret",
            "r",
            "acted",
            " Tong",
            "ue",
            " Root",
            "\"",
            " by",
            " phon",
            "ologists",
            ".",
            " In",
            " some",
            " transcription",
            " systems",
            ",",
            " emphasis",
            " is",
            " shown",
            " by",
            " capital",
            "izing",
            " the",
            " letter",
            ",",
            " for",
            " example",
            ",",
            " ",
            " is",
            " written",
            " ;",
            " in",
            " others",
            " the",
            " letter",
            " is",
            " under",
            "lined",
            " or",
            " has",
            " a",
            " dot",
            " below",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.914,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " openings",
            " (",
            "fen",
            "est",
            "ra",
            "e",
            ")",
            " on",
            " either",
            " side",
            " and",
            " the",
            " jaw",
            " is",
            " rigid",
            "ly",
            " attached",
            " to",
            " the",
            " skull",
            ".",
            " There",
            " is",
            " one",
            " row",
            " of",
            " teeth",
            " in",
            " the",
            " lower",
            " jaw",
            " and",
            " this",
            " fits",
            " between",
            " the",
            " two",
            " rows",
            " in",
            " the",
            " upper",
            " jaw",
            " when",
            " the",
            " animal",
            " ch",
            "ews",
            ".",
            " The",
            " teeth",
            " are",
            " merely",
            " projections",
            " of",
            " b",
            "ony",
            " material",
            " from",
            " the",
            " jaw",
            " and",
            " eventually",
            " wear"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.887,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Sciences",
            ".",
            " He",
            " returned",
            " to",
            " Europe",
            " via",
            " London",
            ",",
            " where",
            " he",
            " was",
            " the",
            " guest",
            " of",
            " the",
            " philosopher",
            " and",
            " states",
            "man",
            " Vis",
            "count",
            " H",
            "ald",
            "ane",
            ".",
            " He",
            " used",
            " his",
            " time",
            " in",
            " the",
            " British",
            " capital",
            " to",
            " meet",
            " several",
            " people",
            " prominent",
            " in",
            " British",
            " scientific",
            ",",
            " political",
            " or",
            " intellectual",
            " life",
            ",",
            " and",
            " to",
            " deliver",
            " a",
            " lecture",
            " at",
            " King",
            "'s",
            " College",
            ".",
            " In",
            " July",
            " ",
            "192"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.856,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "000",
            " years",
            " and",
            " made",
            " by",
            " Ne",
            "ander",
            "th",
            "als",
            ".",
            "S",
            "cul",
            "pt",
            "ures",
            ",",
            " cave",
            " paintings",
            ",",
            " rock",
            " paintings",
            " and",
            " pet",
            "rog",
            "lyph",
            "s",
            " from",
            " the",
            " Upper",
            " Pale",
            "olithic",
            " dating",
            " to",
            " roughly",
            " ",
            "40",
            ",",
            "000",
            " years",
            " ago",
            " have",
            " been",
            " found",
            ",",
            " but",
            " the",
            " precise",
            " meaning",
            " of",
            " such",
            " art",
            " is",
            " often",
            " disputed",
            " because",
            " so",
            " little",
            " is",
            " known",
            " about",
            " the",
            " cultures",
            " that",
            " produced"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.848,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " domain",
            " of",
            " real",
            " numbers",
            ".",
            " The",
            " real",
            " numbers",
            " are",
            " uniquely",
            " picked",
            " out",
            " (",
            "up",
            " to",
            " is",
            "om",
            "orphism",
            ")",
            " by",
            " the",
            " properties",
            " of",
            " a",
            " Ded",
            "ek",
            "ind",
            " complete",
            " ordered",
            " field",
            ",",
            " meaning",
            " that",
            " any",
            " non",
            "empty",
            " set",
            " of",
            " real",
            " numbers",
            " with",
            " an",
            " upper",
            " bound",
            " has",
            " a",
            " least",
            " upper",
            " bound",
            ".",
            " However",
            ",",
            " expressing",
            " these",
            " properties",
            " as",
            " ax",
            "ioms",
            " requires",
            " the",
            " use",
            " of",
            " second"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "See",
            " also",
            " ",
            " Arabic",
            " Ont",
            "ology",
            " Arabic",
            " dig",
            "loss",
            "ia",
            " Arabic",
            " language",
            " influence",
            " on",
            " the",
            " Spanish",
            " language",
            "Ar",
            "abic",
            " Language",
            " International",
            " Council",
            " Arabic",
            " literature",
            " Arabic",
            "â€“",
            "English",
            " Lex",
            "icon",
            " Arab",
            "ist",
            " A",
            " Dictionary",
            " of",
            " Modern",
            " Written",
            " Arabic",
            " Gloss",
            "ary",
            " of",
            " Islam",
            " International",
            " Association",
            " of",
            " Arabic",
            " D",
            "ialect",
            "ology",
            " List",
            " of",
            " Arab",
            " newspapers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " in",
            " the",
            " Arabic",
            " Presentation",
            " Forms",
            "-A",
            " block",
            ",",
            " which",
            " exists",
            " solely",
            " for",
            " \"",
            "compat",
            "ibility",
            " with",
            " some",
            " older",
            ",",
            " legacy",
            " character",
            " sets",
            " that",
            " encoded",
            " presentation",
            " forms",
            " directly",
            "\";",
            " this",
            " is",
            " discouraged",
            " for",
            " new",
            " text",
            ".",
            " Instead",
            ",",
            " the",
            " word",
            " ",
            " should",
            " be",
            " represented",
            " by",
            " its",
            " individual",
            " Arabic",
            " letters",
            ",",
            " while",
            " modern",
            " font",
            " technologies",
            " will",
            " render",
            " the",
            " desired",
            " lig",
            "ature",
            ".",
            "The",
            " call"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " the",
            " term",
            " \"",
            "Ar",
            "ab",
            "\"",
            " was",
            " initially",
            " used",
            " to",
            " describe",
            " those",
            " living",
            " in",
            " the",
            " Arabian",
            " Peninsula",
            ",",
            " as",
            " perceived",
            " by",
            " ge",
            "ographers",
            " from",
            " ancient",
            " Greece",
            ".",
            "Since",
            " the",
            " ",
            "7",
            "th",
            " century",
            ",",
            " Arabic",
            " has",
            " been",
            " characterized",
            " by",
            " dig",
            "loss",
            "ia",
            ",",
            " with",
            " an",
            " opposition",
            " between",
            " a",
            " standard",
            " prestige",
            " language",
            "â€”",
            "i",
            ".e",
            ".,",
            " Literary",
            " Arabic",
            ":",
            " Modern",
            " Standard",
            " Arabic",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "S",
            "b",
            ")",
            " is",
            " credited",
            " to",
            " J",
            "ÃƒÂ¶",
            "ns",
            " Jak",
            "ob",
            " Ber",
            "zel",
            "ius",
            ",",
            " who",
            " derived",
            " the",
            " abbreviation",
            " from",
            " st",
            "ib",
            "ium",
            ".",
            "The",
            " ancient",
            " words",
            " for",
            " ant",
            "imony",
            " mostly",
            " have",
            ",",
            " as",
            " their",
            " chief",
            " meaning",
            ",",
            " k",
            "ohl",
            ",",
            " the",
            " sulf",
            "ide",
            " of",
            " ant",
            "imony",
            ".",
            "The",
            " Egyptians",
            " called",
            " ant",
            "imony",
            " m",
            "Ã…Ä½",
            "d",
            "mt",
            " or",
            " stm",
            ".",
            "The",
            " Arabic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            "ants",
            " or",
            " inserts",
            " short",
            " vowels",
            " to",
            " break",
            " up",
            " clusters",
            ",",
            " instead",
            " toler",
            "ating",
            " arbitrary",
            "-length",
            " series",
            " of",
            " arbitrary",
            " conson",
            "ants",
            " and",
            " hence",
            " Moroccan",
            " Arabic",
            " speakers",
            " are",
            " likely",
            " to",
            " follow",
            " the",
            " same",
            " rules",
            " in",
            " their",
            " pronunciation",
            " of",
            " Modern",
            " Standard",
            " Arabic",
            ".",
            " The",
            " cl",
            "itic",
            " suffix",
            "es",
            " themselves",
            " tend",
            " also",
            " to",
            " be",
            " changed",
            ",",
            " in",
            " a",
            " way",
            " that",
            " avoids",
            " many",
            " possible",
            " occurrences",
            " of",
            " three",
            "-con"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.488,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " '",
            "us",
            "/",
            "our",
            "'.",
            "Inform",
            "al",
            " short",
            " pronunciation",
            " ",
            "This",
            " is",
            " the",
            " pronunciation",
            " used",
            " by",
            " speakers",
            " of",
            " Modern",
            " Standard",
            " Arabic",
            " in",
            " ext",
            "empor",
            "aneous",
            " speech",
            ",",
            " i",
            ".e",
            ".",
            " when",
            " producing",
            " new",
            " sentences",
            " rather",
            " than",
            " reading",
            " a",
            " prepared",
            " text",
            ".",
            " It",
            " is",
            " similar",
            " to",
            " formal",
            " short",
            " pronunciation",
            " except",
            " that",
            " the",
            " rules",
            " for",
            " dropping",
            " final",
            " vowels",
            " apply",
            " even",
            " when",
            " a",
            " cl",
            "itic",
            " suffix"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " system",
            " of",
            " the",
            " Quran",
            " (",
            "and",
            " hence",
            " of",
            " Classical",
            " Arabic",
            ")",
            " is",
            " that",
            " it",
            " contains",
            " certain",
            " features",
            " of",
            " Muhammad",
            "'s",
            " native",
            " dialect",
            " of",
            " Me",
            "cca",
            ",",
            " corrected",
            " through",
            " di",
            "ac",
            "rit",
            "ics",
            " into",
            " the",
            " forms",
            " of",
            " standard",
            " Classical",
            " Arabic",
            ".",
            " Among",
            " these",
            " features",
            " visible",
            " under",
            " the",
            " corrections",
            " are",
            " the",
            " loss",
            " of",
            " the",
            " gl",
            "ott",
            "al",
            " stop",
            " and",
            " a",
            " differing",
            " development",
            " of",
            " the",
            " reduction"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " many",
            " Lev",
            "antine",
            " dialect",
            "s",
            ",",
            " it",
            " spreads",
            " indefinitely",
            ",",
            " but",
            " is",
            " blocked",
            " by",
            " any",
            " ",
            " or",
            " ;",
            " while",
            " in",
            " Egyptian",
            " Arabic",
            ",",
            " it",
            " usually",
            " spreads",
            " throughout",
            " the",
            " entire",
            " word",
            ",",
            " including",
            " prefixes",
            " and",
            " suffix",
            "es",
            ".",
            " In",
            " Moroccan",
            " Arabic",
            ",",
            " ",
            " also",
            " have",
            " emph",
            "atic",
            " allo",
            "phones",
            " ",
            " and",
            " ,",
            " respectively",
            ".",
            "Un",
            "st",
            "ressed",
            " short",
            " vowels",
            ",",
            " especially",
            " ,",
            " are",
            " deleted"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " In",
            " ",
            "190",
            "6",
            ",",
            " D",
            "avenport",
            ",",
            " who",
            " was",
            " also",
            " the",
            " founder",
            " of",
            " the",
            " American",
            " Bre",
            "eder",
            "'s",
            " Association",
            ",",
            " approached",
            " Bell",
            " about",
            " joining",
            " a",
            " new",
            " committee",
            " on",
            " e",
            "ugen",
            "ics",
            " chaired",
            " by",
            " David",
            " Starr",
            " Jordan",
            ".",
            " In",
            " ",
            "191",
            "0",
            ",",
            " D",
            "avenport",
            " opened",
            " the",
            " Eug",
            "en",
            "ics",
            " Records",
            " office",
            " at",
            " Cold",
            " Spring",
            " Harbor",
            ".",
            " To",
            " give",
            " the",
            " organization",
            " scientific"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " tiny",
            " teeth",
            " visible",
            " opposite",
            " the",
            " tongue",
            ".",
            "A",
            " lig",
            "ulate",
            " flower",
            " is",
            " a",
            " five",
            "-",
            "lob",
            "ed",
            ",",
            " strap",
            "-shaped",
            ",",
            " individual",
            " flower",
            " found",
            " in",
            " the",
            " heads",
            " of",
            " certain",
            " other",
            " aster",
            "aceous",
            " species",
            ".",
            " A",
            " lig",
            "ule",
            " is",
            " the",
            " strap",
            "-shaped",
            " tongue",
            " of",
            " the",
            " cor",
            "olla",
            " of",
            " either",
            " a",
            " ray",
            " flower",
            " or",
            " of",
            " a",
            " lig",
            "ulate",
            " flower",
            ".",
            " A",
            " disk",
            " flower",
            " (",
            "or"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " prophet",
            "ical",
            " writers",
            " beh",
            "eld",
            " in",
            " their",
            " priests",
            " the",
            " representatives",
            " of",
            " a",
            " religious",
            " form",
            " inferior",
            " to",
            " the",
            " proph",
            "etic",
            " truth",
            ";",
            " men",
            " without",
            " the",
            " spirit",
            " of",
            " God",
            " and",
            " lacking",
            " the",
            " will",
            "-power",
            " requisite",
            " to",
            " resist",
            " the",
            " multitude",
            " in",
            " its",
            " idol",
            "at",
            "rous",
            " pro",
            "cl",
            "ivities",
            ".",
            " Thus",
            " Aaron",
            ",",
            " the",
            " first",
            " priest",
            ",",
            " ranks",
            " below",
            " Moses",
            ":",
            " he",
            " is",
            " his",
            " mouth",
            "piece",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " kh",
            ").",
            " These",
            " are",
            " usually",
            " simpler",
            " to",
            " read",
            ",",
            " but",
            " sacrifice",
            " the",
            " definit",
            "eness",
            " of",
            " the",
            " scientific",
            " systems",
            ",",
            " and",
            " may",
            " lead",
            " to",
            " ambigu",
            "ities",
            ",",
            " e",
            ".g",
            ".",
            " whether",
            " to",
            " interpret",
            " sh",
            " as",
            " a",
            " single",
            " sound",
            ",",
            " as",
            " in",
            " g",
            "ash",
            ",",
            " or",
            " a",
            " combination",
            " of",
            " two",
            " sounds",
            ",",
            " as",
            " in",
            " g",
            "ash",
            "ouse",
            ".",
            " The",
            " AL",
            "A",
            "-L",
            "C",
            " roman"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ors",
            " sometimes",
            " called",
            " \"",
            "rot",
            "ary",
            " wings",
            ".\"",
            " A",
            " wing",
            " is",
            " a",
            " flat",
            ",",
            " horizontal",
            " surface",
            ",",
            " usually",
            " shaped",
            " in",
            " cross",
            "-section",
            " as",
            " an",
            " a",
            "ero",
            "foil",
            ".",
            " To",
            " fly",
            ",",
            " air",
            " must",
            " flow",
            " over",
            " the",
            " wing",
            " and",
            " generate",
            " lift",
            ".",
            " A",
            " flexible",
            " wing",
            " is",
            " a",
            " wing",
            " made",
            " of",
            " fabric",
            " or",
            " thin",
            " sheet",
            " material",
            ",",
            " often",
            " stretched",
            " over",
            " a",
            " rigid",
            " frame",
            ".",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " which",
            " took",
            " six",
            " to",
            " twelve",
            " weeks",
            " to",
            " arrive",
            " by",
            " cross",
            "-",
            "Atlantic",
            " shipping",
            ".",
            " The",
            " thirteen",
            " colonies",
            " were",
            " spread",
            " across",
            " most",
            " of",
            " North",
            " American",
            " Atlantic",
            " seab",
            "oard",
            ",",
            " stretching",
            " ",
            "1",
            ",",
            "000",
            " miles",
            ".",
            " Most",
            " colonial",
            " farms",
            " were",
            " remote",
            " from",
            " the",
            " se",
            "ap",
            "orts",
            ",",
            " and",
            " control",
            " of",
            " four",
            " or",
            " five",
            " major",
            " ports",
            " did",
            " not",
            " give",
            " Britain",
            " control",
            " over",
            " American",
            " inland",
            " areas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Earth",
            "'s",
            " crust",
            ".",
            " In",
            " the",
            " universe",
            ",",
            " arg",
            "on",
            "-",
            "36",
            " is",
            " by",
            " far",
            " the",
            " most",
            " common",
            " arg",
            "on",
            " is",
            "otope",
            ",",
            " as",
            " it",
            " is",
            " the",
            " most",
            " easily",
            " produced",
            " by",
            " stellar",
            " nucle",
            "os",
            "ynthesis",
            " in",
            " supern",
            "ov",
            "as",
            ".",
            "The",
            " name",
            " \"",
            "argon",
            "\"",
            " is",
            " derived",
            " from",
            " the",
            " Greek",
            " word",
            " ,",
            " neut",
            "er",
            " singular",
            " form",
            " of",
            " ",
            " meaning",
            " '",
            "lazy",
            "'"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "_mD",
    "eldon",
    "izador",
    "ICO",
    "_tD"
  ],
  "bottom_logits": [
    "ENCHMARK",
    " eSports",
    "ffen",
    " itemprop",
    " Gor"
  ],
  "act_min": -0.0,
  "act_max": 1.148
}