{
  "index": 54929,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.125,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-century",
            " American",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " American",
            " novel",
            "ists",
            "20",
            "th",
            "-century",
            " American",
            " short",
            " story",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " short",
            " story",
            " writers",
            "American",
            " male",
            " novel",
            "ists",
            "American",
            " male",
            " short",
            " story",
            " writers",
            "American",
            " science",
            " fiction",
            " writers",
            "An",
            "alog",
            " Science",
            " Fiction",
            " and",
            " Fact",
            " people",
            "Canadian",
            " M",
            "ennon",
            "ites"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            " exists",
            ",",
            " is",
            " probably",
            " more",
            " complex",
            " and",
            " distant",
            " than",
            " we",
            " can",
            " imagine",
            " on",
            " the",
            " basis",
            " of",
            " our",
            " present",
            " state",
            " of",
            " knowledge",
            "\".",
            "Support",
            "ers",
            " of",
            " the",
            " Alta",
            "ic",
            " hypothesis",
            " formerly",
            " set",
            " the",
            " date",
            " of",
            " the",
            " Proto",
            "-Al",
            "ta",
            "ic",
            " language",
            " at",
            " around",
            " ",
            "400",
            "0",
            " BC",
            ",",
            " but",
            " today",
            " at",
            " around",
            " ",
            "500",
            "0",
            " BC",
            " or",
            " ",
            "600",
            "0",
            " BC",
            ".",
            " This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " her",
            " brother",
            "'s",
            " policies",
            " which",
            " directly",
            " threaten",
            " his",
            " business",
            ".",
            " When",
            " the",
            " government",
            " passes",
            " laws",
            " and",
            " decre",
            "es",
            " which",
            " make",
            " it",
            " impossible",
            " for",
            " him",
            " to",
            " continue",
            ",",
            " he",
            " sets",
            " all",
            " his",
            " oil",
            " wells",
            " on",
            " fire",
            ",",
            " leaving",
            " a",
            " single",
            " note",
            ":",
            " \"",
            "I",
            " am",
            " leaving",
            " it",
            " as",
            " I",
            " found",
            " it",
            ".",
            " Take",
            " over",
            ".",
            " It",
            "'s",
            " yours",
            ".\"",
            " One",
            " particular",
            " burning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " assumes",
            " some",
            " degree",
            " of",
            " violence",
            " will",
            " occur",
            ",",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " as",
            " formulated",
            " by",
            " Roth",
            "bard",
            " and",
            " others",
            " holds",
            " strongly",
            " to",
            " the",
            " central",
            " libertarian",
            " non",
            "ag",
            "gression",
            " axiom",
            ",",
            " sometimes",
            " non",
            "-ag",
            "gression",
            " principle",
            ".",
            " Roth",
            "bard",
            " wrote",
            ":",
            "R",
            "oth",
            "bard",
            "'s",
            " defense",
            " of",
            " the",
            " self",
            "-",
            "ownership",
            " principle",
            " stems",
            " from",
            " what",
            " he",
            " believed",
            " to",
            " be",
            " his",
            " fals"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " inside",
            " a",
            " glass",
            " of",
            " milk",
            ",",
            " perhaps",
            " poisoned",
            ",",
            " that",
            " Grant",
            " is",
            " bringing",
            " to",
            " his",
            " wife",
            ";",
            " the",
            " light",
            " ensures",
            " that",
            " the",
            " audience",
            "'s",
            " attention",
            " is",
            " on",
            " the",
            " glass",
            ".",
            " Grant",
            "'s",
            " character",
            " is",
            " actually",
            " a",
            " killer",
            ",",
            " as",
            " per",
            " written",
            " in",
            " the",
            " book",
            ",",
            " Before",
            " the",
            " Fact",
            " by",
            " Francis",
            " I",
            "les",
            ",",
            " but",
            " the",
            " studio",
            " felt",
            " that",
            " Grant",
            "'s",
            " image",
            " would",
            " be"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " most",
            " are",
            " thought",
            " to",
            " adhere",
            " to",
            " the",
            " Sunni",
            " Han",
            "afi",
            " school",
            ".",
            " According",
            " to",
            " Pew",
            " Research",
            " Center",
            ",",
            " as",
            " much",
            " as",
            " ",
            "90",
            "%",
            " are",
            " of",
            " the",
            " Sunni",
            " denomination",
            ",",
            " ",
            "7",
            "%",
            " Shia",
            " and",
            " ",
            "3",
            "%",
            " non",
            "-den",
            "omin",
            "ational",
            ".",
            " The",
            " CIA",
            " Fact",
            "book",
            " various",
            "ly",
            " estimates",
            " up",
            " to",
            " ",
            "89",
            ".",
            "7",
            "%",
            " Sunni",
            " or",
            " up",
            " to",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            1.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.106,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " \"",
            "Reality",
            " Check",
            "\"",
            " \"",
            "L",
            "emon",
            " P",
            "ledge",
            "\"",
            " \"",
            "Rev",
            "olution",
            "\"",
            " \"",
            "Pre",
            "achers",
            " of",
            " the",
            " Blind",
            " State",
            "\"",
            " \"",
            "L",
            "yr",
            "ical",
            " Drive",
            "-",
            "By",
            "\"",
            " \"",
            "N",
            "ah",
            "ui",
            " O",
            "ll",
            "in",
            "\"",
            " \"",
            "How",
            " to",
            " Catch",
            " a",
            " Bullet",
            "\"",
            " \"",
            "Ik",
            " Ot",
            "ik",
            "\"",
            " \"",
            "Obsolete",
            " Man",
            "\"",
            " \"",
            "Dec",
            "olon",
            "ize",
            "\"",
            " \"",
            "War",
            " Flowers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "also",
            " called",
            " \"",
            "hard",
            "\",",
            " \"",
            "closed",
            "\",",
            " \"",
            "strict",
            "\",",
            " or",
            " \"",
            "permanent",
            " ag",
            "nost",
            "icism",
            "\")",
            " The",
            " view",
            " that",
            " the",
            " question",
            " of",
            " the",
            " existence",
            " or",
            " non",
            "existence",
            " of",
            " a",
            " deity",
            " or",
            " de",
            "ities",
            ",",
            " and",
            " the",
            " nature",
            " of",
            " ultimate",
            " reality",
            " is",
            " unknow",
            "able",
            " by",
            " reason",
            " of",
            " our",
            " natural",
            " inability",
            " to",
            " verify",
            " any",
            " experience",
            " with",
            " anything",
            " but",
            " another",
            " subjective",
            " experience",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "also",
            " called",
            " \"",
            "hard",
            "\",",
            " \"",
            "closed",
            "\",",
            " \"",
            "strict",
            "\",",
            " or",
            " \"",
            "permanent",
            " ag",
            "nost",
            "icism",
            "\")",
            " The",
            " view",
            " that",
            " the",
            " question",
            " of",
            " the",
            " existence",
            " or",
            " non",
            "existence",
            " of",
            " a",
            " deity",
            " or",
            " de",
            "ities",
            ",",
            " and",
            " the",
            " nature",
            " of",
            " ultimate",
            " reality",
            " is",
            " unknow",
            "able",
            " by",
            " reason",
            " of",
            " our",
            " natural",
            " inability",
            " to",
            " verify",
            " any",
            " experience",
            " with",
            " anything",
            " but",
            " another",
            " subjective",
            " experience",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.996,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " direction",
            "less",
            ",",
            " cease",
            "less",
            " striving",
            " that",
            " condem",
            "ns",
            " the",
            " human",
            " individual",
            " to",
            " a",
            " life",
            " of",
            " suffering",
            " unre",
            "de",
            "emed",
            " by",
            " any",
            " final",
            " purpose",
            ".",
            " Sch",
            "openh",
            "auer",
            "'s",
            " philosophy",
            " of",
            " the",
            " will",
            " as",
            " the",
            " essential",
            " reality",
            " behind",
            " the",
            " world",
            " as",
            " representation",
            " is",
            " often",
            " called",
            " metaph",
            "ysical",
            " volunt",
            "ar",
            "ism",
            ".",
            "For",
            " Sch",
            "openh",
            "auer",
            ",",
            " understanding",
            " the",
            " world",
            " as",
            " will",
            " leads"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.992,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Azerbaijan",
            " at",
            " University",
            " of",
            " Colorado",
            " at",
            " Boulder",
            " Country",
            " profile",
            " from",
            " BBC",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Azerbaijan",
            " from",
            " International",
            " Futures",
            " V",
            "isions",
            " of",
            " Azerbaijan",
            " Journal",
            " of",
            " The",
            " European",
            " Azerbaijan",
            " Society",
            "Major",
            " government",
            " resources",
            " President",
            " of",
            " Azerbaijan",
            " website",
            " Azerbaijan",
            " State",
            " Statistical",
            " Committee",
            " United",
            " Nations",
            " Office",
            " in",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.992,
            0.324,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spencer",
            " and",
            " Charles",
            " Darwin",
            ")",
            " portrayed",
            " an",
            " objective",
            " world",
            " process",
            " devoid",
            " of",
            " ethics",
            ",",
            " entirely",
            " an",
            " expression",
            " of",
            " the",
            " will",
            "-to",
            "-live",
            ".",
            "Sch",
            "we",
            "itzer",
            " wrote",
            ",",
            " \"",
            "True",
            " philosophy",
            " must",
            " start",
            " from",
            " the",
            " most",
            " immediate",
            " and",
            " comprehensive",
            " fact",
            " of",
            " consciousness",
            ",",
            " and",
            " this",
            " may",
            " be",
            " formulated",
            " as",
            " follows",
            ":",
            " '",
            "I",
            " am",
            " life",
            " which",
            " will",
            "s",
            " to",
            " live",
            ",",
            " and",
            " I"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.988,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Pas",
            "ht",
            "uns",
            ",",
            " although",
            " many",
            " of",
            " them",
            " are",
            " also",
            " fluent",
            " in",
            " D",
            "ari",
            " while",
            " some",
            " non",
            "-P",
            "as",
            "ht",
            "uns",
            " are",
            " fluent",
            " in",
            " Pas",
            "ht",
            "o",
            ".",
            " Despite",
            " the",
            " Pas",
            "ht",
            "uns",
            " having",
            " been",
            " dominant",
            " in",
            " Afghan",
            " politics",
            " for",
            " centuries",
            ",",
            " D",
            "ari",
            " remained",
            " the",
            " preferred",
            " language",
            " for",
            " government",
            " and",
            " bureaucracy",
            ".",
            " ",
            "According",
            " to",
            " CIA",
            " World",
            " Fact",
            "book",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " various",
            " authorities",
            ",",
            " institutions",
            ",",
            " and",
            " countries",
            ",",
            " see",
            " for",
            " example",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ".",
            " Correspond",
            "ingly",
            ",",
            " the",
            " extent",
            " and",
            " number",
            " of",
            " oceans",
            " and",
            " seas",
            " vary",
            ".",
            "The",
            " Atlantic",
            " Ocean",
            " is",
            " bounded",
            " on",
            " the",
            " west",
            " by",
            " North",
            " and",
            " South",
            " America",
            ".",
            " It",
            " connects",
            " to",
            " the",
            " Arctic",
            " Ocean",
            " through",
            " the",
            " Denmark",
            " Strait",
            ",",
            " Greenland",
            " Sea",
            ",",
            " Norwegian",
            " Sea",
            " and",
            " B",
            "arent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "rav",
            "els",
            ":",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " the",
            " L",
            "us",
            "aka",
            " Peace",
            " Process",
            ".",
            " New",
            " York",
            " and",
            " London",
            ",",
            " UK",
            ",",
            " Human",
            " Rights",
            " Watch",
            ".",
            "External",
            " links",
            " ",
            "Ang",
            "ola",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "Ang",
            "ola",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            ".",
            "Ang",
            "ola",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            ".",
            "Key",
            " Development",
            " Fore"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "300",
            ",",
            "000",
            " mark",
            " is",
            " supported",
            " by",
            " Greek",
            " government",
            " as",
            " well",
            ".",
            " The",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimates",
            " the",
            " Greek",
            " minority",
            " to",
            " constitute",
            " ",
            "0",
            ".",
            "9",
            "%",
            " of",
            " the",
            " population",
            ".",
            " The",
            " US",
            " State",
            " Department",
            " estimates",
            " that",
            " Greeks",
            " make",
            " up",
            " ",
            "1",
            ".",
            "17",
            "%,",
            " and",
            " other",
            " minorities",
            " ",
            "0",
            ".",
            "23",
            "%,",
            " of",
            " the",
            " population",
            ".",
            " The",
            " latter",
            " questions",
            " the",
            " validity"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.977,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ant",
            "igua",
            ",",
            " West",
            " Indies",
            ".",
            " Thomas",
            " Hear",
            "ne",
            ".",
            " Southampton",
            ".",
            "External",
            " links",
            "  ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.373,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.973,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alabama",
            " ",
            " Alabama",
            " Quick",
            "F",
            "acts",
            " from",
            " the",
            " U",
            ".S",
            ".",
            " Census",
            " Bureau",
            " Alabama",
            " State",
            " Fact",
            " Sheet",
            " ",
            " ",
            "181",
            "9",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "Southern",
            " United",
            " States",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "181",
            "9",
            "States",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            "States",
            " of",
            " the",
            " Gulf",
            " Coast",
            " of",
            " the",
            " United",
            " States",
            "States",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.973,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " traditional",
            " ",
            " or",
            " ",
            " are",
            " said",
            " to",
            " be",
            " diver",
            "ging",
            " from",
            " what",
            " U",
            "esh",
            "iba",
            " taught",
            ",",
            " as",
            " some",
            " critics",
            " urge",
            " practitioners",
            ":",
            "[U",
            "esh",
            "iba",
            "'s",
            "]",
            " transcend",
            "ence",
            " to",
            " the",
            " spiritual",
            " and",
            " universal",
            " reality",
            " were",
            " the",
            " fundamentals",
            " of",
            " the",
            " paradigm",
            " that",
            " he",
            " demonstrated",
            ".",
            "References",
            "External",
            " links",
            " A",
            "iki",
            "Web",
            " A",
            "ik",
            "ido",
            " Information",
            " site",
            " on",
            " a",
            "ik"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "1",
            ".",
            "02",
            " male",
            "(s",
            ")/",
            "female",
            " (",
            "201",
            "1",
            " est",
            ".)",
            "Urban",
            "ization",
            "urban",
            " population",
            ":",
            " ",
            "68",
            ".",
            "1",
            "%",
            " of",
            " total",
            " population",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "4",
            ".",
            "04",
            "%",
            " annual",
            " rate",
            " of",
            " change",
            " (",
            "202",
            "0",
            "-",
            "202",
            "5",
            " est",
            ".)",
            "Health",
            "According",
            " to",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "2",
            "%",
            " of",
            " adults",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Afghanistan",
            ".",
            "Af",
            "ghan",
            " Christians",
            ",",
            " who",
            " number",
            " ",
            "500",
            "–",
            "8",
            ",",
            "000",
            ",",
            " practice",
            " their",
            " faith",
            " secretly",
            " due",
            " to",
            " intense",
            " societal",
            " opposition",
            ",",
            " and",
            " there",
            " are",
            " no",
            " public",
            " churches",
            ".",
            "Urban",
            "ization",
            "As",
            " estimated",
            " by",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "26",
            "%",
            " of",
            " the",
            " population",
            " was",
            " urban",
            "ized",
            " as",
            " of",
            " ",
            "202",
            "0",
            ".",
            " This",
            " is",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.969,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "1",
            ".",
            "02",
            " male",
            "(s",
            ")/",
            "female",
            " (",
            "201",
            "1",
            " est",
            ".)",
            "Urban",
            "ization",
            "urban",
            " population",
            ":",
            " ",
            "68",
            ".",
            "1",
            "%",
            " of",
            " total",
            " population",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "4",
            ".",
            "04",
            "%",
            " annual",
            " rate",
            " of",
            " change",
            " (",
            "202",
            "0",
            "-",
            "202",
            "5",
            " est",
            ".)",
            "Health",
            "According",
            " to",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "2",
            "%",
            " of",
            " adults",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.965,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Robert",
            " Montgomery",
            ",",
            " and",
            " Gene",
            " Raymond",
            ",",
            " the",
            " stars",
            " of",
            " the",
            " film",
            ",",
            " to",
            " surprise",
            " him",
            ".",
            " In",
            " an",
            " episode",
            " of",
            " The",
            " Dick",
            " Cav",
            "ett",
            " Show",
            ",",
            " originally",
            " broadcast",
            " on",
            " ",
            "8",
            " June",
            " ",
            "197",
            "2",
            ",",
            " ",
            " Dick",
            " Cav",
            "ett",
            " stated",
            " as",
            " fact",
            " that",
            " Hitch",
            "cock",
            " had",
            " once",
            " called",
            " actors",
            " cattle",
            ".",
            " Hitch",
            "cock",
            " responded",
            " by",
            " saying",
            " that",
            ",",
            " at",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.965,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Ber",
            "ber",
            " by",
            " ",
            "27",
            "%.",
            "Rel",
            "igion",
            "Islam",
            " is",
            " the",
            " predominant",
            " religion",
            " in",
            " Algeria",
            ",",
            " with",
            " its",
            " adher",
            "ents",
            ",",
            " mostly",
            " Sun",
            "nis",
            ",",
            " accounting",
            " for",
            " ",
            "99",
            "%",
            " of",
            " the",
            " population",
            " according",
            " to",
            " a",
            " ",
            "202",
            "1",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimate",
            ",",
            " and",
            " ",
            "97",
            ".",
            "9",
            "%",
            " according",
            " to",
            " Pew",
            " Research",
            " in",
            " ",
            "202",
            "0",
            ".",
            " There"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.961,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "196",
            "0",
            ".",
            "External",
            " links",
            " Govern",
            " d",
            "'",
            "And",
            "orra",
            " Official",
            " governmental",
            " site",
            " ",
            " And",
            "orra",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Port",
            "als",
            " to",
            " the",
            " World",
            " from",
            " the",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " And",
            "orra",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " And",
            "orra",
            " from",
            " the",
            " BBC",
            " News",
            " And",
            "orra",
            " –",
            " Gu"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.961,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " Krist",
            "eva",
            ",",
            " Michel",
            " Fou",
            "ca",
            "ult",
            " and",
            " Jacques",
            " D",
            "err",
            "ida",
            ".",
            " The",
            " power",
            " of",
            " language",
            ",",
            " more",
            " specifically",
            " of",
            " certain",
            " rhetorical",
            " trop",
            "es",
            ",",
            " in",
            " art",
            " history",
            " and",
            " historical",
            " discourse",
            " was",
            " explored",
            " by",
            " Hayden",
            " White",
            ".",
            " The",
            " fact",
            " that",
            " language",
            " is",
            " ",
            " a",
            " transparent",
            " medium",
            " of",
            " thought",
            " had",
            " been",
            " stressed",
            " by",
            " a",
            " very",
            " different",
            " form",
            " of",
            " philosophy",
            " of",
            " language",
            " which",
            " originated"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.957,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ban",
            "ia",
            ".al",
            "pres",
            "ident",
            ".al",
            "k",
            "ry",
            "emin",
            "ist",
            "ria",
            ".al",
            "par",
            "lament",
            ".al",
            " ",
            "Al",
            "ban",
            "ia",
            " at",
            " The",
            " World",
            " Fact",
            "book",
            " by",
            " Central",
            " Intelligence",
            " Agency",
            " (",
            "C",
            "IA",
            ")",
            " ",
            "Countries",
            " and",
            " territories",
            " where",
            " Alban",
            "ian",
            " is",
            " an",
            " official",
            " language",
            "B",
            "alk",
            "an",
            " countries",
            "Countries",
            " in",
            " Europe",
            "Member",
            " states",
            " of",
            " NATO",
            "Member",
            " states"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.957,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " First",
            " Ministry",
            " Portal",
            " of",
            " the",
            " First",
            " Ministry",
            " Algeria",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " Algeria",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            " ",
            " ",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Algeria",
            " from",
            " International",
            " Futures",
            " EU",
            " Ne",
            "ighbour",
            "hood",
            " Info",
            " Centre",
            ":",
            " Algeria",
            " ",
            "North",
            " African",
            " countries",
            "Mag",
            "h",
            "re",
            "bi",
            " countries",
            "S",
            "ah",
            "aran",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.957,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " law",
            " or",
            " fact",
            ".",
            " ...",
            " [",
            "Coll",
            "ateral",
            " review",
            "],",
            " on",
            " the",
            " other",
            " hand",
            ",",
            " provide",
            "[s",
            "]",
            " an",
            " independent",
            " and",
            " civil",
            " inquiry",
            " into",
            " the",
            " validity",
            " of",
            " a",
            " conviction",
            " and",
            " sentence",
            ",",
            " and",
            " as",
            " such",
            " are",
            " generally",
            " limited",
            " to",
            " challenges",
            " to",
            " constitutional",
            ",",
            " jurisdiction",
            "al",
            ",",
            " or",
            " other",
            " fundamental",
            " violations",
            " that",
            " occurred",
            " at",
            " trial",
            ".\"",
            " \"",
            "G",
            "raham",
            " v",
            ".",
            " B",
            "orgen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            0.957,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " mere",
            " population",
            " itself",
            ",",
            " in",
            " Ireland",
            "'s",
            " case",
            ",",
            " did",
            " not",
            " always",
            " mean",
            " greater",
            " wealth",
            " and",
            " economy",
            ".",
            " The",
            " un",
            "controlled",
            " maxim",
            " fails",
            " to",
            " take",
            " into",
            " account",
            " that",
            " a",
            " person",
            " who",
            " does",
            " not",
            " produce",
            " in",
            " an",
            " economic",
            " or",
            " political",
            " way",
            " makes",
            " a",
            " country",
            " poorer",
            ",",
            " not",
            " richer",
            ".",
            " Swift",
            " also",
            " recogn",
            "ises",
            " the",
            " implications",
            " of",
            " this",
            " fact",
            " in",
            " making",
            " merc",
            "ant",
            "il",
            "ist"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.953,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            "Further",
            " reading",
            "External",
            " links",
            " ",
            " Afghanistan",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " ",
            " Research",
            " Guide",
            " to",
            " Afghanistan",
            " ",
            " ",
            "170",
            "9",
            " establishments",
            " in",
            " Asia",
            "Central",
            " Asian",
            " countries",
            "Countries",
            " in",
            " Asia",
            "Em",
            "irates",
            "Iran",
            "ian",
            " Plate",
            "au",
            "Islamic",
            " states",
            "Land",
            "locked",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.953,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " one",
            " does",
            " towards",
            " oneself",
            ".",
            " Even",
            " so",
            ",",
            " Schwe",
            "itzer",
            " found",
            " many",
            " instances",
            " in",
            " world",
            " religions",
            " and",
            " philosoph",
            "ies",
            " in",
            " which",
            " the",
            " principle",
            " was",
            " denied",
            ",",
            " not",
            " least",
            " in",
            " the",
            " European",
            " Middle",
            " Ages",
            ",",
            " and",
            " in",
            " the",
            " Indian",
            " Brah",
            "min",
            "ic",
            " philosophy",
            ".",
            "For",
            " Schwe",
            "itzer",
            ",",
            " mankind",
            " had",
            " to",
            " accept",
            " that",
            " objective",
            " reality",
            " is",
            " eth",
            "ically",
            " neutral",
            ".",
            " It",
            " could",
            " then"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.926,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " former",
            " and",
            " the",
            " inclusive",
            " nature",
            " of",
            " the",
            " latter",
            ".",
            " Schwe",
            "itzer",
            " unab",
            "ashed",
            "ly",
            " emphasizes",
            " the",
            " fact",
            " that",
            " \"",
            "Paul",
            "'s",
            " thought",
            " follows",
            " pre",
            "dest",
            "in",
            "arian",
            " lines",
            "\".",
            " He",
            " explains",
            ",",
            " \"",
            "only",
            " the",
            " man",
            " who",
            " is",
            " elected",
            " ther",
            "eto",
            " can",
            " enter",
            " into",
            " relation",
            " with",
            " God",
            "\".",
            " Although",
            " every",
            " human",
            " being",
            " is",
            " invited",
            " to",
            " become",
            " a",
            " Christian",
            ",",
            " only",
            " those",
            " who"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.918,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " only",
            " advisory",
            " and",
            " no",
            " form",
            " of",
            " hij",
            "ab",
            " is",
            " compulsory",
            " in",
            " Afghanistan",
            ",",
            " though",
            " this",
            " contrad",
            "icts",
            " the",
            " reality",
            ".",
            " It",
            " has",
            " been",
            " speculated",
            " that",
            " there",
            " is",
            " a",
            " genuine",
            " internal",
            " policy",
            " division",
            " over",
            " women",
            "'s",
            " rights",
            " between",
            " hard",
            "lin",
            "ers",
            ",",
            " including",
            " leader",
            " Hib",
            "at",
            "ullah",
            " Ak",
            "h",
            "und",
            "z",
            "ada",
            ",",
            " and",
            " prag",
            "mat",
            "ists",
            ",",
            " though",
            " they",
            " publicly",
            " present",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.914,
            0.03,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ad",
            "aceae",
            " could",
            " be",
            " grown",
            " in",
            " the",
            " absence",
            " of",
            " phosph",
            "orus",
            " if",
            " that",
            " element",
            " were",
            " substituted",
            " with",
            " arsen",
            "ic",
            ",",
            " exploiting",
            " the",
            " fact",
            " that",
            " the",
            " arsen",
            "ate",
            " and",
            " phosphate",
            " an",
            "ions",
            " are",
            " similar",
            " struct",
            "urally",
            ".",
            " The",
            " study",
            " was",
            " widely",
            " criticised",
            " and",
            " subsequently",
            " ref",
            "uted",
            " by",
            " independent",
            " researcher",
            " groups",
            ".",
            "Ess",
            "ential",
            " trace",
            " element",
            " in",
            " higher",
            " animals",
            " ",
            "Ars",
            "enic",
            " is",
            " understood",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.898,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " formally",
            " object",
            " at",
            " the",
            " time",
            ",",
            " to",
            " what",
            " one",
            " views",
            " as",
            " improper",
            " action",
            " in",
            " the",
            " lower",
            " court",
            ",",
            " may",
            " result",
            " in",
            " the",
            " affirm",
            "ance",
            " of",
            " the",
            " lower",
            " court",
            "'s",
            " judgment",
            " on",
            " the",
            " grounds",
            " that",
            " one",
            " did",
            " not",
            " \"",
            "preserve",
            " the",
            " issue",
            " for",
            " appeal",
            "\"",
            " by",
            " object",
            "ing",
            ".",
            "In",
            " cases",
            " where",
            " a",
            " judge",
            " rather",
            " than",
            " a",
            " jury",
            " decided",
            " issues",
            " of",
            " fact",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.891,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " chatter",
            "boxes",
            " who",
            " pretend",
            " to",
            " speak",
            " in",
            " the",
            " name",
            " of",
            " the",
            " people",
            ".\"",
            " After",
            " France",
            "'s",
            " liberation",
            ",",
            " Cam",
            "us",
            " remarked",
            ",",
            " \"",
            "This",
            " country",
            " does",
            " not",
            " need",
            " a",
            " Tal",
            "ley",
            "rand",
            ",",
            " but",
            " a",
            " Saint",
            "-",
            "Just",
            ".\"",
            " The",
            " reality",
            " of",
            " the",
            " post",
            "war",
            " trib",
            "un",
            "als",
            " soon",
            " changed",
            " his",
            " mind",
            ":",
            " Cam",
            "us",
            " publicly",
            " reversed",
            " himself",
            " and",
            " became",
            " a",
            " lifelong",
            " opponent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.754,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "ive",
            " products",
            " are",
            " distinguished",
            " from",
            " the",
            " conventional",
            " ones",
            " by",
            " the",
            " use",
            " of",
            " one",
            " or",
            " more",
            " enabling",
            " technologies",
            " (",
            "for",
            " instance",
            ",",
            " artificial",
            " intelligence",
            ",",
            " Internet",
            " of",
            " Things",
            ",",
            " advanced",
            " sensors",
            ",",
            " new",
            " material",
            ",",
            " additive",
            " manufacturing",
            ",",
            " advanced",
            " robotics",
            ",",
            " augmented",
            " and",
            " virtual",
            " reality",
            ")",
            " or",
            " by",
            " the",
            " inclusion",
            " of",
            " implant",
            "able",
            " products",
            "/components",
            ".",
            " Such",
            " emerging",
            " assist",
            "ive",
            " products",
            " are",
            " either",
            " more"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.738,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " close",
            " third",
            ",",
            " despite",
            " the",
            " fact",
            " that",
            " the",
            " M",
            "HP",
            " is",
            " politically",
            " stronger",
            " than",
            " the",
            " CHP",
            " in",
            " almost",
            " every",
            " other",
            " district",
            ".",
            " Overall",
            ",",
            " the",
            " AK",
            " Party",
            " enjoys",
            " the",
            " most",
            " support",
            " throughout",
            " the",
            " city",
            ".",
            " The",
            " electorate",
            " of",
            " Ankara",
            " thus",
            " tend",
            " to",
            " vote",
            " in",
            " favor",
            " of",
            " the",
            " political",
            " right",
            ",",
            " far",
            " more",
            " so",
            " than",
            " the",
            " other",
            " main",
            " cities",
            " of",
            " Istanbul",
            " and",
            " Ä°zmir"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            "201",
            "0",
            ").",
            " The",
            " Alps",
            ":",
            " People",
            " and",
            " pressures",
            " in",
            " the",
            " mountains",
            ",",
            " the",
            " facts",
            " at",
            " a",
            " glance",
            " All",
            "aby",
            ",",
            " Michael",
            " et",
            " al",
            ".",
            " The",
            " Encyclopedia",
            " of",
            " Earth",
            ".",
            " (",
            "200",
            "8",
            ").",
            " Berkeley",
            ":",
            " University",
            " of",
            " California",
            " Press",
            ".",
            " ",
            " Be",
            "att",
            "ie",
            ",",
            " Andrew",
            ".",
            " (",
            "200",
            "6",
            ").",
            " The",
            " Alps",
            ":",
            " A",
            " Cultural",
            " History",
            ".",
            " New",
            " York",
            ":"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            0.031,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " example",
            ",",
            " the",
            " Bra",
            "hm",
            "ans",
            ",",
            " the",
            " Inc",
            "as",
            ",",
            " and",
            " the",
            " rulers",
            " of",
            " the",
            " South",
            " Sea",
            " Islands",
            ".",
            " All",
            " this",
            " is",
            " due",
            " to",
            " the",
            " fact",
            " that",
            " necessity",
            " is",
            " the",
            " mother",
            " of",
            " invention",
            " because",
            " those",
            " tribes",
            " that",
            " em",
            "igrated",
            " early",
            " to",
            " the",
            " north",
            ",",
            " and",
            " there",
            " gradually",
            " became",
            " white",
            ",",
            " had",
            " to",
            " develop",
            " all",
            " their",
            " intellectual",
            " powers",
            " and",
            " invent",
            " and",
            " perfect"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " dog",
            " weights",
            " (",
            "meaning",
            " the",
            " group",
            " is",
            " relatively",
            " homogeneous",
            ")",
            " and",
            " (",
            "b",
            ")",
            " the",
            " mean",
            " of",
            " each",
            " group",
            " is",
            " distinct",
            " (",
            "if",
            " two",
            " groups",
            " have",
            " the",
            " same",
            " mean",
            ",",
            " then",
            " it",
            " isn",
            "'t",
            " reasonable",
            " to",
            " conclude",
            " that",
            " the",
            " groups",
            " are",
            ",",
            " in",
            " fact",
            ",",
            " separate",
            " in",
            " any",
            " meaningful",
            " way",
            ").",
            "In",
            " the",
            " illustrations",
            " to",
            " the",
            " right",
            ",",
            " groups",
            " are",
            " identified",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " element",
            " undergo",
            " beta",
            " decay",
            ",",
            " though",
            " nuclear",
            " mass",
            " measurements",
            " indicate",
            " that",
            " ",
            "215",
            "At",
            " is",
            " in",
            " fact",
            " beta",
            "-st",
            "able",
            ",",
            " as",
            " it",
            " has",
            " the",
            " lowest",
            " mass",
            " of",
            " all",
            " is",
            "ob",
            "ars",
            " with",
            " A",
            "Âł",
            "=",
            "Âł",
            "215",
            ".",
            " A",
            " beta",
            " decay",
            " mode",
            " has",
            " been",
            " found",
            " for",
            " all",
            " other",
            " a",
            "stat",
            "ine",
            " isot",
            "opes",
            " except",
            " for",
            " a",
            "stat",
            "ine",
            "-",
            "213",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " this",
            " reason",
            ".",
            "E",
            "instein",
            " argued",
            " that",
            " this",
            " is",
            " true",
            " for",
            " a",
            " fundamental",
            " reason",
            ":",
            " the",
            " gravitational",
            " field",
            " could",
            " be",
            " made",
            " to",
            " vanish",
            " by",
            " a",
            " choice",
            " of",
            " coordinates",
            ".",
            " He",
            " maintained",
            " that",
            " the",
            " non",
            "-c",
            "ov",
            "ariant",
            " energy",
            " momentum",
            " pseud",
            "ot",
            "ensor",
            " was",
            ",",
            " in",
            " fact",
            ",",
            " the",
            " best",
            " description",
            " of",
            " the",
            " energy",
            " momentum",
            " distribution",
            " in",
            " a",
            " gravitational",
            " field",
            ".",
            " While",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            0.073,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " lines",
            " while",
            " Johnston",
            " charged",
            " well",
            " in",
            " advance",
            " of",
            " his",
            " soldiers",
            ".",
            " Al",
            "on",
            "zo",
            " Ridley",
            " of",
            " Los",
            " Angeles",
            " commanded",
            " the",
            " body",
            "guard",
            " \"",
            "the",
            " Guides",
            "\"",
            " of",
            " Gen",
            ".",
            " A",
            ".",
            " S",
            ".",
            " Johnston",
            " and",
            " was",
            " by",
            " his",
            " side",
            " when",
            " he",
            " fell",
            ".",
            "John",
            "ston",
            " was",
            " the",
            " highest",
            "-ranking",
            " fat",
            "ality",
            " of",
            " the",
            " war",
            " on",
            " either",
            " side",
            " and",
            " his",
            " death",
            " was",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ality",
            ".",
            "Politics",
            "M",
            "unicip",
            "al",
            " council",
            "A",
            "alborg",
            "'s",
            " municipal",
            " council",
            " consists",
            " of",
            " ",
            "31",
            " members",
            ",",
            " elected",
            " every",
            " four",
            " years",
            ".",
            "Below",
            " are",
            " the",
            " municipal",
            " councils",
            " elected",
            " since",
            " the",
            " Municipal",
            " Reform",
            " of",
            " ",
            "200",
            "7",
            ".",
            "T",
            "win",
            " towns",
            " –",
            " sister",
            " cities",
            "A",
            "alborg",
            " is",
            " tw",
            "inned",
            " with",
            " ",
            "34",
            " cities",
            ",",
            " more",
            " than",
            " any",
            " other",
            " city",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.237,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " mixed",
            " regiment",
            " of",
            " ",
            "250",
            " dr",
            "ago",
            "ons",
            " and",
            " ",
            "200",
            " infantry",
            " supported",
            " by",
            " batteries",
            " of",
            " flying",
            " artillery",
            ".",
            " It",
            " was",
            " commanded",
            " by",
            " Ban",
            "ast",
            "re",
            " Tar",
            "leton",
            " and",
            " gained",
            " a",
            " fears",
            "ome",
            " reputation",
            " in",
            " the",
            " colonies",
            " for",
            " \"",
            "br",
            "ut",
            "ality",
            " and",
            " needless",
            " slaughter",
            "\".",
            "In",
            " May",
            " ",
            "177",
            "9",
            " the",
            " British",
            " Legion",
            " was",
            " one",
            " of",
            " five",
            " reg",
            "iments",
            " that",
            " formed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.236,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " being",
            " passed",
            " out",
            " periodically",
            " through",
            " the",
            " clo",
            "ac",
            "al",
            " vent",
            ".",
            " Lar",
            "vae",
            " and",
            " most",
            " aquatic",
            " adult",
            " amphib",
            "ians",
            " ex",
            "crete",
            " the",
            " nitrogen",
            " as",
            " ammonia",
            " in",
            " large",
            " quantities",
            " of",
            " dil",
            "ute",
            " urine",
            ",",
            " while",
            " terrestrial",
            " species",
            ",",
            " with",
            " a",
            " greater",
            " need",
            " to",
            " conserve",
            " water",
            ",",
            " ex",
            "crete",
            " the",
            " less",
            " toxic",
            " product",
            " ure",
            "a",
            ".",
            " Some",
            " tree",
            " frogs",
            " with",
            " limited",
            " access",
            " to",
            " water",
            " ex"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " she",
            " reiterated",
            " her",
            " hopes",
            " for",
            " reunion",
            " during",
            " an",
            " interview",
            " with",
            " Die",
            " Zeit",
            ",",
            " stating",
            ":",
            " \"",
            "If",
            " they",
            " ask",
            " me",
            ",",
            " I",
            "'ll",
            " say",
            " yes",
            ".\"",
            "In",
            " a",
            " May",
            " ",
            "201",
            "3",
            " interview",
            ",",
            " F",
            "Ã¤l",
            "ts",
            "k",
            "og",
            ",",
            " aged",
            " ",
            "63",
            " at",
            " the",
            " time",
            ",",
            " stated",
            " that",
            " an",
            " AB",
            "BA",
            " reunion",
            " would",
            " never",
            " occur",
            ":",
            " \"",
            "I",
            " think",
            " we",
            " have",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "act",
            "ically",
            " un",
            "ambiguous",
            " phrase",
            " has",
            " a",
            " semantic",
            " ambiguity",
            ";",
            " for",
            " example",
            ",",
            " the",
            " lexical",
            " ambiguity",
            " in",
            " \"",
            "Your",
            " boss",
            " is",
            " a",
            " funny",
            " man",
            "\"",
            " is",
            " purely",
            " semantic",
            ",",
            " leading",
            " to",
            " the",
            " response",
            " \"",
            "Funny",
            " ha",
            "-h",
            "a",
            " or",
            " funny",
            " peculiar",
            "?\"",
            "Sp",
            "oken",
            " language",
            " can",
            " contain",
            " many",
            " more",
            " types",
            " of",
            " ambigu",
            "ities",
            " which",
            " are",
            " called",
            " phon",
            "ological",
            " ambigu",
            "ities",
            ",",
            " where",
            " there"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " festival",
            " Hy",
            "acin",
            "th",
            "ia",
            " was",
            " a",
            " national",
            " celebration",
            " of",
            " Sp",
            "arta",
            ",",
            " which",
            " commemor",
            "ated",
            " the",
            " death",
            " and",
            " reb",
            "irth",
            " of",
            " Hy",
            "acin",
            "thus",
            ".",
            "Another",
            " male",
            " lover",
            " was",
            " C",
            "yp",
            "ar",
            "iss",
            "us",
            ",",
            " a",
            " descendant",
            " of",
            " Her",
            "acles",
            ".",
            " Apollo",
            " gave",
            " him",
            " a",
            " tame",
            " deer",
            " as",
            " a",
            " companion",
            " but",
            " C",
            "yp",
            "ar",
            "iss",
            "us",
            " accidentally",
            " killed",
            " it",
            " with",
            " a",
            " jav"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Rocket",
            "-powered",
            " aircraft",
            " have",
            " occasionally",
            " been",
            " experimented",
            " with",
            ",",
            " and",
            " the",
            " Mess",
            "ers",
            "ch",
            "mitt",
            " Me",
            " ",
            "163",
            " K",
            "omet",
            " fighter",
            " even",
            " saw",
            " action",
            " in",
            " the",
            " Second",
            " World",
            " War",
            ".",
            " Since",
            " then",
            ",",
            " they",
            " have",
            " been",
            " restricted",
            " to",
            " research",
            " aircraft",
            ",",
            " such",
            " as",
            " the",
            " North",
            " American",
            " X",
            "-",
            "15",
            ",",
            " which",
            " traveled",
            " up",
            " into",
            " space",
            " where",
            " air",
            "-bre",
            "athing",
            " engines",
            " cannot",
            " work"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " people",
            " in",
            " a",
            " war",
            " with",
            " the",
            " king",
            " of",
            " Ar",
            "ad",
            ",",
            " in",
            " consequence",
            " of",
            " which",
            " the",
            " Israel",
            "ites",
            " fled",
            ",",
            " marching",
            " seven",
            " stations",
            " backward",
            " to",
            " Mos",
            "era",
            ",",
            " where",
            " they",
            " performed",
            " the",
            " rites",
            " of",
            " mourning",
            " for",
            " Aaron",
            ";",
            " where",
            "fore",
            " it",
            " is",
            " said",
            ":",
            " \"",
            "There",
            " [",
            "at",
            " Mos",
            "era",
            "]",
            " died",
            " Aaron",
            ".\"",
            "The",
            " r",
            "abb",
            "is",
            " particularly",
            " praise",
            " the",
            " brother"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " help",
            " disrupt",
            " and",
            " shorten",
            " the",
            " intense",
            " lobbying",
            " and",
            " ad",
            " campaigns",
            " associated",
            " with",
            " Oscar",
            " season",
            " in",
            " the",
            " film",
            " industry",
            ".",
            " Another",
            " reason",
            " was",
            " because",
            " of",
            " the",
            " growing",
            " TV",
            " ratings",
            " success",
            " coinc",
            "iding",
            " with",
            " the",
            " NCAA",
            " basketball",
            " tournament",
            ",",
            " which",
            " would",
            " cut",
            " into",
            " the",
            " Academy",
            " Awards",
            " audience",
            ".",
            " (",
            "In",
            " ",
            "197",
            "6",
            " and",
            " ",
            "197",
            "7",
            ",",
            " ABC",
            "'s",
            " regained",
            " Oscars",
            " were",
            " moved"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Living",
            ",",
            " Rand",
            "'s",
            " later",
            " success",
            " allowed",
            " her",
            " to",
            " get",
            " a",
            " revised",
            " version",
            " published",
            " in",
            " ",
            "194",
            "6",
            ",",
            " and",
            " this",
            " sold",
            " over",
            " ",
            "3",
            ".",
            "5",
            "Âł",
            "million",
            " copies",
            ".",
            "The",
            " Fountain",
            "head",
            " and",
            " political",
            " activism",
            "During",
            " the",
            " ",
            "194",
            "0",
            "s",
            ",",
            " Rand",
            " became",
            " politically",
            " active",
            ".",
            " She",
            " and",
            " her",
            " husband",
            " were",
            " full",
            "-time",
            " volunteers",
            " for",
            " Republican",
            " Wend",
            "ell"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "onde",
    " Scient",
    "ajas",
    ".Companion",
    "dens"
  ],
  "bottom_logits": [
    " Erf",
    "ī",
    " Forces",
    " Downing",
    " Frederick"
  ],
  "act_min": -0.0,
  "act_max": 1.125
}