{
  "index": 65555,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " Eli",
            "ot",
            " wrote",
            " back",
            " to",
            " Orwell",
            " praising",
            " the",
            " book",
            "'s",
            " \"",
            "good",
            " writing",
            "\"",
            " and",
            " \"",
            "fund",
            "amental",
            " integrity",
            "\",",
            " but",
            " declared",
            " that",
            " they",
            " would",
            " only",
            " accept",
            " it",
            " for",
            " publication",
            " if",
            " they",
            " had",
            " some",
            " sympathy",
            " for",
            " the",
            " viewpoint",
            " \"",
            "which",
            " I",
            " take",
            " to",
            " be",
            " generally",
            " Trotsky",
            "ite",
            "\".",
            " Eli",
            "ot",
            " said",
            " he",
            " found",
            " the",
            " view",
            " \"",
            "not",
            " convincing",
            "\",",
            " and",
            " cont",
            "ended"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " Eli",
            "ot",
            " wrote",
            " back",
            " to",
            " Orwell",
            " praising",
            " the",
            " book",
            "'s",
            " \"",
            "good",
            " writing",
            "\"",
            " and",
            " \"",
            "fund",
            "amental",
            " integrity",
            "\",",
            " but",
            " declared",
            " that",
            " they",
            " would",
            " only",
            " accept",
            " it",
            " for",
            " publication",
            " if",
            " they",
            " had",
            " some",
            " sympathy",
            " for",
            " the",
            " viewpoint",
            " \"",
            "which",
            " I",
            " take",
            " to",
            " be",
            " generally",
            " Trotsky",
            "ite",
            "\".",
            " Eli",
            "ot",
            " said",
            " he",
            " found",
            " the",
            " view",
            " \"",
            "not",
            " convincing",
            "\",",
            " and",
            " cont",
            "ended"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.336,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Syracuse",
            " so",
            " that",
            " he",
            " might",
            " learn",
            " about",
            " Athen",
            "ian",
            " life",
            " and",
            " government",
            ".",
            "Latin",
            " translations",
            " of",
            " the",
            " plays",
            " by",
            " Andreas",
            " Div",
            "us",
            " (",
            "Ven",
            "ice",
            " ",
            "152",
            "8",
            ")",
            " were",
            " circulated",
            " widely",
            " throughout",
            " Europe",
            " in",
            " the",
            " Renaissance",
            " and",
            " these",
            " were",
            " soon",
            " followed",
            " by",
            " translations",
            " and",
            " adaptations",
            " in",
            " modern",
            " languages",
            ".",
            " Rac",
            "ine",
            ",",
            " for",
            " example",
            ",",
            " drew",
            " Les",
            " Pla",
            "ide",
            "urs",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Syracuse",
            " University",
            "Works",
            " ",
            "  ",
            " ",
            "190",
            "9",
            " births",
            "197",
            "1",
            " deaths",
            "University",
            " of",
            " Wisconsin",
            "â€“",
            "Mad",
            "ison",
            " alumni",
            "American",
            " short",
            " story",
            " writers",
            "American",
            " mystery",
            " writers",
            "American",
            " speculative",
            " fiction",
            " editors",
            "20",
            "th",
            "-century",
            " American",
            " novel",
            "ists",
            "C",
            "th",
            "ul",
            "hu",
            " Myth",
            "os",
            " writers",
            "American",
            " horror",
            " writers",
            "People",
            " from",
            " Sau",
            "k",
            " City",
            ",",
            " Wisconsin"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "from",
            " T",
            ".",
            " S",
            ".",
            " Eli",
            "ot",
            "'s",
            " Four",
            " Quart",
            "ets",
            "),",
            " Post",
            "ern",
            " of",
            " Fate",
            " (",
            "from",
            " James",
            " El",
            "roy",
            " Fle",
            "cker",
            "'s",
            " \"",
            "G",
            "ates",
            " of",
            " Damascus",
            "\"),",
            " End",
            "less",
            " Night",
            " (",
            "from",
            " William",
            " Blake",
            "'s",
            " \"",
            "Aug",
            "uries",
            " of",
            " Innoc",
            "ence",
            "\"),",
            " N",
            " or",
            " M",
            "?",
            " (",
            "from",
            " the",
            " Book",
            " of",
            " Common",
            " Prayer",
            "),",
            " and",
            " Come",
            ",",
            " Tell",
            " Me"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            "199",
            "3",
            "  ",
            " â€“",
            " Dip",
            "a",
            " K",
            "arm",
            "ak",
            "ar",
            ",",
            " Indian",
            " gymn",
            "ast",
            "199",
            "4",
            " â€“",
            " K",
            "elli",
            " Hub",
            "ly",
            ",",
            " American",
            " soccer",
            " player",
            " ",
            " ",
            "199",
            "4",
            "  ",
            " â€“",
            " King",
            " Von",
            ",",
            " American",
            " rapper",
            " (",
            "d",
            ".",
            " ",
            "202",
            "0",
            ")",
            "199",
            "5",
            " â€“",
            " Eli",
            " Apple",
            ",",
            " American",
            " football",
            " player",
            " ",
            " ",
            "199",
            "5",
            "  ",
            " â€“",
            " Justice",
            " Smith"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " measures",
            " ",
            "10",
            ".",
            "48",
            " x",
            " ",
            "41",
            ".",
            "63",
            " m",
            " and",
            " the",
            " number",
            " of",
            " p",
            "ter",
            "on",
            " columns",
            " was",
            " ",
            "6",
            " x",
            " ",
            "17",
            ".",
            " There",
            " was",
            " a",
            " port",
            "ico",
            " with",
            " a",
            " second",
            " row",
            " of",
            " columns",
            ",",
            " which",
            " is",
            " also",
            " att",
            "ested",
            " for",
            " the",
            " temple",
            " at",
            " Syracuse",
            ".",
            "Del",
            "phi",
            ":",
            " The",
            " first",
            " temple",
            " dedicated",
            " to",
            " Apollo",
            ",",
            " was",
            " built",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "ury",
            " at",
            " Syracuse",
            " University",
            " ",
            " Anat",
            "ole",
            " France",
            ",",
            " his",
            " work",
            " in",
            " audio",
            " version",
            "   ",
            " ",
            "184",
            "4",
            " births",
            "192",
            "4",
            " deaths",
            "W",
            "riters",
            " from",
            " Paris",
            "French",
            " bibli",
            "oph",
            "iles",
            "Coll",
            "ÃƒÂ¨ge",
            " Stan",
            "is",
            "las",
            " de",
            " Paris",
            " alumni",
            "French",
            " fantasy",
            " writers",
            "French",
            " Nobel",
            " laure",
            "ates",
            "19",
            "th",
            "-century",
            " French",
            " poets",
            "French",
            " sat",
            "ir",
            "ists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " .",
            " ",
            "  ",
            "â€”â€”â€”â€”",
            "â€”â€”",
            "â€”",
            "Notes",
            "External",
            " links",
            " Award",
            "-winning",
            " documentary",
            " about",
            " him",
            " ",
            " Albert",
            " Schwe",
            "itzer",
            " info",
            " at",
            " Internet",
            " Archive",
            " ",
            "  ",
            " Albert",
            " Schwe",
            "itzer",
            " Papers",
            " at",
            " Syracuse",
            " University",
            " John",
            " D",
            ".",
            " Reg",
            "ester",
            " Collection",
            " on",
            " Albert",
            " Schwe",
            "itzer",
            " The",
            " H",
            "elf",
            "fer",
            "ich",
            " Collection",
            ",",
            " collected",
            " by",
            " Reg",
            "inal",
            "d",
            " H",
            ".",
            " H",
            "elf",
            "fer"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " burgeoning",
            " Beat",
            " generation",
            "'s",
            " period",
            " of",
            " development",
            ".",
            " As",
            " a",
            " Barn",
            "ard",
            " student",
            ",",
            " El",
            "ise",
            " Cow",
            "en",
            " extensively",
            " read",
            " the",
            " poetry",
            " of",
            " Ezra",
            " Pound",
            " and",
            " T",
            ".",
            " S",
            ".",
            " Eli",
            "ot",
            ",",
            " when",
            " she",
            " met",
            " Joyce",
            " Johnson",
            " and",
            " Leo",
            " Sk",
            "ir",
            ",",
            " among",
            " other",
            " Beat",
            " players",
            ".",
            " As",
            " Cow",
            "en",
            " had",
            " felt",
            " a",
            " strong",
            " attraction",
            " to",
            " darker",
            " poetry",
            " most",
            " of",
            " the",
            " time"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " became",
            " clear",
            " to",
            " Orwell",
            " that",
            " anti",
            "-S",
            "ov",
            "iet",
            " literature",
            " was",
            " not",
            " something",
            " which",
            " most",
            " major",
            " publishing",
            " houses",
            " would",
            " touch",
            "Ã‚Å‚",
            "â€“",
            " including",
            " his",
            " regular",
            " publisher",
            " G",
            "oll",
            "anc",
            "z",
            ".",
            " He",
            " also",
            " submitted",
            " the",
            " manuscript",
            " to",
            " Fab",
            "er",
            " and",
            " Fab",
            "er",
            ",",
            " where",
            " the",
            " poet",
            " T",
            ".",
            " S",
            ".",
            " Eli",
            "ot",
            " (",
            "who",
            " was",
            " a",
            " director",
            " of",
            " the",
            " firm",
            ")",
            " rejected",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "al",
            "izing",
            " all",
            " nearby",
            " conson",
            "ants",
            " and",
            " triggering",
            " the",
            " back",
            " allo",
            "phone",
            " ",
            " in",
            " all",
            " nearby",
            " low",
            " vowels",
            ".",
            " The",
            " extent",
            " of",
            " emphasis",
            " spreading",
            " varies",
            ".",
            " For",
            " example",
            ",",
            " in",
            " Moroccan",
            " Arabic",
            ",",
            " it",
            " spreads",
            " as",
            " far",
            " as",
            " the",
            " first",
            " full",
            " vowel",
            " (",
            "i",
            ".e",
            ".",
            " sound",
            " derived",
            " from",
            " a",
            " long",
            " vowel",
            " or",
            " d",
            "iph",
            "th",
            "ong",
            ")",
            " on",
            " either",
            " side",
            ";",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " all",
            " went",
            " something",
            " like",
            ":",
            " '",
            "The",
            " good",
            " king",
            " rode",
            " forth",
            " from",
            " his",
            " castle",
            ",",
            " saw",
            " the",
            " suffering",
            " workers",
            " and",
            " healed",
            " them",
            ".'\"",
            " Of",
            " his",
            " father",
            " Gins",
            "berg",
            " said",
            ":",
            " \"",
            "My",
            " father",
            " would",
            " go",
            " around",
            " the",
            " house",
            " either",
            " rec",
            "iting",
            " Emily",
            " Dickinson",
            " and",
            " Long",
            "f",
            "ellow",
            " under",
            " his",
            " breath",
            " or",
            " attacking",
            " T",
            ".",
            " S",
            ".",
            " Eli",
            "ot",
            " for",
            " ruining",
            " poetry",
            " with",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " basic",
            " conson",
            "antal",
            " symbol",
            " was",
            " considered",
            " to",
            " have",
            " an",
            " inherent",
            " \"",
            "a",
            "\"",
            " vowel",
            " sound",
            ".",
            " Hooks",
            " or",
            " short",
            " lines",
            " attached",
            " to",
            " various",
            " parts",
            " of",
            " the",
            " basic",
            " letter",
            " modify",
            " the",
            " vowel",
            ".",
            " In",
            " this",
            " way",
            ",",
            " the",
            " South",
            " Arabian",
            " ab",
            "jad",
            " evolved",
            " into",
            " the",
            " Ge",
            "'",
            "ez",
            " ab",
            "ug",
            "ida",
            " of",
            " Ethiopia",
            " between",
            " the",
            " ",
            "5",
            "th",
            " century",
            " BC",
            " and",
            " the",
            " ",
            "5"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ;",
            " also",
            " ab",
            "g",
            "ad",
            ")",
            " is",
            " a",
            " writing",
            " system",
            " in",
            " which",
            " only",
            " conson",
            "ants",
            " are",
            " represented",
            ",",
            " leaving",
            " vowel",
            " sounds",
            " to",
            " be",
            " inferred",
            " by",
            " the",
            " reader",
            ".",
            " This",
            " contrasts",
            " with",
            " alph",
            "ab",
            "ets",
            ",",
            " which",
            " provide",
            " graph",
            "emes",
            " for",
            " both",
            " conson",
            "ants",
            " and",
            " vowels",
            ".",
            " The",
            " term",
            " was",
            " introduced",
            " in",
            " ",
            "199",
            "0",
            " by",
            " Peter",
            " T",
            ".",
            " Daniels",
            ".",
            " Other",
            " terms"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " that",
            " are",
            " fused",
            " with",
            " the",
            " conson",
            "ants",
            " to",
            " the",
            " point",
            " that",
            " they",
            " must",
            " be",
            " considered",
            " modifications",
            " of",
            " the",
            " form",
            " of",
            " the",
            " letters",
            ".",
            " Children",
            " learn",
            " each",
            " modification",
            " separately",
            ",",
            " as",
            " in",
            " a",
            " syll",
            "ab",
            "ary",
            ";",
            " nonetheless",
            ",",
            " the",
            " graphic",
            " similarities",
            " between",
            " syll",
            "ables",
            " with",
            " the",
            " same",
            " conson",
            "ant",
            " are",
            " readily",
            " apparent",
            ",",
            " unlike",
            " the",
            " case",
            " in",
            " a",
            " true",
            " syll",
            "ab",
            "ary"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " lig",
            "ature",
            ",",
            " or",
            " otherwise",
            " change",
            " their",
            " shapes",
            ".",
            " Rare",
            "ly",
            ",",
            " one",
            " of",
            " the",
            " conson",
            "ants",
            " may",
            " be",
            " replaced",
            " by",
            " a",
            " gem",
            "ination",
            " mark",
            ",",
            " e",
            ".g",
            ".",
            " the",
            " G",
            "urm",
            "uk",
            "hi",
            " add",
            "ak",
            ".",
            "When",
            " they",
            " are",
            " arranged",
            " vertically",
            ",",
            " as",
            " in",
            " Bur",
            "m",
            "ese",
            " or",
            " Kh",
            "mer",
            ",",
            " they",
            " are",
            " said",
            " to",
            " be",
            " '",
            "stack",
            "ed",
            "'.",
            " Often"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            0.488,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " doubled",
            " conson",
            "ants",
            " are",
            " held",
            " twice",
            " as",
            " long",
            " as",
            " short",
            " conson",
            "ants",
            ".",
            " This",
            " conson",
            "ant",
            " length",
            "ening",
            " is",
            " phon",
            "em",
            "ically",
            " contrast",
            "ive",
            ":",
            "  ",
            " '",
            "he",
            " accepted",
            "'",
            " vs",
            ".",
            "  ",
            " '",
            "he",
            " kissed",
            "'.",
            "S",
            "yll",
            "able",
            " structure",
            " ",
            "Ar",
            "abic",
            " has",
            " two",
            " kinds",
            " of",
            " syll",
            "ables",
            ":",
            " open",
            " syll",
            "ables",
            " (",
            "CV",
            ")",
            " and",
            " (",
            "CV",
            "V",
            ")â€”"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " or",
            " after",
            " the",
            " conson",
            "ant",
            ".",
            " Two",
            " exceptions",
            " were",
            " Y",
            " and",
            " Z",
            ",",
            " which",
            " were",
            " borrowed",
            " from",
            " the",
            " Greek",
            " alphabet",
            " rather",
            " than",
            " E",
            "tr",
            "us",
            "can",
            ".",
            " They",
            " were",
            " known",
            " as",
            " Y",
            " Gra",
            "eca",
            " \"",
            "Greek",
            " Y",
            "\"",
            " and",
            " z",
            "eta",
            " (",
            "from",
            " Greek",
            ")â€”",
            "this",
            " discrepancy",
            " was",
            " inherited",
            " by",
            " many",
            " European",
            " languages",
            ",",
            " as",
            " in",
            " the",
            " term",
            " z",
            "ed",
            " for",
            " Z",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " nearby",
            " conson",
            "ants",
            " (",
            "especially",
            " vel",
            "ar",
            " conson",
            "ants",
            " and",
            " uv",
            "ular",
            " conson",
            "ants",
            "),",
            " and",
            " then",
            " short",
            " /",
            "a",
            " i",
            " u",
            "/",
            " all",
            " merge",
            " into",
            " ,",
            " which",
            " is",
            " deleted",
            " in",
            " many",
            " contexts",
            ".",
            " (",
            "The",
            " lab",
            "ial",
            "ization",
            " plus",
            " ",
            " is",
            " sometimes",
            " interpreted",
            " as",
            " an",
            " underlying",
            " phon",
            "eme",
            " .",
            ")",
            " This",
            " essentially",
            " causes",
            " the",
            " wholesale",
            " loss",
            " of",
            " the",
            " short",
            "-long",
            " vowel"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ;",
            " also",
            " ab",
            "g",
            "ad",
            ")",
            " is",
            " a",
            " writing",
            " system",
            " in",
            " which",
            " only",
            " conson",
            "ants",
            " are",
            " represented",
            ",",
            " leaving",
            " vowel",
            " sounds",
            " to",
            " be",
            " inferred",
            " by",
            " the",
            " reader",
            ".",
            " This",
            " contrasts",
            " with",
            " alph",
            "ab",
            "ets",
            ",",
            " which",
            " provide",
            " graph",
            "emes",
            " for",
            " both",
            " conson",
            "ants",
            " and",
            " vowels",
            ".",
            " The",
            " term",
            " was",
            " introduced",
            " in",
            " ",
            "199",
            "0",
            " by",
            " Peter",
            " T",
            ".",
            " Daniels",
            ".",
            " Other",
            " terms"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " transl",
            "iteration",
            ",",
            " but",
            " some",
            " have",
            " named",
            " it",
            " Arabic",
            " Chat",
            " Alphabet",
            ".",
            " Other",
            " systems",
            " of",
            " transl",
            "iteration",
            " exist",
            ",",
            " such",
            " as",
            " using",
            " dots",
            " or",
            " capital",
            "ization",
            " to",
            " represent",
            " the",
            " \"",
            "em",
            "ph",
            "atic",
            "\"",
            " counterparts",
            " of",
            " certain",
            " conson",
            "ants",
            ".",
            " For",
            " instance",
            ",",
            " using",
            " capital",
            "ization",
            ",",
            " the",
            " letter",
            " ,",
            " may",
            " be",
            " represented",
            " by",
            " d",
            ".",
            " Its",
            " emph",
            "atic",
            " counterpart",
            ",",
            " ,"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " least",
            " by",
            " the",
            " ",
            "9",
            "th",
            " century",
            " BC",
            ")",
            " it",
            " and",
            " most",
            " of",
            " the",
            " contemporary",
            " Sem",
            "itic",
            " ab",
            "j",
            "ads",
            " had",
            " begun",
            " to",
            " overload",
            " a",
            " few",
            " of",
            " the",
            " conson",
            "ant",
            " symbols",
            " with",
            " a",
            " secondary",
            " function",
            " as",
            " vowel",
            " markers",
            ",",
            " called",
            " mat",
            "res",
            " le",
            "ction",
            "is",
            ".",
            " This",
            " practice",
            " was",
            " at",
            " first",
            " rare",
            " and",
            " limited",
            " in",
            " scope",
            " but",
            " became",
            " increasingly",
            " common",
            " and",
            " more",
            " developed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            "rit",
            "ics",
            " modify",
            " to",
            " represent",
            " vowels",
            ",",
            " like",
            " in",
            " Dev",
            "an",
            "ag",
            "ari",
            " and",
            " other",
            " South",
            " Asian",
            " scripts",
            ",",
            " an",
            " ab",
            "jad",
            ",",
            " in",
            " which",
            " letters",
            " predominantly",
            " or",
            " exclusively",
            " represent",
            " conson",
            "ants",
            " such",
            " as",
            " the",
            " original",
            " Ph",
            "oen",
            "ician",
            ",",
            " Hebrew",
            " or",
            " Arabic",
            ",",
            " and",
            " an",
            " alphabet",
            ",",
            " a",
            " set",
            " of",
            " graph",
            "emes",
            " that",
            " represent",
            " both",
            " conson",
            "ants",
            " and",
            " vowels",
            ".",
            " In",
            " this"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            "di",
            "ac",
            "rit",
            "ics",
            "'.",
            ")",
            "An",
            " alph",
            "asy",
            "ll",
            "ab",
            "ary",
            " is",
            " defined",
            " as",
            " \"",
            "a",
            " type",
            " of",
            " writing",
            " system",
            " in",
            " which",
            " the",
            " vowels",
            " are",
            " den",
            "oted",
            " by",
            " subsidiary",
            " symbols",
            ",",
            " not",
            " all",
            " of",
            " which",
            " occur",
            " in",
            " a",
            " linear",
            " order",
            " (",
            "with",
            " relation",
            " to",
            " the",
            " conson",
            "ant",
            " symbols",
            ")",
            " that",
            " is",
            " congr",
            "uent",
            " with",
            " their",
            " temporal",
            " order",
            " in",
            " speech",
            "\".",
            " Bright",
            " did"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " in",
            " which",
            " vowels",
            " are",
            " marked",
            " with",
            " di",
            "ac",
            "rit",
            "ics",
            " and",
            " syll",
            "able",
            "-final",
            " conson",
            "ants",
            ",",
            " when",
            " they",
            " occur",
            ",",
            " are",
            " indicated",
            " with",
            " lig",
            "atures",
            ",",
            " di",
            "ac",
            "rit",
            "ics",
            ",",
            " or",
            " with",
            " a",
            " special",
            " vowel",
            "-cancel",
            "ing",
            " mark",
            ".",
            "In",
            " the",
            " Ethi",
            "opic",
            " family",
            ",",
            " vowels",
            " are",
            " marked",
            " by",
            " modifying",
            " the",
            " shapes",
            " of",
            " the",
            " conson",
            "ants",
            ",",
            " and",
            " one",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " there",
            " has",
            " been",
            " a",
            " change",
            " to",
            " writing",
            " the",
            " two",
            " conson",
            "ants",
            " side",
            " by",
            " side",
            ".",
            " In",
            " the",
            " latter",
            " case",
            ",",
            " this",
            " combination",
            " may",
            " be",
            " indicated",
            " by",
            " a",
            " di",
            "ac",
            "ritic",
            " on",
            " one",
            " of",
            " the",
            " conson",
            "ants",
            " or",
            " a",
            " change",
            " in",
            " the",
            " form",
            " of",
            " one",
            " of",
            " the",
            " conson",
            "ants",
            ",",
            " e",
            ".g",
            ".",
            " the",
            " half",
            " forms",
            " of",
            " Dev",
            "an",
            "ag",
            "ari",
            ".",
            " Generally",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " reading",
            " order",
            " of",
            " stacked",
            " conson",
            "ants",
            " is",
            " top",
            " to",
            " bottom",
            ",",
            " or",
            " the",
            " general",
            " reading",
            " order",
            " of",
            " the",
            " script",
            ",",
            " but",
            " sometimes",
            " the",
            " reading",
            " order",
            " can",
            " be",
            " reversed",
            ".",
            "The",
            " division",
            " of",
            " a",
            " word",
            " into",
            " syll",
            "ables",
            " for",
            " the",
            " purposes",
            " of",
            " writing",
            " does",
            " not",
            " always",
            " accord",
            " with",
            " the",
            " natural",
            " phon",
            "etics",
            " of",
            " the",
            " language",
            ".",
            " For",
            " example",
            ",",
            " Brah",
            "mic",
            " scripts",
            " commonly"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "v",
            "owel",
            " indication",
            "\"",
            " using",
            " the",
            " positioning",
            " or",
            " choice",
            " of",
            " conson",
            "ant",
            " signs",
            " so",
            " that",
            " writing",
            " vowel",
            "-m",
            "arks",
            " can",
            " be",
            " disp",
            "ensed",
            " with",
            ".",
            "Development",
            " ",
            "As",
            " the",
            " term",
            " alph",
            "asy",
            "ll",
            "ab",
            "ary",
            " suggests",
            ",",
            " ab",
            "ug",
            "idas",
            " have",
            " been",
            " considered",
            " an",
            " intermediate",
            " step",
            " between",
            " alph",
            "ab",
            "ets",
            " and",
            " syll",
            "ab",
            "aries",
            ".",
            " Histor",
            "ically",
            ",",
            " ab",
            "ug",
            "idas",
            " appear"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " often",
            " written",
            " by",
            " combining",
            " the",
            " two",
            " conson",
            "ants",
            ".",
            " In",
            " the",
            " Ind",
            "ic",
            " scripts",
            ",",
            " the",
            " earliest",
            " method",
            " was",
            " simply",
            " to",
            " arrange",
            " them",
            " vertically",
            ",",
            " writing",
            " the",
            " second",
            " conson",
            "ant",
            " of",
            " the",
            " cluster",
            " below",
            " the",
            " first",
            " one",
            ".",
            " The",
            " two",
            " conson",
            "ants",
            " may",
            " also",
            " merge",
            " as",
            " conj",
            "unct",
            " conson",
            "ant",
            " letters",
            ",",
            " where",
            " two",
            " or",
            " more",
            " letters",
            " are",
            " graph",
            "ically",
            " joined",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.488,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " ,",
            " respectively",
            ",",
            " in",
            " many",
            " dialect",
            "s",
            ".",
            "The",
            " definition",
            " of",
            " both",
            " \"",
            "em",
            "ph",
            "atic",
            "\"",
            " and",
            " \"",
            "neighbor",
            "hood",
            "\"",
            " vary",
            " in",
            " ways",
            " that",
            " reflect",
            " (",
            "to",
            " some",
            " extent",
            ")",
            " corresponding",
            " variations",
            " in",
            " the",
            " spoken",
            " dialect",
            "s",
            ".",
            " Generally",
            ",",
            " the",
            " conson",
            "ants",
            " triggering",
            " \"",
            "em",
            "ph",
            "atic",
            "\"",
            " allo",
            "phones",
            " are",
            " the",
            " ph",
            "ary",
            "nge",
            "al",
            "ized",
            " conson",
            "ants"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " letter",
            " to",
            " represent",
            " the",
            " gl",
            "ott",
            "al",
            " stop",
            "â€”the",
            " conson",
            "ant",
            " sound",
            " that",
            " the",
            " letter",
            " den",
            "oted",
            " in",
            " Ph",
            "oen",
            "ician",
            " and",
            " other",
            " Sem",
            "itic",
            " languages",
            ",",
            " and",
            " that",
            " was",
            " the",
            " first",
            " phon",
            "eme",
            " of",
            " the",
            " Ph",
            "oen",
            "ician",
            " pronunciation",
            " of",
            " the",
            " letter",
            "â€”",
            "so",
            " they",
            " used",
            " their",
            " version",
            " of",
            " the",
            " sign",
            " to",
            " represent",
            " the",
            " vowel",
            " ,",
            " and",
            " called",
            " it",
            " by",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " same",
            " original",
            " Proto",
            "-A",
            "ust",
            "ro",
            "asi",
            "atic",
            " prefixes",
            ",",
            " such",
            " as",
            " the",
            " caus",
            "ative",
            " prefix",
            ",",
            " ranging",
            " from",
            " C",
            "VC",
            " syll",
            "ables",
            " to",
            " conson",
            "ant",
            " clusters",
            " to",
            " single",
            " conson",
            "ants",
            " among",
            " the",
            " modern",
            " languages",
            ".",
            " As",
            " for",
            " word",
            " formation",
            ",",
            " most",
            " Aust",
            "ro",
            "asi",
            "atic",
            " languages",
            " have",
            " a",
            " variety",
            " of",
            " deriv",
            "ational",
            " prefixes",
            ",",
            " many",
            " have",
            " infix",
            "es",
            ",",
            " but"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.488,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " e",
            "jective",
            ",",
            " and",
            "/or",
            " imp",
            "los",
            "ive",
            " conson",
            "ants",
            " in",
            " the",
            " different",
            " branches",
            ".",
            " It",
            " is",
            " generally",
            " agreed",
            " that",
            " only",
            " the",
            " ob",
            "stru",
            "ents",
            " had",
            " a",
            " contrast",
            " between",
            " voice",
            "less",
            " and",
            " voiced",
            " forms",
            " in",
            " Proto",
            "-A",
            "f",
            "ro",
            "asi",
            "atic",
            ",",
            " whereas",
            " continu",
            "ants",
            " were",
            " voice",
            "less",
            ".",
            "A",
            " form",
            " of",
            " long",
            "-distance",
            " conson",
            "ant",
            " assim",
            "ilation",
            " known",
            " as",
            " conson",
            "ant",
            " harmony"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " handle",
            " a",
            " phon",
            "etic",
            " sequence",
            " C",
            "VC",
            "-C",
            "V",
            " as",
            " CV",
            "-",
            "CC",
            "V",
            " or",
            " CV",
            "-C",
            "-C",
            "V",
            ".",
            " However",
            ",",
            " sometimes",
            " phon",
            "etic",
            " C",
            "VC",
            " syll",
            "ables",
            " are",
            " handled",
            " as",
            " single",
            " units",
            ",",
            " and",
            " the",
            " final",
            " conson",
            "ant",
            " may",
            " be",
            " represented",
            ":",
            "in",
            " much",
            " the",
            " same",
            " way",
            " as",
            " the",
            " second",
            " conson",
            "ant",
            " in",
            " CC",
            "V",
            ",",
            " e",
            ".g",
            ".",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "Marg",
            "aret",
            " Me",
            "ad",
            "M",
            "erv",
            "yn",
            " Meg",
            "g",
            "itt",
            "Jose",
            "f",
            " M",
            "enge",
            "le",
            "Nich",
            "olas",
            " Mik",
            "lou",
            "ho",
            "-M",
            "acl",
            "ay",
            "Emily",
            " Martin",
            "Hor",
            "ace",
            " Mitchell",
            " Miner",
            "Sid",
            "ney",
            " Mint",
            "z",
            "Ash",
            "ley",
            " Mont",
            "agu",
            "James",
            " Mo",
            "oney",
            "Hen",
            "ri",
            "etta",
            " L",
            ".",
            " Moore",
            "John",
            " H",
            ".",
            " Moore",
            "Lewis",
            " H"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.396,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " (),",
            " this",
            " was",
            " the",
            " name",
            " of",
            " a",
            " shrine",
            " of",
            " Apollo",
            " at",
            " Athens",
            " near",
            " the",
            " Il",
            "isos",
            " river",
            ".",
            " It",
            " was",
            " created",
            " by",
            " Pe",
            "is",
            "istr",
            "atos",
            ",",
            " and",
            " trip",
            "ods",
            " were",
            " placed",
            " there",
            " by",
            " those",
            " who",
            " had",
            " won",
            " in",
            " the",
            " cyclic",
            " chorus",
            " at",
            " the",
            " Th",
            "arg",
            "elia",
            ".",
            "Set",
            "ae",
            " (",
            "Ly",
            "dia",
            "):",
            " The",
            " temple",
            " of",
            " Apollo",
            " A",
            "ks",
            "y",
            "ros",
            " located"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " over",
            "est",
            "imates",
            " himself",
            " and",
            " under",
            "est",
            "imates",
            " his",
            " aunt",
            "'s",
            " mental",
            " ac",
            "uity",
            ".",
            " Miss",
            " Mar",
            "ple",
            " employs",
            " young",
            " women",
            " (",
            "including",
            " Clara",
            ",",
            " Emily",
            ",",
            " Alice",
            ",",
            " Esther",
            ",",
            " G",
            "w",
            "enda",
            ",",
            " and",
            " Amy",
            ")",
            " from",
            " a",
            " nearby",
            " orphan",
            "age",
            ",",
            " whom",
            " she",
            " trains",
            " for",
            " service",
            " as",
            " general",
            " house",
            "maids",
            " after",
            " the",
            " retirement",
            " of",
            " her",
            " long",
            "-time",
            " maid",
            "-house",
            "keeper"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.387,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " twentieth",
            " century",
            " to",
            " the",
            " public",
            " at",
            " large",
            "\".",
            " Political",
            " scientist",
            " Andrew",
            " K",
            "opp",
            "elman",
            " called",
            " her",
            " \"",
            "the",
            " most",
            " widely",
            " read",
            " libertarian",
            "\".",
            " Histor",
            "ian",
            " Jennifer",
            " Burns",
            " referred",
            " to",
            " her",
            " as",
            " \"",
            "the",
            " ultimate",
            " gateway",
            " drug",
            " to",
            " life",
            " on",
            " the",
            " right",
            "\".",
            "The",
            " political",
            " figures",
            " who",
            " cite",
            " Rand",
            " as",
            " an",
            " influence",
            " are",
            " usually",
            " conservatives",
            " (",
            "often",
            " members",
            " of",
            " the",
            " Republican",
            " Party"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " Europe",
            ".",
            " Today",
            ",",
            " it",
            " is",
            " the",
            " most",
            " widely",
            " used",
            " script",
            " in",
            " the",
            " world",
            ".",
            "The",
            " E",
            "tr",
            "us",
            "can",
            " alphabet",
            " remained",
            " nearly",
            " unchanged",
            " for",
            " several",
            " hundred",
            " years",
            ".",
            " Only",
            " evolving",
            " once",
            " the",
            " E",
            "tr",
            "us",
            "can",
            " language",
            " changed",
            " itself",
            ".",
            " The",
            " letters",
            " used",
            " for",
            " non",
            "-existent",
            " phon",
            "emes",
            " were",
            " dropped",
            ".",
            " Afterwards",
            ",",
            " however",
            ",",
            " the",
            " alphabet",
            " went",
            " through",
            " many",
            " different",
            " changes"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " with",
            " a",
            " ratio",
            " of",
            " ",
            "6",
            ".",
            "75",
            " percent",
            ".",
            " The",
            " oil",
            " and",
            " gas",
            " industry",
            " dominates",
            " the",
            " Al",
            "askan",
            " economy",
            ",",
            " with",
            " more",
            " than",
            " ",
            "80",
            "%",
            " of",
            " the",
            " state",
            "'s",
            " revenues",
            " derived",
            " from",
            " petroleum",
            " extraction",
            ".",
            " Alaska",
            "'s",
            " main",
            " export",
            " product",
            " (",
            "excluding",
            " oil",
            " and",
            " natural",
            " gas",
            ")",
            " is",
            " seafood",
            ",",
            " primarily",
            " salmon",
            ",",
            " cod",
            ",",
            " pol",
            "lock",
            " and",
            " crab",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.139,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " based",
            " in",
            " Los",
            " Angeles",
            "196",
            "7",
            " establishments",
            " in",
            " California",
            "Educ",
            "ational",
            " organizations",
            " established",
            " in",
            " ",
            "196",
            "7",
            "F",
            "IA",
            "F",
            "-aff",
            "iliated",
            " institutions",
            "Ar",
            "ts",
            " organizations",
            " established",
            " in",
            " ",
            "196",
            "7",
            "<|begin_of_text|>",
            "was",
            " a",
            " Japanese",
            " filmmaker",
            " and",
            " painter",
            " who",
            " directed",
            " ",
            "30",
            " films",
            " in",
            " a",
            " career",
            " spanning",
            " over",
            " five",
            " decades",
            ".",
            " He",
            " is",
            " widely",
            " regarded",
            " as",
            " one",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " Furthermore",
            ",",
            " the",
            " royal",
            " Il",
            "ly",
            "rian",
            " tom",
            "bs",
            ",",
            " the",
            " remains",
            " of",
            " Ap",
            "oll",
            "onia",
            ",",
            " the",
            " ancient",
            " Amph",
            "ithe",
            "atre",
            " of",
            " D",
            "urr",
            "ÃƒÂ«",
            "s",
            " and",
            " the",
            " Fortress",
            " of",
            " Bas",
            "ht",
            "ov",
            "ÃƒÂ«",
            " has",
            " been",
            " included",
            " on",
            " the",
            " tentative",
            " list",
            " of",
            " Albania",
            ".",
            "C",
            "uisine",
            " ",
            "Throughout",
            " the",
            " centuries",
            ",",
            " Alban",
            "ian",
            " cuisine",
            " has",
            " been",
            " widely",
            " influenced",
            " by",
            " Alban",
            "ian",
            " culture"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " It",
            " is",
            " sometimes",
            " combined",
            " with",
            " an",
            " overhead",
            " strike",
            " as",
            " .",
            " ,",
            " when",
            " the",
            " lap",
            "el",
            " is",
            " grabbed",
            ";",
            " sometimes",
            " referred",
            " to",
            " as",
            " .",
            "Basic",
            " techniques",
            "The",
            " following",
            " are",
            " a",
            " sample",
            " of",
            " the",
            " basic",
            " or",
            " widely",
            " practiced",
            " throws",
            " and",
            " pins",
            ".",
            " Many",
            " of",
            " these",
            " techniques",
            " derive",
            " from",
            " D",
            "ait",
            "Ã…Ä¯",
            "-",
            "ry",
            "Ã…Â«",
            " A",
            "iki",
            "-j",
            "Ã…Â«",
            "j",
            "uts",
            "u",
            ",",
            " but",
            " some"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "Some",
            " systems",
            ",",
            " e",
            ".g",
            ".",
            " for",
            " scholarly",
            " use",
            ",",
            " are",
            " intended",
            " to",
            " accurately",
            " and",
            " un",
            "amb",
            "igu",
            "ously",
            " represent",
            " the",
            " phon",
            "emes",
            " of",
            " Arabic",
            ",",
            " generally",
            " making",
            " the",
            " phon",
            "etics",
            " more",
            " explicit",
            " than",
            " the",
            " original",
            " word",
            " in",
            " the",
            " Arabic",
            " script",
            ".",
            " These",
            " systems",
            " are",
            " heavily",
            " reliant",
            " on",
            " di",
            "ac",
            "ritical",
            " marks",
            " such",
            " as",
            " \"",
            "Ã…Â¡",
            "\"",
            " for",
            " the",
            " sound",
            " equival",
            "ently"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.197,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " his",
            " brother",
            ",",
            " S",
            "ule",
            "iman",
            " Mir",
            "za",
            ".",
            " Tim",
            "ur",
            " Shah",
            " D",
            "urr",
            "ani",
            " asc",
            "ended",
            " to",
            " the",
            " throne",
            " in",
            " November",
            " ",
            "177",
            "2",
            ",",
            " having",
            " defeated",
            " a",
            " coalition",
            " under",
            " Shah",
            " W",
            "ali",
            " Khan",
            " and",
            " Hum",
            "ay",
            "un",
            " Mir",
            "za",
            ".",
            " Tim",
            "ur",
            " Shah",
            " began",
            " his",
            " reign",
            " by",
            " consolid",
            "ating",
            " power",
            " toward",
            " himself",
            " and",
            " people",
            " loyal",
            " to",
            " him",
            ",",
            " pur",
            "ging"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.194,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " linear",
            " model",
            " approach",
            ".",
            " Few",
            " statist",
            "icians",
            " object",
            " to",
            " model",
            "-based",
            " analysis",
            " of",
            " balanced",
            " randomized",
            " experiments",
            ".",
            "Stat",
            "istical",
            " models",
            " for",
            " observational",
            " data",
            "However",
            ",",
            " when",
            " applied",
            " to",
            " data",
            " from",
            " non",
            "-random",
            "ized",
            " experiments",
            " or",
            " observational",
            " studies",
            ",",
            " model",
            "-based",
            " analysis",
            " lacks",
            " the",
            " warrant",
            " of",
            " random",
            "ization",
            ".",
            " For",
            " observational",
            " data",
            ",",
            " the",
            " derivation",
            " of",
            " confidence",
            " intervals",
            " must",
            " use",
            " subjective",
            " models",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " fellow",
            " gang",
            " members",
            " are",
            " Dim",
            ",",
            " a",
            " slow",
            "-w",
            "itted",
            " br",
            "uis",
            "er",
            ",",
            " who",
            " is",
            " the",
            " gang",
            "'s",
            " muscle",
            ";",
            " Georg",
            "ie",
            ",",
            " an",
            " ambitious",
            " second",
            "-in",
            "-command",
            ";",
            " and",
            " Pete",
            ",",
            " who",
            " mostly",
            " plays",
            " along",
            " as",
            " the",
            " dro",
            "ogs",
            " indulge",
            " their",
            " taste",
            " for",
            " \"",
            "ul",
            "tra",
            "-viol",
            "ence",
            "\"",
            " (",
            "random",
            ",",
            " violent",
            " may",
            "hem",
            ").",
            " Character",
            "ised",
            " as",
            " a",
            " soci"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.158,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " production",
            " of",
            " the",
            " artistic",
            " object",
            ".",
            " In",
            " conceptual",
            " art",
            ",",
            " Marcel",
            " Duch",
            "amp",
            "'s",
            " Fountain",
            " is",
            " among",
            " the",
            " first",
            " examples",
            " of",
            " pieces",
            " wherein",
            " the",
            " artist",
            " used",
            " found",
            " objects",
            " (\"",
            "ready",
            "-made",
            "\")",
            " and",
            " exercised",
            " no",
            " traditionally",
            " recognised",
            " set",
            " of",
            " skills",
            ".",
            " Trace",
            "y",
            " Emin",
            "'s",
            " My",
            " Bed",
            ",",
            " or",
            " Damien",
            " H",
            "irst",
            "'s",
            " The",
            " Physical",
            " Im",
            "poss",
            "ibility",
            " of",
            " Death",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.097,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.125,
            -0.0,
            0.14,
            -0.0,
            -0.0,
            -0.0,
            0.105,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "External",
            " links",
            "American",
            " Film",
            " Institute",
            " on",
            " Internet",
            " Archive",
            "AF",
            "I",
            " Los",
            " Angeles",
            " Film",
            " Festival",
            " â€“",
            " history",
            " and",
            " information",
            " (",
            "arch",
            "ived",
            " ",
            "17",
            " July",
            " ",
            "200",
            "9",
            ")",
            " ",
            "Ar",
            "ts",
            " organizations",
            " based",
            " in",
            " California",
            "C",
            "inema",
            " of",
            " Southern",
            " California",
            "Culture",
            " of",
            " Hollywood",
            ",",
            " Los",
            " Angeles",
            "Los",
            " Fel",
            "iz",
            ",",
            " Los",
            " Angeles",
            "Organ",
            "izations"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " municipality",
            ".",
            " The",
            " region",
            " is",
            " a",
            " major",
            " agricultural",
            " producer",
            ",",
            " with",
            " many",
            " large",
            " farms",
            " in",
            " the",
            " out",
            "lying",
            " districts",
            ".",
            " People",
            " commute",
            " to",
            " A",
            "arhus",
            " from",
            " as",
            " far",
            " away",
            " as",
            " Rand",
            "ers",
            ",",
            " Sil",
            "ke",
            "borg",
            " and",
            " Sk",
            "ander",
            "borg",
            " and",
            " almost",
            " a",
            " third",
            " of",
            " those",
            " employed",
            " within",
            " the",
            " A",
            "arhus",
            " municipality",
            " commute",
            " from",
            " neighbouring",
            " communities",
            ".",
            " A",
            "arhus",
            " is",
            " a",
            " centre",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Got",
            "th",
            "ard",
            " Pass",
            ",",
            " the",
            " Sem",
            "mer",
            "ing",
            " Pass",
            ",",
            " the",
            " Simpl",
            "on",
            " Pass",
            ",",
            " and",
            " the",
            " St",
            "el",
            "vio",
            " Pass",
            ".",
            "Cross",
            "ing",
            " the",
            " Italian",
            "-A",
            "ust",
            "rian",
            " border",
            ",",
            " the",
            " Bren",
            "ner",
            " Pass",
            " separates",
            " the",
            " ÃƒÄ¸",
            "t",
            "zt",
            "al",
            " Alps",
            " and",
            " Z",
            "ill",
            "ert",
            "al",
            " Alps",
            " and",
            " has",
            " been",
            " in",
            " use",
            " as",
            " a",
            " trading",
            " route",
            " since",
            " the",
            " ",
            "14",
            "th"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "im",
            " Yus",
            "ov",
            " from",
            " ",
            "195",
            "8",
            " to",
            " ",
            "197",
            "2",
            ",",
            " and",
            " much",
            " of",
            " the",
            " visual",
            " style",
            " of",
            " T",
            "ark",
            "ovsky",
            "'s",
            " films",
            " can",
            " be",
            " attributed",
            " to",
            " this",
            " collaboration",
            ".",
            " T",
            "ark",
            "ovsky",
            " would",
            " spend",
            " two",
            " days",
            " preparing",
            " for",
            " Yus",
            "ov",
            " to",
            " film",
            " a",
            " single",
            " long",
            " take",
            ",",
            " and",
            " due",
            " to",
            " the",
            " preparation",
            ",",
            " usually",
            " only",
            " a",
            " single",
            " take",
            " was",
            " needed",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " appeal",
            " can",
            " be",
            ":",
            "Aff",
            "irmed",
            ":",
            " Where",
            " the",
            " reviewing",
            " court",
            " basically",
            " agrees",
            " with",
            " the",
            " result",
            " of",
            " the",
            " lower",
            " courts",
            "'",
            " ruling",
            "(s",
            ").",
            " ",
            "Re",
            "versed",
            ":",
            " Where",
            " the",
            " reviewing",
            " court",
            " basically",
            " disagrees",
            " with",
            " the",
            " result",
            " of",
            " the",
            " lower",
            " courts",
            "'",
            " ruling",
            "(s",
            "),",
            " and",
            " overturn",
            "s",
            " their",
            " decision",
            ".",
            "Vac",
            "ated",
            ":",
            " Where",
            " the",
            " reviewing",
            " court",
            " overturn",
            "s",
            " the",
            " lower",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " a",
            " narrow",
            " size",
            " shoe",
            ",",
            " or",
            " a",
            " small",
            " cup",
            " size",
            " in",
            " a",
            " brass",
            "iere",
            ".",
            "Related",
            " characters",
            "Desc",
            "endants",
            " and",
            " related",
            " characters",
            " in",
            " the",
            " Latin",
            " alphabet",
            " ",
            "ÃƒÄ¨",
            " ÃƒÂ¦",
            " :",
            " Latin",
            " AE",
            " lig",
            "ature",
            "A",
            " with",
            " di",
            "ac",
            "rit",
            "ics",
            ":",
            " ÃƒÄ§",
            " ÃƒÂ¥",
            " ",
            "Ã‡",
            "Âº",
            " ",
            "Ã‡",
            "Â»",
            " Ã¡",
            "Â¸",
            "Ä¢",
            " Ã¡",
            "Â¸",
            "Ä£",
            " Ã¡Âº",
            "Ä¼",
            " Ã„",
            "Ä¤",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "contri",
    " koc",
    "ezi",
    "ingly",
    "atrice"
  ],
  "bottom_logits": [
    " eyeb",
    " complete",
    " hur",
    "bolt",
    " Schro"
  ],
  "act_min": -0.0,
  "act_max": 0.602
}