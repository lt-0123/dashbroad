{
  "index": 53546,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            "ery",
            ",",
            " music",
            " and",
            " dance",
            ",",
            " truth",
            " and",
            " prophecy",
            ",",
            " healing",
            " and",
            " diseases",
            ",",
            " the",
            " Sun",
            " and",
            " light",
            ",",
            " poetry",
            ",",
            " and",
            " more",
            ".",
            " One",
            " of",
            " the",
            " most",
            " important",
            " and",
            " complex",
            " of",
            " the",
            " Greek",
            " gods",
            ",",
            " he",
            " is",
            " the",
            " son",
            " of",
            " Zeus",
            " and",
            " Let",
            "o",
            ",",
            " and",
            " the",
            " twin",
            " brother",
            " of",
            " Artem",
            "is",
            ",",
            " goddess",
            " of",
            " the",
            " hunt",
            ".",
            " He",
            " is",
            " considered",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            "ery",
            ",",
            " music",
            " and",
            " dance",
            ",",
            " truth",
            " and",
            " prophecy",
            ",",
            " healing",
            " and",
            " diseases",
            ",",
            " the",
            " Sun",
            " and",
            " light",
            ",",
            " poetry",
            ",",
            " and",
            " more",
            ".",
            " One",
            " of",
            " the",
            " most",
            " important",
            " and",
            " complex",
            " of",
            " the",
            " Greek",
            " gods",
            ",",
            " he",
            " is",
            " the",
            " son",
            " of",
            " Zeus",
            " and",
            " Let",
            "o",
            ",",
            " and",
            " the",
            " twin",
            " brother",
            " of",
            " Artem",
            "is",
            ",",
            " goddess",
            " of",
            " the",
            " hunt",
            ".",
            " He",
            " is",
            " considered",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " a",
            " name",
            " director",
            " with",
            " his",
            " first",
            " thriller",
            ",",
            " The",
            " Lod",
            "ger",
            ":",
            " A",
            " Story",
            " of",
            " the",
            " London",
            " Fog",
            " (",
            "192",
            "7",
            ").",
            " The",
            " film",
            " concerns",
            " the",
            " hunt",
            " for",
            " a",
            " Jack",
            " the",
            " Rip",
            "per",
            "-style",
            " serial",
            " killer",
            " who",
            ",",
            " wearing",
            " a",
            " black",
            " cloak",
            " and",
            " carrying",
            " a",
            " black",
            " bag",
            ",",
            " is",
            " murdering",
            " young",
            " blonde",
            " women",
            " in",
            " London",
            ",",
            " and",
            " only",
            " on",
            " Tues",
            "days",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " hunting",
            ".",
            "In",
            " fact",
            ",",
            " tad",
            "po",
            "les",
            " developing",
            " in",
            " ponds",
            " and",
            " streams",
            " are",
            " typically",
            " herb",
            "iv",
            "orous",
            ".",
            " Pond",
            " tad",
            "po",
            "les",
            " tend",
            " to",
            " have",
            " deep",
            " bodies",
            ",",
            " large",
            " ca",
            "ud",
            "al",
            " fins",
            " and",
            " small",
            " mouths",
            ";",
            " they",
            " swim",
            " in",
            " the",
            " quiet",
            " waters",
            " feeding",
            " on",
            " growing",
            " or",
            " loose",
            " fragments",
            " of",
            " vegetation",
            ".",
            " Stream",
            " dwell",
            "ers",
            " mostly",
            " have",
            " larger",
            " mouths",
            ",",
            " shallow",
            " bodies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " not",
            " found",
            " in",
            " Madagascar",
            ".",
            "Ec",
            "ology",
            " and",
            " behaviour",
            "A",
            "ard",
            "v",
            "arks",
            " live",
            " for",
            " up",
            " to",
            " ",
            "23",
            "Âł",
            "years",
            " in",
            " captivity",
            ".",
            " Its",
            " keen",
            " hearing",
            " warns",
            " it",
            " of",
            " predators",
            ":",
            " lions",
            ",",
            " le",
            "op",
            "ards",
            ",",
            " che",
            "et",
            "ah",
            "s",
            ",",
            " African",
            " wild",
            " dogs",
            ",",
            " hy",
            "enas",
            ",",
            " and",
            " py",
            "th",
            "ons",
            ".",
            " Some",
            " humans",
            " also",
            " hunt",
            " a",
            "ard"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " manner",
            " of",
            " noble",
            " Maced",
            "onian",
            " youths",
            ",",
            " learning",
            " to",
            " read",
            ",",
            " play",
            " the",
            " ly",
            "re",
            ",",
            " ride",
            ",",
            " fight",
            ",",
            " and",
            " hunt",
            ".",
            "When",
            " Alexander",
            " was",
            " ten",
            " years",
            " old",
            ",",
            " a",
            " trader",
            " from",
            " Th",
            "ess",
            "aly",
            " brought",
            " Philip",
            " a",
            " horse",
            ",",
            " which",
            " he",
            " offered",
            " to",
            " sell",
            " for",
            " thirteen",
            " talents",
            ".",
            " The",
            " horse",
            " refused",
            " to",
            " be",
            " mounted",
            ",",
            " and",
            " Philip",
            " ordered",
            " it",
            " away"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " not",
            " poisonous",
            ",",
            " but",
            " mim",
            "ics",
            " the",
            " appearance",
            " of",
            " other",
            " toxic",
            " species",
            " in",
            " its",
            " locality",
            ",",
            " a",
            " strategy",
            " that",
            " may",
            " deceive",
            " predators",
            ".",
            "Many",
            " amphib",
            "ians",
            " are",
            " noct",
            "urnal",
            " and",
            " hide",
            " during",
            " the",
            " day",
            ",",
            " thereby",
            " avoiding",
            " di",
            "urnal",
            " predators",
            " that",
            " hunt",
            " by",
            " sight",
            ".",
            " Other",
            " amphib",
            "ians",
            " use",
            " camouflage",
            " to",
            " avoid",
            " being",
            " detected",
            ".",
            " They",
            " have",
            " various",
            " colour",
            "ings",
            " such",
            " as",
            " m"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " British",
            " medical",
            " personnel",
            " who",
            " had",
            " read",
            " Christie",
            "'s",
            " book",
            " and",
            " recognised",
            " the",
            " symptoms",
            " she",
            " described",
            ".",
            "The",
            " British",
            " intelligence",
            " agency",
            " MI",
            "5",
            " investigated",
            " Christie",
            " after",
            " a",
            " character",
            " called",
            " Major",
            " B",
            "let",
            "ch",
            "ley",
            " appeared",
            " in",
            " her",
            " ",
            "194",
            "1",
            " thriller",
            " N",
            " or",
            " M",
            "?,",
            " which",
            " was",
            " about",
            " a",
            " hunt",
            " for",
            " a",
            " pair",
            " of",
            " deadly",
            " fifth",
            " column",
            "ists",
            " in",
            " wartime",
            " England",
            ".",
            " MI",
            "5"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " such",
            " as",
            " the",
            " Bu",
            "fo",
            " spp",
            ".",
            " to",
            "ads",
            ",",
            " actively",
            " search",
            " for",
            " prey",
            ",",
            " while",
            " the",
            " Argentine",
            " horn",
            "ed",
            " frog",
            " (",
            "C",
            "er",
            "at",
            "oph",
            "rys",
            " orn",
            "ata",
            ")",
            " l",
            "ures",
            " in",
            "quis",
            "itive",
            " prey",
            " closer",
            " by",
            " raising",
            " its",
            " hind",
            " feet",
            " over",
            " its",
            " back",
            " and",
            " vibrating",
            " its",
            " yellow",
            " toes",
            ".",
            " Among",
            " leaf",
            " litter",
            " frogs",
            " in",
            " Panama",
            ",",
            " frogs",
            " that",
            " actively",
            " hunt",
            " prey"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " since",
            " Algeria",
            "'s",
            " democracy",
            " in",
            " ",
            "198",
            "9",
            ".",
            " Te",
            "bb",
            "ou",
            "ne",
            " is",
            " accused",
            " to",
            " be",
            " close",
            " to",
            " the",
            " military",
            " and",
            " being",
            " loyal",
            " to",
            " the",
            " de",
            "posed",
            " president",
            ".",
            " Te",
            "bb",
            "ou",
            "ne",
            " rejects",
            " these",
            " accusations",
            ",",
            " claiming",
            " to",
            " be",
            " the",
            " victim",
            " of",
            " a",
            " witch",
            " hunt",
            ".",
            " He",
            " also",
            " reminds",
            " his",
            " detr",
            "actors",
            " that",
            " he",
            " was",
            " expelled",
            " from",
            " the",
            " Government",
            " in",
            " August"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " arch",
            "aic",
            " lifestyle",
            " rev",
            "olved",
            " around",
            " a",
            " food",
            " economy",
            " based",
            " on",
            " fishing",
            ",",
            " hunting",
            ",",
            " and",
            " gathering",
            ",",
            " with",
            " a",
            " strong",
            " emphasis",
            " on",
            " marine",
            " resources",
            ".",
            " Cer",
            "amics",
            " were",
            " absent",
            ",",
            " as",
            " was",
            " h",
            "ort",
            "iculture",
            " and",
            " agriculture",
            ".",
            " These",
            " people",
            " not",
            " only",
            " ch",
            "ipped",
            " stones",
            " but",
            " also",
            " polished",
            " and",
            " sharpen",
            "ed",
            " them",
            ".",
            " Weapons",
            " and",
            " tools",
            " were",
            " predominantly",
            " crafted",
            " from",
            " stone",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "ads",
            ",",
            " sal",
            "am",
            "anders",
            " and",
            " ca",
            "ec",
            "ilians",
            " also",
            " use",
            " smell",
            " to",
            " detect",
            " prey",
            ".",
            " This",
            " response",
            " is",
            " mostly",
            " secondary",
            " because",
            " sal",
            "am",
            "anders",
            " have",
            " been",
            " observed",
            " to",
            " remain",
            " stationary",
            " near",
            " odor",
            "ifer",
            "ous",
            " prey",
            " but",
            " only",
            " feed",
            " if",
            " it",
            " moves",
            ".",
            " Cave",
            "-d",
            "w",
            "elling",
            " amphib",
            "ians",
            " normally",
            " hunt",
            " by",
            " smell",
            ".",
            " Some",
            " sal",
            "am",
            "anders",
            " seem",
            " to",
            " have",
            " learned",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " located",
            " in",
            " Ton",
            "atan",
            "v",
            "ally",
            ",",
            " \"",
            "The",
            " Valley",
            "\",",
            " near",
            " D",
            "ug",
            "ort",
            ",",
            " in",
            " the",
            " northeast",
            " of",
            " Ach",
            "ill",
            " Island",
            ".",
            " The",
            " present",
            " building",
            " sits",
            " on",
            " the",
            " site",
            " of",
            " a",
            " hunting",
            " lodge",
            " built",
            " by",
            " the",
            " Earl",
            " of",
            " C",
            "avan",
            " in",
            " the",
            " ",
            "19",
            "th",
            " century",
            ".",
            " Its",
            " not",
            "ori",
            "ety",
            " arises",
            " from",
            " an",
            " incident",
            " in",
            " ",
            "189",
            "4",
            " in",
            " which"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " sent",
            " her",
            " an",
            " introduction",
            " to",
            " his",
            " own",
            " literary",
            " agent",
            ",",
            " Hughes",
            " Mass",
            "ie",
            ",",
            " who",
            " also",
            " rejected",
            " Snow",
            " Upon",
            " the",
            " Desert",
            " but",
            " suggested",
            " a",
            " second",
            " novel",
            ".",
            "Meanwhile",
            ",",
            " Christie",
            "'s",
            " social",
            " activities",
            " expanded",
            ",",
            " with",
            " country",
            " house",
            " parties",
            ",",
            " riding",
            ",",
            " hunting",
            ",",
            " dances",
            ",",
            " and",
            " roller",
            " skating",
            ".",
            " She",
            " had",
            " short",
            "-lived",
            " relationships",
            " with",
            " four",
            " men",
            " and",
            " an",
            " engagement",
            " to",
            " another"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "Unlike",
            " many",
            " of",
            " its",
            " relatives",
            " in",
            " the",
            " order",
            " Carn",
            "iv",
            "ora",
            ",",
            " the",
            " a",
            "ard",
            "wolf",
            " does",
            " not",
            " hunt",
            " large",
            " animals",
            ".",
            " It",
            " eats",
            " insects",
            " and",
            " their",
            " larvae",
            ",",
            " mainly",
            " ter",
            "mites",
            ";",
            " one",
            " a",
            "ard",
            "wolf",
            " can",
            " lap",
            " up",
            " as",
            " many",
            " as",
            " ",
            "300",
            ",",
            "000",
            " ter",
            "mites",
            " during",
            " a",
            " single",
            " night",
            " using",
            " its",
            " long",
            ",",
            " sticky",
            " tongue",
            ".",
            " The",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " who",
            " have",
            " police",
            " training",
            " but",
            " do",
            " not",
            " carry",
            " firearms",
            ".",
            " In",
            " much",
            " of",
            " the",
            " state",
            ",",
            " the",
            " tro",
            "opers",
            " serve",
            " as",
            " the",
            " only",
            " police",
            " force",
            " available",
            ".",
            " In",
            " addition",
            " to",
            " enforcing",
            " traffic",
            " and",
            " criminal",
            " law",
            ",",
            " wildlife",
            " Tro",
            "opers",
            " enforce",
            " hunting",
            " and",
            " fishing",
            " regulations",
            ".",
            " Due",
            " to",
            " the",
            " varied",
            " terrain",
            " and",
            " wide",
            " scope",
            " of",
            " the",
            " Tro",
            "opers",
            "'",
            " duties",
            ",",
            " they",
            " employ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " land",
            " that",
            " would",
            " later",
            " become",
            " Alberta",
            ",",
            " and",
            " the",
            " Crown",
            " committed",
            " to",
            " the",
            " ongoing",
            " support",
            " of",
            " the",
            " First",
            " Nations",
            " and",
            " guaranteed",
            " their",
            " hunting",
            " and",
            " fishing",
            " rights",
            ".",
            " The",
            " most",
            " significant",
            " treaties",
            " for",
            " Alberta",
            " are",
            " Treaty",
            " ",
            "6",
            " (",
            "187",
            "6",
            "),",
            " Treaty",
            " ",
            "7",
            " (",
            "187",
            "7",
            ")",
            " and",
            " Treaty",
            " ",
            "8",
            " (",
            "189",
            "9",
            ").",
            "The",
            " District",
            " of",
            " Alberta",
            " was",
            " created",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " stranger",
            " aspects",
            " of",
            " P",
            "oi",
            "rot",
            "'s",
            " world",
            ".",
            " He",
            " first",
            " met",
            " P",
            "oi",
            "rot",
            " in",
            " Belgium",
            " in",
            " ",
            "190",
            "4",
            ",",
            " during",
            " the",
            " Aber",
            "c",
            "rom",
            "bie",
            " Forg",
            "ery",
            ".",
            " Later",
            " that",
            " year",
            " they",
            " joined",
            " forces",
            " again",
            " to",
            " hunt",
            " down",
            " a",
            " criminal",
            " known",
            " as",
            " Baron",
            " Alt",
            "ara",
            ".",
            " They",
            " also",
            " meet",
            " in",
            " England",
            " where",
            " P",
            "oi",
            "rot",
            " often",
            " helps",
            " J",
            "app",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " Somerset",
            ".",
            " The",
            " Manor",
            " had",
            " been",
            " built",
            " as",
            " a",
            " hunting",
            " lodge",
            " in",
            " ",
            "179",
            "9",
            " and",
            " was",
            " improved",
            " by",
            " King",
            " in",
            " preparation",
            " for",
            " their",
            " honeymoon",
            ".",
            " It",
            " later",
            " became",
            " their",
            " summer",
            " retreat",
            " and",
            " was",
            " further",
            " improved",
            " during",
            " this",
            " time",
            ".",
            " From",
            " ",
            "184",
            "5",
            ",",
            " the",
            " family",
            "'s",
            " main",
            " house",
            " was",
            " H",
            "ors",
            "ley",
            " Towers",
            ",",
            " built",
            " in",
            " the",
            " Tud",
            "orb",
            "eth",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " First",
            " Nations",
            " peoples",
            ",",
            " including",
            " the",
            " Plains",
            " Indians",
            " of",
            " southern",
            " Alberta",
            " such",
            " as",
            " those",
            " of",
            " the",
            " Black",
            "foot",
            " Confeder",
            "acy",
            " and",
            " the",
            " Plains",
            " Cree",
            ",",
            " who",
            " generally",
            " lived",
            " by",
            " hunting",
            " buffalo",
            ",",
            " and",
            " the",
            " more",
            " nor",
            "ther",
            "ly",
            " tribes",
            " such",
            " as",
            " the",
            " Wood",
            "land",
            " Cree",
            " and",
            " Chip",
            "ew",
            "yan",
            " who",
            " hunted",
            ",",
            " trapped",
            ",",
            " and",
            " f",
            "ished",
            " for",
            " a",
            " living",
            ".",
            "The",
            " first"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " companions",
            " fighting",
            " the",
            " Pers",
            "ians",
            " and",
            " hunting",
            ".",
            " It",
            " was",
            " originally",
            " thought",
            " to",
            " have",
            " been",
            " the",
            " sarc",
            "oph",
            "agus",
            " of",
            " Abd",
            "al",
            "onym",
            "us",
            " (",
            "d",
            "ied",
            " ",
            "311",
            " BC",
            "),",
            " the",
            " king",
            " of",
            " Sid",
            "on",
            " appointed",
            " by",
            " Alexander",
            " immediately",
            " following",
            " the",
            " battle",
            " of",
            " Iss",
            "us",
            " in",
            " ",
            "331",
            ".",
            " However",
            ",",
            " more",
            " recently",
            ",",
            " it",
            " has",
            " been",
            " suggested",
            " that",
            " it",
            " may"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " giant",
            " Ty",
            "phon",
            ".",
            " In",
            " most",
            " of",
            " the",
            " traditions",
            ",",
            " Apollo",
            " was",
            " still",
            " a",
            " child",
            " when",
            " he",
            " killed",
            " Python",
            ".",
            "Python",
            " was",
            " sent",
            " by",
            " Hera",
            " to",
            " hunt",
            " the",
            " pregnant",
            " Let",
            "o",
            " to",
            " death",
            ",",
            " and",
            " assaulted",
            " her",
            ".",
            " To",
            " a",
            "venge",
            " the",
            " trouble",
            " given",
            " to",
            " his",
            " mother",
            ",",
            " Apollo",
            " went",
            " in",
            " search",
            " of",
            " Python",
            " and",
            " killed",
            " it",
            " in",
            " the",
            " sacred",
            " cave",
            " at",
            " Del"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.324,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "La",
            " Bal",
            "ma",
            " de",
            " la",
            " Marg",
            "ined",
            "a",
            ",",
            " found",
            " by",
            " archae",
            "ologists",
            " at",
            " Sant",
            " Juli",
            "Ãł",
            " de",
            " L",
            "Ã²",
            "ria",
            ",",
            " was",
            " settled",
            " in",
            " ",
            "9",
            ",",
            "500",
            " BCE",
            " as",
            " a",
            " passing",
            " place",
            " between",
            " the",
            " two",
            " sides",
            " of",
            " the",
            " Py",
            "rene",
            "es",
            ".",
            " The",
            " seasonal",
            " camp",
            " was",
            " perfectly",
            " located",
            " for",
            " hunting",
            " and",
            " fishing",
            " by",
            " the",
            " groups",
            " of",
            " hunter",
            "-g",
            "ather",
            "ers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " civil",
            " time",
            "keeping",
            " all",
            " over",
            " the",
            " Earth",
            "'s",
            " surface",
            " and",
            " which",
            " has",
            " leap",
            " seconds",
            ".",
            "UTC",
            " dev",
            "iates",
            " from",
            " T",
            "AI",
            " by",
            " a",
            " number",
            " of",
            " whole",
            " seconds",
            ".",
            " ,",
            " when",
            " another",
            " leap",
            " second",
            " was",
            " put",
            " into",
            " effect",
            ",",
            " UTC",
            " is",
            " currently",
            " exactly",
            " ",
            "37",
            " seconds",
            " behind",
            " T",
            "AI",
            ".",
            " The",
            " ",
            "37",
            " seconds",
            " result",
            " from",
            " the",
            " initial",
            " difference",
            " of",
            " ",
            "10",
            " seconds",
            " at"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            "D",
            "ikt",
            "yn",
            "na",
            "),",
            " the",
            " M",
            "ino",
            "an",
            " \"",
            "M",
            "ist",
            "ress",
            " of",
            " the",
            " animals",
            "\".",
            " In",
            " her",
            " earliest",
            " dep",
            "ictions",
            " she",
            " was",
            " accompanied",
            " by",
            " the",
            " \"",
            "Master",
            " of",
            " the",
            " animals",
            "\",",
            " a",
            " bow",
            "-w",
            "ielding",
            " god",
            " of",
            " hunting",
            " whose",
            " name",
            " has",
            " been",
            " lost",
            ";",
            " aspects",
            " of",
            " this",
            " figure",
            " may",
            " have",
            " been",
            " absorbed",
            " into",
            " the",
            " more",
            " popular",
            " Apollo",
            ".",
            "An",
            "at",
            "olian"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            "D",
            "ikt",
            "yn",
            "na",
            "),",
            " the",
            " M",
            "ino",
            "an",
            " \"",
            "M",
            "ist",
            "ress",
            " of",
            " the",
            " animals",
            "\".",
            " In",
            " her",
            " earliest",
            " dep",
            "ictions",
            " she",
            " was",
            " accompanied",
            " by",
            " the",
            " \"",
            "Master",
            " of",
            " the",
            " animals",
            "\",",
            " a",
            " bow",
            "-w",
            "ielding",
            " god",
            " of",
            " hunting",
            " whose",
            " name",
            " has",
            " been",
            " lost",
            ";",
            " aspects",
            " of",
            " this",
            " figure",
            " may",
            " have",
            " been",
            " absorbed",
            " into",
            " the",
            " more",
            " popular",
            " Apollo",
            ".",
            "An",
            "at",
            "olian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " not",
            " a",
            " vegetarian",
            " in",
            " his",
            " personal",
            " life",
            " nor",
            " imposed",
            " it",
            " on",
            " his",
            " missionary",
            " hospital",
            " but",
            " he",
            " did",
            " help",
            " animals",
            " and",
            " was",
            " opposed",
            " to",
            " hunting",
            ".",
            " St",
            "amos",
            " noted",
            " that",
            " Schwe",
            "itzer",
            " held",
            " the",
            " view",
            " that",
            " evolution",
            " ingr",
            "ained",
            " humans",
            " with",
            " an",
            " instinct",
            " for",
            " meat",
            " so",
            " it",
            " was",
            " useless",
            " in",
            " trying",
            " to",
            " deny",
            " it",
            ".",
            "The",
            " Albert",
            " Schwe",
            "itzer",
            " Fellowship",
            " was",
            " founded",
            " in",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " ability",
            ".",
            " The",
            " research",
            " found",
            " that",
            " good",
            " hunters",
            " have",
            " higher",
            " reproductive",
            " success",
            " and",
            " more",
            " adul",
            "ter",
            "ous",
            " relations",
            " even",
            " if",
            " they",
            " receive",
            " no",
            " more",
            " of",
            " the",
            " hunted",
            " meat",
            " than",
            " anyone",
            " else",
            ".",
            " Similarly",
            ",",
            " holding",
            " large",
            " fe",
            "asts",
            " and",
            " giving",
            " large",
            " donations",
            " are",
            " ways",
            " of",
            " demonstrating",
            " one",
            "'s",
            " resources",
            ".",
            " Hero",
            "ic",
            " risk",
            "-taking",
            " has",
            " also",
            " been",
            " interpreted",
            " as",
            " a",
            " costly",
            " signal",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " games",
            " were",
            " popular",
            " with",
            " children",
            ",",
            " and",
            " wrestling",
            " is",
            " also",
            " documented",
            " in",
            " a",
            " tomb",
            " at",
            " Ben",
            "i",
            " Hasan",
            ".",
            " The",
            " wealthy",
            " members",
            " of",
            " ancient",
            " Egyptian",
            " society",
            " enjoyed",
            " hunting",
            ",",
            " fishing",
            ",",
            " and",
            " bo",
            "ating",
            " as",
            " well",
            ".",
            "The",
            " excavation",
            " of",
            " the",
            " workers",
            "'",
            " village",
            " of",
            " De",
            "ir",
            " el",
            "-M",
            "ed",
            "ina",
            " has",
            " resulted",
            " in",
            " one",
            " of",
            " the",
            " most",
            " thoroughly",
            " documented",
            " accounts",
            " of",
            " community"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " C",
            "ret",
            "an",
            " priests",
            " to",
            " Del",
            "phi",
            ",",
            " where",
            " they",
            " evidently",
            " transferred",
            " their",
            " religious",
            " practices",
            ".",
            " Apollo",
            " Del",
            "phin",
            "ios",
            " or",
            " Del",
            "ph",
            "id",
            "ios",
            " was",
            " a",
            " sea",
            "-g",
            "od",
            " especially",
            " worsh",
            "ipped",
            " in",
            " Cre",
            "te",
            " and",
            " in",
            " the",
            " islands",
            ".",
            " Apollo",
            "'s",
            " sister",
            " Artem",
            "is",
            ",",
            " who",
            " was",
            " the",
            " Greek",
            " goddess",
            " of",
            " hunting",
            ",",
            " is",
            " identified",
            " with",
            " Brit",
            "om",
            "art",
            "is",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " C",
            "ret",
            "an",
            " priests",
            " to",
            " Del",
            "phi",
            ",",
            " where",
            " they",
            " evidently",
            " transferred",
            " their",
            " religious",
            " practices",
            ".",
            " Apollo",
            " Del",
            "phin",
            "ios",
            " or",
            " Del",
            "ph",
            "id",
            "ios",
            " was",
            " a",
            " sea",
            "-g",
            "od",
            " especially",
            " worsh",
            "ipped",
            " in",
            " Cre",
            "te",
            " and",
            " in",
            " the",
            " islands",
            ".",
            " Apollo",
            "'s",
            " sister",
            " Artem",
            "is",
            ",",
            " who",
            " was",
            " the",
            " Greek",
            " goddess",
            " of",
            " hunting",
            ",",
            " is",
            " identified",
            " with",
            " Brit",
            "om",
            "art",
            "is",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " in",
            " practice",
            " often",
            " excluded",
            ".",
            "It",
            " may",
            " also",
            " be",
            " broadly",
            " decom",
            "posed",
            " into",
            " plant",
            " agriculture",
            ",",
            " which",
            " concerns",
            " the",
            " cultivation",
            " of",
            " useful",
            " plants",
            ",",
            " and",
            " animal",
            " agriculture",
            ",",
            " the",
            " production",
            " of",
            " agricultural",
            " animals",
            ".",
            "History",
            "Orig",
            "ins",
            " ",
            "The",
            " development",
            " of",
            " agriculture",
            " enabled",
            " the",
            " human",
            " population",
            " to",
            " grow",
            " many",
            " times",
            " larger",
            " than",
            " could",
            " be",
            " sustained",
            " by",
            " hunting",
            " and",
            " gathering",
            ".",
            " Agriculture"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " abdomen",
            " and",
            " was",
            " the",
            " names",
            "ake",
            " of",
            " the",
            " ",
            "17",
            "th",
            " lunar",
            " mansion",
            ",",
            " which",
            " represented",
            " gran",
            "aries",
            ".",
            " Delta",
            " and",
            " Z",
            "eta",
            " Ari",
            "et",
            "is",
            " were",
            " a",
            " part",
            " of",
            " the",
            " constellation",
            " Tian",
            "y",
            "in",
            " (",
            "å¤©",
            "éĻ°",
            "),",
            " thought",
            " to",
            " represent",
            " the",
            " Emperor",
            "'s",
            " hunting",
            " partner",
            ".",
            " Zu",
            "og",
            "eng",
            " (",
            "å·¦",
            "æĽ´",
            "),",
            " a",
            " constellation",
            " depicting",
            " a",
            " marsh",
            " and",
            " pond",
            " inspector"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " ",
            "179",
            "2",
            " above",
            " Kle",
            "ine",
            " Sche",
            "ide",
            "gg",
            " by",
            " three",
            " hunters",
            " from",
            " Gr",
            "ind",
            "el",
            "wald",
            ".",
            "Many",
            " rodents",
            " such",
            " as",
            " vo",
            "les",
            " live",
            " underground",
            ".",
            " M",
            "arm",
            "ots",
            " live",
            " almost",
            " exclusively",
            " above",
            " the",
            " tree",
            " line",
            " as",
            " high",
            " as",
            " .",
            " They",
            " h",
            "ibernate",
            " in",
            " large",
            " groups",
            " to",
            " provide",
            " warmth",
            ",",
            " and",
            " can",
            " be",
            " found",
            " in",
            " all",
            " areas",
            " of",
            " the",
            " Alps",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " hundreds",
            " of",
            " years",
            " and",
            " began",
            " to",
            " be",
            " classified",
            " in",
            " the",
            " ",
            "18",
            "th",
            " century",
            ".",
            " Leon",
            "hard",
            " Euler",
            " studied",
            " the",
            " shapes",
            " of",
            " crystals",
            ",",
            " and",
            " by",
            " the",
            " ",
            "19",
            "th",
            "-century",
            " crystal",
            " hunting",
            " was",
            " common",
            " in",
            " Alpine",
            " regions",
            ".",
            " David",
            " Friedrich",
            " W",
            "iser",
            " amassed",
            " a",
            " collection",
            " of",
            " ",
            "800",
            "0",
            " crystals",
            " that",
            " he",
            " studied",
            " and",
            " documented",
            ".",
            " In",
            " the",
            " ",
            "20",
            "th",
            " century"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " known",
            " as",
            " the",
            " \"",
            "Heart",
            " of",
            " Dix",
            "ie",
            "\"",
            " and",
            " the",
            " \"",
            "C",
            "otton",
            " State",
            "\".",
            " The",
            " state",
            " tree",
            " is",
            " the",
            " long",
            "leaf",
            " pine",
            ",",
            " and",
            " the",
            " state",
            " flower",
            " is",
            " the",
            " cam",
            "ell",
            "ia",
            ".",
            " Alabama",
            "'s",
            " capital",
            " is",
            " Montgomery",
            ",",
            " and",
            " its",
            " largest",
            " city",
            " by",
            " population",
            " and",
            " area",
            " is",
            " Hunts",
            "ville",
            ".",
            " Its",
            " oldest",
            " city",
            " is",
            " Mobile",
            ",",
            " founded",
            " by",
            " French",
            " colon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " suggested",
            " for",
            " Ap",
            "ali",
            "unas",
            " makes",
            " Apollo",
            " \"",
            "The",
            " One",
            " of",
            " Entr",
            "ap",
            "ment",
            "\",",
            " perhaps",
            " in",
            " the",
            " sense",
            " of",
            " \"",
            "Hunter",
            "\".",
            "Gre",
            "co",
            "-R",
            "oman",
            " epith",
            "ets",
            "Apollo",
            "'s",
            " chief",
            " epith",
            "et",
            " was",
            " Ph",
            "o",
            "eb",
            "us",
            " (",
            " ;",
            " ,",
            " Ph",
            "o",
            "ib",
            "os",
            " ),",
            " literally",
            " \"",
            "bright",
            "\".",
            " It",
            " was",
            " very",
            " commonly",
            " used",
            " by",
            " both",
            " the",
            " Greeks",
            " and",
            " Romans"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " ",
            "21",
            ".",
            " The",
            " \"",
            "Kr",
            "aven",
            "'s",
            " Last",
            " Hunt",
            "\"",
            " storyline",
            " by",
            " writer",
            " J",
            ".M",
            ".",
            " De",
            "Mat",
            "te",
            "is",
            " and",
            " artists",
            " Mike",
            " Ze",
            "ck",
            " and",
            " Bob",
            " Mc",
            "Leod",
            " crossed",
            " over",
            " into",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            " #",
            "293",
            " and",
            " ",
            "294",
            ".",
            " Issue",
            " No",
            ".",
            "298",
            " (",
            "Mar",
            ".",
            " ",
            "198",
            "8",
            ")",
            " was",
            " the",
            " first",
            " Spider",
            "-Man",
            " comic",
            " to",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " University",
            " of",
            " Alabama",
            " at",
            " Birmingham",
            " at",
            " ",
            "73",
            ").",
            "According",
            " to",
            " the",
            " ",
            "201",
            "2",
            " U",
            ".S",
            ".",
            " News",
            " &",
            " World",
            " Report",
            ",",
            " Alabama",
            " had",
            " four",
            " tier",
            " one",
            " universities",
            " (",
            "University",
            " of",
            " Alabama",
            ",",
            " Auburn",
            " University",
            ",",
            " University",
            " of",
            " Alabama",
            " at",
            " Birmingham",
            " and",
            " University",
            " of",
            " Alabama",
            " in",
            " Hunts",
            "ville",
            ").",
            "Media",
            " ",
            "Major",
            " newspapers",
            " include",
            " Birmingham",
            " News",
            ",",
            " Mobile",
            " Press",
            "-",
            "Register"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " prim",
            "acy",
            " of",
            " the",
            " Pope",
            " on",
            " the",
            " other",
            ",",
            " resulted",
            " in",
            " an",
            " association",
            " of",
            " churches",
            " that",
            " was",
            " both",
            " deliberately",
            " vague",
            " about",
            " doctr",
            "inal",
            " principles",
            ",",
            " yet",
            " bold",
            " in",
            " developing",
            " parameters",
            " of",
            " acceptable",
            " deviation",
            ".",
            " These",
            " parameters",
            " were",
            " most",
            " clearly",
            " articulated",
            " in",
            " the",
            " various",
            " rub",
            "rics",
            " of",
            " the",
            " successive",
            " prayer",
            " books",
            ",",
            " as",
            " well",
            " as",
            " the",
            " Thirty",
            "-nine",
            " Articles",
            " of",
            " Religion",
            " (",
            "156"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.281,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.132,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.12,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.162,
            -0.0,
            -0.0,
            -0.0,
            0.12,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " more",
            " heating",
            ".",
            "Snow",
            "Snow",
            " al",
            "bedo",
            " is",
            " highly",
            " variable",
            ",",
            " ranging",
            " from",
            " as",
            " high",
            " as",
            " ",
            "0",
            ".",
            "9",
            " for",
            " freshly",
            " fallen",
            " snow",
            ",",
            " to",
            " about",
            " ",
            "0",
            ".",
            "4",
            " for",
            " melting",
            " snow",
            ",",
            " and",
            " as",
            " low",
            " as",
            " ",
            "0",
            ".",
            "2",
            " for",
            " dirty",
            " snow",
            ".",
            " Over",
            " Antarctica",
            " snow",
            " al",
            "bedo",
            " averages",
            " a",
            " little",
            " more",
            " than",
            " ",
            "0",
            ".",
            "8",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.279,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " wind",
            "mill",
            " is",
            " rebuilt",
            " and",
            " another",
            " wind",
            "mill",
            " is",
            " constructed",
            ",",
            " which",
            " makes",
            " the",
            " farm",
            " a",
            " good",
            " amount",
            " of",
            " income",
            ".",
            " However",
            ",",
            " the",
            " ideals",
            " that",
            " Snow",
            "ball",
            " discussed",
            ",",
            " including",
            " stalls",
            " with",
            " electric",
            " lighting",
            ",",
            " heating",
            ",",
            " and",
            " running",
            " water",
            ",",
            " are",
            " forgotten",
            ",",
            " with",
            " Napoleon",
            " advocating",
            " that",
            " the",
            " happiest",
            " animals",
            " live",
            " simple",
            " lives",
            ".",
            " Snow",
            "ball",
            " has",
            " been",
            " forgotten",
            ",",
            " alongside"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.248,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Napoleon",
            " is",
            " the",
            " leader",
            " of",
            " Animal",
            " Farm",
            ".",
            " Snow",
            "ball",
            "Âł",
            "–",
            " Napoleon",
            "'s",
            " rival",
            " and",
            " original",
            " head",
            " of",
            " the",
            " farm",
            " after",
            " Jones",
            "'s",
            " overthrow",
            ".",
            " His",
            " life",
            " parallels",
            " that",
            " of",
            " Leon",
            " Trotsky",
            ",",
            " although",
            " there",
            " is",
            " no",
            " reference",
            " to",
            " Snow",
            "ball",
            " having",
            " been",
            " murdered",
            " (",
            "as",
            " Trotsky",
            " was",
            ");",
            " he",
            " may",
            " also",
            " combine",
            " some",
            " elements",
            " from",
            " Lenin",
            ".",
            " S",
            "que",
            "aler"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " mean",
            " precipitation",
            " in",
            " the",
            " Alps",
            " ranges",
            " from",
            " a",
            " low",
            " of",
            " ",
            " per",
            " year",
            " to",
            " ",
            " per",
            " year",
            ",",
            " with",
            " the",
            " higher",
            " levels",
            " occurring",
            " at",
            " high",
            " alt",
            "itudes",
            ".",
            " At",
            " alt",
            "itudes",
            " between",
            " ,",
            " snow",
            "fall",
            " begins",
            " in",
            " November",
            " and",
            " accum",
            "ulates",
            " through",
            " to",
            " April",
            " or",
            " May",
            " when",
            " the",
            " melt",
            " begins",
            ".",
            " Snow",
            " lines",
            " vary",
            " from",
            " ,",
            " above",
            " which",
            " the",
            " snow",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.238,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "Battle",
            " of",
            " the",
            " Cow",
            "shed",
            "\"),",
            " Snow",
            "ball",
            " announces",
            " his",
            " plans",
            " to",
            " modern",
            "ise",
            " the",
            " farm",
            " by",
            " building",
            " a",
            " wind",
            "mill",
            ".",
            " Napoleon",
            " disputes",
            " this",
            " idea",
            ",",
            " and",
            " matters",
            " come",
            " to",
            " a",
            " head",
            ",",
            " which",
            " cul",
            "min",
            "ates",
            " in",
            " Napoleon",
            "'s",
            " dogs",
            " chasing",
            " Snow",
            "ball",
            " away",
            " and",
            " Napoleon",
            " effectively",
            " declaring",
            " himself",
            " supreme",
            " commander",
            ".",
            "N",
            "ap",
            "oleon",
            " en",
            "acts",
            " changes",
            " to",
            " the",
            " governance"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.25,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " is",
            " replaced",
            " with",
            " \"",
            "Animal",
            " Farm",
            "\",",
            " while",
            " an",
            " anthem",
            " glor",
            "ifying",
            " Napoleon",
            ",",
            " who",
            " is",
            " presumably",
            " adopting",
            " the",
            " lifestyle",
            " of",
            " a",
            " man",
            " (\"",
            "Com",
            "rade",
            " Napoleon",
            "\"),",
            " is",
            " composed",
            " and",
            " sung",
            ".",
            " Napoleon",
            " then",
            " conducts",
            " a",
            " second",
            " purge",
            ",",
            " during",
            " which",
            " many",
            " animals",
            " who",
            " are",
            " alleged",
            " to",
            " be",
            " helping",
            " Snow",
            "ball",
            " in",
            " plots",
            " are",
            " executed",
            " by",
            " Napoleon",
            "'s",
            " dogs",
            ",",
            " which"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " exported",
            " back",
            " to",
            " Egypt",
            ".",
            "By",
            " the",
            " Second",
            " Dynasty",
            " at",
            " latest",
            ",",
            " ancient",
            " Egyptian",
            " trade",
            " with",
            " By",
            "b",
            "los",
            " yielded",
            " a",
            " critical",
            " source",
            " of",
            " quality",
            " timber",
            " not",
            " found",
            " in",
            " Egypt",
            ".",
            " By",
            " the",
            " Fifth",
            " Dynasty",
            ",",
            " trade",
            " with",
            " P",
            "unt",
            " provided",
            " gold",
            ",",
            " aromatic",
            " res",
            "ins",
            ",",
            " ebony",
            ",",
            " ivory",
            ",",
            " and",
            " wild",
            " animals",
            " such",
            " as",
            " monkeys",
            " and",
            " bab",
            "oons",
            ".",
            " Egypt"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " days",
            " tend",
            " to",
            " be",
            " clearer",
            ".",
            " On",
            " average",
            ",",
            " Anch",
            "orage",
            " receives",
            " ",
            " of",
            " precipitation",
            " a",
            " year",
            ",",
            " with",
            " around",
            " ",
            " of",
            " snow",
            ",",
            " although",
            " there",
            " are",
            " areas",
            " in",
            " the",
            " south",
            " central",
            " which",
            " receive",
            " far",
            " more",
            " snow",
            ".",
            " It",
            " is",
            " a",
            " sub",
            "ar",
            "ctic",
            " climate",
            " (",
            "K",
            "Ã¶",
            "pp",
            "en",
            ":",
            " D",
            "fc",
            ")",
            " due",
            " to",
            " its",
            " brief",
            ",",
            " cool",
            " summers",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " included",
            " all",
            " the",
            " Cycl",
            "ades",
            " except",
            " My",
            "kon",
            "os",
            " and",
            " T",
            "inos",
            ".",
            " The",
            " Empire",
            " of",
            " N",
            "ica",
            "ea",
            ",",
            " a",
            " Byz",
            "antine",
            " r",
            "ump",
            " state",
            ",",
            " managed",
            " to",
            " effect",
            " the",
            " Rec",
            "apture",
            " of",
            " Constantin",
            "ople",
            " from",
            " the",
            " Lat",
            "ins",
            " in",
            " ",
            "126",
            "1",
            " and",
            " defeat",
            " E",
            "pir",
            "us",
            ".",
            " Byz",
            "antine",
            " successes",
            " were",
            " not",
            " to",
            " last",
            ";",
            " the",
            " Ott",
            "om",
            "ans",
            " would"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.123,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " whom",
            " they",
            " also",
            " named",
            " Py",
            "th",
            "ias",
            ".",
            " In",
            " ",
            "343",
            " BC",
            ",",
            " Aristotle",
            " was",
            " invited",
            " by",
            " Philip",
            " II",
            " of",
            " Maced",
            "on",
            " to",
            " become",
            " the",
            " tutor",
            " to",
            " his",
            " son",
            " Alexander",
            ".",
            "A",
            "rist",
            "otle",
            " was",
            " appointed",
            " as",
            " the",
            " head",
            " of",
            " the",
            " royal",
            " Academy",
            " of",
            " Maced",
            "on",
            ".",
            " During",
            " Aristotle",
            "'s",
            " time",
            " in",
            " the",
            " Maced",
            "onian",
            " court",
            ",",
            " he",
            " gave",
            " lessons",
            " not",
            " only"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.09,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " eliminated",
            ".",
            "During",
            " the",
            " Nap",
            "ole",
            "onic",
            " Wars",
            " in",
            " the",
            " late",
            " ",
            "18",
            "th",
            " century",
            " and",
            " early",
            " ",
            "19",
            "th",
            " century",
            ",",
            " Napoleon",
            " annex",
            "ed",
            " territory",
            " formerly",
            " controlled",
            " by",
            " the",
            " House",
            " of",
            " H",
            "abs",
            "burg",
            ",",
            " and",
            " the",
            " House",
            " of",
            " Sav",
            "oy",
            ".",
            " In",
            " ",
            "179",
            "8",
            ",",
            " the",
            " Hel",
            "v",
            "etic",
            " Republic",
            " was",
            " established",
            ",",
            " two",
            " years",
            " later",
            " an",
            " army",
            " across"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " Strawberry",
            " Press",
            ",",
            " ",
            "199",
            "2",
            ".",
            " .",
            "Also",
            " available",
            " on",
            " Wik",
            "is",
            "ource",
            ":",
            " The",
            " M",
            "ene",
            "b",
            "rea",
            " article",
            ",",
            " The",
            " notes",
            " by",
            " Ada",
            " Lov",
            "el",
            "ace",
            ".",
            "Publication",
            " history",
            " ",
            "Six",
            " copies",
            " of",
            " the",
            " ",
            "184",
            "3",
            " first",
            " edition",
            " of",
            " Sketch",
            " of",
            " the",
            " Analy",
            "tical",
            " Engine",
            " with",
            " Ada",
            " Lov",
            "el",
            "ace",
            "'s",
            " \"",
            "Notes",
            "\"",
            " have",
            " been",
            " located",
            ".",
            " Three"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ipro",
            "to",
            "zo",
            "an",
            " drugs",
            ".",
            " Pot",
            "assium",
            " ant",
            "imony",
            "l",
            " tar",
            "tr",
            "ate",
            ",",
            " or",
            " tart",
            "ar",
            " em",
            "etic",
            ",",
            " was",
            " once",
            " used",
            " as",
            " an",
            " anti",
            "-s",
            "ch",
            "ist",
            "osomal",
            " drug",
            " from",
            " ",
            "191",
            "9",
            " on",
            ".",
            " It",
            " was",
            " subsequently",
            " replaced",
            " by",
            " pr",
            "az",
            "iqu",
            "ant",
            "el",
            ".",
            " Ant",
            "imony",
            " and",
            " its",
            " compounds",
            " are",
            " used",
            " in",
            " several",
            " veterinary",
            " preparations",
            ",",
            " such",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " those",
            " spacecraft",
            ".",
            " The",
            " increased",
            " cabin",
            " space",
            " in",
            " the",
            " Apollo",
            " command",
            " module",
            " afforded",
            " astronauts",
            " greater",
            " freedom",
            " of",
            " movement",
            ",",
            " contributing",
            " to",
            " symptoms",
            " of",
            " space",
            " sickness",
            " for",
            " B",
            "orman",
            " and",
            ",",
            " later",
            ",",
            " astronaut",
            " Rust",
            "y",
            " Schwe",
            "ick",
            "art",
            " during",
            " Apollo",
            "9",
            ".",
            "The",
            " cruise",
            " phase",
            " was",
            " a",
            " relatively",
            " un",
            "event",
            "ful",
            " part",
            " of",
            " the",
            " flight",
            ",",
            " except",
            " for",
            " the",
            " crew",
            "'s",
            " checking",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " least",
            " once",
            ".",
            " In",
            " ",
            "198",
            "4",
            ",",
            " the",
            " Baltimore",
            " Colts",
            " relocated",
            " to",
            " Indianapolis",
            ".",
            " In",
            " ",
            "199",
            "5",
            ",",
            " the",
            " Cleveland",
            " Browns",
            " had",
            " attempted",
            " to",
            " move",
            " to",
            " Baltimore",
            ";",
            " the",
            " resulting",
            " dispute",
            " between",
            " Cleveland",
            " and",
            " the",
            " team",
            " led",
            " to",
            " Mod",
            "ell",
            " establishing",
            " the",
            " Baltimore",
            " Ravens",
            " with",
            " the",
            " players",
            " and",
            " personnel",
            " from",
            " the",
            " Browns",
            ",",
            " while",
            " the",
            " Browns",
            " were",
            " placed",
            " in",
            " suspended",
            " operations"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "å¢ĥ",
    "ckill",
    "yna",
    "arie",
    "uos"
  ],
  "bottom_logits": [
    "Ð»Ð¸ÑĨ",
    "MS",
    " Rudd",
    " addCriterion",
    "-em"
  ],
  "act_min": -0.0,
  "act_max": 0.602
}