{
  "index": 64408,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.879,
            -0.0,
            0.455,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " based",
            " first",
            " of",
            " all",
            " upon",
            " a",
            " corpus",
            " of",
            " poetic",
            " texts",
            ",",
            " in",
            " addition",
            " to",
            " Qur",
            "'an",
            " usage",
            " and",
            " Bed",
            "ou",
            "in",
            " inform",
            "ants",
            " whom",
            " he",
            " considered",
            " to",
            " be",
            " reliable",
            " speakers",
            " of",
            " the",
            " ",
            "Ê",
            "¿",
            "ar",
            "abi",
            "yy",
            "a",
            ".",
            "Spread",
            " ",
            "Ar",
            "abic",
            " spread",
            " with",
            " the",
            " spread",
            " of",
            " Islam",
            ".",
            " Following",
            " the",
            " early",
            " Muslim",
            " conquest",
            "s",
            ",",
            " Arabic",
            " gained",
            " vocabulary",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.875,
            0.088,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "Rel",
            "igious",
            " and",
            " philosophical",
            " beliefs",
            "As",
            " a",
            " young",
            " man",
            ",",
            " Lincoln",
            " was",
            " a",
            " religious",
            " skeptic",
            ".",
            " He",
            " was",
            " deeply",
            " familiar",
            " with",
            " the",
            " Bible",
            ",",
            " quoting",
            " and",
            " praising",
            " it",
            ".",
            " He",
            " was",
            " private",
            " about",
            " his",
            " position",
            " on",
            " organized",
            " religion",
            " and",
            " respected",
            " the",
            " beliefs",
            " of",
            " others",
            ".",
            " He",
            " never",
            " made",
            " a",
            " clear",
            " profession",
            " of",
            " Christian",
            " beliefs",
            ".",
            " Throughout",
            " his",
            " public",
            " career",
            ",",
            " Lincoln"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.867,
            -0.0,
            -0.0,
            0.289,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " \\",
            " H",
            "2",
            " \\",
            "up",
            "arrow",
            "Reaction",
            " with",
            " carbon",
            "L",
            "ith",
            "ium",
            " is",
            " the",
            " only",
            " metal",
            " that",
            " reacts",
            " directly",
            " with",
            " carbon",
            " to",
            " give",
            " dil",
            "ith",
            "ium",
            " ac",
            "ety",
            "lide",
            ".",
            " Na",
            " and",
            " K",
            " can",
            " react",
            " with",
            " ac",
            "ety",
            "lene",
            " to",
            " give",
            " ac",
            "etyl",
            "ides",
            ".",
            "2",
            " Li",
            " \\",
            " +",
            " \\",
            " ",
            "2",
            " C",
            " \\",
            " \\",
            "longrightarrow",
            " \\",
            " Li",
            "2",
            "C",
            "2"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.867,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " A",
            " strong",
            " ag",
            "nostic",
            " would",
            " say",
            ",",
            " \"",
            "I",
            " cannot",
            " know",
            " whether",
            " a",
            " deity",
            " exists",
            " or",
            " not",
            ",",
            " and",
            " neither",
            " can",
            " you",
            ".\"",
            "Weak",
            " ag",
            "nost",
            "icism",
            " (",
            "also",
            " called",
            " \"",
            "soft",
            "\",",
            " \"",
            "open",
            "\",",
            " \"",
            "emp",
            "irical",
            "\",",
            " \"",
            "hope",
            "ful",
            "\"",
            " or",
            " \"",
            "temp",
            "oral",
            " ag",
            "nost",
            "icism",
            "\")",
            " The",
            " view",
            " that",
            " the",
            " existence",
            " or",
            " non",
            "existence",
            " of",
            " any",
            " de"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.083,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.859,
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "2",
            "C",
            "2",
            " \\",
            " +",
            " \\",
            " H",
            "2",
            "Reaction",
            " with",
            " water",
            "On",
            " reaction",
            " with",
            " water",
            ",",
            " they",
            " generate",
            " hydro",
            "x",
            "ide",
            " ions",
            " and",
            " hydrogen",
            " gas",
            ".",
            " This",
            " reaction",
            " is",
            " vigorous",
            " and",
            " highly",
            " ex",
            "other",
            "mic",
            " and",
            " the",
            " hydrogen",
            " resulted",
            " may",
            " ignite",
            " in",
            " air",
            " or",
            " even",
            " explode",
            " in",
            " the",
            " case",
            " of",
            " R",
            "b",
            " and",
            " Cs",
            ".",
            "Na",
            " +",
            " H",
            "2",
            "O",
            " âĨĴ"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.856,
            -0.0,
            -0.0,
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " polar",
            "isable",
            " radical",
            " an",
            "ions",
            ",",
            " such",
            " as",
            " the",
            " dark",
            "-green",
            " sodium",
            " n",
            "aph",
            "thal",
            "en",
            "ide",
            ",",
            " Na",
            "+[",
            "C",
            "10",
            "H",
            "8",
            "âĢ¢",
            "]",
            "âĪĴ",
            ",",
            " a",
            " strong",
            " reducing",
            " agent",
            ".",
            "Represent",
            "ative",
            " reactions",
            " of",
            " alk",
            "ali",
            " metals",
            "Reaction",
            " with",
            " oxygen",
            "Upon",
            " reacting",
            " with",
            " oxygen",
            ",",
            " alk",
            "ali",
            " metals",
            " form",
            " ox",
            "ides",
            ",",
            " per",
            "ox",
            "ides",
            ",",
            " super"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.141,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "NH",
            "3",
            " âĨĴ",
            " Na",
            "+",
            " +",
            " e",
            "(N",
            "H",
            "3",
            ")x",
            "âĪĴ",
            "Due",
            " to",
            " the",
            " presence",
            " of",
            " sol",
            "v",
            "ated",
            " electrons",
            ",",
            " these",
            " solutions",
            " are",
            " very",
            " powerful",
            " reducing",
            " agents",
            " used",
            " in",
            " organic",
            " synthesis",
            ".",
            "Reaction",
            " ",
            "1",
            ")",
            " is",
            " known",
            " as",
            " Birch",
            " reduction",
            ".",
            "Other",
            " reductions",
            " that",
            " can",
            " be",
            " carried",
            " by",
            " these",
            " solutions",
            " are",
            ":",
            "S",
            "8",
            " +",
            " ",
            "2",
            "e",
            "âĪĴ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " with",
            " ammon",
            "ium",
            " hydro",
            "x",
            "ide",
            " at",
            " ",
            "100",
            "0",
            "Âł",
            "Â°C",
            ".",
            " However",
            ",",
            " in",
            " contrast",
            " to",
            " the",
            " oxy",
            "flu",
            "or",
            "ide",
            ",",
            " the",
            " oxy",
            "chlor",
            "ide",
            " could",
            " well",
            " be",
            " synthesized",
            " by",
            " ign",
            "iting",
            " a",
            " solution",
            " of",
            " act",
            "inium",
            " tr",
            "ich",
            "lor",
            "ide",
            " in",
            " hydro",
            "chlor",
            "ic",
            " acid",
            " with",
            " ammonia",
            ".",
            "Reaction",
            " of",
            " aluminium",
            " brom",
            "ide",
            " and",
            " act",
            "inium",
            " oxide",
            " yields",
            " act"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            0.021
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "ster",
            "'s",
            " Millions",
            " (",
            "194",
            "5",
            ")",
            "R",
            "endez",
            "vous",
            " with",
            " Annie",
            " (",
            "194",
            "6",
            ")",
            "Dr",
            "ift",
            "wood",
            " (",
            "194",
            "7",
            ")",
            "Calendar",
            " Girl",
            " (",
            "194",
            "7",
            ")",
            "North",
            "west",
            " Out",
            "post",
            " (",
            "194",
            "7",
            ")",
            " also",
            " associate",
            " producer",
            "The",
            " Inside",
            " Story",
            " (",
            "194",
            "8",
            ")",
            "Angel",
            " in",
            " Ex",
            "ile",
            " (",
            "194",
            "8",
            ")",
            " (",
            "with",
            " Philip",
            " Ford",
            ")",
            "S",
            "ands"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.089,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.848,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " S",
            " in",
            " size",
            ".",
            "Con",
            "verse",
            " Sch",
            "r",
            "Ã¶",
            "der",
            "–",
            "Bern",
            "stein",
            " theorem",
            ":",
            " if",
            " two",
            " sets",
            " have",
            " sur",
            "jections",
            " to",
            " each",
            " other",
            ",",
            " they",
            " are",
            " equ",
            "in",
            "umer",
            "ous",
            ".",
            "Weak",
            " partition",
            " principle",
            ":",
            " if",
            " there",
            " is",
            " a",
            " injection",
            " and",
            " a",
            " sur",
            "jection",
            " from",
            " A",
            " to",
            " B",
            ",",
            " then",
            " A",
            " and",
            " B",
            " are",
            " equ",
            "in",
            "umer",
            "ous",
            ".",
            " Equ",
            "ival"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.059,
            -0.0,
            -0.0,
            -0.0,
            0.277,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.354,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.848
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " âĨĴ",
            " Na",
            "2",
            "S",
            "2",
            "...",
            "Na",
            "2",
            "S",
            "7",
            "Because",
            " alk",
            "ali",
            " metal",
            " sulf",
            "ides",
            " are",
            " essentially",
            " salts",
            " of",
            " a",
            " weak",
            " acid",
            " and",
            " a",
            " strong",
            " base",
            ",",
            " they",
            " form",
            " basic",
            " solutions",
            ".",
            "S",
            "2",
            "-",
            " +",
            " H",
            "2",
            "O",
            " âĨĴ",
            " HS",
            "âĪĴ",
            " +",
            " HO",
            "âĪĴ",
            "HS",
            "âĪĴ",
            " +",
            " H",
            "2",
            "O",
            " âĨĴ",
            " H",
            "2",
            "S",
            " +",
            " HO",
            "âĪĴ",
            "Reaction"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            0.076,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            "es",
            "Spanish",
            "-speaking",
            " countries",
            " and",
            " territories",
            "Special",
            " economic",
            " zones",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "127",
            "8",
            "Rel",
            "igion",
            " and",
            " politics",
            "<|begin_of_text|>",
            "In",
            " mathematics",
            " and",
            " statistics",
            ",",
            " the",
            " arithmetic",
            " mean",
            " (",
            " ),",
            " arithmetic",
            " average",
            ",",
            " or",
            " just",
            " the",
            " mean",
            " or",
            " average",
            " (",
            "when",
            " the",
            " context",
            " is",
            " clear",
            ")",
            " is",
            " the",
            " sum",
            " of",
            " a",
            " collection",
            " of",
            " numbers",
            " divided",
            " by",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            0.844,
            0.029,
            0.531,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " asked",
            " about",
            " the",
            " allegations",
            ",",
            " an",
            " Apple",
            " representative",
            " referred",
            " the",
            " reporter",
            " to",
            " a",
            " section",
            " of",
            " the",
            " company",
            " policy",
            " for",
            " law",
            " enforcement",
            " guidelines",
            ",",
            " which",
            " stated",
            ",",
            " \"",
            "We",
            " review",
            " every",
            " data",
            " request",
            " for",
            " legal",
            " suff",
            "iciency",
            " and",
            " use",
            " advanced",
            " systems",
            " and",
            " processes",
            " to",
            " validate",
            " law",
            " enforcement",
            " requests",
            " and",
            " detect",
            " abuse",
            ".\"",
            "Corporate",
            " affairs",
            "Leaders",
            "hip",
            "Senior",
            " management",
            " ",
            "As",
            " of",
            " March"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            0.085,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " customs",
            ",",
            " language",
            ",",
            " and",
            " place",
            " names",
            ".",
            "Rel",
            "igion",
            " ",
            "The",
            " anthropology",
            " of",
            " religion",
            " involves",
            " the",
            " study",
            " of",
            " religious",
            " institutions",
            " in",
            " relation",
            " to",
            " other",
            " social",
            " institutions",
            ",",
            " and",
            " the",
            " comparison",
            " of",
            " religious",
            " beliefs",
            " and",
            " practices",
            " across",
            " cultures",
            ".",
            " Modern",
            " anthropology",
            " assumes",
            " that",
            " there",
            " is",
            " complete",
            " continuity",
            " between",
            " magical",
            " thinking",
            " and",
            " religion",
            ",",
            " and",
            " that",
            " every",
            " religion",
            " is",
            " a",
            " cultural",
            " product",
            ",",
            " created"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            0.081,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " in",
            " online",
            " experiments",
            " is",
            " contagious",
            " –",
            " people",
            " im",
            "itate",
            " the",
            " generosity",
            " they",
            " observe",
            " in",
            " others",
            ".",
            "Rel",
            "igious",
            " viewpoints",
            "Most",
            ",",
            " if",
            " not",
            " all",
            ",",
            " of",
            " the",
            " world",
            "'s",
            " religions",
            " promote",
            " altru",
            "ism",
            " as",
            " a",
            " very",
            " important",
            " moral",
            " value",
            ".",
            " Buddhism",
            ",",
            " Christianity",
            ",",
            " Hindu",
            "ism",
            ",",
            " Islam",
            ",",
            " Jain",
            "ism",
            ",",
            " Judaism",
            ",",
            " and",
            " Sikh",
            "ism",
            ",",
            " etc",
            ".,",
            " place"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " salt",
            ".",
            " An",
            " example",
            " is",
            " the",
            " weak",
            "ly",
            " acidic",
            " ammon",
            "ium",
            " chloride",
            ",",
            " which",
            " is",
            " produced",
            " from",
            " the",
            " strong",
            " acid",
            " hydrogen",
            " chloride",
            " and",
            " the",
            " weak",
            " base",
            " ammonia",
            ".",
            " Conversely",
            ",",
            " neutral",
            "izing",
            " a",
            " weak",
            " acid",
            " with",
            " a",
            " strong",
            " base",
            " gives",
            " a",
            " weak",
            "ly",
            " basic",
            " salt",
            " (",
            "e",
            ".g",
            ".,",
            " sodium",
            " fluoride",
            " from",
            " hydrogen",
            " fluoride",
            " and",
            " sodium",
            " hydro",
            "x",
            "ide",
            ").",
            "Weak",
            " acid",
            "–"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " mean",
            " but",
            " also",
            " the",
            " median",
            " mentioned",
            " above",
            " and",
            " the",
            " mode",
            " (",
            "the",
            " three",
            " Ms",
            "),",
            " are",
            " equal",
            ".",
            " This",
            " equality",
            " does",
            " not",
            " hold",
            " for",
            " other",
            " probability",
            " distributions",
            ",",
            " as",
            " illustrated",
            " for",
            " the",
            " log",
            "-normal",
            " distribution",
            " here",
            ".",
            "Angles",
            "Part",
            "icular",
            " care",
            " is",
            " needed",
            " when",
            " using",
            " cyclic",
            " data",
            ",",
            " such",
            " as",
            " phases",
            " or",
            " angles",
            ".",
            " Taking",
            " the",
            " arithmetic",
            " mean",
            " of",
            " ",
            "1",
            "Â°"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " Na",
            "OH",
            " +",
            " ",
            "1",
            "/",
            "2",
            "H",
            "2",
            "Reaction",
            " with",
            " other",
            " salts",
            "The",
            " alk",
            "ali",
            " metals",
            " are",
            " very",
            " good",
            " reducing",
            " agents",
            ".",
            " They",
            " can",
            " reduce",
            " metal",
            " c",
            "ations",
            " that",
            " are",
            " less",
            " elect",
            "rop",
            "os",
            "itive",
            ".",
            " Titanium",
            " is",
            " produced",
            " industri",
            "ally",
            " by",
            " the",
            " reduction",
            " of",
            " titanium",
            " t",
            "etr",
            "ach",
            "lor",
            "ide",
            " with",
            " Na",
            " at",
            " ",
            "400",
            "0",
            "C",
            " (",
            "van"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            0.836,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.199,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " northeastern",
            " China",
            ",",
            " only",
            " becoming",
            " pastoral",
            "ists",
            " later",
            " on",
            ".",
            "Against",
            " the",
            " grouping",
            "Weak",
            "ness",
            " of",
            " lexical",
            " and",
            " typ",
            "ological",
            " data",
            "According",
            " to",
            " G",
            ".",
            " Claus",
            "on",
            " (",
            "195",
            "6",
            "),",
            " G",
            ".",
            " Do",
            "er",
            "fer",
            " (",
            "196",
            "3",
            "),",
            " and",
            " A",
            ".",
            " Sh",
            "cher",
            "bak",
            " (",
            "196",
            "3",
            "),",
            " many",
            " of",
            " the",
            " typ",
            "ological",
            " features",
            " of",
            " the",
            " supposed",
            " Alta",
            "ic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.074,
            -0.0,
            0.35,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " engaged",
            " in",
            " a",
            " government",
            " sponsored",
            " re",
            "organization",
            " and",
            " central",
            "ization",
            " of",
            " Japanese",
            " martial",
            " arts",
            ".",
            "Rel",
            "igious",
            " influences",
            "After",
            " U",
            "esh",
            "iba",
            " left",
            " Hok",
            "k",
            "aid",
            "Åį",
            " in",
            " ",
            "191",
            "9",
            ",",
            " he",
            " met",
            " and",
            " was",
            " profoundly",
            " influenced",
            " by",
            " On",
            "is",
            "ab",
            "uro",
            " Deg",
            "uchi",
            ",",
            " the",
            " spiritual",
            " leader",
            " of",
            " the",
            " Å",
            "Į",
            "m",
            "oto",
            "-",
            "ky",
            "Åį",
            " religion",
            " (",
            "a",
            " neo"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " can",
            " mimic",
            " it",
            " can",
            " be",
            " found",
            " historically",
            " in",
            " the",
            " assim",
            "ilation",
            " of",
            " two",
            " concepts",
            ":",
            " simul",
            "ac",
            "ra",
            " (",
            "devices",
            " that",
            " exhibit",
            " likeness",
            ")",
            " and",
            " autom",
            "ata",
            " (",
            "devices",
            " that",
            " have",
            " independence",
            ").",
            "Projects",
            "Several",
            " projects",
            " aiming",
            " to",
            " create",
            " android",
            "s",
            " that",
            " look",
            ",",
            " and",
            ",",
            " to",
            " a",
            " certain",
            " degree",
            ",",
            " speak",
            " or",
            " act",
            " like",
            " a",
            " human",
            " being",
            " have",
            " been",
            " launched"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.002,
            0.088,
            -0.0,
            0.114,
            -0.0,
            0.096,
            -0.0,
            0.075,
            -0.0,
            0.598,
            -0.0,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.183,
            -0.0,
            -0.0,
            0.045,
            -0.0,
            0.019,
            -0.0,
            0.461,
            0.475,
            0.316,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            0.033
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Mediterranean",
            " Lev",
            "ant",
            " Near",
            " East",
            " Pan",
            "-",
            "Asian",
            "ism",
            "Lists",
            ":",
            " List",
            " of",
            " cities",
            " in",
            " Asia",
            " List",
            " of",
            " metropolitan",
            " areas",
            " in",
            " Asia",
            " by",
            " population",
            " List",
            " of",
            " sovereign",
            " states",
            " and",
            " dependent",
            " territories",
            " in",
            " Asia",
            "Projects",
            " Asian",
            " Highway",
            " Network",
            " Trans",
            "-",
            "Asian",
            " Railway",
            "Notes",
            "References",
            "B",
            "ibli",
            "ography",
            "Further",
            " reading",
            " ",
            " Emb"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "1",
            ").",
            " ",
            " PDF",
            " .",
            "   ",
            "  ",
            "   ",
            "  ",
            " ",
            " (",
            "Ex",
            "cerpt",
            " via",
            " Amazon",
            ").",
            "External",
            " links",
            " ",
            "Understanding",
            " Austrian",
            " Economics",
            " by",
            " Henry",
            " Haz",
            "l",
            "itt",
            " ",
            " ",
            " ",
            "School",
            "s",
            " of",
            " economic",
            " thought",
            "Lib",
            "ert",
            "arian",
            " theory",
            "<|begin_of_text|>",
            "An",
            " abs",
            "cess",
            " is",
            " a",
            " collection",
            " of",
            " pus",
            " that",
            " has",
            " built",
            " up",
            " within",
            " the",
            " tissue",
            " of",
            " the",
            " body",
            ".",
            " Signs",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.836,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " ",
            "Legal",
            " procedure",
            "United",
            " States",
            " procedural",
            " law",
            "<|begin_of_text|>",
            "In",
            " law",
            ",",
            " an",
            " answer",
            " was",
            " originally",
            " a",
            " solemn",
            " assertion",
            " in",
            " opposition",
            " to",
            " someone",
            " or",
            " something",
            ",",
            " and",
            " thus",
            " generally",
            " any",
            " counter",
            "-st",
            "atement",
            " or",
            " defense",
            ",",
            " a",
            " reply",
            " to",
            " a",
            " question",
            " or",
            " response",
            ",",
            " or",
            " objection",
            ",",
            " or",
            " a",
            " correct",
            " solution",
            " of",
            " a",
            " problem",
            ".",
            "In",
            " the",
            " common",
            " law",
            ",",
            " an",
            " answer"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            0.644,
            -0.0,
            0.836,
            -0.0,
            0.641,
            -0.0,
            0.641,
            -0.0,
            0.656,
            -0.0,
            0.734,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            0.723,
            0.064,
            -0.0,
            0.777,
            0.022,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "165",
            "0",
            " images",
            " of",
            " Apollo",
            ")",
            " ",
            "Greek",
            " gods",
            "Roman",
            " gods",
            "Beauty",
            " gods",
            "Health",
            " gods",
            "Knowledge",
            " gods",
            "Light",
            " gods",
            "Maintenance",
            " de",
            "ities",
            "Music",
            " and",
            " singing",
            " gods",
            "Or",
            "acular",
            " gods",
            "Solar",
            " gods",
            "Pl",
            "ague",
            " gods",
            "Drag",
            "ons",
            "layers",
            "My",
            "th",
            "ological",
            " Greek",
            " arch",
            "ers",
            "My",
            "th",
            "ological",
            " rap",
            "ists"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            0.019,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            "ah",
            "ans",
            "h",
            "ah",
            "s",
            "Male",
            " murder",
            " victims",
            "<|begin_of_text|>",
            "The",
            " American",
            " Film",
            " Institute",
            " (",
            "AF",
            "I",
            ")",
            " is",
            " an",
            " American",
            " nonprofit",
            " film",
            " organization",
            " that",
            " educ",
            "ates",
            " filmmakers",
            " and",
            " honors",
            " the",
            " heritage",
            " of",
            " the",
            " motion",
            " picture",
            " arts",
            " in",
            " the",
            " United",
            " States",
            ".",
            " A",
            "FI",
            " is",
            " supported",
            " by",
            " private",
            " funding",
            " and",
            " public",
            " membership",
            " fees",
            ".",
            "Leaders",
            "hip",
            "The",
            " institute",
            " is",
            " composed",
            " of",
            " leaders"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            0.644,
            -0.0,
            0.836,
            -0.0,
            0.641,
            -0.0,
            0.641,
            -0.0,
            0.656,
            -0.0,
            0.734,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            0.723,
            0.064,
            -0.0,
            0.777,
            0.022,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "165",
            "0",
            " images",
            " of",
            " Apollo",
            ")",
            " ",
            "Greek",
            " gods",
            "Roman",
            " gods",
            "Beauty",
            " gods",
            "Health",
            " gods",
            "Knowledge",
            " gods",
            "Light",
            " gods",
            "Maintenance",
            " de",
            "ities",
            "Music",
            " and",
            " singing",
            " gods",
            "Or",
            "acular",
            " gods",
            "Solar",
            " gods",
            "Pl",
            "ague",
            " gods",
            "Drag",
            "ons",
            "layers",
            "My",
            "th",
            "ological",
            " Greek",
            " arch",
            "ers",
            "My",
            "th",
            "ological",
            " rap",
            "ists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.836,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " ",
            "Legal",
            " procedure",
            "United",
            " States",
            " procedural",
            " law",
            "<|begin_of_text|>",
            "In",
            " law",
            ",",
            " an",
            " answer",
            " was",
            " originally",
            " a",
            " solemn",
            " assertion",
            " in",
            " opposition",
            " to",
            " someone",
            " or",
            " something",
            ",",
            " and",
            " thus",
            " generally",
            " any",
            " counter",
            "-st",
            "atement",
            " or",
            " defense",
            ",",
            " a",
            " reply",
            " to",
            " a",
            " question",
            " or",
            " response",
            ",",
            " or",
            " objection",
            ",",
            " or",
            " a",
            " correct",
            " solution",
            " of",
            " a",
            " problem",
            ".",
            "In",
            " the",
            " common",
            " law",
            ",",
            " an",
            " answer"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " been",
            " nominated",
            " and",
            " rewarded",
            " in",
            " other",
            " categories",
            ",",
            " relatively",
            " often",
            " for",
            " Best",
            " Original",
            " Song",
            " and",
            " Best",
            " Original",
            " Score",
            ".",
            "Beauty",
            " and",
            " the",
            " Beast",
            " was",
            " the",
            " first",
            " animated",
            " film",
            " nominated",
            " for",
            " Best",
            " Picture",
            ",",
            " in",
            " ",
            "199",
            "1",
            ".",
            " Up",
            " (",
            "200",
            "9",
            ")",
            " and",
            " Toy",
            " Story",
            " ",
            "3",
            " (",
            "201",
            "0",
            ")",
            " also",
            " received",
            " Best",
            " Picture",
            " nominations",
            ",",
            " after",
            " the",
            " academy",
            " expanded",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " moral",
            " outcome",
            " of",
            " our",
            " attachment",
            " to",
            " pleasure",
            ".",
            " Sch",
            "openh",
            "auer",
            " deemed",
            " that",
            " this",
            " truth",
            " was",
            " expressed",
            " by",
            " the",
            " Christian",
            " dog",
            "ma",
            " of",
            " original",
            " sin",
            " and",
            ",",
            " in",
            " Eastern",
            " religions",
            ",",
            " by",
            " the",
            " dog",
            "ma",
            " of",
            " reb",
            "irth",
            ".",
            "Quiet",
            "ism",
            " ",
            "He",
            " who",
            " sees",
            " through",
            " the",
            " princip",
            "ium",
            " individ",
            "uation",
            "is",
            " and",
            " compreh",
            "ends",
            " suffering",
            " in",
            " general",
            " as",
            " his",
            " own",
            " will"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            0.062,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Ber",
            "ber",
            " by",
            " ",
            "27",
            "%.",
            "Rel",
            "igion",
            "Islam",
            " is",
            " the",
            " predominant",
            " religion",
            " in",
            " Algeria",
            ",",
            " with",
            " its",
            " adher",
            "ents",
            ",",
            " mostly",
            " Sun",
            "nis",
            ",",
            " accounting",
            " for",
            " ",
            "99",
            "%",
            " of",
            " the",
            " population",
            " according",
            " to",
            " a",
            " ",
            "202",
            "1",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimate",
            ",",
            " and",
            " ",
            "97",
            ".",
            "9",
            "%",
            " according",
            " to",
            " Pew",
            " Research",
            " in",
            " ",
            "202",
            "0",
            ".",
            " There"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            0.014,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.758,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Aber",
            "de",
            "ens",
            "hire",
            " voted",
            " for",
            " the",
            " Union",
            ",",
            " while",
            " ",
            "39",
            ".",
            "64",
            "%",
            " opted",
            " for",
            " independence",
            ".",
            "Political",
            " control",
            "A",
            "ber",
            "de",
            "ens",
            "hire",
            " Council",
            " has",
            " been",
            " under",
            " no",
            " overall",
            " control",
            " since",
            " its",
            " creation",
            ":",
            "Leaders",
            "hip",
            "The",
            " leaders",
            " of",
            " the",
            " council",
            " since",
            " ",
            "199",
            "6",
            " have",
            " been",
            ":",
            "Composition",
            "Following",
            " the",
            " ",
            "202",
            "2",
            " election",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            0.75,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " lead",
            " electric",
            " guitar",
            " Anders",
            " El",
            "jas",
            " (",
            "197",
            "7",
            ")",
            " keyboards",
            " on",
            " tour",
            " and",
            " all",
            " the",
            " band",
            "'s",
            " orchest",
            "ration",
            " ",
            " (",
            "197",
            "8",
            "–",
            "198",
            "2",
            ")",
            " percussion",
            " ",
            " (",
            "198",
            "0",
            "–",
            "202",
            "1",
            ")",
            " drums",
            "Disc",
            "ography",
            " ",
            "Studio",
            " albums",
            " Ring",
            " Ring",
            " (",
            "197",
            "3",
            ")",
            " Waterloo",
            " (",
            "197",
            "4",
            ")",
            " AB",
            "BA",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.738,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " an",
            " \"",
            "ant",
            "id",
            "ote",
            "\"",
            " to",
            " nihil",
            "ism",
            ".",
            " Cam",
            "us",
            " described",
            " her",
            " as",
            " \"",
            "the",
            " only",
            " great",
            " spirit",
            " of",
            " our",
            " times",
            "\".",
            "Death",
            " ",
            "  ",
            "Cam",
            "us",
            " died",
            " on",
            " ",
            "4",
            " January",
            " ",
            "196",
            "0",
            " at",
            " the",
            " age",
            " of",
            " ",
            "46",
            ",",
            " in",
            " a",
            " car",
            " accident",
            " near",
            " Sens",
            ",",
            " in",
            " Le",
            " Grand",
            " F",
            "oss",
            "ard",
            " in",
            " the",
            " small",
            " town",
            " of",
            " Ville"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.387,
            0.129,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "Right",
            "-lib",
            "ert",
            "arian",
            "ism",
            "Syn",
            "cret",
            "ic",
            " political",
            " movements",
            "M",
            "urray",
            " Roth",
            "bard",
            "<|begin_of_text|>",
            "Events",
            "Pre",
            "-",
            "160",
            "0",
            "48",
            " BC",
            " –",
            " Caesar",
            "'s",
            " Civil",
            " War",
            ":",
            " Battle",
            " of",
            " Ph",
            "ars",
            "alus",
            ":",
            " Julius",
            " Caesar",
            " decis",
            "ively",
            " defeats",
            " Pompe",
            "y",
            " at",
            " Ph",
            "ars",
            "alus",
            " and",
            " Pompe",
            "y",
            " fle",
            "es",
            " to",
            " Egypt",
            ".",
            " ",
            "378",
            " –",
            " Gothic"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " assumption",
            " that",
            " no",
            " key",
            " sent",
            " a",
            " BS",
            " character",
            " allowed",
            " control",
            "+",
            "H",
            " to",
            " be",
            " used",
            " for",
            " other",
            " purposes",
            ",",
            " such",
            " as",
            " the",
            " \"",
            "help",
            "\"",
            " prefix",
            " command",
            " in",
            " GNU",
            " Emacs",
            ".",
            "Escape",
            "Many",
            " more",
            " of",
            " the",
            " control",
            " characters",
            " have",
            " been",
            " assigned",
            " meanings",
            " quite",
            " different",
            " from",
            " their",
            " original",
            " ones",
            ".",
            " The",
            " \"",
            "escape",
            "\"",
            " character",
            " (",
            "ESC",
            ",",
            " code",
            " ",
            "27",
            "),",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            0.547,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " oscill",
            "ation",
            ",",
            " rupt",
            "ured",
            " fuel",
            " lines",
            ",",
            " and",
            " bad",
            " ign",
            "iter",
            " lines",
            " took",
            " place",
            " on",
            " December",
            " ",
            "18",
            ",",
            " three",
            " days",
            " before",
            " the",
            " scheduled",
            " launch",
            ".",
            "Mission",
            "Parameter",
            " summary",
            "As",
            " the",
            " first",
            " crew",
            "ed",
            " spacecraft",
            " to",
            " orbit",
            " more",
            " than",
            " one",
            " celestial",
            " body",
            ",",
            " Apollo",
            "8",
            "'s",
            " profile",
            " had",
            " two",
            " different",
            " sets",
            " of",
            " orbital",
            " parameters",
            ",",
            " separated",
            " by",
            " a",
            " transl",
            "unar"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "ate",
            " that",
            " \"",
            "story",
            "\"",
            " while",
            " it",
            " was",
            " lowered",
            " for",
            " the",
            " other",
            ".",
            " The",
            " multiplication",
            " of",
            " images",
            " ev",
            "oked",
            " War",
            "hol",
            "'s",
            " seminal",
            " silk",
            "-screen",
            " works",
            " of",
            " the",
            " early",
            " ",
            "196",
            "0",
            "s",
            ".",
            "War",
            "hol",
            " was",
            " a",
            " fan",
            " of",
            " filmmaker",
            " Rad",
            "ley",
            " Met",
            "z",
            "ger",
            " film",
            " work",
            " and",
            " commented",
            " that",
            " Met",
            "z",
            "ger",
            "'s",
            " film",
            ",",
            " The",
            " L",
            "icker",
            "ish",
            " Quart",
            "et"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.091,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "oms",
            "en",
            ",",
            " or",
            " possibly",
            " via",
            " K",
            "har",
            "ost",
            "hi",
            " (",
            "cf",
            ".,",
            " Iss",
            "yk",
            " inscription",
            ").",
            "B",
            "rah",
            "mi",
            " script",
            " was",
            " also",
            " possibly",
            " derived",
            " or",
            " inspired",
            " by",
            " Ar",
            "ama",
            "ic",
            ".",
            " Brah",
            "mic",
            " family",
            " of",
            " scripts",
            " includes",
            " Dev",
            "an",
            "ag",
            "ari",
            ".",
            "Languages",
            " using",
            " the",
            " alphabet",
            "Today",
            ",",
            " Biblical",
            " Ar",
            "ama",
            "ic",
            ",",
            " Jewish",
            " Neo",
            "-A",
            "rama",
            "ic",
            " dialect",
            "s",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " moral",
            " debate",
            " increase",
            " the",
            " possibility",
            " of",
            " agreement",
            ".",
            "Applied",
            " ethics",
            " was",
            " later",
            " distinguished",
            " from",
            " the",
            " nas",
            "cent",
            " applied",
            " ep",
            "istem",
            "ology",
            ",",
            " which",
            " is",
            " also",
            " under",
            " the",
            " umbrella",
            " of",
            " applied",
            " philosophy",
            ".",
            " While",
            " the",
            " former",
            " was",
            " concerned",
            " with",
            " the",
            " practical",
            " application",
            " of",
            " moral",
            " considerations",
            ",",
            " the",
            " latter",
            " focuses",
            " on",
            " the",
            " application",
            " of",
            " ep",
            "istem",
            "ology",
            " in",
            " solving",
            " practical",
            " problems",
            ".",
            "See",
            " also"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.672,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " example",
            ".",
            "Common",
            " tools",
            " ",
            "Am",
            "ateur",
            " astronomers",
            " use",
            " a",
            " range",
            " of",
            " instruments",
            " to",
            " study",
            " the",
            " sky",
            ",",
            " depending",
            " on",
            " a",
            " combination",
            " of",
            " their",
            " interests",
            " and",
            " resources",
            ".",
            " ",
            " Methods",
            " include",
            " simply",
            " looking",
            " at",
            " the",
            " night",
            " sky",
            " with",
            " the",
            " naked",
            " eye",
            ",",
            " using",
            " bin",
            "ocular",
            "s",
            ",",
            " and",
            " using",
            " a",
            " variety",
            " of",
            " optical",
            " telesc",
            "opes",
            " of",
            " varying",
            " power",
            " and",
            " quality",
            ",",
            " as",
            " well"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " into",
            " the",
            " origin",
            " of",
            " techniques",
            " and",
            " movements",
            ",",
            " and",
            " reinforces",
            " the",
            " concepts",
            " of",
            " distance",
            ",",
            " timing",
            ",",
            " foot",
            " movement",
            ",",
            " presence",
            " and",
            " connected",
            "ness",
            " with",
            " one",
            "'s",
            " training",
            " partner",
            "(s",
            ").",
            "Multiple",
            " attackers",
            " and",
            " ",
            "One",
            " feature",
            " of",
            " a",
            "ik",
            "ido",
            " is",
            " training",
            " to",
            " defend",
            " against",
            " multiple",
            " attackers",
            ",",
            " often",
            " called",
            " ,",
            " or",
            " .",
            " Fre",
            "estyle",
            " practice",
            " with",
            " multiple",
            " attackers",
            " called",
            " ",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.116,
            -0.0,
            -0.0,
            -0.0,
            0.344,
            -0.0,
            -0.0,
            0.57,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.275,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            "ellen",
            "istic",
            "-era",
            " people",
            "K",
            "ayan",
            "ians",
            "Mon",
            "archs",
            " of",
            " Pers",
            "ia",
            "People",
            " in",
            " the",
            " de",
            "uter",
            "oc",
            "an",
            "onical",
            " books",
            "Ph",
            "araoh",
            "s",
            " of",
            " the",
            " Ar",
            "ge",
            "ad",
            " dynasty",
            "Sh",
            "ah",
            "name",
            "h",
            " characters",
            "Tem",
            "ple",
            " of",
            " Artem",
            "is",
            "<|begin_of_text|>",
            "Al",
            "fred",
            " Hab",
            "d",
            "ank",
            " Sk",
            "arb",
            "ek",
            " Kor",
            "zy",
            "bs",
            "ki",
            " (",
            ",",
            " ;",
            " July"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.461,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            0.602,
            0.159,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "German",
            " atheist",
            " writers",
            "German",
            " scholars",
            " of",
            " Buddhism",
            "German",
            " ethic",
            "ists",
            "German",
            " e",
            "ug",
            "enic",
            "ists",
            "German",
            " fl",
            "aut",
            "ists",
            "German",
            " log",
            "icians",
            "German",
            " male",
            " essay",
            "ists",
            "German",
            " male",
            " non",
            "-fiction",
            " writers",
            "German",
            " monarch",
            "ists",
            "German",
            " people",
            " of",
            " Dutch",
            " descent",
            "German",
            " phil",
            "ologists",
            "Ac",
            "ademic",
            " staff",
            " of",
            " the",
            " Humb",
            "oldt",
            " University",
            " of",
            " Berlin"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            0.578,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ã¶",
            "hl",
            "au",
            ")",
            "External",
            " links",
            "A",
            "ust",
            "rian",
            " German",
            " –",
            " German",
            " Dictionary",
            "Das",
            " Ãĸ",
            "sterreich",
            "ische",
            " Volk",
            "sw",
            "Ã¶r",
            "ter",
            "buch",
            "B",
            "av",
            "arian",
            " language",
            "German",
            " dialect",
            "s",
            "German",
            "National",
            " varieties",
            " of",
            " German",
            "<|begin_of_text|>",
            "In",
            " mathematics",
            ",",
            " the",
            " axiom",
            " of",
            " choice",
            ",",
            " abbreviated",
            " AC",
            " or",
            " Ao",
            "C",
            ",",
            " is",
            " an",
            " axiom",
            " of",
            " set",
            " theory",
            " equivalent",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            0.004,
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " party",
            ",",
            " MPL",
            "A",
            ",",
            " won",
            " another",
            " majority",
            " and",
            " President",
            " Lou",
            "ren",
            "Ã§o",
            " won",
            " a",
            " second",
            " five",
            "-year",
            " term",
            " in",
            " the",
            " election",
            ".",
            " However",
            ",",
            " the",
            " election",
            " was",
            " the",
            " tight",
            "est",
            " in",
            " Angola",
            "'s",
            " history",
            ".",
            "Ge",
            "ography",
            "At",
            " ,",
            " Angola",
            " is",
            " the",
            " world",
            "'s",
            " twenty",
            "-four",
            "th",
            " largest",
            " country",
            " —",
            " comparable",
            " in",
            " size",
            " to",
            " Mali",
            ",",
            " or",
            " twice",
            " the",
            " size",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.012,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " Region",
            ")",
            "North",
            " Mes",
            "opot",
            "am",
            "ian",
            " Arabic",
            " is",
            " a",
            " spoken",
            " north",
            " of",
            " the",
            " Ham",
            "rin",
            " Mountains",
            " in",
            " Iraq",
            ",",
            " in",
            " western",
            " Iran",
            ",",
            " northern",
            " Syria",
            ",",
            " and",
            " in",
            " southeastern",
            " Turkey",
            " (",
            "in",
            " the",
            " eastern",
            " Mediterranean",
            " Region",
            ",",
            " Southeast",
            "ern",
            " Anat",
            "olia",
            " Region",
            ",",
            " and",
            " southern",
            " Eastern",
            " Anat",
            "olia",
            " Region",
            ").",
            "J",
            "ude",
            "o",
            "-M",
            "es",
            "opot",
            "am",
            "ian",
            " Arabic",
            ",",
            " also",
            " known"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " writing",
            " style",
            ",",
            " and",
            " it",
            " is",
            " this",
            " form",
            " of",
            " writing",
            "—",
            "along",
            " with",
            " formal",
            " hier",
            "og",
            "lyph",
            "s",
            "—that",
            " accompany",
            " the",
            " Greek",
            " text",
            " on",
            " the",
            " Ros",
            "etta",
            " Stone",
            ".",
            "Around",
            " the",
            " first",
            " century",
            " AD",
            ",",
            " the",
            " C",
            "optic",
            " alphabet",
            " started",
            " to",
            " be",
            " used",
            " alongside",
            " the",
            " Dem",
            "otic",
            " script",
            ".",
            " C",
            "optic",
            " is",
            " a",
            " modified",
            " Greek",
            " alphabet",
            " with",
            " the",
            " addition",
            " of",
            " some",
            " Dem",
            "otic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.197,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "British",
            ")",
            " English",
            " instead",
            ".",
            "A",
            " significant",
            " number",
            " of",
            " the",
            " words",
            " that",
            " are",
            " utilized",
            " in",
            " the",
            " Ant",
            "ig",
            "uan",
            " dialect",
            " are",
            " derived",
            " from",
            " both",
            " the",
            " British",
            " and",
            " African",
            " languages",
            ".",
            " This",
            " is",
            " readily",
            " apparent",
            " in",
            " phrases",
            " such",
            " as",
            " \"",
            "In",
            "nit",
            "?\"",
            " which",
            " literally",
            " translates",
            " to",
            " \"",
            "Isn",
            "'t",
            " it",
            "?\"",
            " Many",
            " common",
            " island",
            " pro",
            "verbs",
            " can",
            " be",
            " traced",
            " back",
            " to",
            " Africa",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " monument",
            " (",
            "198",
            "9",
            ").",
            " Professor",
            " A",
            ".",
            " Mel",
            "ua",
            " has",
            " provided",
            " funds",
            " for",
            " the",
            " establishment",
            " of",
            " the",
            " monument",
            " (",
            "J",
            ".S",
            ".Co",
            ".",
            " \"",
            "Human",
            "istica",
            "\",",
            " ",
            "199",
            "0",
            "–",
            "199",
            "1",
            ").",
            " The",
            " abstract",
            " metal",
            " sculpture",
            " was",
            " designed",
            " by",
            " local",
            " artists",
            " Sergey",
            " Al",
            "ip",
            "ov",
            " and",
            " Pavel",
            " She",
            "v",
            "chen",
            "ko",
            ",",
            " and",
            " appears",
            " to",
            " be",
            " an",
            " explosion",
            " or",
            " branches"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " at",
            " rest",
            ",",
            " since",
            " Aristotle",
            " does",
            " not",
            " address",
            " friction",
            ".",
            " With",
            " this",
            " understanding",
            ",",
            " it",
            " can",
            " be",
            " observed",
            " that",
            ",",
            " as",
            " Aristotle",
            " stated",
            ",",
            " heavy",
            " objects",
            " (",
            "on",
            " the",
            " ground",
            ",",
            " say",
            ")",
            " require",
            " more",
            " force",
            " to",
            " make",
            " them",
            " move",
            ";",
            " and",
            " objects",
            " pushed",
            " with",
            " greater",
            " force",
            " move",
            " faster",
            ".",
            " This",
            " would",
            " imply",
            " the",
            " equation",
            " ,",
            "incorrect",
            " in",
            " modern",
            " physics",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " defendant",
            "'s",
            " guilt",
            " in",
            " the",
            " matter",
            ".",
            "The",
            " term",
            " \"",
            "alloc",
            "ution",
            "\"",
            " is",
            " used",
            " generally",
            " only",
            " in",
            " jurisdictions",
            " in",
            " the",
            " United",
            " States",
            ",",
            " but",
            " there",
            " are",
            " vaguely",
            " similar",
            " processes",
            " in",
            " other",
            " common",
            " law",
            " countries",
            ".",
            " In",
            " many",
            " other",
            " jurisdictions",
            ",",
            " it",
            " is",
            " for",
            " the",
            " defense",
            " lawyer",
            " to",
            " mitigate",
            " on",
            " their",
            " client",
            "'s",
            " behalf",
            ",",
            " and",
            " the",
            " defendant",
            " rarely",
            " has",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ora",
            "!",
            " T",
            "ora",
            "!,",
            " produced",
            " by",
            " ",
            "20",
            "th",
            " Century",
            " Fox",
            " and",
            " K",
            "uros",
            "awa",
            " Production",
            ",",
            " would",
            " be",
            " a",
            " portrayal",
            " of",
            " the",
            " Japanese",
            " attack",
            " on",
            " Pearl",
            " Harbor",
            " from",
            " both",
            " the",
            " American",
            " and",
            " the",
            " Japanese",
            " points",
            " of",
            " view",
            ",",
            " with",
            " K",
            "uros",
            "awa",
            " hel",
            "ming",
            " the",
            " Japanese",
            " half",
            " and",
            " an",
            " Anglo",
            "ph",
            "onic",
            " film",
            "-maker",
            " directing",
            " the",
            " American",
            " half",
            ".",
            " He",
            " spent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " began",
            " writing",
            " seriously",
            " in",
            " his",
            " early",
            " twenties",
            ",",
            " establishing",
            " himself",
            " as",
            " a",
            " successful",
            " writer",
            " and",
            " social",
            " sat",
            "ir",
            "ist",
            ".",
            " His",
            " first",
            " published",
            " novels",
            " were",
            " social",
            " sat",
            "ires",
            ",",
            " C",
            "rome",
            " Yellow",
            " (",
            "192",
            "1",
            "),",
            " Ant",
            "ic",
            " Hay",
            " (",
            "192",
            "3",
            "),",
            " Those",
            " Bar",
            "ren",
            " Leaves",
            " (",
            "192",
            "5",
            "),",
            " and",
            " Point",
            " Counter",
            " Point",
            " (",
            "192",
            "8",
            ").",
            " Brave",
            " New",
            " World"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "and",
            " the",
            " world",
            " No",
            ".",
            " ",
            "2",
            ")",
            " until",
            " the",
            " mid",
            "-",
            "199",
            "0",
            "s",
            ".",
            " He",
            " fought",
            " Kas",
            "par",
            "ov",
            " in",
            " three",
            " more",
            " world",
            " championship",
            " matches",
            " in",
            " ",
            "198",
            "6",
            " (",
            "held",
            " in",
            " London",
            " and",
            " L",
            "ening",
            "rad",
            "),",
            " ",
            "198",
            "7",
            " (",
            "in",
            " Se",
            "ville",
            "),",
            " and",
            " ",
            "199",
            "0",
            " (",
            "in",
            " New",
            " York",
            " City",
            " and",
            " Lyon",
            ").",
            " All",
            " three"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "achuset",
    "adro",
    " helicopt",
    "Ð¸ÑĨÐ¸Ð½",
    "onth"
  ],
  "bottom_logits": [
    "itchens",
    "eland",
    " Cla",
    " Laf",
    "ter"
  ],
  "act_min": -0.0,
  "act_max": 0.879
}