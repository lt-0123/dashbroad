{
  "index": 66195,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.863,
            -0.0,
            -0.0,
            0.131,
            -0.0,
            -0.0,
            0.158,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "rez",
            ",",
            " Salah",
            " Assad",
            " and",
            " Dj",
            "amel",
            " Z",
            "id",
            "ane",
            ".",
            " The",
            " Algeria",
            " national",
            " football",
            " team",
            " qualified",
            " for",
            " the",
            " ",
            "198",
            "2",
            " FIFA",
            " World",
            " Cup",
            ",",
            " ",
            "198",
            "6",
            " FIFA",
            " World",
            " Cup",
            ",",
            " ",
            "201",
            "0",
            " FIFA",
            " World",
            " Cup",
            " and",
            " ",
            "201",
            "4",
            " FIFA",
            " World",
            " Cup",
            ".",
            " In",
            " addition",
            ",",
            " several",
            " football",
            " clubs",
            " have",
            " won",
            " continental",
            " and",
            " international",
            " trophies",
            " as",
            " the",
            " club",
            " ES"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.859,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.195,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "bles",
            ":",
            " In",
            " Northern",
            " Ireland",
            ",",
            " the",
            " British",
            " authorities",
            " launch",
            " Operation",
            " Dem",
            "etri",
            "us",
            ".",
            " The",
            " operation",
            " involves",
            " the",
            " mass",
            " arrest",
            " and",
            " intern",
            "ment",
            " without",
            " trial",
            " of",
            " individuals",
            " suspected",
            " of",
            " being",
            " affiliated",
            " with",
            " the",
            " Irish",
            " Republican",
            " Army",
            " (",
            "PI",
            "RA",
            ").",
            " Mass",
            " riots",
            " follow",
            ",",
            " and",
            " thousands",
            " of",
            " people",
            " flee",
            " or",
            " are",
            " forced",
            " out",
            " of",
            " their",
            " homes",
            ".",
            "197",
            "3",
            " –",
            " Mars",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "sis",
            " of",
            " sodium",
            " chloride",
            " by",
            " lowering",
            " the",
            " melting",
            " point",
            " of",
            " the",
            " substance",
            " to",
            " below",
            " ",
            "700",
            "Âł",
            "Â°C",
            " through",
            " the",
            " use",
            " of",
            " a",
            " Downs",
            " cell",
            ".",
            " Extremely",
            " pure",
            " sodium",
            " can",
            " be",
            " produced",
            " through",
            " the",
            " thermal",
            " decomposition",
            " of",
            " sodium",
            " az",
            "ide",
            ".",
            " Pot",
            "assium",
            " occurs",
            " in",
            " many",
            " minerals",
            ",",
            " such",
            " as",
            " s",
            "ylv",
            "ite",
            " (",
            "pot",
            "assium",
            " chloride",
            ").",
            " Previously",
            ",",
            " potassium",
            " was",
            " generally"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            0.085,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "sis",
            " cell",
            " are",
            " made",
            " of",
            " carbon",
            "—the",
            " most",
            " resistant",
            " material",
            " against",
            " fluoride",
            " corrosion",
            "—and",
            " either",
            " bake",
            " at",
            " the",
            " process",
            " or",
            " are",
            " pre",
            "b",
            "aked",
            ".",
            " The",
            " former",
            ",",
            " also",
            " called",
            " S",
            "Ã¶",
            "der",
            "berg",
            " an",
            "odes",
            ",",
            " are",
            " less",
            " power",
            "-efficient",
            " and",
            " f",
            "umes",
            " released",
            " during",
            " baking",
            " are",
            " costly",
            " to",
            " collect",
            ",",
            " which",
            " is",
            " why",
            " they",
            " are",
            " being",
            " replaced",
            " by",
            " pre",
            "b",
            "aked"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.82,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "gin",
            "ic",
            " acid",
            " also",
            " has",
            " been",
            " used",
            " in",
            " the",
            " field",
            " of",
            " bi",
            "otechnology",
            " as",
            " a",
            " bi",
            "ocom",
            "patible",
            " medium",
            " for",
            " cell",
            " encaps",
            "ulation",
            " and",
            " cell",
            " immobil",
            "ization",
            ".",
            " Molecular",
            " cuisine",
            " is",
            " also",
            " a",
            " user",
            " of",
            " the",
            " substance",
            " for",
            " its",
            " g",
            "elling",
            " properties",
            ",",
            " by",
            " which",
            " it",
            " becomes",
            " a",
            " delivery",
            " vehicle",
            " for",
            " flavours",
            ".",
            "Between",
            " ",
            "100",
            ",",
            "000",
            " and",
            " ",
            "170",
            ",",
            "000"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.116,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            0.071,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "gin",
            "us",
            ",",
            " G",
            "ai",
            "us",
            " Julius",
            ",",
            " De",
            " Astr",
            "onom",
            "ica",
            ",",
            " in",
            " The",
            " My",
            "ths",
            " of",
            " Hy",
            "gin",
            "us",
            ",",
            " edited",
            " and",
            " translated",
            " by",
            " Mary",
            " A",
            ".",
            " Grant",
            ",",
            " Lawrence",
            ":",
            " University",
            " of",
            " Kansas",
            " Press",
            ",",
            " ",
            "196",
            "0",
            ".",
            " Online",
            " version",
            " at",
            " Top",
            "os",
            "Text",
            ".",
            " Hy",
            "gin",
            "us",
            ",",
            " G",
            "ai",
            "us",
            " Julius",
            ",",
            " Fab",
            "ula",
            "e",
            ",",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " openings",
            " (",
            "fen",
            "est",
            "ra",
            "e",
            ")",
            " on",
            " either",
            " side",
            " and",
            " the",
            " jaw",
            " is",
            " rigid",
            "ly",
            " attached",
            " to",
            " the",
            " skull",
            ".",
            " There",
            " is",
            " one",
            " row",
            " of",
            " teeth",
            " in",
            " the",
            " lower",
            " jaw",
            " and",
            " this",
            " fits",
            " between",
            " the",
            " two",
            " rows",
            " in",
            " the",
            " upper",
            " jaw",
            " when",
            " the",
            " animal",
            " ch",
            "ews",
            ".",
            " The",
            " teeth",
            " are",
            " merely",
            " projections",
            " of",
            " b",
            "ony",
            " material",
            " from",
            " the",
            " jaw",
            " and",
            " eventually",
            " wear"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.061,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.087,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            0.103,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            ",",
            " and",
            " Cecil",
            " Greek",
            ".",
            " L",
            "atham",
            ",",
            " MD",
            ";",
            " Madison",
            ",",
            " NJ",
            ":",
            " Row",
            "man",
            " &",
            " Little",
            "field",
            ";",
            " Fair",
            "leigh",
            " Dickinson",
            " University",
            " Press",
            ".",
            "  ",
            "  ",
            "    ",
            "  ",
            "  ",
            "  ",
            "  ",
            " Slav",
            "oj",
            " Å½",
            "iÅ¾",
            "ek",
            " et",
            " al",
            ".:",
            "Everything",
            " You",
            " Always",
            " Wanted",
            " to",
            " Know",
            " About",
            " Lac",
            "an",
            " But",
            " Were",
            " Af",
            "raid",
            " to",
            " Ask",
            " Hitch",
            "cock",
            "'',",
            " London",
            " and",
            " New"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.076,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            " of",
            " Bold",
            " Monday",
            ".",
            " The",
            " font",
            " began",
            " to",
            " appear",
            " in",
            " Audi",
            "'s",
            " ",
            "200",
            "9",
            " products",
            " and",
            " marketing",
            " materials",
            ".",
            "S",
            "ponsor",
            "ships",
            "A",
            "udi",
            " is",
            " a",
            " strong",
            " partner",
            " of",
            " different",
            " kinds",
            " of",
            " sports",
            ".",
            " In",
            " football",
            ",",
            " long",
            " partnerships",
            " exist",
            " between",
            " Audi",
            " and",
            " domestic",
            " clubs",
            " including",
            " Bayern",
            " Munich",
            ",",
            " H",
            "amburger",
            " SV",
            ",",
            " ",
            "1",
            ".",
            " FC",
            " N",
            "Ã¼rnberg",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            0.443,
            -0.0,
            0.816,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.108,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " i",
            ",",
            " o",
            " and",
            " u",
            " to",
            " create",
            " suffix",
            "es",
            " -",
            "ane",
            ",",
            " -",
            "ene",
            ",",
            " -",
            "ine",
            " (",
            "or",
            " -",
            "yne",
            "),",
            " -",
            "one",
            ",",
            " -",
            "une",
            ",",
            " for",
            " the",
            " hydro",
            "car",
            "bons",
            " C",
            "n",
            "H",
            "2",
            "n",
            "+",
            "2",
            ",",
            " C",
            "n",
            "H",
            "2",
            "n",
            ",",
            " C",
            "n",
            "H",
            "2",
            "n",
            "âĪĴ",
            "2",
            ",",
            " C",
            "n",
            "H",
            "2",
            "n",
            "âĪĴ",
            "4",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            ")",
            " or",
            " entire",
            " capit",
            "ulum",
            " (",
            "e",
            ".g",
            ".",
            " Ar",
            "ct",
            "ium",
            ")",
            " has",
            " hooks",
            ",",
            " sp",
            "ines",
            " or",
            " some",
            " structure",
            " to",
            " attach",
            " to",
            " the",
            " fur",
            " or",
            " plum",
            "age",
            " (",
            "or",
            " even",
            " clothes",
            ",",
            " as",
            " in",
            " the",
            " photo",
            ")",
            " of",
            " an",
            " animal",
            " just",
            " to",
            " fall",
            " off",
            " later",
            " far",
            " from",
            " its",
            " mother",
            " plant",
            ".",
            " ",
            "Some",
            " members",
            " of",
            " Aster",
            "aceae",
            " are",
            " economically",
            " important"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            "em",
            "Ã¼nde",
            " along",
            " with",
            " the",
            " V",
            "-",
            "1",
            " flying",
            " bomb",
            ".",
            " The",
            " V",
            "-",
            "2",
            "'s",
            " first",
            " target",
            " was",
            " Paris",
            " on",
            " ",
            "8",
            " September",
            " ",
            "194",
            "4",
            ".",
            " The",
            " program",
            " while",
            " advanced",
            " proved",
            " to",
            " be",
            " an",
            " imped",
            "iment",
            " to",
            " the",
            " war",
            " economy",
            ".",
            " The",
            " large",
            " capital",
            " investment",
            " was",
            " not",
            " rep",
            "aid",
            " in",
            " military",
            " effectiveness",
            ".",
            " The",
            " rockets",
            " were",
            " built",
            " at",
            " an",
            " underground"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.057,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            ",",
            " rag",
            "we",
            "eds",
            ",",
            " th",
            "istles",
            ",",
            " and",
            " d",
            "andel",
            "ion",
            ".",
            " D",
            "andel",
            "ion",
            " was",
            " introduced",
            " into",
            " North",
            " America",
            " by",
            " European",
            " settlers",
            " who",
            " used",
            " the",
            " young",
            " leaves",
            " as",
            " a",
            " salad",
            " green",
            ".",
            " A",
            " number",
            " of",
            " species",
            " are",
            " toxic",
            " to",
            " grazing",
            " animals",
            ".",
            "Uses",
            " ",
            "A",
            "ster",
            "aceae",
            " is",
            " an",
            " economically",
            " important",
            " family",
            ",",
            " providing",
            " products",
            " such",
            " as",
            " cooking",
            " oils",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            0.055,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            "ory",
            " at",
            "ax",
            "ia",
            " presents",
            " itself",
            " with",
            " an",
            " un",
            "steady",
            " \"",
            "st",
            "om",
            "ping",
            "\"",
            " g",
            "ait",
            " with",
            " heavy",
            " heel",
            " strikes",
            ",",
            " as",
            " well",
            " as",
            " a",
            " post",
            "ural",
            " instability",
            " that",
            " is",
            " usually",
            " wors",
            "ened",
            " when",
            " the",
            " lack",
            " of",
            " proprio",
            "ceptive",
            " input",
            " cannot",
            " be",
            " compensated",
            " for",
            " by",
            " visual",
            " input",
            ",",
            " such",
            " as",
            " in",
            " poorly",
            " lit",
            " environments",
            ".",
            "Phys",
            "icians",
            " can",
            " find",
            " evidence",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.158,
            0.028,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            "hire",
            " was",
            " the",
            " location",
            " of",
            " more",
            " fighting",
            ",",
            " cent",
            "red",
            " on",
            " the",
            " Mar",
            "qu",
            "ess",
            " of",
            " Mont",
            "rose",
            " and",
            " the",
            " English",
            " Civil",
            " Wars",
            ".",
            " This",
            " period",
            " also",
            " saw",
            " increased",
            " wealth",
            " due",
            " to",
            " the",
            " increase",
            " in",
            " trade",
            " with",
            " Germany",
            ",",
            " Poland",
            ",",
            " and",
            " the",
            " Low",
            " Countries",
            ".",
            "The",
            " present",
            " council",
            " area",
            " is",
            " named",
            " after",
            " the",
            " historic",
            " county",
            " of",
            " Aber",
            "de",
            "ens",
            "hire",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            0.248,
            -0.0,
            -0.0,
            0.008,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            " (",
            "and",
            " smaller",
            " Mao",
            "ist",
            " guerr",
            "illas",
            ")",
            " against",
            " regime",
            " forces",
            " country",
            "wide",
            ".",
            " It",
            " quickly",
            " turned",
            " into",
            " a",
            " proxy",
            " war",
            " as",
            " the",
            " Pakistani",
            " government",
            " provided",
            " these",
            " rebels",
            " with",
            " covert",
            " training",
            " centers",
            ",",
            " the",
            " United",
            " States",
            " supported",
            " them",
            " through",
            " Pakistan",
            "'s",
            " Inter",
            "-S",
            "ervices",
            " Intelligence",
            " (",
            "IS",
            "I",
            "),",
            " and",
            " the",
            " Soviet",
            " Union",
            " sent",
            " thousands",
            " of",
            " military",
            " advisers",
            " to",
            " support",
            " the",
            " PD"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.126,
            -0.0,
            -0.0,
            0.12,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            0.085,
            -0.0,
            0.016,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.051,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            " and",
            " Henri",
            "-D",
            "omin",
            "ique",
            " S",
            "aff",
            "rey",
            " (",
            "ed",
            ".),",
            " Les",
            " al",
            "ch",
            "im",
            "istes",
            " g",
            "rec",
            "s",
            ",",
            " t",
            ".",
            " ",
            "4",
            ".",
            "1",
            " :",
            " Z",
            "os",
            "ime",
            " de",
            " Pan",
            "opol",
            "is",
            ".",
            " MÃ©",
            "mo",
            "ires",
            " auth",
            "ent",
            "iques",
            ",",
            " Paris",
            ",",
            " Les",
            " Bel",
            "les",
            " Let",
            "tres",
            ",",
            " ",
            "199",
            "5",
            ".",
            " Andr",
            "Ã©e",
            " Coll",
            "inet",
            " and",
            " Henri",
            "-D",
            "omin"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            0.071,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            " as",
            " beasts",
            " of",
            " burden",
            ",",
            " and",
            " they",
            " were",
            " responsible",
            " for",
            " pl",
            "owing",
            " the",
            " fields",
            " and",
            " tr",
            "ampling",
            " seed",
            " into",
            " the",
            " soil",
            ".",
            " The",
            " slaughter",
            " of",
            " a",
            " fatt",
            "ened",
            " ox",
            " was",
            " also",
            " a",
            " central",
            " part",
            " of",
            " an",
            " offering",
            " ritual",
            ".",
            " H",
            "orses",
            " were",
            " introduced",
            " by",
            " the",
            " Hy",
            "ks",
            "os",
            " in",
            " the",
            " Second",
            " Intermediate",
            " Period",
            ".",
            " Cam",
            "els",
            ",",
            " although",
            " known",
            " from",
            " the",
            " New"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            0.165,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.109,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.116,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            0.153,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            "leh",
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "2",
            ":",
            " Morph",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "196",
            "6",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " III"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ens",
            ",",
            " and",
            " the",
            " N",
            "BI",
            " was",
            " closed",
            ".",
            " She",
            " published",
            " an",
            " article",
            " in",
            " The",
            " Object",
            "ivist",
            " rep",
            "udi",
            "ating",
            " Nath",
            "aniel",
            " Brand",
            "en",
            " for",
            " dishonest",
            "y",
            " and",
            " \"",
            "ir",
            "r",
            "ational",
            " behavior",
            " in",
            " his",
            " private",
            " life",
            "\".",
            " In",
            " subsequent",
            " years",
            ",",
            " Rand",
            " and",
            " several",
            " more",
            " of",
            " her",
            " closest",
            " associates",
            " parted",
            " company",
            ".",
            "Rand",
            " had",
            " surgery",
            " for",
            " lung",
            " cancer",
            " in",
            " ",
            "197",
            "4"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.805,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            " can",
            " be",
            " reduced",
            " to",
            " al",
            "kan",
            "es",
            " by",
            " reaction",
            " with",
            " hy",
            "d",
            "ride",
            " re",
            "agents",
            " such",
            " as",
            " lithium",
            " aluminium",
            " hy",
            "d",
            "ride",
            ".",
            " R",
            "âĪĴ",
            "X",
            " +",
            " H",
            "–",
            " âĨĴ",
            " R",
            "âĪĴ",
            "H",
            " +",
            " X",
            "–",
            "Applications",
            "The",
            " applications",
            " of",
            " al",
            "kan",
            "es",
            " depend",
            " on",
            " the",
            " number",
            " of",
            " carbon",
            " atoms",
            ".",
            " The",
            " first",
            " four",
            " al",
            "kan",
            "es",
            " are",
            " used",
            " mainly",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            0.594,
            0.114,
            0.173,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.805,
            0.151,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.113,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "entially",
            " beings",
            " can",
            " either",
            " '",
            "act",
            "'",
            " (",
            "po",
            "ie",
            "in",
            ")",
            " or",
            " '",
            "be",
            " acted",
            " upon",
            "'",
            " (",
            "pas",
            "chein",
            "),",
            " which",
            " can",
            " be",
            " either",
            " innate",
            " or",
            " learned",
            ".",
            " For",
            " example",
            ",",
            " the",
            " eyes",
            " possess",
            " the",
            " potential",
            "ity",
            " of",
            " sight",
            " (",
            "inn",
            "ate",
            " –",
            " being",
            " acted",
            " upon",
            "),",
            " while",
            " the",
            " capability",
            " of",
            " playing",
            " the",
            " flute",
            " can",
            " be",
            " possessed",
            " by",
            " learning",
            " (",
            "exercise"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.805,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.128,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            " is",
            " in",
            " fact",
            " based",
            " on",
            " a",
            " reading",
            " of",
            " the",
            " plays",
            ".",
            " For",
            " example",
            ",",
            " conversation",
            " among",
            " the",
            " guests",
            " turns",
            " to",
            " the",
            " subject",
            " of",
            " Love",
            " and",
            " Arist",
            "oph",
            "anes",
            " explains",
            " his",
            " notion",
            " of",
            " it",
            " in",
            " terms",
            " of",
            " an",
            " amusing",
            " alleg",
            "ory",
            ",",
            " a",
            " device",
            " he",
            " often",
            " uses",
            " in",
            " his",
            " plays",
            ".",
            " He",
            " is",
            " represented",
            " as",
            " suffering",
            " an",
            " attack",
            " of",
            " h",
            "icc",
            "ups",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.805,
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            "ha",
            " Temple",
            " of",
            " Alaska",
            ",",
            " making",
            " it",
            " the",
            " first",
            " Hindu",
            " Temple",
            " in",
            " Alaska",
            " and",
            " the",
            " northern",
            "most",
            " Hindu",
            " Temple",
            " in",
            " the",
            " world",
            ".",
            " There",
            " are",
            " an",
            " estimated",
            " ",
            "2",
            ",",
            "000",
            "–",
            "3",
            ",",
            "000",
            " Hindus",
            " in",
            " Alaska",
            ".",
            " The",
            " vast",
            " majority",
            " of",
            " Hindus",
            " live",
            " in",
            " Anch",
            "orage",
            " or",
            " Fair",
            "banks",
            ".",
            "Est",
            "imates",
            " for",
            " the",
            " number",
            " of",
            " Muslims",
            " in",
            " Alaska",
            " range"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.805,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            0.087,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.247,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.141,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            0.091,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            " won",
            " second",
            " prize",
            " at",
            " the",
            " City",
            " Dion",
            "ys",
            "ia",
            " in",
            " ",
            "427",
            " BC",
            " with",
            " his",
            " first",
            " play",
            " The",
            " Ban",
            "qu",
            "eters",
            " (",
            "now",
            " lost",
            ").",
            " He",
            " won",
            " first",
            " prize",
            " there",
            " with",
            " his",
            " next",
            " play",
            ",",
            " The",
            " Babylon",
            "ians",
            " (",
            "also",
            " now",
            " lost",
            ").",
            " It",
            " was",
            " usual",
            " for",
            " foreign",
            " dign",
            "it",
            "aries",
            " to",
            " attend",
            " the",
            " City",
            " Dion",
            "ys",
            "ia",
            ",",
            " and",
            " The",
            " Babylon"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.151,
            0.052,
            0.114,
            -0.0,
            -0.0,
            -0.0,
            0.106,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "-",
            "emin",
            "ently",
            " G",
            "ide",
            "an",
            " mode",
            " of",
            " expression",
            ".\"",
            " \"",
            "His",
            " first",
            " novel",
            " emerged",
            " from",
            " G",
            "ide",
            "'s",
            " own",
            " journal",
            ",",
            " and",
            " many",
            " of",
            " the",
            " first",
            "-person",
            " narratives",
            " read",
            " more",
            " or",
            " less",
            " like",
            " journals",
            ".",
            " In",
            " Les",
            " faux",
            "-m",
            "onn",
            "ay",
            "eurs",
            ",",
            " Ed",
            "ou",
            "ard",
            "'s",
            " journal",
            " provides",
            " an",
            " alternative",
            " voice",
            " to",
            " the",
            " narrator",
            "'s",
            ".\"",
            " \"",
            "In",
            " ",
            "194",
            "6"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            0.182,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            "\".",
            "The",
            " song",
            " has",
            " been",
            " performed",
            " as",
            " part",
            " of",
            " the",
            " Indianapolis",
            " ",
            "500",
            " pre",
            "-r",
            "ace",
            " ceremonies",
            " since",
            " ",
            "199",
            "1",
            ".",
            "The",
            " US",
            " singer",
            "/s",
            "ong",
            "writer",
            " Martin",
            " Sext",
            "on",
            " recorded",
            " a",
            " gospel",
            "-",
            "ting",
            "ed",
            " version",
            " on",
            " his",
            " LP",
            " '",
            "Black",
            " Sheep",
            "',",
            " released",
            " in",
            " ",
            "199",
            "6",
            ".",
            " <",
            ">",
            "Pop",
            "ularity",
            " of",
            " the",
            " song",
            " increased",
            " greatly",
            " in",
            " the",
            " decades"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.24,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.07,
            -0.0,
            0.297,
            0.106,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            "fly",
            "),",
            " tow",
            " sack",
            " (",
            "b",
            "url",
            "ap",
            " bag",
            "),",
            " plum",
            " peach",
            " (",
            "cling",
            "stone",
            "),",
            " French",
            " har",
            "p",
            " (",
            "har",
            "mon",
            "ica",
            "),",
            " and",
            " dog",
            " ir",
            "ons",
            " (",
            "and",
            "ir",
            "ons",
            ").",
            "Rel",
            "igion",
            " ",
            "In",
            " the",
            " ",
            "200",
            "8",
            " American",
            " Religious",
            " Identification",
            " Survey",
            ",",
            " ",
            "86",
            "%",
            " of",
            " Alabama",
            " respondents",
            " reported",
            " their",
            " religion",
            " as",
            " Christian",
            ",",
            " including",
            " ",
            "6",
            "%",
            " Catholic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.275,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.129,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "zes",
            "i",
            ")",
            " is",
            " situated",
            " at",
            " the",
            " entrance",
            " of",
            " the",
            " Ankara",
            " Castle",
            ".",
            " It",
            " is",
            " an",
            " old",
            " ",
            "15",
            "th",
            " century",
            " bed",
            "est",
            "en",
            " (",
            "covered",
            " b",
            "azaar",
            ")",
            " that",
            " has",
            " been",
            " restored",
            " and",
            " now",
            " houses",
            " a",
            " collection",
            " of",
            " Pale",
            "olithic",
            ",",
            " Ne",
            "olithic",
            ",",
            " H",
            "atti",
            ",",
            " H",
            "itt",
            "ite",
            ",",
            " Ph",
            "ry",
            "g",
            "ian",
            ",",
            " Ur",
            "art",
            "ian",
            " and",
            " Roman",
            " works"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.104,
            -0.0,
            0.033,
            0.066,
            -0.0,
            0.049,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.086,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.201,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " word",
            " for",
            " the",
            " substance",
            ",",
            " as",
            " opposed",
            " to",
            " the",
            " cosmetic",
            ",",
            " can",
            " appear",
            " as",
            " Ø¥",
            "Ø«",
            "ÙħØ¯",
            " ith",
            "mid",
            ",",
            " ath",
            "m",
            "oud",
            ",",
            " o",
            "th",
            "mod",
            ",",
            " or",
            " ",
            "uth",
            "mod",
            ".",
            " Lit",
            "tr",
            "Ã©",
            " suggests",
            " the",
            " first",
            " form",
            ",",
            " which",
            " is",
            " the",
            " earliest",
            ",",
            " derives",
            " from",
            " st",
            "imm",
            "ida",
            ",",
            " an",
            " accus",
            "ative",
            " for",
            " stim",
            "mi",
            ".",
            " The",
            " Greek",
            " word",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.789,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            0.016,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "zu",
            ").",
            "The",
            " French",
            " filmmaker",
            " Chris",
            " Marker",
            " directed",
            " a",
            " documentary",
            " film",
            " as",
            " a",
            " homage",
            " to",
            " T",
            "ark",
            "ovsky",
            " called",
            " One",
            " Day",
            " in",
            " the",
            " Life",
            " of",
            " Andre",
            "i",
            " Ars",
            "ene",
            "v",
            "ich",
            " and",
            " used",
            " T",
            "ark",
            "ovsky",
            "'s",
            " concept",
            " of",
            " \"",
            "The",
            " Zone",
            "\"",
            " (",
            "from",
            " the",
            " film",
            ",",
            " St",
            "alker",
            ")",
            " for",
            " his",
            " ",
            "198",
            "3",
            " film",
            " essay",
            ",",
            " Sans",
            " Sole",
            "il",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.103,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.07,
            -0.0,
            0.609,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            0.523,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.067,
            -0.0,
            0.758,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            0.041,
            -0.0,
            0.287,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            0.527,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "!",
            " '",
            "throw",
            "!'",
            " second",
            "-person",
            " singular",
            " feminine",
            " past",
            "-t",
            "ense",
            " -",
            "ti",
            " and",
            " likewise",
            " anti",
            " '",
            "you",
            " (",
            "f",
            "em",
            ".",
            " sg",
            ".)",
            "'",
            " sometimes",
            ",",
            " first",
            "-person",
            " singular",
            " past",
            "-t",
            "ense",
            " -",
            "tu",
            " sometimes",
            ",",
            " second",
            "-person",
            " masculine",
            " past",
            "-t",
            "ense",
            " -",
            "ta",
            " and",
            " likewise",
            " ant",
            "a",
            " '",
            "you",
            " (",
            "m",
            "asc",
            ".",
            " sg",
            ".)",
            "'",
            " final",
            " -",
            "a",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.124,
            -0.0,
            0.75,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.75,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " with",
            " the",
            " activity",
            " exceeding",
            " that",
            " of",
            " the",
            " standard",
            " americ",
            "ium",
            "-",
            "ber",
            "yll",
            "ium",
            " and",
            " rad",
            "ium",
            "-",
            "ber",
            "yll",
            "ium",
            " pairs",
            ".",
            " In",
            " all",
            " those",
            " applications",
            ",",
            " ",
            "227",
            "Ac",
            " (",
            "a",
            " beta",
            " source",
            ")",
            " is",
            " merely",
            " a",
            " progen",
            "itor",
            " which",
            " generates",
            " alpha",
            "-em",
            "itting",
            " isot",
            "opes",
            " upon",
            " its",
            " decay",
            ".",
            " B",
            "ery",
            "ll",
            "ium",
            " captures",
            " alpha",
            " particles",
            " and",
            " emits",
            " neut",
            "rons"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            0.18,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.196,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.058
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            "es",
            ")",
            " Order",
            " Not",
            "ac",
            "anth",
            "iform",
            "es",
            " Good",
            "rich",
            " ",
            "190",
            "9",
            " (",
            "hal",
            "osaurs",
            " and",
            " sp",
            "iny",
            " e",
            "els",
            ")",
            " Order",
            " Ang",
            "u",
            "ill",
            "iform",
            "es",
            " Jar",
            "ock",
            "i",
            " ",
            "182",
            "2",
            " sens",
            "u",
            " Good",
            "rich",
            " ",
            "190",
            "9",
            " (",
            "true",
            " e",
            "els",
            ")",
            " Meg",
            "ac",
            "oh",
            "ort",
            " O",
            "ste",
            "og",
            "los",
            "so",
            "ceph",
            "al",
            "ai",
            " sens",
            "u",
            " Arr",
            "at",
            "ia"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.096,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            "Al",
            "uminium",
            " production",
            " is",
            " highly",
            " energy",
            "-consuming",
            ",",
            " and",
            " so",
            " the",
            " producers",
            " tend",
            " to",
            " locate",
            " sm",
            "elters",
            " in",
            " places",
            " where",
            " electric",
            " power",
            " is",
            " both",
            " plentiful",
            " and",
            " inexpensive",
            ".",
            " Production",
            " of",
            " one",
            "Âł",
            "kil",
            "ogram",
            " of",
            " aluminium",
            " requires",
            " ",
            "7",
            "Âł",
            "kil",
            "ograms",
            " of",
            " oil",
            " energy",
            " equivalent",
            ",",
            " as",
            " compared",
            " to",
            " ",
            "1",
            ".",
            "5",
            "Âł",
            "kil",
            "ograms",
            " for",
            " steel",
            " and",
            " ",
            "2",
            "Âł"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.691,
            -0.0,
            -0.0,
            0.095,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.049,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            0.35,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            0.036,
            0.071,
            -0.0,
            -0.0,
            -0.0,
            0.045,
            -0.0,
            0.539,
            0.059,
            0.019,
            -0.0,
            0.016,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "erse",
            " Late",
            " Pale",
            "ozo",
            "ic",
            " and",
            " early",
            " Mes",
            "ozo",
            "ic",
            " grade",
            ",",
            " some",
            " of",
            " which",
            " were",
            " large",
            " predators",
            ")",
            " Sub",
            "class",
            " L",
            "iss",
            "am",
            "ph",
            "ibia",
            " (",
            "all",
            " modern",
            " amphib",
            "ians",
            ",",
            " including",
            " frogs",
            ",",
            " to",
            "ads",
            ",",
            " sal",
            "am",
            "anders",
            ",",
            " new",
            "ts",
            " and",
            " ca",
            "ec",
            "ilians",
            ")",
            " Sal",
            "ient",
            "ia",
            " (",
            "f",
            "ro",
            "gs",
            ",",
            " to",
            "ads",
            " and",
            " relatives",
            "):",
            " Early"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            0.06,
            -0.0,
            -0.0,
            0.088,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            0.119,
            0.011,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.046,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ida",
            " [=",
            " monoc",
            "ot",
            "yled",
            "ons",
            "]",
            " and",
            " includes",
            " only",
            " three",
            " families",
            " as",
            " shown",
            ":",
            " Al",
            "ism",
            "ata",
            "ceae",
            " But",
            "om",
            "aceae",
            " Lim",
            "no",
            "char",
            "it",
            "aceae",
            "C",
            "ron",
            "quist",
            "'s",
            " subclass",
            " Al",
            "ism",
            "at",
            "idae",
            " con",
            "formed",
            " fairly",
            " closely",
            " to",
            " the",
            " order",
            " Al",
            "ism",
            "ata",
            "les",
            " as",
            " defined",
            " by",
            " AP",
            "G",
            ",",
            " minus",
            " the",
            " Ar",
            "aceae",
            ".",
            "The",
            " Dah"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.174,
            -0.0,
            0.199,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "in",
            " the",
            " tradition",
            " of",
            " Ni",
            "obe",
            "'s",
            " offspring",
            ").",
            " The",
            " poem",
            " ends",
            " with",
            " a",
            " description",
            " of",
            " Hector",
            "'s",
            " funeral",
            ",",
            " with",
            " the",
            " doom",
            " of",
            " Troy",
            " and",
            " Achilles",
            " himself",
            " still",
            " to",
            " come",
            ".",
            "Later",
            " epic",
            " accounts",
            ":",
            " fighting",
            " Pent",
            "hes",
            "ile",
            "a",
            " and",
            " Mem",
            "non",
            " ",
            "The",
            " A",
            "eth",
            "i",
            "opis",
            " (",
            "7",
            "th",
            " century",
            " BC",
            ")",
            " and",
            " a",
            " work",
            " named",
            " Post",
            "h"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.194,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " difficult",
            " to",
            " know",
            " which",
            " of",
            " the",
            " al",
            "chem",
            "ists",
            " were",
            " actually",
            " women",
            ".",
            " As",
            " the",
            " six",
            "teenth",
            " century",
            " went",
            " on",
            ",",
            " scientific",
            " culture",
            " flour",
            "ished",
            " and",
            " people",
            " began",
            " collecting",
            " \"",
            "se",
            "crets",
            "\".",
            " During",
            " this",
            " period",
            " \"",
            "se",
            "crets",
            "\"",
            " referred",
            " to",
            " experiments",
            ",",
            " and",
            " the",
            " most",
            " coveted",
            " ones",
            " were",
            " not",
            " those",
            " which",
            " were",
            " bizarre",
            ",",
            " but",
            " the",
            " ones",
            " which",
            " had",
            " been",
            " proven"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.073,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "194",
            "2",
            ",",
            " M",
            "inder",
            ",",
            " in",
            " collaboration",
            " with",
            " the",
            " English",
            " scientist",
            " Alice",
            " Leigh",
            "-Smith",
            ",",
            " announced",
            " the",
            " discovery",
            " of",
            " another",
            " is",
            "otope",
            " of",
            " element",
            " ",
            "85",
            ",",
            " presumed",
            " to",
            " be",
            " the",
            " product",
            " of",
            " thor",
            "ium",
            " A",
            " (",
            "pol",
            "onium",
            "-",
            "216",
            ")",
            " beta",
            " decay",
            ".",
            " They",
            " named",
            " this",
            " substance",
            " \"",
            "ang",
            "lo",
            "-h",
            "el",
            "vet",
            "ium",
            "\",",
            " but",
            " Kar",
            "lik",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.111,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.088,
            -0.0,
            0.598,
            0.031,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.118,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "Coming",
            "-to",
            "-be",
            " is",
            " a",
            " change",
            " where",
            " the",
            " substrate",
            " of",
            " the",
            " thing",
            " that",
            " has",
            " undergone",
            " the",
            " change",
            " has",
            " itself",
            " changed",
            ".",
            " In",
            " that",
            " particular",
            " change",
            " he",
            " introduces",
            " the",
            " concept",
            " of",
            " potential",
            "ity",
            " (",
            "d",
            "ynam",
            "is",
            ")",
            " and",
            " actual",
            "ity",
            " (",
            "ente",
            "le",
            "che",
            "ia",
            ")",
            " in",
            " association",
            " with",
            " the",
            " matter",
            " and",
            " the",
            " form",
            ".",
            " Ref",
            "erring",
            " to",
            " potential",
            "ity",
            ",",
            " this"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.377,
            -0.0,
            -0.0,
            0.59,
            0.113,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " during",
            " the",
            " family",
            "-f",
            "estival",
            " ",
            " ().",
            " According",
            " to",
            " some",
            " scholars",
            ",",
            " the",
            " words",
            " are",
            " derived",
            " from",
            " the",
            " Dor",
            "ic",
            " word",
            " ",
            " (),",
            " which",
            " originally",
            " meant",
            " \"",
            "wall",
            ",\"",
            " \"",
            "f",
            "ence",
            " for",
            " animals",
            "\"",
            " and",
            " later",
            " \"",
            "assembly",
            " within",
            " the",
            " limits",
            " of",
            " the",
            " square",
            ".\"",
            " Ap",
            "ella",
            " ()",
            " is",
            " the",
            " name",
            " of",
            " the",
            " popular",
            " assembly",
            " in",
            " Sp",
            "arta",
            ",",
            " corresponding",
            " to",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Her",
            " suspicion",
            " of",
            " those",
            " around",
            " her",
            " caused",
            " Naomi",
            " to",
            " draw",
            " closer",
            " to",
            " young",
            " Allen",
            ",",
            " \"",
            "her",
            " little",
            " pet",
            ",\"",
            " as",
            " Bill",
            " Morgan",
            " says",
            " in",
            " his",
            " biography",
            " of",
            " Gins",
            "berg",
            ",",
            " titled",
            " I",
            " Celebr",
            "ate",
            " My",
            "self",
            ":",
            " The",
            " Som",
            "ewhat",
            " Private",
            " Life",
            " of",
            " Allen",
            " Gins",
            "berg",
            ".",
            " She",
            " also",
            " tried",
            " to",
            " kill",
            " herself",
            " by",
            " sl",
            "itting",
            " her",
            " wrists",
            " and",
            " was",
            " soon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            0.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.2,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.225,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " .",
            " These",
            " are",
            " dissect",
            "ed",
            " by",
            " three",
            " narrow",
            " valleys",
            " in",
            " a",
            " Y",
            " shape",
            " that",
            " combine",
            " into",
            " one",
            " as",
            " the",
            " main",
            " stream",
            ",",
            " the",
            " Gran",
            " Val",
            "ira",
            " river",
            ",",
            " leaves",
            " the",
            " country",
            " for",
            " Spain",
            " (",
            "at",
            " And",
            "orra",
            "'s",
            " lowest",
            " point",
            " of",
            " ).",
            " And",
            "orra",
            "'s",
            " land",
            " area",
            " is",
            " .",
            "Environment",
            "Ph",
            "yt",
            "oge",
            "ographically",
            ",",
            " And",
            "orra",
            " belongs",
            " to",
            " the",
            " Atlantic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.504,
            0.061,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "vae",
            "us",
            " were",
            " not",
            " satisfied",
            " with",
            " the",
            " outcome",
            ",",
            " so",
            " the",
            " tapes",
            " were",
            " shel",
            "ved",
            " and",
            " the",
            " group",
            " took",
            " a",
            " break",
            " for",
            " the",
            " summer",
            ".",
            "Back",
            " in",
            " the",
            " studio",
            " again",
            " in",
            " early",
            " August",
            ",",
            " the",
            " group",
            " had",
            " changed",
            " plans",
            " for",
            " the",
            " rest",
            " of",
            " the",
            " year",
            ":",
            " they",
            " settled",
            " for",
            " a",
            " Christmas",
            " release",
            " of",
            " a",
            " double",
            " album",
            " compilation",
            " of",
            " all",
            " their",
            " past",
            " single",
            " releases"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.112,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " English",
            " director",
            " and",
            " production",
            " manager",
            " (",
            "d",
            ".",
            " ",
            "199",
            "3",
            ")",
            "194",
            "6",
            " –",
            " Allan",
            " Hold",
            "sworth",
            ",",
            " English",
            " guitarist",
            ",",
            " songwriter",
            ",",
            " and",
            " producer",
            " (",
            "d",
            ".",
            " ",
            "201",
            "7",
            ")",
            "194",
            "7",
            " –",
            " Rad",
            "h",
            "ia",
            " Cous",
            "ot",
            ",",
            " French",
            " computer",
            " scientist",
            " and",
            " academic",
            " (",
            "d",
            ".",
            " ",
            "201",
            "4",
            ")",
            "194",
            "9",
            " –",
            " D",
            "ino",
            " Bravo",
            ",",
            " Italian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " V",
            "ulture",
            " and",
            " Mor",
            "bi",
            "us",
            " [#",
            "622",
            "–",
            "625",
            ";",
            " Web",
            " of",
            " Spider",
            "-Man",
            " (",
            "vol",
            ".",
            " ",
            "2",
            ")",
            " #",
            "2",
            ",",
            " ",
            "5",
            " (",
            "V",
            "ulture",
            " story",
            ")]",
            " ()",
            "The",
            " Ga",
            "untlet",
            " Book",
            " ",
            "4",
            ":",
            " J",
            "ugg",
            "ernaut",
            " [#",
            "229",
            "–",
            "230",
            ",",
            " ",
            "626",
            "–",
            "629",
            "]",
            " ()",
            "The",
            " Ga",
            "untlet",
            " Book",
            " ",
            "5",
            ":",
            " L",
            "izard"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.057,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.356,
            -0.0,
            0.086,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " While",
            " the",
            " other",
            " cities",
            " again",
            " hes",
            "itated",
            ",",
            " The",
            "bes",
            " decided",
            " to",
            " fight",
            ".",
            " The",
            " The",
            "ban",
            " resistance",
            " was",
            " ineffective",
            ",",
            " and",
            " Alexander",
            " raz",
            "ed",
            " the",
            " city",
            " and",
            " divided",
            " its",
            " territory",
            " between",
            " the",
            " other",
            " Bo",
            "e",
            "ot",
            "ian",
            " cities",
            ".",
            " The",
            " end",
            " of",
            " The",
            "bes",
            " c",
            "owed",
            " Athens",
            ",",
            " leaving",
            " all",
            " of",
            " Greece",
            " temporarily",
            " at",
            " peace",
            ".",
            " Alexander",
            " then",
            " set",
            " out",
            " on",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.245,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.177,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " library",
            " and",
            " writings",
            " went",
            " to",
            " The",
            "op",
            "hr",
            "ast",
            "us",
            " (",
            "A",
            "rist",
            "otle",
            "'s",
            " successor",
            " as",
            " head",
            " of",
            " the",
            " Ly",
            "cae",
            "um",
            " and",
            " the",
            " Per",
            "ip",
            "at",
            "etic",
            " school",
            ").",
            " After",
            " the",
            " death",
            " of",
            " The",
            "op",
            "hr",
            "ast",
            "us",
            ",",
            " the",
            " per",
            "ip",
            "at",
            "etic",
            " library",
            " went",
            " to",
            " Ne",
            "le",
            "us",
            " of",
            " S",
            "ce",
            "ps",
            "is",
            ".",
            "Some",
            " time",
            " later",
            ",",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.08,
            0.231,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.128,
            0.077,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "164",
            "8",
            " and",
            " the",
            " Peace",
            " of",
            " N",
            "ij",
            "m",
            "egen",
            " in",
            " ",
            "167",
            "8",
            ",",
            " there",
            " were",
            " ",
            "30",
            " years",
            " of",
            " crisis",
            " in",
            " the",
            " Dutch",
            " Ant",
            "illes",
            " and",
            " the",
            " entire",
            " Caribbean",
            " region",
            ".",
            " By",
            " ",
            "164",
            "8",
            ",",
            " C",
            "ura",
            "Ã§",
            "ao",
            " had",
            " lost",
            " its",
            " importance",
            " as",
            " a",
            " military",
            " outpost",
            ".",
            " Governor",
            " Peter",
            " St",
            "uy",
            "ves",
            "ant",
            " had",
            " a",
            " plan",
            " to",
            " strengthen",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.046,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.216,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " world",
            ";",
            " he",
            " gathered",
            " botanical",
            " specimens",
            " and",
            " ran",
            " experiments",
            " at",
            " an",
            " early",
            " age",
            ".",
            " His",
            " best",
            " friend",
            " was",
            " Ben",
            " Her",
            "d",
            "man",
            ",",
            " a",
            " neighbour",
            " whose",
            " family",
            " operated",
            " a",
            " flour",
            " mill",
            ".",
            " At",
            " the",
            " age",
            " of",
            " ",
            "12",
            ",",
            " Bell",
            " built",
            " a",
            " homemade",
            " device",
            " that",
            " combined",
            " rotating",
            " padd",
            "les",
            " with",
            " sets",
            " of",
            " nail",
            " brushes",
            ",",
            " creating",
            " a",
            " simple",
            " de",
            "hus",
            "king",
            " machine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            0.205,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            0.01,
            -0.0,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.204,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            0.029,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "H",
            ")",
            " and",
            " ar",
            "yl",
            " ket",
            "ones",
            " to",
            " tertiary",
            " car",
            "bin",
            "ols",
            " (",
            "Ar",
            "'",
            "2",
            "C",
            "(A",
            "r",
            ")",
            "OH",
            ").",
            " Finally",
            ",",
            " they",
            " may",
            " be",
            " used",
            " to",
            " synthes",
            "ise",
            " other",
            " organ",
            "omet",
            "al",
            "lic",
            " compounds",
            " through",
            " metal",
            "-h",
            "al",
            "ogen",
            " exchange",
            ".",
            "He",
            "avier",
            " alk",
            "ali",
            " metals",
            " ",
            "Unlike",
            " the",
            " organ",
            "olith",
            "ium",
            " compounds",
            ",",
            " the",
            " organ",
            "omet",
            "al",
            "lic",
            " compounds"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            0.076,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " a",
            " believer",
            " in",
            " spiritual",
            " c",
            "ures",
            ".",
            " This",
            " conversion",
            " came",
            " about",
            " after",
            " he",
            " witnessed",
            " the",
            " inexp",
            "licable",
            " healing",
            " of",
            " Marie",
            " Bail",
            "ly",
            ".",
            " The",
            " Catholic",
            " journal",
            " Le",
            " Nou",
            "vell",
            "iste",
            " reported",
            " that",
            " she",
            " identified",
            " Car",
            "rel",
            " as",
            " the",
            " principal",
            " witness",
            " of",
            " her",
            " cure",
            ".",
            " Despite",
            " facing",
            " opposition",
            " from",
            " his",
            " peers",
            " in",
            " the",
            " medical",
            " community",
            ",",
            " Car",
            "rel",
            " refused",
            " to",
            " dismiss",
            " a",
            " supernatural"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " space",
            " bar",
            " of",
            " a",
            " keyboard",
            ".",
            " Since",
            " the",
            " space",
            " character",
            " is",
            " considered",
            " an",
            " invisible",
            " graphic",
            " (",
            "rather",
            " than",
            " a",
            " control",
            " character",
            ")",
            " it",
            " is",
            " listed",
            " in",
            " the",
            " table",
            " below",
            " instead",
            " of",
            " in",
            " the",
            " previous",
            " section",
            ".",
            "Code",
            " ",
            "7",
            "F",
            "hex",
            " corresponds",
            " to",
            " the",
            " non",
            "-print",
            "able",
            " \"",
            "delete",
            "\"",
            " (",
            "DEL",
            ")",
            " control",
            " character",
            " and",
            " is",
            " therefore",
            " omitted",
            " from",
            " this",
            " chart"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " including",
            " one",
            " of",
            " Orwell",
            "'s",
            " own",
            ",",
            " Victor",
            " G",
            "oll",
            "anc",
            "z",
            ",",
            " which",
            " delayed",
            " its",
            " publication",
            ".",
            " It",
            " became",
            " a",
            " great",
            " commercial",
            " success",
            " when",
            " it",
            " did",
            " appear",
            " as",
            " international",
            " relations",
            " and",
            " public",
            " opinion",
            " were",
            " transformed",
            " as",
            " the",
            " wartime",
            " alliance",
            " gave",
            " way",
            " to",
            " the",
            " Cold",
            " War",
            ".",
            "Time",
            " magazine",
            " chose",
            " the",
            " book",
            " as",
            " one",
            " of",
            " the",
            " ",
            "100",
            " best",
            " English",
            "-language",
            " novels",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "aires",
    "InParameter",
    "edy",
    "Ð»Ð¸ÑĨ",
    "ORE"
  ],
  "bottom_logits": [
    "elden",
    "ur",
    "imenti",
    "rett",
    "nton"
  ],
  "act_min": -0.0,
  "act_max": 0.863
}