{
  "index": 9951,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " geometry",
            " was",
            " complete",
            ",",
            " and",
            " he",
            " applied",
            " his",
            " new",
            " theory",
            " not",
            " just",
            " to",
            " the",
            " behavior",
            " of",
            " the",
            " Sun",
            " as",
            " a",
            " gravitational",
            " lens",
            " but",
            " also",
            " to",
            " another",
            " astronomical",
            " phenomenon",
            ",",
            " the",
            " pre",
            "cession",
            " of",
            " the",
            " per",
            "ih",
            "el",
            "ion",
            " of",
            " Mercury",
            " (",
            "a",
            " slow",
            " drift",
            " in",
            " the",
            " point",
            " in",
            " Mercury",
            "'s",
            " ellipt",
            "ical",
            " orbit",
            " at",
            " which",
            " it",
            " approaches",
            " the",
            " Sun",
            " most",
            " closely",
            ").",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " M",
            "atura",
            " (",
            "grad",
            "uation",
            " for",
            " the",
            " successful",
            " completion",
            " of",
            " higher",
            " secondary",
            " schooling",
            ")",
            " awarded",
            " to",
            " him",
            " in",
            " the",
            " September",
            " of",
            " that",
            " year",
            " acknowledged",
            " him",
            " to",
            " have",
            " performed",
            " well",
            " across",
            " most",
            " of",
            " the",
            " curriculum",
            ",",
            " allot",
            "ting",
            " him",
            " a",
            " top",
            " grade",
            " of",
            " ",
            "6",
            " for",
            " history",
            ",",
            " physics",
            ",",
            " algebra",
            ",",
            " geometry",
            ",",
            " and",
            " descriptive",
            " geometry",
            ".",
            " At",
            " seventeen",
            ",",
            " he",
            " enrolled",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " later",
            " on",
            " in",
            " human",
            " history",
            ".",
            " Tech",
            "ne",
            " included",
            " painting",
            ",",
            " sculpt",
            "ing",
            " and",
            " music",
            ",",
            " but",
            " also",
            " cooking",
            ",",
            " medicine",
            ",",
            " hor",
            "sem",
            "anship",
            ",",
            " geometry",
            ",",
            " carp",
            "entry",
            ",",
            " prophecy",
            ",",
            " and",
            " farming",
            ",",
            " etc",
            ".",
            "New",
            " Crit",
            "icism",
            " and",
            " the",
            " \"",
            "int",
            "ention",
            "al",
            " fall",
            "acy",
            "\"",
            "Following",
            " Duch",
            "amp",
            " during",
            " the",
            " first",
            " half",
            " of",
            " the",
            " ",
            "20",
            "th",
            " century"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " the",
            "ore",
            "ms",
            " of",
            " geometry",
            " on",
            " par",
            " with",
            " scientific",
            " facts",
            ".",
            " As",
            " such",
            ",",
            " they",
            " developed",
            " and",
            " used",
            " the",
            " log",
            "ico",
            "-d",
            "ed",
            "uctive",
            " method",
            " as",
            " a",
            " means",
            " of",
            " avoiding",
            " error",
            ",",
            " and",
            " for",
            " struct",
            "uring",
            " and",
            " communicating",
            " knowledge",
            ".",
            " Aristotle",
            "'s",
            " posterior",
            " analytics",
            " is",
            " a",
            " definitive",
            " exposition",
            " of",
            " the",
            " classical",
            " view",
            ".",
            "An",
            " \"",
            "ax",
            "iom",
            "\",",
            " in",
            " classical",
            " terminology",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " this",
            " magnitude",
            " can",
            ",",
            " depending",
            " on",
            " the",
            " exact",
            " geometry",
            " of",
            " the",
            " impact",
            ",",
            " cause",
            " it",
            " to",
            " miss",
            " the",
            " Earth",
            ".",
            "\"",
            "Project",
            " I",
            "car",
            "us",
            "\"",
            " was",
            " one",
            " of",
            " the",
            " first",
            " projects",
            " designed",
            " in",
            " ",
            "196",
            "7",
            " as",
            " a",
            " contingency",
            " plan",
            " in",
            " case",
            " of",
            " collision",
            " with",
            " ",
            "156",
            "6",
            " I",
            "car",
            "us",
            ".",
            " The",
            " plan",
            " relied",
            " on",
            " the",
            " new",
            " Saturn",
            " V",
            " rocket"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            "master",
            "ed",
            " integral",
            " and",
            " differential",
            " calculus",
            "\"",
            " while",
            " still",
            " just",
            " fourteen",
            ".",
            " His",
            " love",
            " of",
            " algebra",
            " and",
            " geometry",
            " was",
            " so",
            " great",
            " that",
            " at",
            " twelve",
            ",",
            " he",
            " was",
            " already",
            " confident",
            " that",
            " nature",
            " could",
            " be",
            " understood",
            " as",
            " a",
            " \"",
            "math",
            "ematic",
            "al",
            " structure",
            "\".",
            "At",
            " thirteen",
            ",",
            " when",
            " his",
            " range",
            " of",
            " enthus",
            "ias",
            "ms",
            " had",
            " broad",
            "ened",
            " to",
            " include",
            " music",
            " and",
            " philosophy",
            ",",
            " Einstein",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " compute",
            " the",
            " volumes",
            " of",
            " boxes",
            ",",
            " columns",
            " and",
            " py",
            "ramids",
            ".",
            " They",
            " understood",
            " basic",
            " concepts",
            " of",
            " algebra",
            " and",
            " geometry",
            ",",
            " and",
            " could",
            " solve",
            " simple",
            " sets",
            " of",
            " simultaneous",
            " equations",
            ".",
            "Math",
            "ematic",
            "al",
            " notation",
            " was",
            " decimal",
            ",",
            " and",
            " based",
            " on",
            " hier",
            "og",
            "lyph",
            "ic",
            " signs",
            " for",
            " each",
            " power",
            " of",
            " ten",
            " up",
            " to",
            " one",
            " million",
            ".",
            " Each",
            " of",
            " these",
            " could",
            " be",
            " written",
            " as",
            " many",
            " times"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " compute",
            " the",
            " volumes",
            " of",
            " boxes",
            ",",
            " columns",
            " and",
            " py",
            "ramids",
            ".",
            " They",
            " understood",
            " basic",
            " concepts",
            " of",
            " algebra",
            " and",
            " geometry",
            ",",
            " and",
            " could",
            " solve",
            " simple",
            " sets",
            " of",
            " simultaneous",
            " equations",
            ".",
            "Math",
            "ematic",
            "al",
            " notation",
            " was",
            " decimal",
            ",",
            " and",
            " based",
            " on",
            " hier",
            "og",
            "lyph",
            "ic",
            " signs",
            " for",
            " each",
            " power",
            " of",
            " ten",
            " up",
            " to",
            " one",
            " million",
            ".",
            " Each",
            " of",
            " these",
            " could",
            " be",
            " written",
            " as",
            " many",
            " times"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " them",
            ".",
            " Structural",
            " formula",
            "e",
            " that",
            " represent",
            " the",
            " bonds",
            " as",
            " being",
            " at",
            " right",
            " angles",
            " to",
            " one",
            " another",
            ",",
            " while",
            " both",
            " common",
            " and",
            " useful",
            ",",
            " do",
            " not",
            " accurately",
            " depict",
            " the",
            " geometry",
            ".",
            "Con",
            "formation",
            "The",
            " structural",
            " formula",
            " and",
            " the",
            " bond",
            " angles",
            " are",
            " not",
            " usually",
            " sufficient",
            " to",
            " completely",
            " describe",
            " the",
            " geometry",
            " of",
            " a",
            " molecule",
            ".",
            " There",
            " is",
            " a",
            " further",
            " degree",
            " of",
            " freedom",
            " for",
            " each"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " them",
            ".",
            " Structural",
            " formula",
            "e",
            " that",
            " represent",
            " the",
            " bonds",
            " as",
            " being",
            " at",
            " right",
            " angles",
            " to",
            " one",
            " another",
            ",",
            " while",
            " both",
            " common",
            " and",
            " useful",
            ",",
            " do",
            " not",
            " accurately",
            " depict",
            " the",
            " geometry",
            ".",
            "Con",
            "formation",
            "The",
            " structural",
            " formula",
            " and",
            " the",
            " bond",
            " angles",
            " are",
            " not",
            " usually",
            " sufficient",
            " to",
            " completely",
            " describe",
            " the",
            " geometry",
            " of",
            " a",
            " molecule",
            ".",
            " There",
            " is",
            " a",
            " further",
            " degree",
            " of",
            " freedom",
            " for",
            " each"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " them",
            ".",
            " Structural",
            " formula",
            "e",
            " that",
            " represent",
            " the",
            " bonds",
            " as",
            " being",
            " at",
            " right",
            " angles",
            " to",
            " one",
            " another",
            ",",
            " while",
            " both",
            " common",
            " and",
            " useful",
            ",",
            " do",
            " not",
            " accurately",
            " depict",
            " the",
            " geometry",
            ".",
            "Con",
            "formation",
            "The",
            " structural",
            " formula",
            " and",
            " the",
            " bond",
            " angles",
            " are",
            " not",
            " usually",
            " sufficient",
            " to",
            " completely",
            " describe",
            " the",
            " geometry",
            " of",
            " a",
            " molecule",
            ".",
            " There",
            " is",
            " a",
            " further",
            " degree",
            " of",
            " freedom",
            " for",
            " each"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "ness",
            " and",
            " k",
            "urt",
            "osis",
            ",",
            " each",
            " of",
            " which",
            " can",
            " save",
            " substantial",
            " computer",
            " memory",
            " requirements",
            " and",
            " CPU",
            " time",
            " in",
            " certain",
            " applications",
            ".",
            " The",
            " first",
            " approach",
            " is",
            " to",
            " compute",
            " the",
            " statistical",
            " moments",
            " by",
            " separating",
            " the",
            " data",
            " into",
            " bins",
            " and",
            " then",
            " computing",
            " the",
            " moments",
            " from",
            " the",
            " geometry",
            " of",
            " the",
            " resulting",
            " histogram",
            ",",
            " which",
            " effectively",
            " becomes",
            " a",
            " one",
            "-pass",
            " algorithm",
            " for",
            " higher",
            " moments",
            ".",
            " One",
            " benefit"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            0.028,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " One",
            " can",
            " assume",
            " that",
            " exactly",
            " one",
            " parallel",
            " through",
            " a",
            " point",
            " outside",
            " a",
            " line",
            " exists",
            ",",
            " or",
            " that",
            " infinitely",
            " many",
            " exist",
            ".",
            " This",
            " choice",
            " gives",
            " us",
            " two",
            " alternative",
            " forms",
            " of",
            " geometry",
            " in",
            " which",
            " the",
            " interior",
            " angles",
            " of",
            " a",
            " triangle",
            " add",
            " up",
            " to",
            " exactly",
            " ",
            "180",
            " degrees",
            " or",
            " less",
            ",",
            " respectively",
            ",",
            " and",
            " are",
            " known",
            " as",
            " Eu",
            "clidean",
            " and",
            " hyper",
            "b",
            "olic",
            " geomet",
            "ries",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "163",
            "7",
            ",",
            " Ren",
            "Ã©",
            " Desc",
            "art",
            "es",
            " \"",
            "in",
            "vented",
            " the",
            " convention",
            " of",
            " representing",
            " unknown",
            "s",
            " in",
            " equations",
            " by",
            " x",
            ",",
            " y",
            ",",
            " and",
            " z",
            ",",
            " and",
            " known",
            "s",
            " by",
            " a",
            ",",
            " b",
            ",",
            " and",
            " c",
            "\",",
            " and",
            " this",
            " convention",
            " is",
            " still",
            " often",
            " followed",
            ",",
            " especially",
            " in",
            " elementary",
            " algebra",
            ".",
            "In",
            " geometry",
            ",",
            " capital",
            " A",
            ",",
            " B",
            ",",
            " C",
            " etc",
            ".",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "es",
            "For",
            " an",
            " ellipse",
            ",",
            " the",
            " aspect",
            " ratio",
            " denotes",
            " the",
            " ratio",
            " of",
            " the",
            " major",
            " axis",
            " to",
            " the",
            " minor",
            " axis",
            ".",
            " An",
            " ellipse",
            " with",
            " an",
            " aspect",
            " ratio",
            " of",
            " ",
            "1",
            ":",
            "1",
            " is",
            " a",
            " circle",
            ".",
            "Aspect",
            " ratios",
            " of",
            " general",
            " shapes",
            "In",
            " geometry",
            ",",
            " there",
            " are",
            " several",
            " alternative",
            " definitions",
            " to",
            " aspect",
            " ratios",
            " of",
            " general",
            " compact",
            " sets",
            " in",
            " a",
            " d",
            "-dimensional",
            " space",
            ":"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "196",
            "8",
            ",",
            " Animal",
            " Farm",
            " had",
            " been",
            " widely",
            " deemed",
            " a",
            " \"",
            "problem",
            " book",
            "\".",
            " A",
            " censorship",
            " survey",
            " conducted",
            " in",
            " De",
            "Kal",
            "b",
            " County",
            ",",
            " Georgia",
            ",",
            " relating",
            " to",
            " the",
            " years",
            " ",
            "197",
            "9",
            "–",
            "198",
            "2",
            ",",
            " revealed",
            " that",
            " many",
            " schools",
            " had",
            " attempted",
            " to",
            " limit",
            " access",
            " to",
            " Animal",
            " Farm",
            " due",
            " to",
            " its",
            " \"",
            "political",
            " theories",
            "\".",
            " A",
            " superintendent",
            " in",
            " Bay",
            " County",
            ",",
            " Florida"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " G",
            "ork",
            "y",
            "'s",
            " funeral",
            " and",
            " to",
            " tour",
            " the",
            " Soviet",
            " Union",
            " as",
            " a",
            " guest",
            " of",
            " the",
            " Soviet",
            " Union",
            " of",
            " Writers",
            ".",
            " He",
            " encountered",
            " censorship",
            " of",
            " his",
            " speeches",
            " and",
            " was",
            " particularly",
            " disillusion",
            "ed",
            " with",
            " the",
            " state",
            " of",
            " culture",
            " under",
            " Soviet",
            " Commun",
            "ism",
            ".",
            " In",
            " his",
            " work",
            ",",
            " Ret",
            "our",
            " de",
            " L",
            "'",
            "U",
            ".R",
            ".S",
            ".S",
            ".",
            " (",
            "Return",
            " from",
            " the",
            " USSR",
            ",",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "Åį",
            " O",
            "zu",
            ",",
            " who",
            " champion",
            "ed",
            " the",
            " film",
            ",",
            " that",
            " Sans",
            "hiro",
            " Sug",
            "ata",
            " was",
            " finally",
            " accepted",
            " for",
            " release",
            " on",
            " March",
            " ",
            "25",
            ",",
            " ",
            "194",
            "3",
            ".",
            " (",
            "K",
            "uros",
            "awa",
            " had",
            " just",
            " turned",
            " ",
            "33",
            ".)",
            " The",
            " movie",
            " became",
            " both",
            " a",
            " critical",
            " and",
            " commercial",
            " success",
            ".",
            " Nevertheless",
            ",",
            " the",
            " censorship",
            " office",
            " would",
            " later",
            " decide",
            " to",
            " cut",
            " out",
            " some",
            " ",
            "18"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " location",
            " in",
            " Yok",
            "oh",
            "ama",
            " in",
            " December",
            " ",
            "194",
            "2",
            ".",
            " Production",
            " proceeded",
            " smoothly",
            ",",
            " but",
            " getting",
            " the",
            " completed",
            " film",
            " past",
            " the",
            " c",
            "ensors",
            " was",
            " an",
            " entirely",
            " different",
            " matter",
            ".",
            " The",
            " censorship",
            " office",
            " considered",
            " the",
            " work",
            " to",
            " be",
            " objection",
            "ably",
            " \"",
            "British",
            "-American",
            "\"",
            " by",
            " the",
            " standards",
            " of",
            " wartime",
            " Japan",
            ",",
            " and",
            " it",
            " was",
            " only",
            " through",
            " the",
            " intervention",
            " of",
            " director",
            " Yas",
            "uj",
            "ir"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ",",
            " was",
            " renamed",
            " Cedar",
            " Crest",
            " Elementary",
            ".",
            " Johnston",
            " Middle",
            " School",
            " in",
            " Houston",
            ",",
            " Texas",
            ",",
            " was",
            " also",
            " renamed",
            " Meyer",
            "land",
            " Middle",
            " School",
            ".",
            " Three",
            " other",
            " elementary",
            " schools",
            " named",
            " for",
            " Confederate",
            " veterans",
            " were",
            " renamed",
            " simultaneously",
            ".",
            "See",
            " also",
            " Albert",
            " Sidney",
            " Johnston",
            " High",
            " School",
            ",",
            " a",
            " def",
            "unct",
            " public",
            " high",
            " school",
            " in",
            " Austin",
            ",",
            " Texas",
            " Statue",
            " of",
            " Albert",
            " Sidney"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Dallas",
            ",",
            " Texas",
            ".",
            "Music",
            " ",
            "War",
            "hol",
            " strongly",
            " influenced",
            " the",
            " new",
            " wave",
            "/p",
            "unk",
            " rock",
            " band",
            " De",
            "vo",
            ",",
            " as",
            " well",
            " as",
            " David",
            " Bowie",
            ".",
            " Bowie",
            " recorded",
            " a",
            " song",
            " called",
            " \"",
            "Andy",
            " War",
            "hol",
            "\"",
            " for",
            " his",
            " ",
            "197",
            "1",
            " album",
            " H",
            "unky",
            " D",
            "ory",
            ".",
            " Lou",
            " Reed",
            " wrote",
            " the",
            " song",
            " \"",
            "Andy",
            "'s",
            " Chest",
            "\",",
            " about",
            " Valerie",
            " Sol",
            "anas",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            " Dallas",
            ",",
            " Low",
            "nd",
            "es",
            ",",
            " Mare",
            "ngo",
            " and",
            " Perry",
            ".\"",
            "In",
            " ",
            "197",
            "2",
            ",",
            " for",
            " the",
            " first",
            " time",
            " since",
            " ",
            "190",
            "1",
            ",",
            " the",
            " legislature",
            " completed",
            " the",
            " congressional",
            " red",
            "istrict",
            "ing",
            " based",
            " on",
            " the",
            " dec",
            "ennial",
            " census",
            ".",
            " This",
            " benefited",
            " the",
            " urban",
            " areas",
            " that",
            " had",
            " developed",
            ",",
            " as",
            " well",
            " as",
            " all",
            " in",
            " the",
            " population",
            " who",
            " had",
            " been",
            " under",
            "represented",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " which",
            " includes",
            " the",
            " phrase",
            " \"",
            "do",
            " the",
            " ultra",
            "-viol",
            "ent",
            "\".",
            " The",
            " term",
            "'s",
            " association",
            " with",
            " aesthetic",
            " violence",
            " has",
            " led",
            " to",
            " its",
            " use",
            " in",
            " the",
            " media",
            ".",
            "B",
            "anning",
            " and",
            " censorship",
            " history",
            " in",
            " the",
            " US",
            "In",
            " ",
            "197",
            "6",
            ",",
            " A",
            " Clock",
            "work",
            " Orange",
            " was",
            " removed",
            " from",
            " an",
            " Aurora",
            ",",
            " Colorado",
            " high",
            " school",
            " because",
            " of",
            " \"",
            "obj",
            "ection",
            "able",
            " language",
            "\".",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " non",
            "comm",
            "ut",
            "ative",
            " geometry",
            ".",
            " He",
            " is",
            " a",
            " professor",
            " at",
            " the",
            " ,",
            " ,",
            " Ohio",
            " State",
            " University",
            " and",
            " Vanderbilt",
            " University",
            ".",
            " He",
            " was",
            " awarded",
            " the",
            " Fields",
            " Medal",
            " in",
            " ",
            "198",
            "2",
            ".",
            "Career",
            "Al",
            "ain",
            " Con",
            "nes",
            " attended",
            " high",
            " school",
            " at",
            " ",
            " in",
            " Marseille",
            ",",
            " and",
            " was",
            " then",
            " a",
            " student",
            " of",
            " the",
            " classes",
            " prÃ©",
            "par",
            "ato",
            "ires",
            " in",
            " .",
            " Between",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " electoral",
            " code",
            ",",
            " and",
            " the",
            " representation",
            " of",
            " women",
            " in",
            " elected",
            " bodies",
            ".",
            " In",
            " April",
            " ",
            "201",
            "1",
            ",",
            " Bout",
            "ef",
            "li",
            "ka",
            " promised",
            " further",
            " constitutional",
            " and",
            " political",
            " reform",
            ".",
            " However",
            ",",
            " elections",
            " are",
            " routinely",
            " criticised",
            " by",
            " opposition",
            " groups",
            " as",
            " unfair",
            " and",
            " international",
            " human",
            " rights",
            " groups",
            " say",
            " that",
            " media",
            " censorship",
            " and",
            " harassment",
            " of",
            " political",
            " opponents",
            " continue",
            ".",
            "On",
            " ",
            "2",
            " April",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " greatly",
            " impressed",
            " K",
            "uros",
            "awa",
            ",",
            " but",
            " managed",
            " to",
            " alien",
            "ate",
            " most",
            " of",
            " the",
            " other",
            " judges",
            ".",
            "D",
            "run",
            "ken",
            " Angel",
            " is",
            " often",
            " considered",
            " the",
            " director",
            "'s",
            " first",
            " major",
            " work",
            ".",
            " Although",
            " the",
            " script",
            ",",
            " like",
            " all",
            " of",
            " K",
            "uros",
            "awa",
            "'s",
            " occupation",
            "-era",
            " works",
            ",",
            " had",
            " to",
            " go",
            " through",
            " re",
            "writes",
            " due",
            " to",
            " American",
            " censorship",
            ",",
            " K",
            "uros",
            "awa",
            " felt",
            " that",
            " this"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " examples",
            " of",
            " a",
            " detailed",
            " murder",
            " in",
            " the",
            " history",
            " of",
            " film",
            "\".",
            " Both",
            " actors",
            ",",
            " Barbara",
            " Leigh",
            "-H",
            "unt",
            " and",
            " Anna",
            " Mas",
            "sey",
            ",",
            " refused",
            " to",
            " do",
            " the",
            " scenes",
            ",",
            " so",
            " models",
            " were",
            " used",
            " instead",
            ".",
            " Bi",
            "ographers",
            " have",
            " noted",
            " that",
            " Hitch",
            "cock",
            " had",
            " always",
            " pushed",
            " the",
            " limits",
            " of",
            " film",
            " censorship",
            ",",
            " often",
            " managing",
            " to",
            " fool",
            " Joseph",
            " B",
            "reen",
            ",",
            " the",
            " head",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " angle",
            " of",
            " sweep",
            ",",
            " and",
            " any",
            " variations",
            " along",
            " the",
            " span",
            " (",
            "including",
            " the",
            " important",
            " class",
            " of",
            " delta",
            " wings",
            ").",
            " Location",
            " of",
            " the",
            " horizontal",
            " stabil",
            "izer",
            ",",
            " if",
            " any",
            ".",
            " D",
            "ih",
            "edral",
            " angle",
            "Âł",
            "—",
            " positive",
            ",",
            " zero",
            ",",
            " or",
            " negative",
            " (",
            "anh",
            "edral",
            ").",
            "A",
            " variable",
            " geometry",
            " aircraft",
            " can",
            " change",
            " its",
            " wing",
            " configuration",
            " during",
            " flight",
            ".",
            "A",
            " flying",
            " wing",
            " has",
            " no"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " these",
            " basic",
            " assumptions",
            ".",
            " However",
            ",",
            " the",
            " interpretation",
            " of",
            " mathematical",
            " knowledge",
            " has",
            " changed",
            " from",
            " ancient",
            " times",
            " to",
            " the",
            " modern",
            ",",
            " and",
            " consequently",
            " the",
            " terms",
            " axiom",
            " and",
            " post",
            "ulate",
            " hold",
            " a",
            " slightly",
            " different",
            " meaning",
            " for",
            " the",
            " present",
            " day",
            " mathematic",
            "ian",
            ",",
            " than",
            " they",
            " did",
            " for",
            " Aristotle",
            " and",
            " Eu",
            "clid",
            ".",
            "The",
            " ancient",
            " Greeks",
            " considered",
            " geometry",
            " as",
            " just",
            " one",
            " of",
            " several",
            " sciences",
            ",",
            " and",
            " held"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.459,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "External",
            " links",
            " Al",
            "ain",
            " Con",
            "nes",
            " Official",
            " Web",
            " Site",
            " containing",
            " downloadable",
            " papers",
            ",",
            " and",
            " his",
            " book",
            " Non",
            "-comm",
            "ut",
            "ative",
            " geometry",
            ",",
            " .",
            " ",
            " Al",
            "ain",
            " Con",
            "nes",
            "'",
            " Standard",
            " Model",
            " An",
            " interview",
            " with",
            " Al",
            "ain",
            " Con",
            "nes",
            " and",
            " a",
            " discussion",
            " about",
            " it",
            " ",
            " ",
            "194",
            "7",
            " births",
            "Living",
            " people",
            "People",
            " from",
            " Drag",
            "u",
            "ign",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.457,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " standard",
            " structure",
            " is",
            " ",
            " where",
            " ",
            " is",
            " the",
            " set",
            " of",
            " natural",
            " numbers",
            ",",
            " ",
            " is",
            " the",
            " successor",
            " function",
            " and",
            " ",
            " is",
            " naturally",
            " interpreted",
            " as",
            " the",
            " number",
            " ",
            "0",
            ".",
            "Eu",
            "clidean",
            " geometry",
            "Probably",
            " the",
            " oldest",
            ",",
            " and",
            " most",
            " famous",
            ",",
            " list",
            " of",
            " ax",
            "ioms",
            " are",
            " the",
            " ",
            "4",
            " +",
            " ",
            "1",
            " Eu",
            "clid",
            "'s",
            " post",
            "ulates",
            " of",
            " plane",
            " geometry",
            ".",
            " The",
            " ax"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-M",
            "edit",
            "err",
            "anean",
            " Human",
            " Rights",
            " Monitor",
            " issued",
            " a",
            " report",
            " regarding",
            " violation",
            " of",
            " media",
            " freedom",
            " in",
            " Algeria",
            ".",
            " It",
            " clarified",
            " that",
            " the",
            " Alger",
            "ian",
            " government",
            " imposed",
            " restrictions",
            " on",
            " freedom",
            " of",
            " the",
            " press",
            ";",
            " expression",
            ";",
            " and",
            " right",
            " to",
            " peaceful",
            " demonstration",
            ",",
            " protest",
            " and",
            " assembly",
            " as",
            " well",
            " as",
            " intensified",
            " censorship",
            " of",
            " the",
            " media",
            " and",
            " websites",
            ".",
            " Due",
            " to",
            " the",
            " fact",
            " that",
            " the",
            " journalists",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.224,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "alm",
            "ud",
            ",",
            " said",
            " that",
            " only",
            " a",
            " short",
            " time",
            " after",
            " he",
            " had",
            " given",
            " the",
            " twelve",
            " year",
            " old",
            " Einstein",
            " a",
            " geometry",
            " textbook",
            ",",
            " the",
            " boy",
            " \"",
            "had",
            " worked",
            " through",
            " the",
            " whole",
            " book",
            ".",
            " He",
            " there",
            "upon",
            " devoted",
            " himself",
            " to",
            " higher",
            " mathematics",
            "Âł",
            "...",
            " Soon",
            " the",
            " flight",
            " of",
            " his",
            " mathematical",
            " genius",
            " was",
            " so",
            " high",
            " I",
            " could",
            " not",
            " follow",
            ".\"",
            " Einstein",
            " recorded",
            " that",
            " he",
            " had",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " and",
            " might",
            " or",
            " might",
            " not",
            " be",
            " self",
            "-e",
            "vid",
            "ent",
            " in",
            " nature",
            " (",
            "e",
            ".g",
            ".,",
            " the",
            " parallel",
            " post",
            "ulate",
            " in",
            " Eu",
            "clidean",
            " geometry",
            ").",
            " To",
            " axiom",
            "at",
            "ize",
            " a",
            " system",
            " of",
            " knowledge",
            " is",
            " to",
            " show",
            " that",
            " its",
            " claims",
            " can",
            " be",
            " derived",
            " from",
            " a",
            " small",
            ",",
            " well",
            "-under",
            "stood",
            " set",
            " of",
            " sentences",
            " (",
            "the",
            " ax",
            "ioms",
            "),",
            " and",
            " there",
            " are",
            " typically",
            " many"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.44,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " lawsuit",
            " against",
            " Audi",
            " over",
            " the",
            " use",
            " of",
            " the",
            " letter",
            " \"",
            "Q",
            "\"",
            " as",
            " a",
            " model",
            " name",
            ".",
            "A",
            "udi",
            " is",
            " using",
            " the",
            " \"",
            "Q",
            "\"",
            " for",
            " the",
            " designation",
            " of",
            " their",
            " qu",
            "attro",
            " four",
            "-wheel",
            " drive",
            " system",
            ",",
            " used",
            " in",
            " production",
            " cars",
            " for",
            " over",
            " twenty",
            "-five",
            " years",
            " (",
            "A",
            "udi",
            "'s",
            " Qu",
            "attro",
            " trademark",
            " is",
            " actually",
            " an",
            " umbrella",
            " term",
            " for",
            " several",
            " types",
            " of",
            " four"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " fields",
            ",",
            " and",
            " Gal",
            "ois",
            " theory",
            ".",
            "This",
            " list",
            " could",
            " be",
            " expanded",
            " to",
            " include",
            " most",
            " fields",
            " of",
            " mathematics",
            ",",
            " including",
            " measure",
            " theory",
            ",",
            " erg",
            "odic",
            " theory",
            ",",
            " probability",
            ",",
            " representation",
            " theory",
            ",",
            " and",
            " differential",
            " geometry",
            ".",
            "Ar",
            "ithmetic",
            "The",
            " Pe",
            "ano",
            " ax",
            "ioms",
            " are",
            " the",
            " most",
            " widely",
            " used",
            " axiom",
            "at",
            "ization",
            " of",
            " first",
            "-order",
            " arithmetic",
            ".",
            " They",
            " are",
            " a",
            " set",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.418,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " form",
            " the",
            " upper",
            " layer",
            " in",
            " an",
            " alk",
            "ane",
            "–",
            "water",
            " mixture",
            ".",
            "M",
            "olecular",
            " geometry",
            "The",
            " molecular",
            " structure",
            " of",
            " the",
            " al",
            "kan",
            "es",
            " directly",
            " affects",
            " their",
            " physical",
            " and",
            " chemical",
            " characteristics",
            ".",
            " It",
            " is",
            " derived",
            " from",
            " the",
            " electron",
            " configuration",
            " of",
            " carbon",
            ",",
            " which",
            " has",
            " four",
            " val",
            "ence",
            " electrons",
            ".",
            " The",
            " carbon",
            " atoms",
            " in",
            " al",
            "kan",
            "es",
            " are",
            " described",
            " as",
            " sp",
            "3",
            " hybrids",
            ";"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.393,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " orgasm",
            ":",
            " \"",
            "When",
            " he",
            " wasn",
            "'t",
            " being",
            " Andy",
            " War",
            "hol",
            " and",
            " when",
            " you",
            " were",
            " just",
            " alone",
            " with",
            " him",
            " he",
            " was",
            " an",
            " incredibly",
            " generous",
            " and",
            " very",
            " kind",
            " person",
            ".",
            " What",
            " sed",
            "uced",
            " me",
            " was",
            " the",
            " Andy",
            " War",
            "hol",
            " who",
            " I",
            " saw",
            " alone",
            ".",
            " In",
            " fact",
            " when",
            " I",
            " was",
            " with",
            " him",
            " in",
            " public",
            " he",
            " kind",
            " of",
            " got",
            " on",
            " my",
            " nerves",
            "....",
            "I",
            "'d"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " mid",
            "-",
            "160",
            "0",
            "s",
            " because",
            " they",
            " were",
            " not",
            " paid",
            " regularly",
            ",",
            " and",
            " they",
            " repeatedly",
            " revolt",
            "ed",
            " against",
            " the",
            " p",
            "asha",
            ".",
            " As",
            " a",
            " result",
            ",",
            " the",
            " a",
            "gh",
            "a",
            " charged",
            " the",
            " p",
            "asha",
            " with",
            " corruption",
            " and",
            " incompetence",
            " and",
            " seized",
            " power",
            " in",
            " ",
            "165",
            "9",
            ".",
            "Pl",
            "ague",
            " had",
            " repeatedly",
            " struck",
            " the",
            " cities",
            " of",
            " North",
            " Africa",
            ".",
            " Alg",
            "iers",
            " lost",
            " between",
            " ",
            "30"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " exist",
            " in",
            " the",
            " midst",
            " of",
            " life",
            " which",
            " will",
            "s",
            " to",
            " live",
            ".'\"",
            " In",
            " nature",
            " one",
            " form",
            " of",
            " life",
            " must",
            " always",
            " prey",
            " upon",
            " another",
            ".",
            " However",
            ",",
            " human",
            " consciousness",
            " holds",
            " an",
            " awareness",
            " of",
            ",",
            " and",
            " sympathy",
            " for",
            ",",
            " the",
            " will",
            " of",
            " other",
            " beings",
            " to",
            " live",
            ".",
            " An",
            " ethical",
            " human",
            " strives",
            " to",
            " escape",
            " from",
            " this",
            " contradiction",
            " so",
            " far",
            " as",
            " possible",
            ".",
            "Though",
            " we",
            " cannot",
            " perfect"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " based",
            " on",
            " jurisdiction",
            ",",
            " but",
            " may",
            " include",
            " whether",
            " the",
            " pregnancy",
            " is",
            " a",
            " result",
            " of",
            " rape",
            " or",
            " incest",
            ",",
            " the",
            " fetus",
            "'",
            " development",
            " is",
            " impaired",
            ",",
            " the",
            " woman",
            "'s",
            " physical",
            " or",
            " mental",
            " well",
            "-being",
            " is",
            " endangered",
            ",",
            " or",
            " socioeconomic",
            " considerations",
            " make",
            " childbirth",
            " a",
            " hardship",
            ".",
            " In",
            " countries",
            " where",
            " abortion",
            " is",
            " banned",
            " entirely",
            ",",
            " such",
            " as",
            " Nicaragua",
            ",",
            " medical",
            " authorities",
            " have",
            " recorded",
            " rises",
            " in",
            " maternal"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.059,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " frequently",
            " referenced",
            " in",
            " popular",
            " accounts",
            " of",
            " the",
            " war",
            " and",
            " has",
            " much",
            " traction",
            " among",
            " Sou",
            "ther",
            "ners",
            ".",
            " Sou",
            "ther",
            "ners",
            " advocating",
            " se",
            "cession",
            " argued",
            " that",
            " just",
            " as",
            " each",
            " state",
            " had",
            " decided",
            " to",
            " join",
            " the",
            " Union",
            ",",
            " a",
            " state",
            " had",
            " the",
            " right",
            " to",
            " se",
            "cede",
            "—",
            "leave",
            " the",
            " Union",
            "—at",
            " any",
            " time",
            ".",
            " Nor",
            "ther",
            "ners",
            " (",
            "including",
            " pro",
            "-sl",
            "avery",
            " President",
            " Buchanan",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " theory",
            " has",
            " been",
            " criticized",
            " by",
            " some",
            ",",
            " such",
            " as",
            " Andr",
            "zej",
            " Z",
            "ab",
            "ors",
            "ki",
            " and",
            " Alan",
            " Kay",
            "e",
            ",",
            " as",
            " being",
            " too",
            " many",
            " extensions",
            " to",
            " be",
            " realistic",
            ",",
            " though",
            " Z",
            "yg",
            "mont",
            " Fra",
            "j",
            "zy",
            "ng",
            "ier",
            " and",
            " Erin",
            " Shay",
            " note",
            " that",
            " some",
            " Ch",
            "adic",
            " languages",
            " have",
            " as",
            " many",
            " as",
            " twelve",
            " extensions",
            ".",
            "\"N",
            "is",
            "ba",
            "\"",
            " derivation",
            "The",
            " so",
            "-called"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.221,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Each",
            " state",
            " had",
            " established",
            " internal",
            " distribution",
            " systems",
            ".",
            "Each",
            " of",
            " the",
            " thirteen",
            " colonies",
            " also",
            " had",
            " a",
            " long",
            "-established",
            " system",
            " of",
            " local",
            " militia",
            ",",
            " which",
            " were",
            " combat",
            "-tested",
            " in",
            " support",
            " of",
            " British",
            " regular",
            "s",
            " thirteen",
            " years",
            " before",
            " to",
            " secure",
            " an",
            " expanded",
            " British",
            " Empire",
            ".",
            " Together",
            ",",
            " these",
            " militias",
            " denied",
            " France",
            "'s",
            " claims",
            " to",
            " North",
            " America",
            " west",
            " of",
            " the",
            " Mississippi",
            " River",
            " in",
            " the",
            " French"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " residence",
            " Art",
            "istic",
            " freedom",
            " Cultural",
            " tourism",
            " Craft",
            "ivism",
            " Formal",
            " analysis",
            " History",
            " of",
            " art",
            " List",
            " of",
            " artistic",
            " media",
            " List",
            " of",
            " art",
            " techniques",
            " Mathematics",
            " and",
            " art",
            " Street",
            " art",
            " (",
            "or",
            " \"",
            "in",
            "dependent",
            " public",
            " art",
            "\")",
            " Outline",
            " of",
            " the",
            " visual",
            " arts",
            ",",
            " a",
            " guide",
            " to",
            " the",
            " subject",
            " of",
            " art",
            " presented",
            " as",
            " a",
            " tree",
            " structured",
            " list",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.215,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " Apollo",
            ".",
            " Apollo",
            ",",
            " through",
            " the",
            " Py",
            "th",
            "ia",
            ",",
            " commanded",
            " him",
            " to",
            " serve",
            " king",
            " Eur",
            "yst",
            "he",
            "us",
            " for",
            " twelve",
            " years",
            " and",
            " complete",
            " the",
            " ten",
            " tasks",
            " the",
            " king",
            " would",
            " give",
            " him",
            ".",
            " Only",
            " then",
            " would",
            " Al",
            "c",
            "ides",
            " be",
            " abs",
            "olved",
            " of",
            " his",
            " sin",
            ".",
            " Apollo",
            " also",
            " renamed",
            " him",
            " Her",
            "acles",
            ".",
            "To",
            " complete",
            " his",
            " third",
            " task",
            ",",
            " Her",
            "acles",
            " had",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            0.214,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "ment",
            " with",
            " dwind",
            "ling",
            " supplies",
            ",",
            " Cornwall",
            "is",
            " felt",
            " his",
            " situation",
            " was",
            " hopeless",
            " and",
            " on",
            " October",
            " ",
            "16",
            " sent",
            " emiss",
            "aries",
            " to",
            " General",
            " Washington",
            " to",
            " negotiate",
            " their",
            " surrender",
            ";",
            " after",
            " twelve",
            " hours",
            " of",
            " negotiations",
            ",",
            " the",
            " terms",
            " of",
            " surrender",
            " were",
            " finalized",
            " the",
            " following",
            " day",
            ".",
            " Responsibility",
            " for",
            " defeat",
            " was",
            " the",
            " subject",
            " of",
            " fierce",
            " public",
            " debate",
            " between",
            " Cornwall",
            "is",
            ",",
            " Clinton",
            ",",
            " and",
            " Ger"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.213,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " both",
            " the",
            " soup",
            " cans",
            " and",
            " War",
            "hol",
            "'s",
            " dollar",
            " paintings",
            ".",
            " On",
            " November",
            " ",
            "23",
            ",",
            " ",
            "196",
            "1",
            ",",
            " War",
            "hol",
            " wrote",
            " Lat",
            "ow",
            " a",
            " check",
            " for",
            " $",
            "50",
            " which",
            ",",
            " according",
            " to",
            " the",
            " ",
            "200",
            "9",
            " War",
            "hol",
            " biography",
            ",",
            " Pop",
            ",",
            " The",
            " Genius",
            " of",
            " War",
            "hol",
            ",",
            " was",
            " payment",
            " for",
            " coming",
            " up",
            " with",
            " the",
            " idea",
            " of",
            " the",
            " soup",
            " cans",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "5",
            ")",
            "190",
            "6",
            " –",
            " George",
            " Water",
            "house",
            ",",
            " English",
            "-New",
            " Zealand",
            " politician",
            ",",
            " ",
            "7",
            "th",
            " Prime",
            " Minister",
            " of",
            " New",
            " Zealand",
            " (",
            "b",
            ".",
            " ",
            "182",
            "4",
            ")",
            "191",
            "5",
            " –",
            " Jenn",
            "ie",
            " de",
            " la",
            " Mont",
            "agn",
            "ie",
            " Lo",
            "zier",
            ",",
            " American",
            " physician",
            " (",
            "b",
            ".",
            " ",
            "184",
            "1",
            ")",
            "192",
            "0",
            " –",
            " Stefan",
            " Bast",
            "yr",
            ",",
            " Polish",
            " pilot",
            " and",
            " author",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.11,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " Street",
            " Food",
            " and",
            " A",
            "arhus",
            " Central",
            " Food",
            " Market",
            " are",
            " two",
            " indoor",
            " food",
            " courts",
            " from",
            " ",
            "201",
            "6",
            " in",
            " the",
            " city",
            " centre",
            ",",
            " comprising",
            " a",
            " variety",
            " of",
            " street",
            " food",
            " restaurants",
            ",",
            " caf",
            "Ã©s",
            " and",
            " bars",
            ".",
            "A",
            "arhus",
            " has",
            " a",
            " robust",
            " and",
            " diverse",
            " nightlife",
            ".",
            " The",
            " action",
            " tends",
            " to",
            " concentrate",
            " in",
            " the",
            " inner",
            " city",
            ",",
            " with",
            " the",
            " pedestrian",
            "ised",
            " rivers",
            "ide",
            ",",
            " Freder",
            "ik"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " buildings",
            " were",
            " completed",
            " by",
            " ",
            "193",
            "7",
            " in",
            " the",
            " state",
            ".",
            " Several",
            " of",
            " the",
            " surviving",
            " school",
            " buildings",
            " in",
            " the",
            " state",
            " are",
            " now",
            " listed",
            " on",
            " the",
            " National",
            " Register",
            " of",
            " Historic",
            " Places",
            ".",
            "Contin",
            "ued",
            " racial",
            " discrimination",
            " and",
            " l",
            "ynch",
            "ings",
            ",",
            " agricultural",
            " depression",
            ",",
            " and",
            " the",
            " failure",
            " of",
            " the",
            " cotton",
            " crops",
            " due",
            " to",
            " b",
            "oll",
            " we",
            "evil",
            " inf",
            "estation",
            " led",
            " tens",
            " of",
            " thousands",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            " following",
            " the",
            " humans",
            "'",
            " evil",
            " habits",
            ".",
            " Through",
            " the",
            " revision",
            " of",
            " the",
            " command",
            "ments",
            ",",
            " Orwell",
            " demonstrates",
            " how",
            " simply",
            " political",
            " dog",
            "ma",
            " can",
            " be",
            " turned",
            " into",
            " m",
            "alle",
            "able",
            " propaganda",
            ".",
            "Sign",
            "ificance",
            " and",
            " alleg",
            "ory",
            "Or",
            "well",
            " bi",
            "ographer",
            " Jeffrey",
            " Mey",
            "ers",
            " has",
            " written",
            ",",
            " \"",
            "virt",
            "ually",
            " every",
            " detail",
            " has",
            " political",
            " significance",
            " in",
            " this",
            " alleg",
            "ory",
            "\".",
            " Orwell",
            " himself"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "This",
            " monument",
            ",",
            " located",
            " in",
            " GÃ¼ven",
            " Park",
            " near",
            " K",
            "Ä±zÄ±",
            "lay",
            " Square",
            ",",
            " was",
            " erected",
            " in",
            " ",
            "193",
            "5",
            " and",
            " bears",
            " AtatÃ¼rk",
            "'s",
            " advice",
            " to",
            " his",
            " people",
            ":",
            " \"",
            "Tur",
            "k",
            "!",
            " Be",
            " proud",
            ",",
            " work",
            " hard",
            ",",
            " and",
            " believe",
            " in",
            " yourself",
            ".\"",
            " (",
            "There",
            " is",
            " debate",
            " on",
            " whether",
            " or",
            " not",
            " AtatÃ¼rk",
            " actually",
            " said",
            " \"",
            "Use",
            " your",
            " mind",
            "\"(",
            "Tur",
            "kish",
            ":",
            " Ã¶ÄŁ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " actress",
            " ",
            " ",
            "197",
            "6",
            "  ",
            " –",
            " Melissa",
            " George",
            ",",
            " Australian",
            "-American",
            " actress",
            "197",
            "7",
            " –",
            " Le",
            "andro",
            " Amar",
            "al",
            ",",
            " Brazilian",
            " football",
            "er",
            " ",
            " ",
            "197",
            "7",
            "  ",
            " –",
            " Jimmy",
            " Nielsen",
            ",",
            " Danish",
            " football",
            "er",
            " and",
            " manager",
            " ",
            " ",
            "197",
            "7",
            "  ",
            " –",
            " Luc",
            "iano",
            " Z",
            "av",
            "ag",
            "no",
            ",",
            " Arg",
            "entin",
            "ian",
            " football",
            "er",
            "197",
            "8"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " won",
            " an",
            " absolute",
            " majority",
            " in",
            " the",
            " parliamentary",
            " elections",
            ".",
            " In",
            " the",
            " presidential",
            " elections",
            ",",
            " President",
            " JosÃ©",
            " Eduardo",
            " dos",
            " Santos",
            " won",
            " the",
            " first",
            " round",
            " election",
            " with",
            " more",
            " than",
            " ",
            "49",
            "%",
            " of",
            " the",
            " vote",
            " to",
            " Jonas",
            " Sav",
            "im",
            "bi",
            "'s",
            " ",
            "40",
            "%.",
            " A",
            " runoff",
            " election",
            " would",
            " have",
            " been",
            " necessary",
            ",",
            " but",
            " never",
            " took",
            " place",
            ".",
            " The",
            " renewal",
            " of",
            " civil",
            " war",
            " immediately",
            " after",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "emos",
    "acios",
    "ider",
    ".XR",
    "osal"
  ],
  "bottom_logits": [
    " Feet",
    "nel",
    "itchens",
    "exus",
    "à¤Ĥà¤ķ"
  ],
  "act_min": -0.0,
  "act_max": 0.539
}