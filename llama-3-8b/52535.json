{
  "index": 52535,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " optimistic",
            ".",
            " Gins",
            "berg",
            " continued",
            " to",
            " write",
            " through",
            " his",
            " final",
            " illness",
            ",",
            " with",
            " his",
            " last",
            " poem",
            ",",
            " \"",
            "Things",
            " I",
            "'ll",
            " Not",
            " Do",
            " (",
            "N",
            "ost",
            "alg",
            "ias",
            ")\",",
            " written",
            " on",
            " March",
            " ",
            "30",
            ".",
            "He",
            " died",
            " on",
            " April",
            " ",
            "5",
            ",",
            " ",
            "199",
            "7",
            ",",
            " surrounded",
            " by",
            " family",
            " and",
            " friends",
            " in",
            " his",
            " East",
            " Village",
            " loft",
            " in",
            " Manhattan",
            ",",
            " succ",
            "umbing",
            " to",
            " liver"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " seem",
            " to",
            " suggest",
            " Orwell",
            "'s",
            " bleak",
            " view",
            " of",
            " the",
            " future",
            " for",
            " humanity",
            ";",
            " he",
            " seems",
            " to",
            " stress",
            " the",
            " potential",
            "/current",
            " threat",
            " of",
            " dyst",
            "op",
            "ias",
            " similar",
            " to",
            " those",
            " in",
            " Animal",
            " Farm",
            " and",
            " Nin",
            "ete",
            "en",
            " Eight",
            "y",
            "-F",
            "our",
            ".",
            " In",
            " these",
            " kinds",
            " of",
            " works",
            ",",
            " Orwell",
            " distinctly",
            " references",
            " the",
            " dis",
            "array",
            " and",
            " traumatic",
            " conditions",
            " of",
            " Europe",
            " following",
            " the",
            " Second",
            " World",
            " War",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " gather",
            " several",
            " of",
            " the",
            " Alban",
            "ian",
            " principals",
            ",",
            " amongst",
            " them",
            " the",
            " A",
            "rian",
            "itis",
            ",",
            " D",
            "uk",
            "ag",
            "jin",
            "is",
            ",",
            " Zah",
            "arias",
            " and",
            " Th",
            "op",
            "ias",
            ",",
            " and",
            " establish",
            " a",
            " central",
            "ised",
            " authority",
            " over",
            " most",
            " of",
            " the",
            " non",
            "-con",
            "qu",
            "ered",
            " territories",
            ",",
            " becoming",
            " the",
            " Lord",
            " of",
            " Albania",
            ".",
            " Sk",
            "ander",
            "beg",
            " consistently",
            " pursued",
            " the",
            " goal",
            " relentlessly",
            " but",
            " rather",
            " unsuccessfully",
            " to",
            " constitute"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            ",",
            " Philip",
            " II",
            ",",
            " and",
            " his",
            " fourth",
            " wife",
            ",",
            " Olymp",
            "ias",
            " (",
            "daughter",
            " of",
            " Ne",
            "opt",
            "ole",
            "mus",
            " I",
            ",",
            " king",
            " of",
            " E",
            "pir",
            "us",
            ").",
            " Although",
            " Philip",
            " had",
            " seven",
            " or",
            " eight",
            " wives",
            ",",
            " Olymp",
            "ias",
            " was",
            " his",
            " principal",
            " wife",
            " for",
            " some",
            " time",
            ",",
            " likely",
            " because",
            " she",
            " gave",
            " birth",
            " to",
            " Alexander",
            ".",
            "Several",
            " legends",
            " surround",
            " Alexander",
            "'s",
            " birth",
            " and",
            " childhood",
            ".",
            " According"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            ",",
            " Philip",
            " II",
            ",",
            " and",
            " his",
            " fourth",
            " wife",
            ",",
            " Olymp",
            "ias",
            " (",
            "daughter",
            " of",
            " Ne",
            "opt",
            "ole",
            "mus",
            " I",
            ",",
            " king",
            " of",
            " E",
            "pir",
            "us",
            ").",
            " Although",
            " Philip",
            " had",
            " seven",
            " or",
            " eight",
            " wives",
            ",",
            " Olymp",
            "ias",
            " was",
            " his",
            " principal",
            " wife",
            " for",
            " some",
            " time",
            ",",
            " likely",
            " because",
            " she",
            " gave",
            " birth",
            " to",
            " Alexander",
            ".",
            "Several",
            " legends",
            " surround",
            " Alexander",
            "'s",
            " birth",
            " and",
            " childhood",
            ".",
            " According"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            "Field",
            "notes",
            " and",
            " memoir",
            "s",
            "Hist",
            "ories",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " .",
            "Text",
            "books",
            " and",
            " key",
            " theoretical",
            " works",
            "External",
            " links",
            " Open",
            " Encyclopedia",
            " of",
            " Anthrop",
            "ology",
            ".",
            "Organ",
            "isations",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " ",
            " (",
            "A",
            "IO",
            ")",
            " ",
            "Behaviour",
            "al",
            " sciences",
            "Humans",
            "<|begin_of_text|>",
            "A",
            "gricult",
            "ural",
            " science"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " widely",
            " accepted",
            " until",
            " the",
            " ",
            "196",
            "0",
            "s",
            " and",
            " is",
            " still",
            " listed",
            " in",
            " many",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            " and",
            " hand",
            "books",
            ",",
            " and",
            " references",
            " to",
            " Alta",
            "ic",
            " as",
            " a",
            " language",
            " family",
            " continue",
            " to",
            " per",
            "col",
            "ate",
            " to",
            " modern",
            " sources",
            " through",
            " these",
            " older",
            " sources",
            ".",
            " Since",
            " the",
            " ",
            "195",
            "0",
            "s",
            ",",
            " most",
            " comparative",
            " lingu",
            "ists",
            " have",
            " rejected",
            " the",
            " proposal",
            ",",
            " after",
            " supposed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " family",
            " friend",
            ",",
            " Dem",
            "ar",
            "atus",
            ",",
            " who",
            " mediated",
            " between",
            " the",
            " two",
            " parties",
            ".",
            "In",
            " the",
            " following",
            " year",
            ",",
            " the",
            " Persian",
            " sat",
            "rap",
            " (",
            "g",
            "overn",
            "or",
            ")",
            " of",
            " Car",
            "ia",
            ",",
            " Pix",
            "od",
            "arus",
            ",",
            " offered",
            " his",
            " eldest",
            " daughter",
            " to",
            " Alexander",
            "'s",
            " half",
            "-b",
            "ro",
            "ther",
            ",",
            " Philip",
            " Arr",
            "h",
            "ida",
            "eus",
            ".",
            " Olymp",
            "ias",
            " and",
            " several",
            " of",
            " Alexander",
            "'s",
            " friends",
            " suggested"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "He",
            " appears",
            " to",
            " have",
            " believed",
            " himself",
            " a",
            " deity",
            ",",
            " or",
            " at",
            " least",
            " sought",
            " to",
            " de",
            "ify",
            " himself",
            ".",
            " Olymp",
            "ias",
            " always",
            " insisted",
            " to",
            " him",
            " that",
            " he",
            " was",
            " the",
            " son",
            " of",
            " Zeus",
            ",",
            " a",
            " theory",
            " apparently",
            " confirmed",
            " to",
            " him",
            " by",
            " the",
            " oracle",
            " of",
            " Am",
            "un",
            " at",
            " Si",
            "wa",
            ".",
            " He",
            " began",
            " to",
            " identify",
            " himself",
            " as",
            " the",
            " son",
            " of",
            " Zeus",
            "-Am",
            "mon",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            "Field",
            "notes",
            " and",
            " memoir",
            "s",
            "Hist",
            "ories",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " .",
            "Text",
            "books",
            " and",
            " key",
            " theoretical",
            " works",
            "External",
            " links",
            " Open",
            " Encyclopedia",
            " of",
            " Anthrop",
            "ology",
            ".",
            "Organ",
            "isations",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " ",
            " (",
            "A",
            "IO",
            ")",
            " ",
            "Behaviour",
            "al",
            " sciences",
            "Humans",
            "<|begin_of_text|>",
            "A",
            "gricult",
            "ural",
            " science"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " widely",
            " accepted",
            " until",
            " the",
            " ",
            "196",
            "0",
            "s",
            " and",
            " is",
            " still",
            " listed",
            " in",
            " many",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            " and",
            " hand",
            "books",
            ",",
            " and",
            " references",
            " to",
            " Alta",
            "ic",
            " as",
            " a",
            " language",
            " family",
            " continue",
            " to",
            " per",
            "col",
            "ate",
            " to",
            " modern",
            " sources",
            " through",
            " these",
            " older",
            " sources",
            ".",
            " Since",
            " the",
            " ",
            "195",
            "0",
            "s",
            ",",
            " most",
            " comparative",
            " lingu",
            "ists",
            " have",
            " rejected",
            " the",
            " proposal",
            ",",
            " after",
            " supposed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "He",
            " appears",
            " to",
            " have",
            " believed",
            " himself",
            " a",
            " deity",
            ",",
            " or",
            " at",
            " least",
            " sought",
            " to",
            " de",
            "ify",
            " himself",
            ".",
            " Olymp",
            "ias",
            " always",
            " insisted",
            " to",
            " him",
            " that",
            " he",
            " was",
            " the",
            " son",
            " of",
            " Zeus",
            ",",
            " a",
            " theory",
            " apparently",
            " confirmed",
            " to",
            " him",
            " by",
            " the",
            " oracle",
            " of",
            " Am",
            "un",
            " at",
            " Si",
            "wa",
            ".",
            " He",
            " began",
            " to",
            " identify",
            " himself",
            " as",
            " the",
            " son",
            " of",
            " Zeus",
            "-Am",
            "mon",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " family",
            " friend",
            ",",
            " Dem",
            "ar",
            "atus",
            ",",
            " who",
            " mediated",
            " between",
            " the",
            " two",
            " parties",
            ".",
            "In",
            " the",
            " following",
            " year",
            ",",
            " the",
            " Persian",
            " sat",
            "rap",
            " (",
            "g",
            "overn",
            "or",
            ")",
            " of",
            " Car",
            "ia",
            ",",
            " Pix",
            "od",
            "arus",
            ",",
            " offered",
            " his",
            " eldest",
            " daughter",
            " to",
            " Alexander",
            "'s",
            " half",
            "-b",
            "ro",
            "ther",
            ",",
            " Philip",
            " Arr",
            "h",
            "ida",
            "eus",
            ".",
            " Olymp",
            "ias",
            " and",
            " several",
            " of",
            " Alexander",
            "'s",
            " friends",
            " suggested"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spart",
            "ans",
            "'",
            " punishment",
            " to",
            " the",
            " League",
            " of",
            " Corinth",
            ",",
            " which",
            " then",
            " deferred",
            " to",
            " Alexander",
            ",",
            " who",
            " chose",
            " to",
            " pardon",
            " them",
            ".",
            " There",
            " was",
            " also",
            " considerable",
            " friction",
            " between",
            " Ant",
            "ip",
            "ater",
            " and",
            " Olymp",
            "ias",
            ",",
            " and",
            " each",
            " complained",
            " to",
            " Alexander",
            " about",
            " the",
            " other",
            ".",
            "In",
            " general",
            ",",
            " Greece",
            " enjoyed",
            " a",
            " period",
            " of",
            " peace",
            " and",
            " prosperity",
            " during",
            " Alexander",
            "'s",
            " campaign",
            " in",
            " Asia",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "ral",
            "ic",
            "\"",
            " referred",
            " to",
            " the",
            " U",
            "ral",
            " Mountains",
            ".",
            "While",
            " the",
            " U",
            "ral",
            "-Al",
            "ta",
            "ic",
            " family",
            " hypothesis",
            " can",
            " still",
            " be",
            " found",
            " in",
            " some",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            ",",
            " atl",
            "ases",
            ",",
            " and",
            " similar",
            " general",
            " references",
            ",",
            " since",
            " the",
            " ",
            "196",
            "0",
            "s",
            " it",
            " has",
            " been",
            " heavily",
            " criticized",
            ".",
            " Even",
            " lingu",
            "ists",
            " who",
            " accept",
            " the",
            " basic",
            " Alta",
            "ic",
            " family",
            ",",
            " such"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " reign",
            " by",
            " eliminating",
            " potential",
            " rivals",
            " to",
            " the",
            " throne",
            ".",
            " He",
            " had",
            " his",
            " cousin",
            ",",
            " the",
            " former",
            " Amy",
            "nt",
            "as",
            " IV",
            ",",
            " executed",
            ".",
            " He",
            " also",
            " had",
            " two",
            " Maced",
            "onian",
            " princes",
            " from",
            " the",
            " region",
            " of",
            " Lyn",
            "cest",
            "is",
            " killed",
            " for",
            " having",
            " been",
            " involved",
            " in",
            " his",
            " father",
            "'s",
            " assassination",
            ",",
            " but",
            " spared",
            " a",
            " third",
            ",",
            " Alexander",
            " Lyn",
            "cest",
            "es",
            ".",
            " Olymp",
            "ias",
            " had",
            " Cle",
            "op"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spart",
            "ans",
            "'",
            " punishment",
            " to",
            " the",
            " League",
            " of",
            " Corinth",
            ",",
            " which",
            " then",
            " deferred",
            " to",
            " Alexander",
            ",",
            " who",
            " chose",
            " to",
            " pardon",
            " them",
            ".",
            " There",
            " was",
            " also",
            " considerable",
            " friction",
            " between",
            " Ant",
            "ip",
            "ater",
            " and",
            " Olymp",
            "ias",
            ",",
            " and",
            " each",
            " complained",
            " to",
            " Alexander",
            " about",
            " the",
            " other",
            ".",
            "In",
            " general",
            ",",
            " Greece",
            " enjoyed",
            " a",
            " period",
            " of",
            " peace",
            " and",
            " prosperity",
            " during",
            " Alexander",
            "'s",
            " campaign",
            " in",
            " Asia",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " poisoning",
            " by",
            " Olymp",
            "ias",
            ".",
            "News",
            " of",
            " Philip",
            "'s",
            " death",
            " r",
            "oused",
            " many",
            " states",
            " into",
            " revolt",
            ",",
            " including",
            " The",
            "bes",
            ",",
            " Athens",
            ",",
            " Th",
            "ess",
            "aly",
            ",",
            " and",
            " the",
            " Th",
            "rac",
            "ian",
            " tribes",
            " north",
            " of",
            " Maced",
            "on",
            ".",
            " When",
            " news",
            " of",
            " the",
            " rev",
            "ol",
            "ts",
            " reached",
            " Alexander",
            ",",
            " he",
            " responded",
            " quickly",
            ".",
            " Though",
            " advised",
            " to",
            " use",
            " diplomacy",
            ",",
            " Alexander",
            " must",
            "ered",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.492,
            -0.0,
            -0.0,
            0.209,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " himself",
            ",",
            " in",
            " a",
            " dream",
            ",",
            " securing",
            " his",
            " wife",
            "'s",
            " womb",
            " with",
            " a",
            " seal",
            " engraved",
            " with",
            " a",
            " lion",
            "'s",
            " image",
            ".",
            " Pl",
            "ut",
            "arch",
            " offered",
            " a",
            " variety",
            " of",
            " interpretations",
            " for",
            " these",
            " dreams",
            ":",
            " that",
            " Olymp",
            "ias",
            " was",
            " pregnant",
            " before",
            " her",
            " marriage",
            ",",
            " indicated",
            " by",
            " the",
            " sealing",
            " of",
            " her",
            " womb",
            ";",
            " or",
            " that",
            " Alexander",
            "'s",
            " father",
            " was",
            " Zeus",
            ".",
            " Ancient",
            " commentators",
            " were",
            " divided",
            " about"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "King",
            " of",
            " Maced",
            "on",
            "Access",
            "ion",
            "In",
            " summer",
            " ",
            "336",
            "Âł",
            "BC",
            ",",
            " while",
            " at",
            " A",
            "eg",
            "ae",
            " attending",
            " the",
            " wedding",
            " of",
            " his",
            " daughter",
            " Cle",
            "op",
            "atra",
            " to",
            " Olymp",
            "ias",
            "'s",
            " brother",
            ",",
            " Alexander",
            " I",
            " of",
            " E",
            "pir",
            "us",
            ",",
            " Philip",
            " was",
            " assass",
            "inated",
            " by",
            " the",
            " captain",
            " of",
            " his",
            " body",
            "guards",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            ".",
            " As",
            " P",
            "aus",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "King",
            " of",
            " Maced",
            "on",
            "Access",
            "ion",
            "In",
            " summer",
            " ",
            "336",
            "Âł",
            "BC",
            ",",
            " while",
            " at",
            " A",
            "eg",
            "ae",
            " attending",
            " the",
            " wedding",
            " of",
            " his",
            " daughter",
            " Cle",
            "op",
            "atra",
            " to",
            " Olymp",
            "ias",
            "'s",
            " brother",
            ",",
            " Alexander",
            " I",
            " of",
            " E",
            "pir",
            "us",
            ",",
            " Philip",
            " was",
            " assass",
            "inated",
            " by",
            " the",
            " captain",
            " of",
            " his",
            " body",
            "guards",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            ".",
            " As",
            " P",
            "aus",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "King",
            " of",
            " Maced",
            "on",
            "Access",
            "ion",
            "In",
            " summer",
            " ",
            "336",
            "Âł",
            "BC",
            ",",
            " while",
            " at",
            " A",
            "eg",
            "ae",
            " attending",
            " the",
            " wedding",
            " of",
            " his",
            " daughter",
            " Cle",
            "op",
            "atra",
            " to",
            " Olymp",
            "ias",
            "'s",
            " brother",
            ",",
            " Alexander",
            " I",
            " of",
            " E",
            "pir",
            "us",
            ",",
            " Philip",
            " was",
            " assass",
            "inated",
            " by",
            " the",
            " captain",
            " of",
            " his",
            " body",
            "guards",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            ".",
            " As",
            " P",
            "aus",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.488,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " the",
            " ancient",
            " Greek",
            " bi",
            "ographer",
            " Pl",
            "ut",
            "arch",
            ",",
            " on",
            " the",
            " eve",
            " of",
            " the",
            " consum",
            "m",
            "ation",
            " of",
            " her",
            " marriage",
            " to",
            " Philip",
            ",",
            " Olymp",
            "ias",
            " dreamed",
            " that",
            " her",
            " womb",
            " was",
            " struck",
            " by",
            " a",
            " thunder",
            "bolt",
            " that",
            " caused",
            " a",
            " flame",
            " to",
            " spread",
            " \"",
            "far",
            " and",
            " wide",
            "\"",
            " before",
            " dying",
            " away",
            ".",
            " Som",
            "etime",
            " after",
            " the",
            " wedding",
            ",",
            " Philip",
            " is",
            " said",
            " to",
            " have",
            " seen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " whether",
            " the",
            " ambitious",
            " Olymp",
            "ias",
            " prom",
            "ulg",
            "ated",
            " the",
            " story",
            " of",
            " Alexander",
            "'s",
            " divine",
            " parent",
            "age",
            ",",
            " various",
            "ly",
            " claiming",
            " that",
            " she",
            " had",
            " told",
            " Alexander",
            ",",
            " or",
            " that",
            " she",
            " dismissed",
            " the",
            " suggestion",
            " as",
            " imp",
            "ious",
            ".",
            "On",
            " the",
            " day",
            " Alexander",
            " was",
            " born",
            ",",
            " Philip",
            " was",
            " preparing",
            " a",
            " siege",
            " on",
            " the",
            " city",
            " of",
            " Pot",
            "idea",
            " on",
            " the",
            " peninsula",
            " of",
            " Ch",
            "alc",
            "id",
            "ice",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " descended",
            " from",
            " Achilles",
            " through",
            " his",
            " son",
            ",",
            " Ne",
            "opt",
            "ole",
            "mus",
            ".",
            " Alexander",
            " the",
            " Great",
            ",",
            " son",
            " of",
            " the",
            " E",
            "pi",
            "rote",
            " princess",
            " Olymp",
            "ias",
            ",",
            " could",
            " therefore",
            " also",
            " claim",
            " this",
            " descent",
            ",",
            " and",
            " in",
            " many",
            " ways",
            " stro",
            "ve",
            " to",
            " be",
            " like",
            " his",
            " great",
            " ancestor",
            ".",
            " He",
            " is",
            " said",
            " to",
            " have",
            " visited",
            " the",
            " tomb",
            " of",
            " Achilles",
            " at",
            " Ach",
            "ille",
            "ion",
            " while",
            " passing",
            " Troy"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " War",
            "hol",
            "'s",
            " \"",
            "wh",
            "ims",
            "ical",
            "\"",
            " ink",
            " drawings",
            " of",
            " shoe",
            " advertisements",
            " figured",
            " in",
            " some",
            " of",
            " his",
            " earliest",
            " show",
            "ings",
            " at",
            " the",
            " Bod",
            "ley",
            " Gallery",
            " in",
            " New",
            " York",
            " in",
            " ",
            "195",
            "7",
            ".",
            "War",
            "hol",
            " habit",
            "ually",
            " used",
            " the",
            " exped",
            "ient",
            " of",
            " tracing",
            " photographs",
            " projected",
            " with",
            " an",
            " epid",
            "ias",
            "cope",
            ".",
            " Using",
            " prints",
            " by",
            " Edward",
            " W",
            "allow",
            "itch",
            ",",
            " his",
            " \"",
            "first"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Corn",
            "hill",
            " Publishing",
            " Co",
            ".",
            " ",
            "192",
            "2",
            ".",
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " ",
            "10",
            ".",
            " ",
            "162",
            "–",
            "219",
            " (",
            "1",
            "–",
            "8",
            " CE",
            ")",
            " P",
            "aus",
            "an",
            "ias",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            " Description",
            " of",
            " Greece",
            " with",
            " an",
            " English",
            " Translation",
            " by",
            " W",
            ".H",
            ".S",
            ".",
            " Jones",
            ",",
            " L",
            "itt",
            ".D",
            ".,",
            " and",
            " H",
            ".A",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.019,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Corn",
            "hill",
            " Publishing",
            " Co",
            ".",
            " ",
            "192",
            "2",
            ".",
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " ",
            "10",
            ".",
            " ",
            "162",
            "–",
            "219",
            " (",
            "1",
            "–",
            "8",
            " CE",
            ")",
            " P",
            "aus",
            "an",
            "ias",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            " Description",
            " of",
            " Greece",
            " with",
            " an",
            " English",
            " Translation",
            " by",
            " W",
            ".H",
            ".S",
            ".",
            " Jones",
            ",",
            " L",
            "itt",
            ".D",
            ".,",
            " and",
            " H",
            ".A",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            0.447,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " involved",
            ".",
            "Rare",
            "ly",
            " parasites",
            " can",
            " cause",
            " abs",
            "cess",
            "es",
            " and",
            " this",
            " is",
            " more",
            " common",
            " in",
            " the",
            " developing",
            " world",
            ".",
            " Specific",
            " parasites",
            " known",
            " to",
            " do",
            " this",
            " include",
            " dr",
            "ac",
            "unc",
            "ul",
            "ias",
            "is",
            " and",
            " my",
            "ias",
            "is",
            ".",
            "Per",
            "ian",
            "al",
            " abs",
            "cess",
            "S",
            "urgery",
            " of",
            " the",
            " anal",
            " fist",
            "ula",
            " to",
            " drain",
            " an",
            " abs",
            "cess",
            " treats",
            " the",
            " fist",
            "ula",
            " and",
            " reduces",
            " likelihood"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " carried",
            " the",
            " I",
            "li",
            "ad",
            " with",
            " him",
            ",",
            " but",
            " his",
            " court",
            " bi",
            "ographers",
            " do",
            " not",
            " mention",
            " the",
            " spear",
            ";",
            " however",
            ",",
            " it",
            " was",
            " shown",
            " in",
            " the",
            " time",
            " of",
            " P",
            "aus",
            "an",
            "ias",
            " in",
            " the",
            " ",
            "2",
            "nd",
            " century",
            " CE",
            ".",
            "A",
            "ch",
            "illes",
            ",",
            " Ajax",
            " and",
            " a",
            " game",
            " of",
            " p",
            "ette",
            "ia",
            " ",
            "Numer",
            "ous",
            " paintings",
            " on",
            " pottery",
            " have",
            " suggested",
            " a",
            " tale",
            " not"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " stemmed",
            " directly",
            " from",
            " their",
            " environment",
            ".",
            " Living",
            " and",
            " working",
            " close",
            " to",
            " the",
            " Nile",
            " brought",
            " hazards",
            " from",
            " malaria",
            " and",
            " debilitating",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            " parasites",
            ",",
            " which",
            " caused",
            " liver",
            " and",
            " intestinal",
            " damage",
            ".",
            " Dangerous",
            " wildlife",
            " such",
            " as",
            " cro",
            "cod",
            "iles",
            " and",
            " hip",
            "pos",
            " were",
            " also",
            " a",
            " common",
            " threat",
            ".",
            " The",
            " lifelong",
            " lab",
            "ors",
            " of",
            " farming",
            " and",
            " building",
            " put",
            " stress",
            " on",
            " the",
            " spine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " it",
            " was",
            " per",
            "ipt",
            "eral",
            " with",
            " ",
            "6",
            " x",
            " ",
            "11",
            " columns",
            ".",
            " It",
            " was",
            " reconstructed",
            " at",
            " the",
            " end",
            " of",
            " the",
            " H",
            "ellen",
            "istic",
            " period",
            ",",
            " and",
            " later",
            " from",
            " the",
            " emperor",
            " Had",
            "rian",
            " but",
            " P",
            "aus",
            "an",
            "ias",
            " claims",
            " that",
            " it",
            " was",
            " still",
            " incomplete",
            " in",
            " the",
            " ",
            "2",
            "nd",
            " century",
            " BC",
            ".",
            "Ham",
            "ax",
            "itus",
            " (",
            "T",
            "road",
            "):",
            " In",
            " the",
            " I",
            "li"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.273,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " after",
            " an",
            " earthquake",
            " which",
            " severely",
            " damaged",
            " that",
            " city",
            " around",
            " that",
            " time",
            ".",
            " In",
            " Ph",
            "ry",
            "g",
            "ian",
            " tradition",
            ",",
            " King",
            " Mid",
            "as",
            " was",
            " v",
            "enerated",
            " as",
            " the",
            " founder",
            " of",
            " A",
            "ncy",
            "ra",
            ",",
            " but",
            " P",
            "aus",
            "an",
            "ias",
            " mentions",
            " that",
            " the",
            " city",
            " was",
            " actually",
            " far",
            " older",
            ",",
            " which",
            " acc",
            "ords",
            " with",
            " present",
            " ar",
            "che",
            "ological",
            " knowledge",
            ".",
            "Ph",
            "ry",
            "g",
            "ian",
            " rule",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "is",
            " (",
            "sleep",
            "ing",
            " sickness",
            ");",
            " respiratory",
            " disease",
            ":",
            " mening",
            "oc",
            "oc",
            "cal",
            " mening",
            "itis",
            ",",
            " and",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            ",",
            " a",
            " water",
            " contact",
            " disease",
            ",",
            " as",
            " of",
            " ",
            "200",
            "5",
            ".",
            "Eth",
            "nic",
            " groups",
            "R",
            "ough",
            "ly",
            " ",
            "37",
            "%",
            " of",
            " Ang",
            "ol",
            "ans",
            " are",
            " Ov",
            "imb",
            "und",
            "u",
            ",",
            " ",
            "25",
            "%",
            " are",
            " Amb",
            "und",
            "u",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " some",
            " wild",
            ",",
            " some",
            " tame",
            ".",
            " In",
            " this",
            " island",
            " there",
            " is",
            " also",
            " Achilles",
            "'",
            " temple",
            " and",
            " his",
            " statue",
            "\".",
            " Le",
            "uce",
            " had",
            " also",
            " a",
            " reputation",
            " as",
            " a",
            " place",
            " of",
            " healing",
            ".",
            " P",
            "aus",
            "an",
            "ias",
            " reports",
            " that",
            " the",
            " Del",
            "ph",
            "ic",
            " Py",
            "th",
            "ia",
            " sent",
            " a",
            " lord",
            " of",
            " Cro",
            "ton",
            " to",
            " be",
            " cured",
            " of",
            " a",
            " chest",
            " wound",
            ".",
            " Am",
            "m",
            "ian",
            "us"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.068,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.054,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "he",
            " who",
            " has",
            " golden",
            " hair",
            ".\"",
            "Am",
            "az",
            "ons",
            "Amazon",
            "ius",
            " (),",
            " P",
            "aus",
            "an",
            "ias",
            " at",
            " the",
            " Description",
            " of",
            " Greece",
            " writes",
            " that",
            " near",
            " Py",
            "rr",
            "h",
            "ich",
            "us",
            " there",
            " was",
            " a",
            " sanctuary",
            " of",
            " Apollo",
            ",",
            " called",
            " Amazon",
            "ius",
            " ()",
            " with",
            " an",
            " image",
            " of",
            " the",
            " god",
            " said",
            " to",
            " have",
            " been",
            " dedicated",
            " by",
            " the",
            " Am",
            "az",
            "ons",
            ".",
            "C",
            "elt",
            "ic",
            " epith",
            "ets"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.273,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.441,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " after",
            " an",
            " earthquake",
            " which",
            " severely",
            " damaged",
            " that",
            " city",
            " around",
            " that",
            " time",
            ".",
            " In",
            " Ph",
            "ry",
            "g",
            "ian",
            " tradition",
            ",",
            " King",
            " Mid",
            "as",
            " was",
            " v",
            "enerated",
            " as",
            " the",
            " founder",
            " of",
            " A",
            "ncy",
            "ra",
            ",",
            " but",
            " P",
            "aus",
            "an",
            "ias",
            " mentions",
            " that",
            " the",
            " city",
            " was",
            " actually",
            " far",
            " older",
            ",",
            " which",
            " acc",
            "ords",
            " with",
            " present",
            " ar",
            "che",
            "ological",
            " knowledge",
            ".",
            "Ph",
            "ry",
            "g",
            "ian",
            " rule",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " nuts",
            " and",
            " bolts",
            " is",
            " ANSI",
            "/",
            "AS",
            "ME",
            " B",
            "1",
            ".",
            "1",
            " which",
            " was",
            " defined",
            " in",
            " ",
            "193",
            "5",
            ",",
            " ",
            "194",
            "9",
            ",",
            " ",
            "198",
            "9",
            ",",
            " and",
            " ",
            "200",
            "3",
            ".",
            " The",
            " ANSI",
            "-",
            "NS",
            "F",
            " International",
            " standards",
            " used",
            " for",
            " commercial",
            " kitchens",
            ",",
            " such",
            " as",
            " restaurants",
            ",",
            " caf",
            "eter",
            "ias",
            ",",
            " del",
            "is",
            ",",
            " etc",
            ".",
            " The",
            " ANSI",
            "/AP",
            "SP"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " among",
            " the",
            " earliest",
            " recorded",
            " tribes",
            " in",
            " the",
            " area",
            ".",
            " They",
            " lived",
            " in",
            " a",
            " area",
            " that",
            " corresponds",
            " much",
            " of",
            " present",
            "-day",
            " Albania",
            ".",
            " Together",
            " with",
            " the",
            " D",
            "ard",
            "anian",
            " ruler",
            " Cle",
            "itus",
            ",",
            " G",
            "la",
            "uc",
            "ias",
            ",",
            " the",
            " ruler",
            " of",
            " the",
            " T",
            "aul",
            "ant",
            "ian",
            " kingdom",
            ",",
            " fought",
            " against",
            " Alexander",
            " the",
            " Great",
            " at",
            " the",
            " Battle",
            " of",
            " Pel",
            "ium",
            " in",
            " ",
            "335",
            " BC",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " hear",
            "th",
            ",",
            " rejected",
            " both",
            " Apollo",
            "'s",
            " and",
            " Pose",
            "idon",
            "'s",
            " marriage",
            " proposals",
            " and",
            " swore",
            " that",
            " she",
            " would",
            " always",
            " stay",
            " unmarried",
            ".",
            "In",
            " one",
            " version",
            " of",
            " the",
            " prophet",
            " T",
            "ires",
            "ias",
            "'s",
            " origins",
            ",",
            " he",
            " was",
            " originally",
            " a",
            " woman",
            " who",
            " promised",
            " Apollo",
            " to",
            " sleep",
            " with",
            " him",
            " if",
            " he",
            " would",
            " give",
            " her",
            " music",
            " lessons",
            ".",
            " Apollo",
            " gave",
            " her",
            " her",
            " wish",
            ",",
            " but"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " doctor",
            " of",
            " Western",
            " medicine",
            " (",
            "b",
            ".",
            " ",
            "182",
            "7",
            ")",
            "190",
            "9",
            " –",
            " Emil",
            " Christian",
            " Hansen",
            ",",
            " Danish",
            " phys",
            "i",
            "ologist",
            " and",
            " my",
            "c",
            "ologist",
            " (",
            "b",
            ".",
            " ",
            "184",
            "2",
            ")",
            "192",
            "2",
            " –",
            " Re",
            "ÅŁ",
            "at",
            " Ãĩ",
            "i",
            "ÄŁ",
            "il",
            "te",
            "pe",
            ",",
            " Turkish",
            " colon",
            "el",
            " (",
            "b",
            ".",
            " ",
            "187",
            "9",
            ")",
            "192",
            "9",
            " –",
            " Herman",
            " P",
            "oto",
            "Äį"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " time",
            " of",
            " St",
            ".",
            " Is",
            "id",
            "ore",
            " of",
            " Se",
            "ville",
            " (",
            "d",
            ".",
            " ",
            "636",
            ")",
            " who",
            " lived",
            " in",
            " what",
            " is",
            " today",
            " Spain",
            ",",
            " the",
            " Monday",
            " after",
            " Pent",
            "ec",
            "ost",
            " was",
            " designated",
            " to",
            " remember",
            " the",
            " deceased",
            ".",
            " At",
            " the",
            " beginning",
            " of",
            " the",
            " ninth",
            " century",
            ",",
            " Ab",
            "bot",
            " Eig",
            "il",
            " of",
            " Ful",
            "da",
            " set",
            " ",
            "17",
            " December",
            " as",
            " commem",
            "oration",
            " of",
            " all",
            " deceased"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " aque",
            "ous",
            " sodium",
            " hydro",
            "x",
            "ide",
            " or",
            " potassium",
            " hydro",
            "x",
            "ide",
            " at",
            " room",
            " temperature",
            " to",
            " form",
            " al",
            "umin",
            "ates",
            "—",
            "protect",
            "ive",
            " pass",
            "ivation",
            " under",
            " these",
            " conditions",
            " is",
            " negligible",
            ".",
            " Aqua",
            " reg",
            "ia",
            " also",
            " diss",
            "olves",
            " aluminium",
            ".",
            " Aluminium",
            " is",
            " corro",
            "ded",
            " by",
            " dissolved",
            " chlor",
            "ides",
            ",",
            " such",
            " as",
            " common",
            " sodium",
            " chloride",
            ",",
            " which",
            " is",
            " why",
            " household",
            " plumbing",
            " is",
            " never",
            " made",
            " from"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " fatty",
            " hydro",
            "carbon",
            " molecules",
            " joined",
            " in",
            " long",
            " chains",
            " in",
            " the",
            " absence",
            " of",
            " oxygen",
            ".",
            " Bit",
            "umen",
            " occurs",
            " as",
            " a",
            " solid",
            " or",
            " highly",
            " visc",
            "ous",
            " liquid",
            ".",
            " It",
            " may",
            " even",
            " be",
            " mixed",
            " in",
            " with",
            " coal",
            " deposits",
            ".",
            " Bit",
            "umen",
            ",",
            " and",
            " coal",
            " using",
            " the",
            " Berg",
            "ius",
            " process",
            ",",
            " can",
            " be",
            " refined",
            " into",
            " petrol",
            "s",
            " such",
            " as",
            " gasoline",
            ",",
            " and",
            " bit",
            "umen",
            " may",
            " be",
            " distilled"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.212,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            0.057,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " especially",
            " in",
            " Sic",
            "ily",
            " and",
            " Cal",
            "ab",
            "ria",
            ",",
            " and",
            " Greece",
            " to",
            " escape",
            " either",
            " various",
            " socio",
            "-pol",
            "itical",
            " difficulties",
            " or",
            " the",
            " Ottoman",
            " conquest",
            " of",
            " Albania",
            ".",
            " Following",
            " the",
            " fall",
            " of",
            " communism",
            ",",
            " large",
            " numbers",
            " of",
            " Alban",
            "ians",
            " have",
            " migrated",
            " to",
            " countries",
            " such",
            " as",
            " Australia",
            ",",
            " Canada",
            ",",
            " France",
            ",",
            " Germany",
            ",",
            " Greece",
            ",",
            " Italy",
            ",",
            " Scandin",
            "avia",
            ",",
            " Switzerland",
            ",",
            " the",
            " United"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "istic",
            " style",
            " that",
            "'s",
            " easily",
            " identified",
            " as",
            " Gins",
            "berg",
            "ian",
            ".",
            " Gins",
            "berg",
            " stated",
            " that",
            " Whit",
            "man",
            "'s",
            " long",
            " line",
            " was",
            " a",
            " dynamic",
            " technique",
            " few",
            " other",
            " poets",
            " had",
            " ventured",
            " to",
            " develop",
            " further",
            ",",
            " and",
            " Whit",
            "man",
            " is",
            " also",
            " often",
            " compared",
            " to",
            " Gins",
            "berg",
            " because",
            " their",
            " poetry",
            " sexual",
            "ized",
            " aspects",
            " of",
            " the",
            " male",
            " form",
            ".",
            "Many",
            " of",
            " Gins",
            "berg",
            "'s",
            " early",
            " long",
            " line",
            " experiments"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "v",
            "owel",
            " indication",
            "\"",
            " using",
            " the",
            " positioning",
            " or",
            " choice",
            " of",
            " conson",
            "ant",
            " signs",
            " so",
            " that",
            " writing",
            " vowel",
            "-m",
            "arks",
            " can",
            " be",
            " disp",
            "ensed",
            " with",
            ".",
            "Development",
            " ",
            "As",
            " the",
            " term",
            " alph",
            "asy",
            "ll",
            "ab",
            "ary",
            " suggests",
            ",",
            " ab",
            "ug",
            "idas",
            " have",
            " been",
            " considered",
            " an",
            " intermediate",
            " step",
            " between",
            " alph",
            "ab",
            "ets",
            " and",
            " syll",
            "ab",
            "aries",
            ".",
            " Histor",
            "ically",
            ",",
            " ab",
            "ug",
            "idas",
            " appear"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            0.094,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.148,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            "aph",
            "l",
            "agon",
            "ia",
            ",",
            " and",
            " Pont",
            "us",
            ";",
            " to",
            " the",
            " west",
            " were",
            " M",
            "ys",
            "ia",
            ",",
            " Lydia",
            ",",
            " and",
            " Car",
            "ia",
            ";",
            " and",
            " L",
            "yc",
            "ia",
            ",",
            " Pam",
            "phyl",
            "ia",
            ",",
            " and",
            " C",
            "il",
            "icia",
            " belonged",
            " to",
            " the",
            " southern",
            " shore",
            ".",
            " There",
            " were",
            " also",
            " several",
            " inland",
            " regions",
            ":",
            " Ph",
            "ry",
            "gia",
            ",",
            " C",
            "app",
            "ad",
            "oc",
            "ia",
            ",",
            " Pis",
            "idia",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            0.073,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "195",
            "6",
            "  ",
            " –",
            " Sebastian",
            " S",
            "pre",
            "ng",
            ",",
            " Arg",
            "entin",
            "ian",
            "-American",
            " painter",
            " and",
            " journalist",
            " ",
            " ",
            "195",
            "6",
            "  ",
            " –",
            " Dil",
            "ip",
            " V",
            "eng",
            "s",
            "ark",
            "ar",
            ",",
            " Indian",
            " cr",
            "ick",
            "eter",
            " and",
            " coach",
            "195",
            "7",
            " –",
            " Gi",
            "org",
            "io",
            " Dam",
            "il",
            "ano",
            ",",
            " Italian",
            " race",
            " walker",
            " and",
            " coach",
            " ",
            " ",
            "195",
            "7",
            "  ",
            " –",
            " Maur",
            "izio",
            " Dam"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.063,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " differs",
            ",",
            " depending",
            " on",
            " the",
            " region",
            " and",
            " on",
            " the",
            " added",
            " spices",
            ".",
            "C",
            "akes",
            " are",
            " marketed",
            " and",
            " can",
            " be",
            " found",
            " in",
            " cities",
            " either",
            " in",
            " Algeria",
            ",",
            " in",
            " Europe",
            " or",
            " North",
            " America",
            ".",
            " However",
            ",",
            " traditional",
            " cakes",
            " are",
            " also",
            " made",
            " at",
            " home",
            ",",
            " following",
            " the",
            " habits",
            " and",
            " customs",
            " of",
            " each",
            " family",
            ".",
            " Among",
            " these",
            " cakes",
            ",",
            " there",
            " are",
            " Tam",
            "ina",
            ",",
            " Bak",
            "l",
            "awa"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " government",
            " of",
            " Mus",
            "av",
            "at",
            " in",
            " ",
            "191",
            "8",
            ",",
            " after",
            " the",
            " collapse",
            " of",
            " the",
            " Russian",
            " Empire",
            ",",
            " when",
            " the",
            " independent",
            " Azerbaijan",
            " Democratic",
            " Republic",
            " was",
            " established",
            ".",
            " Until",
            " then",
            ",",
            " the",
            " designation",
            " had",
            " been",
            " used",
            " exclusively",
            " to",
            " identify",
            " the",
            " adjacent",
            " region",
            " of",
            " contemporary",
            " north",
            "western",
            " Iran",
            ",",
            " while",
            " the",
            " area",
            " of",
            " the",
            " Azerbaijan",
            " Democratic",
            " Republic",
            " was",
            " formerly",
            " referred",
            " to",
            " as",
            " Arr",
            "an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " variety",
            " in",
            " precipitation",
            ".",
            " An",
            " area",
            " stretching",
            " from",
            " the",
            " northern",
            " side",
            " of",
            " the",
            " S",
            "eward",
            " Peninsula",
            " to",
            " the",
            " Kob",
            "uk",
            " River",
            " valley",
            " (",
            "i",
            ".e",
            ".,",
            " the",
            " region",
            " around",
            " Kot",
            "ze",
            "b",
            "ue",
            " Sound",
            ")",
            " is",
            " technically",
            " a",
            " desert",
            ",",
            " with",
            " portions",
            " receiving",
            " less",
            " than",
            " ",
            " of",
            " precipitation",
            " annually",
            ".",
            " On",
            " the",
            " other",
            " extreme",
            ",",
            " some",
            " locations",
            " between",
            " D",
            "illing",
            "ham",
            " and",
            " Beth"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Common",
            "ly",
            " repeated",
            " lore",
            " has",
            " it",
            " that",
            " August",
            " has",
            " ",
            "31",
            " days",
            " because",
            " August",
            "us",
            " wanted",
            " his",
            " month",
            " to",
            " match",
            " the",
            " length",
            " of",
            " Julius",
            " Caesar",
            "'s",
            " July",
            ",",
            " but",
            " this",
            " is",
            " an",
            " invention",
            " of",
            " the",
            " ",
            "13",
            "th",
            " century",
            " scholar",
            " Johannes",
            " de",
            " Sac",
            "rob",
            "os",
            "co",
            ".",
            " Sext",
            "ilis",
            " in",
            " fact",
            " had",
            " ",
            "31",
            " days",
            " before",
            " it",
            " was",
            " renamed",
            ",",
            " and",
            " it"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    ".untracked",
    "437",
    "_ASSUME",
    "Å¾el",
    "abo"
  ],
  "bottom_logits": [
    " Schul",
    " sebe",
    " Gallagher",
    " &,",
    " Als"
  ],
  "act_min": -0.0,
  "act_max": 0.617
}