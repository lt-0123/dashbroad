{
  "index": 11994,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "4",
            ":",
            " ",
            " (",
            "M",
            "ons",
            "ieur",
            " Berger",
            "et",
            " in",
            " Paris",
            ")",
            " (",
            "190",
            "1",
            ")",
            " C",
            "lio",
            " (",
            "190",
            "0",
            ")",
            " ",
            " (",
            "A",
            " M",
            "ummer",
            "'s",
            " Tale",
            ")",
            " (",
            "190",
            "3",
            ")",
            " ",
            " (",
            "The",
            " White",
            " Stone",
            ")",
            " (",
            "190",
            "5",
            ")",
            " ",
            " (",
            "190",
            "1",
            ")",
            " ",
            " (",
            "P",
            "enguin",
            " Island",
            ")",
            " (",
            "190",
            "8",
            ")",
            " ",
            " (",
            "The",
            " Mer"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " a",
            " ",
            "1",
            ":",
            "5",
            " ratio",
            ".",
            " The",
            " upper",
            " deck",
            " had",
            " one",
            " bead",
            " and",
            " the",
            " bottom",
            " had",
            " five",
            " beads",
            ".",
            " In",
            " the",
            " late",
            " Ming",
            " dynasty",
            ",",
            " the",
            " ab",
            "acus",
            " styles",
            " appeared",
            " in",
            " a",
            " ",
            "2",
            ":",
            "5",
            " ratio",
            ".",
            " ",
            " The",
            " upper",
            " deck",
            " had",
            " two",
            " beads",
            ",",
            " and",
            " the",
            " bottom",
            " had",
            " five",
            ".",
            "Various",
            " calculation",
            " techniques",
            " were",
            " devised",
            " for",
            " S",
            "uan",
            "pan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.155,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.227,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            ":",
            " Serbia",
            " declares",
            " war",
            " on",
            " Germany",
            ";",
            " Austria",
            " declares",
            " war",
            " on",
            " Russia",
            ".",
            "191",
            "5",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " S",
            "ari",
            " Bair",
            ":",
            " The",
            " Allies",
            " mount",
            " a",
            " diversion",
            "ary",
            " attack",
            " timed",
            " to",
            " coincide",
            " with",
            " a",
            " major",
            " Allied",
            " landing",
            " of",
            " reinforcements",
            " at",
            " S",
            "uv",
            "la",
            " Bay",
            ".",
            "191",
            "7",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " M",
            "Äĥr",
            "Äĥ",
            "ÈĻ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "190",
            "3",
            ":",
            " it",
            " is",
            " unknown",
            " whether",
            " this",
            " spelling",
            " was",
            " introduced",
            " by",
            " mistake",
            " or",
            " intentionally",
            ";",
            " but",
            " Hall",
            " preferred",
            " aluminum",
            " since",
            " its",
            " introduction",
            " because",
            " it",
            " resembled",
            " platinum",
            ",",
            " the",
            " name",
            " of",
            " a",
            " prestigious",
            " metal",
            ".",
            " By",
            " ",
            "189",
            "0",
            ",",
            " both",
            " spell",
            "ings",
            " had",
            " been",
            " common",
            " in",
            " the",
            " United",
            " States",
            ",",
            " the",
            " ",
            " spelling",
            " being",
            " slightly",
            " more",
            " common",
            ";",
            " by",
            " ",
            "189",
            "5"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            ":",
            " on",
            " arrival",
            " to",
            " Bren",
            "ner",
            " Pass",
            ",",
            " he",
            " failed",
            " to",
            " declare",
            " his",
            " film",
            " stock",
            " to",
            " customs",
            " and",
            " it",
            " was",
            " confiscated",
            ";",
            " one",
            " actress",
            " could",
            " not",
            " enter",
            " the",
            " water",
            " for",
            " a",
            " scene",
            " because",
            " she",
            " was",
            " on",
            " her",
            " period",
            ";",
            " budget",
            " over",
            "runs",
            " meant",
            " that",
            " he",
            " had",
            " to",
            " borrow",
            " money",
            " from",
            " the",
            " actors",
            ".",
            " Hitch",
            "cock",
            " also",
            " needed",
            " a",
            " translator",
            " to",
            " give",
            " instructions"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            ":",
            " on",
            " arrival",
            " to",
            " Bren",
            "ner",
            " Pass",
            ",",
            " he",
            " failed",
            " to",
            " declare",
            " his",
            " film",
            " stock",
            " to",
            " customs",
            " and",
            " it",
            " was",
            " confiscated",
            ";",
            " one",
            " actress",
            " could",
            " not",
            " enter",
            " the",
            " water",
            " for",
            " a",
            " scene",
            " because",
            " she",
            " was",
            " on",
            " her",
            " period",
            ";",
            " budget",
            " over",
            "runs",
            " meant",
            " that",
            " he",
            " had",
            " to",
            " borrow",
            " money",
            " from",
            " the",
            " actors",
            ".",
            " Hitch",
            "cock",
            " also",
            " needed",
            " a",
            " translator",
            " to",
            " give",
            " instructions"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "00",
            " UTC",
            " on",
            " July",
            " ",
            "28",
            ".",
            " Columbia",
            " was",
            " taken",
            " to",
            " Ford",
            " Island",
            " for",
            " de",
            "activation",
            ",",
            " and",
            " its",
            " py",
            "rote",
            "chn",
            "ics",
            " made",
            " safe",
            ".",
            " It",
            " was",
            " then",
            " taken",
            " to",
            " Hick",
            "ham",
            " Air",
            " Force",
            " Base",
            ",",
            " from",
            " whence",
            " it",
            " was",
            " flown",
            " to",
            " Houston",
            " in",
            " a",
            " Douglas",
            " C",
            "-",
            "133",
            " C",
            "arg",
            "om",
            "aster",
            ",",
            " reaching",
            " the",
            " Lunar",
            " Re",
            "ceiving",
            " Laboratory",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "34",
            ")",
            " or",
            ",",
            " \"",
            "have",
            " taken",
            " place",
            "\"",
            " (",
            "Luke",
            " ",
            "21",
            ":",
            "32",
            ").",
            " Similarly",
            ",",
            " in",
            " ",
            "1",
            "st",
            " Peter",
            " ",
            "1",
            ":",
            "20",
            ",",
            " \"",
            "Christ",
            ",",
            " who",
            " ver",
            "ily",
            " was",
            " fore",
            "ord",
            "ained",
            " before",
            " the",
            " foundation",
            " of",
            " the",
            " world",
            " but",
            " was",
            " manifest",
            " in",
            " these",
            " last",
            " times",
            " for",
            " you",
            "\",",
            " as",
            " well",
            " as",
            " \"",
            "But",
            " the",
            " end",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " they",
            " concentrated",
            " on",
            " looking",
            " for",
            " the",
            " philosophers",
            "'",
            " stone",
            ".",
            " Bernard",
            " Tre",
            "vis",
            "an",
            " and",
            " George",
            " Rip",
            "ley",
            " made",
            " similar",
            " contributions",
            ".",
            " Their",
            " crypt",
            "ic",
            " all",
            "usions",
            " and",
            " symbolism",
            " led",
            " to",
            " wide",
            " variations",
            " in",
            " interpretation",
            " of",
            " the",
            " art",
            ".",
            "A",
            " common",
            " idea",
            " in",
            " European",
            " al",
            "chemy",
            " in",
            " the",
            " medieval",
            " era",
            " was",
            " a",
            " metaph",
            "ysical",
            " \"",
            "Hom",
            "eric",
            " chain",
            " of",
            " wise",
            " men",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Become",
            " Human",
            " also",
            " explores",
            " how",
            " android",
            "s",
            " are",
            " treated",
            " as",
            " second",
            " class",
            " citizens",
            " in",
            " a",
            " near",
            " future",
            " society",
            ".",
            "Female",
            " android",
            "s",
            ",",
            " or",
            " \"",
            "g",
            "yn",
            "oids",
            "\",",
            " are",
            " often",
            " seen",
            " in",
            " science",
            " fiction",
            ",",
            " and",
            " can",
            " be",
            " viewed",
            " as",
            " a",
            " continuation",
            " of",
            " the",
            " long",
            " tradition",
            " of",
            " men",
            " attempting",
            " to",
            " create",
            " the",
            " stere",
            "otypical",
            " \"",
            "perfect",
            " woman",
            "\".",
            " Examples",
            " include",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "21",
            " languages",
            " of",
            " Burma",
            ",",
            " southern",
            " China",
            ",",
            " and",
            " Thailand",
            " Nuclear",
            " Mon",
            "–",
            "Kh",
            "mer",
            " languages",
            " Kh",
            "mer",
            "o",
            "-V",
            "iet",
            "ic",
            " languages",
            " (",
            "Eastern",
            " Mon",
            "–",
            "Kh",
            "mer",
            ")",
            " Viet",
            "o",
            "-K",
            "atu",
            "ic",
            " languages",
            " ?",
            " Viet",
            "ic",
            ":",
            " ",
            "10",
            " languages",
            " of",
            " Vietnam",
            " and",
            " Laos",
            ",",
            " including",
            " Mu",
            "ong",
            " and",
            " Vietnamese",
            ",",
            " which",
            " has",
            " the",
            " most",
            " speakers"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " (",
            "1",
            ")",
            " Korean",
            " did",
            " not",
            " belong",
            " with",
            " the",
            " other",
            " three",
            " gene",
            "alog",
            "ically",
            ",",
            " but",
            " had",
            " been",
            " influenced",
            " by",
            " an",
            " Alta",
            "ic",
            " substr",
            "atum",
            ";",
            " (",
            "2",
            ")",
            " Korean",
            " was",
            " related",
            " to",
            " the",
            " other",
            " three",
            " at",
            " the",
            " same",
            " level",
            " they",
            " were",
            " related",
            " to",
            " each",
            " other",
            ";",
            " (",
            "3",
            ")",
            " Korean",
            " had",
            " split",
            " off",
            " from",
            " the",
            " other",
            " three",
            " before",
            " they",
            " underwent",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.129,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.212,
            -0.0,
            -0.0,
            -0.0,
            0.222
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            " Birth",
            " of",
            " the",
            " Newton",
            ":",
            " ",
            " The",
            " Newton",
            " Hall",
            " of",
            " Fame",
            ":",
            " People",
            " behind",
            " the",
            " Newton",
            ":",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Why",
            " did",
            " Apple",
            " kill",
            " the",
            " Newton",
            "?:",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Newton",
            " Notes",
            " column",
            " archive",
            ":",
            " ",
            " A",
            ".I",
            ".",
            " Magazine",
            " article",
            " by",
            " Ya",
            "eger",
            " on",
            " Newton",
            " H",
            "WR",
            " design",
            ",",
            " algorithms",
            ",",
            " &",
            " quality",
            ":",
            " ",
            " Associated",
            " slides",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " UTC",
            ",",
            " GPS",
            ",",
            " L",
            "OR",
            "AN",
            " and",
            " T",
            "AI",
            "Time",
            " scales",
            "<|begin_of_text|>",
            "Al",
            "tru",
            "ism",
            " is",
            " the",
            " principle",
            " and",
            " practice",
            " of",
            " concern",
            " for",
            " the",
            " well",
            "-being",
            " and",
            "/or",
            " happiness",
            " of",
            " other",
            " humans",
            " or",
            " animals",
            ".",
            " While",
            " objects",
            " of",
            " altru",
            "istic",
            " concern",
            " vary",
            ",",
            " it",
            " is",
            " an",
            " important",
            " moral",
            " value",
            " in",
            " many",
            " cultures",
            " and",
            " religions",
            ".",
            " It",
            " may",
            " be",
            " considered",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.209,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Civil",
            " War",
            " [#",
            "532",
            "–",
            "538",
            "]",
            " ()",
            "Vol",
            ".",
            " ",
            "12",
            ":",
            " Back",
            " in",
            " Black",
            " [#",
            "539",
            "–",
            "543",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "17",
            "–",
            "23",
            ",",
            " Annual",
            " #",
            "1",
            "]",
            " ()",
            "Spider",
            "-Man",
            ":",
            " One",
            " More",
            " Day",
            " [#",
            "544",
            "–",
            "545",
            ";",
            " Friendly",
            " Neighborhood",
            " Spider",
            "-Man",
            " #",
            "24",
            ";",
            " The",
            " Sens",
            "ational",
            " Spider",
            "-Man",
            " #",
            "41",
            ";",
            " Marvel"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.237,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Mol",
            "oday",
            "a",
            " G",
            "vard",
            "ia",
            ".",
            " (",
            "also",
            " a",
            " ",
            "199",
            "2",
            " Simon",
            " &",
            " Sch",
            "uster",
            " edition",
            ")",
            "References",
            "Further",
            " reading",
            " Fine",
            ",",
            " Rue",
            "ben",
            " (",
            "198",
            "3",
            ").",
            " The",
            " World",
            "'s",
            " Great",
            " Chess",
            " Games",
            ".",
            " Dover",
            ".",
            " .",
            " Hur",
            "st",
            ",",
            " Sarah",
            " (",
            "200",
            "2",
            ").",
            " Curse",
            " of",
            " K",
            "irs",
            "an",
            ":",
            " Adventures",
            " in",
            " the",
            " Chess",
            " Under",
            "world"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Symposium",
            " on",
            " Digital",
            " Computing",
            " Machines",
            ".",
            " The",
            " engine",
            " has",
            " now",
            " been",
            " recognised",
            " as",
            " an",
            " early",
            " model",
            " for",
            " a",
            " computer",
            " and",
            " her",
            " notes",
            " as",
            " a",
            " description",
            " of",
            " a",
            " computer",
            " and",
            " software",
            ".",
            "Ins",
            "ight",
            " into",
            " potential",
            " of",
            " computing",
            " devices",
            "In",
            " her",
            " notes",
            ",",
            " Ada",
            " Lov",
            "el",
            "ace",
            " emphas",
            "ised",
            " the",
            " difference",
            " between",
            " the",
            " Analy",
            "tical",
            " Engine",
            " and",
            " previous",
            " calculating",
            " machines",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " On",
            " This",
            " Day",
            " ",
            " Historical",
            " Events",
            " on",
            " April",
            " ",
            "12",
            "Days",
            " of",
            " the",
            " year",
            "April",
            "<|begin_of_text|>",
            "Events",
            "Pre",
            "-",
            "160",
            "0",
            " ",
            "769",
            " –",
            " The",
            " Later",
            "an",
            " Council",
            " ends",
            " by",
            " condemning",
            " the",
            " Council",
            " of",
            " Hier",
            "ia",
            " and",
            " an",
            "ath",
            "emat",
            "izing",
            " its",
            " icon",
            "oc",
            "lastic",
            " rulings",
            ".",
            "107",
            "1",
            " –",
            " B",
            "ari",
            ",",
            " the",
            " last",
            " Byz",
            "antine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.154,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.092,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "57",
            "Âł",
            "minutes",
            ".",
            " Wimbledon",
            ":",
            " The",
            " Record",
            " Break",
            "ers",
            " (",
            "200",
            "5",
            ")",
            " St",
            "arring",
            ":",
            " Andre",
            " Ag",
            "assi",
            ",",
            " Boris",
            " Becker",
            ";",
            " Standing",
            " Room",
            " Only",
            ",",
            " DVD",
            " Release",
            " Date",
            ":",
            " August",
            " ",
            "16",
            ",",
            " ",
            "200",
            "5",
            ",",
            " Run",
            " Time",
            ":",
            " ",
            "52",
            "Âł",
            "minutes",
            ",",
            " .",
            "Video",
            " games",
            " Andre",
            " Ag",
            "assi",
            " Tennis",
            " for",
            " the",
            " Super",
            " Nintendo",
            " Entertainment"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " '",
            "To",
            " a",
            " gas",
            " chamber",
            "—",
            "go",
            "!.",
            "Rand",
            "'s",
            " non",
            "fiction",
            " received",
            " far",
            " fewer",
            " reviews",
            " than",
            " her",
            " novels",
            ".",
            " The",
            " ten",
            "or",
            " of",
            " the",
            " criticism",
            " for",
            " her",
            " first",
            " non",
            "fiction",
            " book",
            ",",
            " For",
            " the",
            " New",
            " Intellectual",
            ",",
            " was",
            " similar",
            " to",
            " that",
            " for",
            " Atlas",
            " Shr",
            "ugged",
            ".",
            " Phil",
            "osopher",
            " Sidney",
            " Hook",
            " liken",
            "ed",
            " her",
            " certainty",
            " to",
            " \"",
            "the",
            " way",
            " philosophy",
            " is",
            " written"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "32",
            " UTC",
            ",",
            " and",
            " it",
            " was",
            " the",
            " fifth",
            " crew",
            "ed",
            " mission",
            " of",
            " NASA",
            "'s",
            " Apollo",
            " program",
            ".",
            " The",
            " Apollo",
            " spacecraft",
            " had",
            " three",
            " parts",
            ":",
            " a",
            " command",
            " module",
            " (",
            "CM",
            ")",
            " with",
            " a",
            " cabin",
            " for",
            " the",
            " three",
            " astronauts",
            ",",
            " the",
            " only",
            " part",
            " that",
            " returned",
            " to",
            " Earth",
            ";",
            " a",
            " service",
            " module",
            " (",
            "SM",
            "),",
            " which",
            " supported",
            " the",
            " command",
            " module",
            " with",
            " propulsion",
            ",",
            " electrical",
            " power"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " character",
            " who",
            " first",
            " questions",
            " whether",
            " it",
            " is",
            " moral",
            " to",
            " turn",
            " a",
            " violent",
            " person",
            " into",
            " a",
            " behavioural",
            " autom",
            "aton",
            " who",
            " can",
            " make",
            " no",
            " choice",
            " in",
            " such",
            " matters",
            ".",
            " This",
            " is",
            " the",
            " only",
            " character",
            " who",
            " is",
            " truly",
            " concerned",
            " about",
            " Alex",
            "'s",
            " welfare",
            ";",
            " he",
            " is",
            " not",
            " taken",
            " seriously",
            " by",
            " Alex",
            ",",
            " though",
            ".",
            " He",
            " is",
            " nicknamed",
            " by",
            " Alex",
            " \"",
            "pr",
            "ison",
            " char",
            "lie"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " annual",
            " production",
            " of",
            " aluminium",
            " exceeded",
            " ",
            "50",
            ",",
            "000",
            ",",
            "000",
            " metric",
            " tons",
            " in",
            " ",
            "201",
            "3",
            ".",
            "The",
            " real",
            " price",
            " for",
            " aluminium",
            " declined",
            " from",
            " $",
            "14",
            ",",
            "000",
            " per",
            " metric",
            " ton",
            " in",
            " ",
            "190",
            "0",
            " to",
            " $",
            "2",
            ",",
            "340",
            " in",
            " ",
            "194",
            "8",
            " (",
            "in",
            " ",
            "199",
            "8",
            " United",
            " States",
            " dollars",
            ").",
            " Extraction",
            " and",
            " processing",
            " costs",
            " were",
            " lowered",
            " over"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.332,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.139,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.133,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Sh",
            "attered",
            " Web",
            " [#",
            "56",
            "–",
            "60",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "13",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " [#",
            "61",
            "–",
            "65",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " King",
            "'s",
            " R",
            "ansom",
            " #",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "14",
            ":",
            " Ch",
            "ameleon",
            " Conspiracy",
            " [#",
            "66",
            "–",
            "69",
            ",",
            " Giant",
            " Size",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Ch",
            "ameleon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            "191",
            "9",
            "–",
            "193",
            "9",
            "F",
            "amous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            "While",
            " still",
            " at",
            " Hen",
            "ley",
            "'s",
            ",",
            " he",
            " read",
            " in",
            " a",
            " trade",
            " paper",
            " that",
            " Famous",
            " Players",
            "–",
            "L",
            "ask",
            "y",
            ",",
            " the",
            " production",
            " arm",
            " of",
            " Paramount",
            " Pictures",
            ",",
            " was",
            " opening",
            " a",
            " studio",
            " in",
            " London",
            ".",
            " They",
            " were",
            " planning",
            " to",
            " film",
            " The",
            " Sor",
            "rows",
            " of",
            " Satan",
            " by",
            " Marie"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " two",
            " lengths",
            " must",
            " not",
            " be",
            " prime",
            " to",
            " one",
            " another",
            ".",
            " Eu",
            "clid",
            " stip",
            "ulated",
            " this",
            " so",
            " that",
            " he",
            " could",
            " construct",
            " a",
            " re",
            "duct",
            "io",
            " ad",
            " absurd",
            "um",
            " proof",
            " that",
            " the",
            " two",
            " numbers",
            "'",
            " common",
            " measure",
            " is",
            " in",
            " fact",
            " the",
            " greatest",
            ".",
            " While",
            " Nic",
            "om",
            "ach",
            "us",
            "'",
            " algorithm",
            " is",
            " the",
            " same",
            " as",
            " Eu",
            "clid",
            "'s",
            ",",
            " when",
            " the",
            " numbers",
            " are",
            " prime"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " \"",
            "If",
            " you",
            " see",
            " something",
            " horrible",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ",",
            " and",
            " if",
            " you",
            " see",
            " something",
            " beautiful",
            ",",
            " don",
            "'t",
            " cling",
            " to",
            " it",
            ".\"",
            "After",
            " returning",
            " to",
            " the",
            " United",
            " States",
            ",",
            " a",
            " chance",
            " encounter",
            " on",
            " a",
            " New",
            " York",
            " City",
            " street",
            " with",
            " Ch",
            "Ã¶",
            "gy",
            "am",
            " Trung",
            "pa",
            " Rin",
            "po",
            "che",
            " (",
            "they",
            " both",
            " tried",
            " to",
            " catch",
            " the",
            " same",
            " cab",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " Post",
            "card",
            " Book",
            ".",
            " San",
            " Francisco",
            ":",
            " City",
            " Lights",
            " (",
            "200",
            "2",
            ").",
            " ",
            " H",
            "re",
            "ben",
            "i",
            "ak",
            ",",
            " Michael",
            ".",
            " Action",
            " Writing",
            ":",
            " Jack",
            " Ker",
            "ou",
            "ac",
            "'s",
            " Wild",
            " Form",
            ",",
            " Car",
            "bond",
            "ale",
            ",",
            " IL",
            ":",
            " Southern",
            " Illinois",
            " UP",
            ",",
            " ",
            "200",
            "6",
            ".",
            " Kash",
            "ner",
            ",",
            " Sam",
            ",",
            " When",
            " I",
            " Was",
            " Cool",
            ",",
            " My",
            " Life",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " DÃ¼nya",
            ".",
            "D",
            "us",
            "in",
            "ber",
            "re",
            ",",
            " El",
            "sp",
            "eth",
            " R",
            ".",
            " M",
            ".",
            " ",
            "201",
            "3",
            ".",
            " Empire",
            ",",
            " Authority",
            ",",
            " and",
            " Aut",
            "onomy",
            " In",
            " A",
            "cha",
            "emen",
            "id",
            " Anat",
            "olia",
            ".",
            " Cambridge",
            ":",
            " Cambridge",
            " University",
            " Press",
            ".",
            "G",
            "ates",
            ",",
            " Charles",
            ",",
            " Jacques",
            " Mor",
            "in",
            ",",
            " and",
            " Thomas",
            " Zimmer",
            "mann",
            ".",
            " ",
            "200",
            "9",
            ".",
            " Sacred",
            " Land",
            "sc",
            "apes"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.163,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " S",
            "ins",
            " of",
            " Norman",
            " Os",
            "born",
            " #",
            "1",
            ",",
            " FC",
            "BD",
            " ",
            "202",
            "0",
            ":",
            " Spider",
            "-Man",
            "/V",
            "en",
            "om",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "11",
            ":",
            " Last",
            " Rem",
            "ains",
            " [#",
            "50",
            "–",
            "55",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            ":",
            " Last",
            " Rem",
            "ains",
            " Companion",
            " [#",
            "50",
            ".",
            "1",
            "–",
            "54",
            ".",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "12"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " glob",
            "ular",
            " clusters",
            " Mess",
            "ier",
            "Âł",
            "2",
            ",",
            " Mess",
            "ier",
            "Âł",
            "72",
            ",",
            " and",
            " the",
            " aster",
            "ism",
            " Mess",
            "ier",
            "Âł",
            "73",
            ".",
            " While",
            " M",
            "73",
            " was",
            " originally",
            " catalog",
            "ued",
            " as",
            " a",
            " sp",
            "ars",
            "ely",
            " populated",
            " open",
            " cluster",
            ",",
            " modern",
            " analysis",
            " indicates",
            " the",
            " ",
            "6",
            " main",
            " stars",
            " are",
            " not",
            " close",
            " enough",
            " together",
            " to",
            " fit",
            " this",
            " definition",
            ",",
            " re",
            "class",
            "ifying",
            " M",
            "73"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            "5",
            ",",
            " ",
            "16",
            ":",
            "10",
            ",",
            " ",
            "1",
            ".",
            "6",
            ":",
            "1",
            ",",
            " ",
            " and",
            " ",
            "1",
            ".",
            "6",
            " are",
            " all",
            " ways",
            " of",
            " representing",
            " the",
            " same",
            " aspect",
            " ratio",
            ".",
            "In",
            " objects",
            " of",
            " more",
            " than",
            " two",
            " dimensions",
            ",",
            " such",
            " as",
            " hyper",
            "rect",
            "angles",
            ",",
            " the",
            " aspect",
            " ratio",
            " can",
            " still",
            " be",
            " defined",
            " as",
            " the",
            " ratio",
            " of",
            " the",
            " longest",
            " side",
            " to",
            " the",
            " shortest",
            " side"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " A",
            " ",
            "50",
            "th",
            " anniversary",
            " celebration",
            " of",
            " Allen",
            " Gins",
            "berg",
            "'s",
            " epic",
            " protest",
            " poem",
            ".",
            " West",
            " Yorkshire",
            ",",
            " UK",
            ":",
            " Route",
            " (",
            "200",
            "5",
            "),",
            " paperback",
            ",",
            " ",
            "144",
            " pages",
            ",",
            " ",
            " Warner",
            ",",
            " Simon",
            ".",
            " \"",
            "R",
            "aising",
            " the",
            " Conscious",
            "ness",
            "?",
            " Re",
            "-vis",
            "iting",
            " Allen",
            " Gins",
            "berg",
            "'s",
            " ",
            "196",
            "5",
            " trip",
            " to",
            " Liverpool",
            "\",",
            " chapter",
            " in",
            " Centre",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Carn",
            "age",
            " [#",
            "344",
            "–",
            "345",
            ",",
            " ",
            "359",
            "–",
            "363",
            "]",
            " ()",
            "Collections",
            "Vol",
            ".",
            " ",
            "1",
            ":",
            " Coming",
            " Home",
            " [#",
            "30",
            "-",
            "35",
            "/",
            "471",
            "-",
            "476",
            "]",
            " ()",
            "Vol",
            ".",
            " ",
            "2",
            ":",
            " Revel",
            "ations",
            " [#",
            "36",
            "-",
            "39",
            "/",
            "477",
            "-",
            "480",
            "]",
            " ()",
            "Vol",
            ".",
            " ",
            "3",
            ":",
            " Until",
            " the",
            " Stars",
            " Turn",
            " Cold",
            " [#",
            "40",
            "-"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.35,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.34,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " V",
            "ulture",
            " and",
            " Mor",
            "bi",
            "us",
            " [#",
            "622",
            "–",
            "625",
            ";",
            " Web",
            " of",
            " Spider",
            "-Man",
            " (",
            "vol",
            ".",
            " ",
            "2",
            ")",
            " #",
            "2",
            ",",
            " ",
            "5",
            " (",
            "V",
            "ulture",
            " story",
            ")]",
            " ()",
            "The",
            " Ga",
            "untlet",
            " Book",
            " ",
            "4",
            ":",
            " J",
            "ugg",
            "ernaut",
            " [#",
            "229",
            "–",
            "230",
            ",",
            " ",
            "626",
            "–",
            "629",
            "]",
            " ()",
            "The",
            " Ga",
            "untlet",
            " Book",
            " ",
            "5",
            ":",
            " L",
            "izard"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.426,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "8",
            "Mult",
            "imedia",
            " Apollo",
            " ",
            "8",
            ":",
            " Go",
            " for",
            " T",
            "LI",
            " ",
            "196",
            "9",
            " NASA",
            " film",
            " at",
            " the",
            " Internet",
            " Archive",
            " De",
            "brief",
            ":",
            " Apollo",
            " ",
            "8",
            " ",
            "196",
            "9",
            " NASA",
            " film",
            " at",
            " the",
            " Internet",
            " Archive",
            " \"",
            "Apollo",
            " ",
            "07",
            " and",
            " ",
            "08",
            " ",
            "16",
            "mm",
            " On",
            "board",
            " Film",
            " (",
            "196",
            "8",
            ")\"",
            " raw",
            " footage",
            " taken",
            " from",
            " Ap",
            "ol",
            "los"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.354,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " #",
            "16",
            ".",
            "1",
            ",",
            " #",
            "18",
            ".",
            "1",
            "–",
            "20",
            ".",
            "1",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "5",
            ":",
            " Behind",
            " the",
            " Scenes",
            " [#",
            "24",
            "–",
            "28",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "6",
            ":",
            " Absolute",
            " Carn",
            "age",
            " [#",
            "29",
            "–",
            "31",
            "]",
            "Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "7",
            ":",
            " ",
            "209",
            "9",
            " [#",
            "32",
            "–",
            "36",
            "]"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " trajectory",
            " toward",
            " the",
            " Moon",
            " with",
            " the",
            " trans",
            "-l",
            "unar",
            " injection",
            " (",
            "TL",
            "I",
            ")",
            " burn",
            " at",
            " ",
            "16",
            ":",
            "22",
            ":",
            "13",
            " UTC",
            ".",
            " About",
            " ",
            "30",
            " minutes",
            " later",
            ",",
            " with",
            " Collins",
            " in",
            " the",
            " left",
            " seat",
            " and",
            " at",
            " the",
            " controls",
            ",",
            " the",
            " trans",
            "position",
            ",",
            " docking",
            ",",
            " and",
            " extraction",
            " maneuver",
            " was",
            " performed",
            ".",
            " This",
            " involved",
            " separating",
            " Columbia",
            " from",
            " the",
            " spent",
            " S",
            "-",
            "IV"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " Disney",
            ".",
            " Nearly",
            " one",
            " million",
            " tourists",
            " enter",
            " this",
            " port",
            " per",
            " year",
            ".",
            " Ar",
            "uba",
            " Ports",
            " Authority",
            ",",
            " owned",
            " and",
            " operated",
            " by",
            " the",
            " Ar",
            "ub",
            "an",
            " government",
            ",",
            " runs",
            " these",
            " se",
            "ap",
            "orts",
            ".",
            "Ar",
            "ub",
            "us",
            " is",
            " a",
            " government",
            "-owned",
            " bus",
            " company",
            ".",
            " Its",
            " buses",
            " operate",
            " from",
            " ",
            "3",
            ":",
            "30",
            "Âł",
            "a",
            ".m",
            ".",
            " until",
            " ",
            "12",
            ":",
            "30",
            "Âł",
            "a",
            ".m"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.383,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.357,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "200",
            "3",
            ".",
            "  ",
            "Er",
            "nst",
            " Z",
            "erm",
            "elo",
            ",",
            " \"",
            "Unt",
            "ers",
            "uch",
            "ungen",
            " Ã¼ber",
            " die",
            " Grund",
            "lagen",
            " der",
            " Meng",
            "en",
            "leh",
            "re",
            " I",
            ",\"",
            " Math",
            "em",
            "atische",
            " Ann",
            "alen",
            " ",
            "65",
            ":",
            " (",
            "190",
            "8",
            ")",
            " pp",
            ".",
            "Âł",
            "261",
            "–",
            "81",
            ".",
            " PDF",
            " download",
            " via",
            " dig",
            "ize",
            "its",
            "chr",
            "if",
            "ten",
            ".de",
            "Translated",
            " in",
            ":",
            " Jean",
            " van",
            " He"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"(",
            "K",
            "le",
            "ene",
            " ",
            "195",
            "2",
            ":",
            "317",
            ")",
            " (",
            "i",
            ".e",
            ".,",
            " the",
            " Church",
            " thesis",
            ").",
            "  ",
            " ",
            " Kos",
            "ovsky",
            ",",
            " N",
            ".K",
            ".",
            " Elements",
            " of",
            " Mathematical",
            " Logic",
            " and",
            " its",
            " Application",
            " to",
            " the",
            " theory",
            " of",
            " Sub",
            "recursive",
            " Algorithms",
            ",",
            " LSU",
            " Pub",
            "l",
            ".,",
            " L",
            "ening",
            "rad",
            ",",
            " ",
            "198",
            "1",
            " ",
            " A",
            ".A",
            ".",
            " Mark",
            "ov",
            " (",
            "195",
            "4",
            ")",
            " Theory"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " groups",
            " (",
            "although",
            " with",
            " exceptions",
            ")",
            " are",
            ":",
            " the",
            " mode",
            " of",
            " micro",
            "spor",
            "ogenesis",
            " and",
            " the",
            " position",
            " of",
            " the",
            " ov",
            "ary",
            ".",
            " The",
            " '",
            "lower",
            " As",
            "par",
            "ag",
            "ales",
            "'",
            " typically",
            " have",
            " simultaneous",
            " micro",
            "spor",
            "ogenesis",
            " (",
            "i",
            ".e",
            ".",
            " cell",
            " walls",
            " develop",
            " only",
            " after",
            " both",
            " me",
            "iotic",
            " divisions",
            "),",
            " which",
            " appears",
            " to",
            " be",
            " an",
            " ap",
            "omor",
            "phy",
            " within",
            " the",
            " monoc",
            "ots",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.095
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " revised",
            " his",
            " Sp",
            "and",
            "au",
            " writings",
            " into",
            " two",
            " autobi",
            "ographical",
            " books",
            ",",
            " Inside",
            " the",
            " Third",
            " Reich",
            " (",
            "in",
            " German",
            ",",
            " Er",
            "inner",
            "ungen",
            ",",
            " or",
            " Rem",
            "in",
            "isc",
            "ences",
            ")",
            " and",
            " Sp",
            "and",
            "au",
            ":",
            " The",
            " Secret",
            " Di",
            "aries",
            ".",
            " He",
            " later",
            " published",
            " a",
            " work",
            " about",
            " Him",
            "ml",
            "er",
            " and",
            " the",
            " SS",
            " which",
            " has",
            " been",
            " published",
            " in",
            " English",
            " as",
            " The",
            " Slave",
            " State",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " body",
            " is",
            " an",
            " aircraft",
            " body",
            " shaped",
            " to",
            " produce",
            " lift",
            ".",
            " If",
            " there",
            " are",
            " any",
            " wings",
            ",",
            " they",
            " are",
            " too",
            " small",
            " to",
            " provide",
            " significant",
            " lift",
            " and",
            " are",
            " used",
            " only",
            " for",
            " stability",
            " and",
            " control",
            ".",
            " L",
            "ifting",
            " bodies",
            " are",
            " not",
            " efficient",
            ":",
            " they",
            " suffer",
            " from",
            " high",
            " drag",
            ",",
            " and",
            " must",
            " also",
            " travel",
            " at",
            " high",
            " speed",
            " to",
            " generate",
            " enough",
            " lift",
            " to",
            " fly",
            ".",
            " Many",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.237,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " realised",
            ".",
            " Walter",
            " Isaac",
            "son",
            " as",
            "cribes",
            " Ada",
            "'s",
            " insight",
            " regarding",
            " the",
            " application",
            " of",
            " computing",
            " to",
            " any",
            " process",
            " based",
            " on",
            " logical",
            " symbols",
            " to",
            " an",
            " observation",
            " about",
            " textiles",
            ":",
            " \"",
            "When",
            " she",
            " saw",
            " some",
            " mechanical",
            " lo",
            "oms",
            " that",
            " used",
            " punch",
            "cards",
            " to",
            " direct",
            " the",
            " weaving",
            " of",
            " beautiful",
            " patterns",
            ",",
            " it",
            " reminded",
            " her",
            " of",
            " how",
            " B",
            "abbage",
            "'s",
            " engine",
            " used",
            " punched",
            " cards",
            " to",
            " make",
            " calculations"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.235,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.141,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Sid",
            " Me",
            "ier",
            "'s",
            " Ant",
            "iet",
            "am",
            "!",
            " (",
            "199",
            "9",
            ",",
            " US",
            ")",
            " American",
            " Con",
            "q",
            "est",
            ":",
            " Div",
            "ided",
            " Nation",
            " (",
            "200",
            "6",
            ",",
            " US",
            ")",
            " Forge",
            " of",
            " Freedom",
            ":",
            " The",
            " American",
            " Civil",
            " War",
            " (",
            "200",
            "6",
            ",",
            " US",
            ")",
            " The",
            " History",
            " Channel",
            ":",
            " Civil",
            " War",
            " –",
            " A",
            " Nation",
            " Div",
            "ided",
            " (",
            "200",
            "6",
            ",",
            " US",
            ")",
            " Age",
            "od",
            "'s",
            " American"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.131,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.206,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "201",
            "0",
            ").",
            " The",
            " Alps",
            ":",
            " People",
            " and",
            " pressures",
            " in",
            " the",
            " mountains",
            ",",
            " the",
            " facts",
            " at",
            " a",
            " glance",
            " All",
            "aby",
            ",",
            " Michael",
            " et",
            " al",
            ".",
            " The",
            " Encyclopedia",
            " of",
            " Earth",
            ".",
            " (",
            "200",
            "8",
            ").",
            " Berkeley",
            ":",
            " University",
            " of",
            " California",
            " Press",
            ".",
            " ",
            " Be",
            "att",
            "ie",
            ",",
            " Andrew",
            ".",
            " (",
            "200",
            "6",
            ").",
            " The",
            " Alps",
            ":",
            " A",
            " Cultural",
            " History",
            ".",
            " New",
            " York",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ray",
            ",",
            " who",
            " was",
            " post",
            "hum",
            "ously",
            " awarded",
            " the",
            " Ak",
            "ira",
            " K",
            "uros",
            "awa",
            " Award",
            " for",
            " Lifetime",
            " Achievement",
            " in",
            " Direct",
            "ing",
            " at",
            " the",
            " San",
            " Francisco",
            " International",
            " Film",
            " Festival",
            " in",
            " ",
            "199",
            "2",
            ",",
            " had",
            " said",
            " earlier",
            " of",
            " Rash",
            "omon",
            ":",
            " ",
            "Roman",
            " Pol",
            "ans",
            "ki",
            " considered",
            " K",
            "uros",
            "awa",
            " to",
            " be",
            " among",
            " the",
            " three",
            " film",
            "-makers",
            " he",
            " favored",
            " most",
            ",",
            " along",
            " with",
            " Fell"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.184,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " =",
            " noise",
            " or",
            " error",
            " associated",
            " with",
            " the",
            " particular",
            " ij",
            " data",
            " value",
            "That",
            " is",
            ",",
            " we",
            " envision",
            " an",
            " additive",
            " model",
            " that",
            " says",
            " every",
            " data",
            " point",
            " can",
            " be",
            " represented",
            " by",
            " sum",
            "ming",
            " three",
            " quantities",
            ":",
            " the",
            " true",
            " mean",
            ",",
            " averaged",
            " over",
            " all",
            " factor",
            " levels",
            " being",
            " investigated",
            ",",
            " plus",
            " an",
            " incremental",
            " component",
            " associated",
            " with",
            " the",
            " particular",
            " column",
            " (",
            "factor",
            " level",
            "),",
            " plus",
            " a",
            " final",
            " component"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.162,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "201",
            "0",
            " –",
            " Flash",
            " floods",
            " across",
            " a",
            " large",
            " part",
            " of",
            " J",
            "ammu",
            " and",
            " Kashmir",
            ",",
            " India",
            ",",
            " damages",
            " ",
            "71",
            " towns",
            " and",
            " kills",
            " at",
            " least",
            " ",
            "255",
            " people",
            ".",
            "201",
            "1",
            " –",
            " War",
            " in",
            " Afghanistan",
            ":",
            " A",
            " United",
            " States",
            " military",
            " helicopter",
            " is",
            " shot",
            " down",
            ",",
            " killing",
            " ",
            "30",
            " American",
            " special",
            " forces",
            " members",
            " and",
            " a",
            " working",
            " dog",
            ",",
            " seven",
            " Afghan",
            " soldiers",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " words",
            " for",
            " cultural",
            " artifacts",
            ",",
            " which",
            " are",
            " reconstruct",
            "ible",
            " in",
            " Proto",
            "-A",
            "ust",
            "ro",
            "asi",
            "atic",
            ")",
            " form",
            " part",
            " of",
            " the",
            " later",
            " str",
            "atum",
            ".",
            "Roger",
            " Bl",
            "ench",
            " (",
            "201",
            "7",
            ")",
            " suggests",
            " that",
            " vocabulary",
            " related",
            " to",
            " aquatic",
            " subs",
            "istence",
            " strategies",
            " (",
            "such",
            " as",
            " boats",
            ",",
            " water",
            "ways",
            ",",
            " river",
            " fauna",
            ",",
            " and",
            " fish",
            " capture",
            " techniques",
            ")",
            " can",
            " be",
            " reconstructed",
            " for",
            " Proto",
            "-A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " town",
            " of",
            " Santa",
            " Col",
            "oma",
            " d",
            "'",
            "And",
            "orra",
            ",",
            " are",
            " recognized",
            " by",
            " the",
            " ,",
            " the",
            " governing",
            " body",
            " of",
            " cast",
            "ells",
            ".",
            "See",
            " also",
            " Index",
            " of",
            " And",
            "orra",
            "-related",
            " articles",
            " Outline",
            " of",
            " And",
            "orra",
            " Bibli",
            "ography",
            " of",
            " And",
            "orra",
            "Ex",
            "plan",
            "atory",
            " notes",
            "C",
            "itations",
            "General",
            " bibliography",
            "Further",
            " reading",
            " Ber",
            "th",
            "et",
            ",",
            " El"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " external",
            " rather",
            " than",
            " internal",
            " fertil",
            "ization",
            ".",
            " Of",
            " the",
            " ov",
            "ipar",
            "ous",
            " tele",
            "ost",
            "s",
            ",",
            " most",
            " (",
            "79",
            "%)",
            " do",
            " not",
            " provide",
            " parental",
            " care",
            ".",
            " Viv",
            "ip",
            "arity",
            ",",
            " ov",
            "ov",
            "iv",
            "ip",
            "arity",
            ",",
            " or",
            " some",
            " form",
            " of",
            " parental",
            " care",
            " for",
            " eggs",
            ",",
            " whether",
            " by",
            " the",
            " male",
            ",",
            " the",
            " female",
            ",",
            " or",
            " both",
            " parents",
            " is",
            " seen",
            " in",
            " a",
            " significant",
            " fraction",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Apollo",
            " and",
            " Artem",
            "is",
            " as",
            " twins",
            ".",
            " Here",
            ",",
            " Aster",
            "ia",
            " is",
            " also",
            " stated",
            " to",
            " be",
            " Let",
            "o",
            "'s",
            " sister",
            ".",
            " Want",
            "ing",
            " escape",
            " Zeus",
            "'",
            " advances",
            ",",
            " she",
            " fl",
            "ung",
            " herself",
            " into",
            " the",
            " sea",
            " and",
            " became",
            " a",
            " floating",
            " rock",
            " called",
            " Or",
            "ty",
            "gia",
            " until",
            " the",
            " twins",
            " were",
            " born",
            ".",
            " When",
            " Let",
            "o",
            " stepped",
            " on",
            " the",
            " rock",
            ",",
            " four",
            " pillars",
            " with",
            " adam",
            "antine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "elfth",
            " of",
            " the",
            " mass",
            " of",
            " a",
            " free",
            " neutral",
            " atom",
            " of",
            " carbon",
            "-",
            "12",
            ",",
            " which",
            " is",
            " approximately",
            " .",
            " Hydro",
            "gen",
            "-",
            "1",
            " (",
            "the",
            " light",
            "est",
            " is",
            "otope",
            " of",
            " hydrogen",
            " which",
            " is",
            " also",
            " the",
            " nu",
            "clide",
            " with",
            " the",
            " lowest",
            " mass",
            ")",
            " has",
            " an",
            " atomic",
            " weight",
            " of",
            " ",
            "1",
            ".",
            "007",
            "825",
            "Âł",
            "Da",
            ".",
            " The",
            " value",
            " of",
            " this",
            " number",
            " is",
            " called",
            " the",
            " atomic"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "hait",
    "shima",
    "uml",
    "IRMWARE",
    "ruba"
  ],
  "bottom_logits": [
    " doc",
    " Fors",
    "WAR",
    " force",
    " feature"
  ],
  "act_min": -0.0,
  "act_max": 0.59
}