{
  "index": 88376,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ieces",
            " and",
            " As",
            "pects",
            " of",
            " the",
            " War",
            " (",
            "186",
            "6",
            ")",
            " poetry",
            " by",
            " Herman",
            " Mel",
            "ville",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " the",
            " Confederate",
            " Government",
            " (",
            "188",
            "1",
            ")",
            " by",
            " Jefferson",
            " Davis",
            " The",
            " Private",
            " History",
            " of",
            " a",
            " Campaign",
            " That",
            " Failed",
            " (",
            "188",
            "5",
            ")",
            " by",
            " Mark",
            " Tw",
            "ain",
            " Tex",
            "ar",
            "'s",
            " Revenge",
            ",",
            " or",
            ",",
            " North",
            " Against",
            " South",
            " (",
            "188",
            "7"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "atches",
            " on",
            " to",
            " a",
            " nipple",
            " and",
            " completes",
            " its",
            " development",
            ".",
            "Human",
            " anatomy",
            "Humans",
            " have",
            " the",
            " overall",
            " body",
            " plan",
            " of",
            " a",
            " mamm",
            "al",
            ".",
            " Humans",
            " have",
            " a",
            " head",
            ",",
            " neck",
            ",",
            " trunk",
            " (",
            "which",
            " includes",
            " the",
            " thor",
            "ax",
            " and",
            " abdomen",
            "),",
            " two",
            " arms",
            " and",
            " hands",
            ",",
            " and",
            " two",
            " legs",
            " and",
            " feet",
            ".",
            "Generally",
            ",",
            " students",
            " of",
            " certain",
            " biological",
            " sciences",
            ",",
            " param",
            "edics",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "atches",
            " on",
            " to",
            " a",
            " nipple",
            " and",
            " completes",
            " its",
            " development",
            ".",
            "Human",
            " anatomy",
            "Humans",
            " have",
            " the",
            " overall",
            " body",
            " plan",
            " of",
            " a",
            " mamm",
            "al",
            ".",
            " Humans",
            " have",
            " a",
            " head",
            ",",
            " neck",
            ",",
            " trunk",
            " (",
            "which",
            " includes",
            " the",
            " thor",
            "ax",
            " and",
            " abdomen",
            "),",
            " two",
            " arms",
            " and",
            " hands",
            ",",
            " and",
            " two",
            " legs",
            " and",
            " feet",
            ".",
            "Generally",
            ",",
            " students",
            " of",
            " certain",
            " biological",
            " sciences",
            ",",
            " param",
            "edics",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "riters",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "People",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "G",
            "eras",
            "im",
            "ov",
            " Institute",
            " of",
            " Cin",
            "emat",
            "ography",
            " alumni",
            "Ac",
            "ademic",
            " staff",
            " of",
            " High",
            " Courses",
            " for",
            " Script",
            "writers",
            " and",
            " Film",
            " Directors",
            "People",
            "'s",
            " Artists",
            " of",
            " the",
            " RS",
            "FS",
            "R",
            "Rec",
            "ipients",
            " of",
            " the",
            " Lenin",
            " Prize",
            "C",
            "annes",
            " Film",
            " Festival",
            " Award",
            " for",
            " Best"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "riters",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "People",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "G",
            "eras",
            "im",
            "ov",
            " Institute",
            " of",
            " Cin",
            "emat",
            "ography",
            " alumni",
            "Ac",
            "ademic",
            " staff",
            " of",
            " High",
            " Courses",
            " for",
            " Script",
            "writers",
            " and",
            " Film",
            " Directors",
            "People",
            "'s",
            " Artists",
            " of",
            " the",
            " RS",
            "FS",
            "R",
            "Rec",
            "ipients",
            " of",
            " the",
            " Lenin",
            " Prize",
            "C",
            "annes",
            " Film",
            " Festival",
            " Award",
            " for",
            " Best"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ublished",
            " with",
            " an",
            " appendix",
            " on",
            " the",
            " state",
            " of",
            " the",
            " organ",
            "-building",
            " industry",
            " in",
            " ",
            "192",
            "7",
            ")",
            " effectively",
            " launched",
            " the",
            " ",
            "20",
            "th",
            "-century",
            " Org",
            "el",
            "bew",
            "eg",
            "ung",
            ",",
            " which",
            " turned",
            " away",
            " from",
            " romantic",
            " extremes",
            " and",
            " redis",
            "covered",
            " bar",
            "oque",
            " principles",
            "—",
            "although",
            " this",
            " sweeping",
            " reform",
            " movement",
            " in",
            " organ",
            " building",
            " eventually",
            " went",
            " further",
            " than",
            " Schwe",
            "itzer",
            " had",
            " intended",
            ".",
            " In",
            " ",
            "190"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eres",
            " and",
            " sent",
            " his",
            " results",
            " to",
            " von",
            " Zach",
            ".",
            " On",
            " ",
            "31",
            " December",
            " ",
            "180",
            "1",
            ",",
            " von",
            " Zach",
            " and",
            " fellow",
            " celestial",
            " policeman",
            " Hein",
            "rich",
            " W",
            ".",
            " M",
            ".",
            " Ol",
            "bers",
            " found",
            " C",
            "eres",
            " near",
            " the",
            " predicted",
            " position",
            " and",
            " thus",
            " recovered",
            " it",
            ".",
            " At",
            " ",
            "2",
            ".",
            "8",
            " AU",
            " from",
            " the",
            " Sun",
            ",",
            " C",
            "eres",
            " appeared",
            " to",
            " fit",
            " the",
            " Tit",
            "ius",
            "–",
            "B"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.185,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eres",
            ":",
            " high",
            " in",
            " carbon",
            " and",
            " silicon",
            ",",
            " and",
            " perhaps",
            " partially",
            " differentiated",
            ".",
            " P",
            "allas",
            " is",
            " the",
            " parent",
            " body",
            " of",
            " the",
            " Pall",
            "adian",
            " family",
            " of",
            " asteroids",
            ".",
            "Hy",
            "gie",
            "a",
            " is",
            " the",
            " largest",
            " carbon",
            "aceous",
            " asteroid",
            " and",
            ",",
            " unlike",
            " the",
            " other",
            " largest",
            " asteroids",
            ",",
            " lies",
            " relatively",
            " close",
            " to",
            " the",
            " plane",
            " of",
            " the",
            " ecl",
            "ipt",
            "ic",
            ".",
            " It",
            " is",
            " the",
            " largest",
            " member",
            " and",
            " presumed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eres",
            " a",
            " total",
            " of",
            " ",
            "24",
            " times",
            ",",
            " the",
            " final",
            " time",
            " on",
            " ",
            "11",
            " February",
            " ",
            "180",
            "1",
            ",",
            " when",
            " illness",
            " interrupted",
            " his",
            " work",
            ".",
            " He",
            " announced",
            " his",
            " discovery",
            " on",
            " ",
            "24",
            " January",
            " ",
            "180",
            "1",
            " in",
            " letters",
            " to",
            " only",
            " two",
            " fellow",
            " astronomers",
            ",",
            " his",
            " comp",
            "atri",
            "ot",
            " Barn",
            "aba",
            " Ori",
            "ani",
            " of",
            " Milan",
            " and",
            " B",
            "ode",
            " in",
            " Berlin",
            ".",
            " He",
            " reported",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eres",
            " a",
            " total",
            " of",
            " ",
            "24",
            " times",
            ",",
            " the",
            " final",
            " time",
            " on",
            " ",
            "11",
            " February",
            " ",
            "180",
            "1",
            ",",
            " when",
            " illness",
            " interrupted",
            " his",
            " work",
            ".",
            " He",
            " announced",
            " his",
            " discovery",
            " on",
            " ",
            "24",
            " January",
            " ",
            "180",
            "1",
            " in",
            " letters",
            " to",
            " only",
            " two",
            " fellow",
            " astronomers",
            ",",
            " his",
            " comp",
            "atri",
            "ot",
            " Barn",
            "aba",
            " Ori",
            "ani",
            " of",
            " Milan",
            " and",
            " B",
            "ode",
            " in",
            " Berlin",
            ".",
            " He",
            " reported",
            " it"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "arine",
            " Lee",
            " Bates",
            "'",
            " words",
            ".",
            " Arch",
            "ival",
            " collection",
            " of",
            " America",
            " the",
            " Beautiful",
            " lantern",
            " slides",
            " from",
            " the",
            " ",
            "193",
            "0",
            "s",
            ".",
            " Another",
            " free",
            " sheet",
            " music",
            "189",
            "5",
            " songs",
            "American",
            " Christian",
            " hym",
            "ns",
            "American",
            " patriotic",
            " songs",
            "P",
            "ikes",
            " Peak",
            "History",
            " of",
            " Colorado",
            " Springs",
            ",",
            " Colorado",
            "Songs",
            " based",
            " on",
            " poems",
            "Gram",
            "my",
            " Hall",
            " of",
            " Fame",
            " Award",
            " recipients"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "useum",
            " Wol",
            "fs",
            "bug",
            ",",
            " ",
            "199",
            "9",
            ".",
            "  ",
            "  ",
            " Doyle",
            ",",
            " Jennifer",
            ",",
            " Jonathan",
            " Flat",
            "ley",
            ",",
            " and",
            " JosÃ©",
            " Est",
            "eb",
            "an",
            " Mu",
            "Ã±",
            "oz",
            ",",
            " eds",
            " (",
            "199",
            "6",
            ").",
            " Pop",
            " Out",
            ":",
            " Que",
            "er",
            " War",
            "hol",
            ".",
            " Durham",
            ":",
            " Duke",
            " University",
            " Press",
            ".",
            " Duncan",
            " F",
            "allow",
            "ell",
            ",",
            " ",
            "20",
            "th",
            " Century",
            " Characters",
            ",",
            " ch",
            ".",
            " Andy",
            " Lives",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "arine",
            " Lee",
            " Bates",
            "'",
            " words",
            ".",
            " Arch",
            "ival",
            " collection",
            " of",
            " America",
            " the",
            " Beautiful",
            " lantern",
            " slides",
            " from",
            " the",
            " ",
            "193",
            "0",
            "s",
            ".",
            " Another",
            " free",
            " sheet",
            " music",
            "189",
            "5",
            " songs",
            "American",
            " Christian",
            " hym",
            "ns",
            "American",
            " patriotic",
            " songs",
            "P",
            "ikes",
            " Peak",
            "History",
            " of",
            " Colorado",
            " Springs",
            ",",
            " Colorado",
            "Songs",
            " based",
            " on",
            " poems",
            "Gram",
            "my",
            " Hall",
            " of",
            " Fame",
            " Award",
            " recipients"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "useum",
            " Wol",
            "fs",
            "bug",
            ",",
            " ",
            "199",
            "9",
            ".",
            "  ",
            "  ",
            " Doyle",
            ",",
            " Jennifer",
            ",",
            " Jonathan",
            " Flat",
            "ley",
            ",",
            " and",
            " JosÃ©",
            " Est",
            "eb",
            "an",
            " Mu",
            "Ã±",
            "oz",
            ",",
            " eds",
            " (",
            "199",
            "6",
            ").",
            " Pop",
            " Out",
            ":",
            " Que",
            "er",
            " War",
            "hol",
            ".",
            " Durham",
            ":",
            " Duke",
            " University",
            " Press",
            ".",
            " Duncan",
            " F",
            "allow",
            "ell",
            ",",
            " ",
            "20",
            "th",
            " Century",
            " Characters",
            ",",
            " ch",
            ".",
            " Andy",
            " Lives",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " G",
            "ide",
            "'s",
            " work",
            " '",
            "ess",
            "entially",
            " modern",
            "':",
            " the",
            " '",
            "per",
            "pet",
            "ual",
            " renewal",
            " of",
            " the",
            " values",
            " by",
            " which",
            " one",
            " lives",
            ".'\"",
            " G",
            "ide",
            " wrote",
            " in",
            " his",
            " Journal",
            " in",
            " ",
            "193",
            "0",
            ":",
            " \"",
            "The",
            " only",
            " drama",
            " that",
            " really",
            " interests",
            " me",
            " and",
            " that",
            " I",
            " should",
            " always",
            " be",
            " willing",
            " to",
            " depict",
            " anew",
            ",",
            " is",
            " the",
            " debate",
            " of",
            " the",
            " individual",
            " with",
            " whatever",
            " keeps",
            " him"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eward",
            ",",
            " and",
            " others",
            " at",
            " Hampton",
            " Roads",
            ".",
            " Lincoln",
            " refused",
            " to",
            " negotiate",
            " with",
            " the",
            " Confeder",
            "acy",
            " as",
            " a",
            " co",
            "equal",
            ";",
            " his",
            " objective",
            " to",
            " end",
            " the",
            " fighting",
            " was",
            " not",
            " realized",
            ".",
            " On",
            " April",
            " ",
            "1",
            ",",
            " ",
            "186",
            "5",
            ",",
            " Grant",
            " nearly",
            " enc",
            "irc",
            "led",
            " Petersburg",
            " in",
            " a",
            " siege",
            ".",
            " The",
            " Confederate",
            " government",
            " evacuated",
            " Richmond",
            " and",
            " Lincoln",
            " visited",
            " the",
            " conquered",
            " capital",
            ".",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eward",
            ",",
            " worked",
            " to",
            " block",
            " this",
            " and",
            " threatened",
            " war",
            " if",
            " any",
            " country",
            " officially",
            " recognized",
            " the",
            " existence",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            ".",
            " In",
            " ",
            "186",
            "1",
            ",",
            " Sou",
            "ther",
            "ners",
            " voluntarily",
            " embargo",
            "ed",
            " cotton",
            " shipments",
            ",",
            " hoping",
            " to",
            " start",
            " an",
            " economic",
            " depression",
            " in",
            " Europe",
            " that",
            " would",
            " force",
            " Britain",
            " to",
            " enter",
            " the",
            " war",
            " to",
            " get",
            " cotton",
            ",",
            " but",
            " this",
            " did",
            " not",
            " work",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.492,
            0.081,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.061,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ess",
            "ive",
            " need",
            " to",
            " control",
            " their",
            " surroundings",
            " (",
            "the",
            " psychological",
            " aspect",
            "),",
            " and",
            " an",
            " unusual",
            " ability",
            " to",
            " process",
            " fast",
            "-moving",
            " information",
            " (",
            "phys",
            "iological",
            ").",
            " In",
            " this",
            ",",
            " researchers",
            " have",
            " noted",
            " a",
            " strong",
            " correlation",
            " between",
            " rac",
            "ers",
            "'",
            " psychological",
            " profiles",
            " and",
            " those",
            " of",
            " fighter",
            " pilots",
            ".",
            " In",
            " tests",
            " comparing",
            " rac",
            "ers",
            " to",
            " members",
            " of",
            " the",
            " general",
            " public",
            ",",
            " the",
            " greater",
            " the",
            " complexity",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ess",
            " of",
            " For",
            "l",
            "Ã¬",
            " and",
            " Lady",
            " of",
            " Im",
            "ola",
            ",",
            " is",
            " one",
            " of",
            " the",
            " few",
            " confirmed",
            " female",
            " al",
            "chem",
            "ists",
            " after",
            " Mary",
            " the",
            " Jew",
            "ess",
            ".",
            " As",
            " she",
            " owned",
            " an",
            " ap",
            "oth",
            "ec",
            "ary",
            ",",
            " she",
            " would",
            " practice",
            " science",
            " and",
            " conduct",
            " experiments",
            " in",
            " her",
            " bot",
            "anic",
            " gardens",
            " and",
            " laboratories",
            ".",
            " Being",
            " knowledgeable",
            " in",
            " al",
            "chemy",
            " and",
            " pharmac",
            "ology",
            ",",
            " she",
            " recorded",
            " all"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.488,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.07,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ee",
            "vi",
            ",",
            " Indian",
            " film",
            " actor",
            ",",
            " producer",
            " and",
            " politician",
            "195",
            "6",
            " –",
            " Paul",
            " Mol",
            "itor",
            ",",
            " American",
            " baseball",
            " player",
            " and",
            " coach",
            " ",
            " ",
            "195",
            "6",
            "  ",
            " –",
            " Peter",
            " Taylor",
            ",",
            " Australian",
            " cr",
            "ick",
            "eter",
            "195",
            "7",
            " –",
            " Steve",
            " Davis",
            ",",
            " English",
            " sn",
            "ook",
            "er",
            " player",
            ",",
            " sport",
            "sc",
            "aster",
            ",",
            " and",
            " author",
            " ",
            " ",
            "195",
            "7",
            "  ",
            " –"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ires",
            ":",
            "According",
            " to",
            " an",
            " assistant",
            ",",
            " Stanley",
            " Kub",
            "rick",
            " considered",
            " K",
            "uros",
            "awa",
            " to",
            " be",
            " \"",
            "one",
            " of",
            " the",
            " great",
            " film",
            " directors",
            "\"",
            " and",
            " spoke",
            " of",
            " him",
            " \"",
            "cons",
            "ist",
            "ently",
            " and",
            " adm",
            "iring",
            "ly",
            "\",",
            " to",
            " the",
            " point",
            " that",
            " a",
            " letter",
            " from",
            " him",
            " \"",
            "me",
            "ant",
            " more",
            " than",
            " any",
            " Oscar",
            "\"",
            " and",
            " caused",
            " him",
            " to",
            " agon",
            "ize",
            " for",
            " months",
            " over",
            " drafting"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ures",
            " came",
            " to",
            " dominate",
            " agricultural",
            " output",
            ".",
            "Today",
            ",",
            " small",
            " farms",
            " produce",
            " about",
            " a",
            " third",
            " of",
            " the",
            " world",
            "'s",
            " food",
            ",",
            " but",
            " large",
            " farms",
            " are",
            " prevalent",
            ".",
            " The",
            " largest",
            " one",
            " percent",
            " of",
            " farms",
            " in",
            " the",
            " world",
            " are",
            " greater",
            " than",
            " ",
            "50",
            " hectares",
            " and",
            " operate",
            " more",
            " than",
            " ",
            "70",
            " percent",
            " of",
            " the",
            " world",
            "'s",
            " far",
            "mland",
            ".",
            " Nearly",
            " ",
            "40",
            " percent",
            " of",
            " agricultural",
            " land"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "inally",
            " between",
            " Europe",
            " and",
            " Africa",
            " to",
            " the",
            " east",
            ",",
            " and",
            " the",
            " Americas",
            " to",
            " the",
            " west",
            ".",
            " As",
            " one",
            " component",
            " of",
            " the",
            " interconnected",
            " World",
            " Ocean",
            ",",
            " it",
            " is",
            " connected",
            " in",
            " the",
            " north",
            " to",
            " the",
            " Arctic",
            " Ocean",
            ",",
            " to",
            " the",
            " Pacific",
            " Ocean",
            " in",
            " the",
            " southwest",
            ",",
            " the",
            " Indian",
            " Ocean",
            " in",
            " the",
            " southeast",
            ",",
            " and",
            " the",
            " Southern",
            " Ocean",
            " in",
            " the",
            " south",
            " (",
            "other",
            " definitions",
            " describe",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ishes",
            " some",
            " members",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            " from",
            " L",
            "ilia",
            "les",
            ".",
            " Micro",
            "spor",
            "ogenesis",
            " involves",
            " a",
            " cell",
            " dividing",
            " twice",
            " (",
            "me",
            "iot",
            "ically",
            ")",
            " to",
            " form",
            " four",
            " daughter",
            " cells",
            ".",
            " There",
            " are",
            " two",
            " kinds",
            " of",
            " micro",
            "spor",
            "ogenesis",
            ":",
            " successive",
            " and",
            " simultaneous",
            " (",
            "although",
            " intermedi",
            "ates",
            " exist",
            ").",
            " In",
            " successive",
            " micro",
            "spor",
            "ogenesis",
            ",",
            " walls",
            " are",
            " laid",
            " down",
            " separating",
            " the",
            " daughter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " represents",
            " one",
            " of",
            " the",
            " fastest",
            " growing",
            " and",
            " dynamic",
            " sectors",
            " in",
            " Albania",
            ".",
            " V",
            "odafone",
            " Albania",
            ",",
            " Tele",
            "kom",
            " Albania",
            " and",
            " Alb",
            "tele",
            "com",
            " are",
            " the",
            " three",
            " large",
            " providers",
            " of",
            " mobile",
            " and",
            " internet",
            " in",
            " Albania",
            ".",
            " As",
            " of",
            " the",
            " Electronic",
            " and",
            " Postal",
            " Communications",
            " Authority",
            " (",
            "AKE",
            "P",
            ")",
            " in",
            " ",
            "201",
            "8",
            ",",
            " the",
            " country",
            " had",
            " approximately",
            " ",
            "2",
            ".",
            "7",
            " million",
            " active",
            " mobile"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ewish",
            " American",
            " military",
            " personnel",
            "J",
            "ewish",
            " American",
            " poets",
            "L",
            "GBT",
            " Buddh",
            "ists",
            "L",
            "GBT",
            " Jews",
            "L",
            "GBT",
            " people",
            " from",
            " Colorado",
            "L",
            "GBT",
            " people",
            " from",
            " New",
            " Jersey",
            "L",
            "GBT",
            " people",
            " from",
            " New",
            " York",
            " (",
            "state",
            ")",
            "Loc",
            "ust",
            " Music",
            " artists",
            "Military",
            " personnel",
            " from",
            " New",
            " Jersey",
            "Mont",
            "clair",
            " State",
            " University",
            " alumni",
            "National",
            " Book",
            " Award",
            " winners"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "enger",
            "'s",
            " contributions",
            " to",
            " economic",
            " theory",
            " were",
            " closely",
            " followed",
            " by",
            " those",
            " of",
            " Eug",
            "en",
            " B",
            "Ã¶",
            "hm",
            " von",
            " B",
            "aw",
            "erk",
            " and",
            " Friedrich",
            " von",
            " W",
            "ies",
            "er",
            ".",
            " These",
            " three",
            " economists",
            " became",
            " what",
            " is",
            " known",
            " as",
            " the",
            " \"",
            "first",
            " wave",
            "\"",
            " of",
            " the",
            " Austrian",
            " School",
            ".",
            " B",
            "Ã¶",
            "hm",
            "-B",
            "aw",
            "erk",
            " wrote",
            " extensive",
            " critiques",
            " of",
            " Karl",
            " Marx",
            " in",
            " the",
            " ",
            "188",
            "0"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.036,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ewish",
            " women",
            " writers",
            "Met",
            "aph",
            "ys",
            "icians",
            "Nov",
            "el",
            "ists",
            " from",
            " New",
            " York",
            " (",
            "state",
            ")",
            "Object",
            "iv",
            "ists",
            "People",
            " with",
            " acquired",
            " American",
            " citizenship",
            "Phil",
            "osoph",
            "ers",
            " from",
            " New",
            " York",
            " (",
            "state",
            ")",
            "Political",
            " philosophers",
            "P",
            "seud",
            "onymous",
            " women",
            " writers",
            "Saint",
            " Petersburg",
            " State",
            " University",
            " alumni",
            "Screen",
            "writers",
            " from",
            " New",
            " York",
            " (",
            "state",
            ")",
            "S",
            "ov",
            "iet"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ick",
            ",",
            " librarian",
            " and",
            " president",
            " of",
            " the",
            " Academy",
            ",",
            " may",
            " have",
            " said",
            " she",
            " named",
            " it",
            " after",
            " her",
            " supposed",
            " uncle",
            " Oscar",
            " in",
            " ",
            "192",
            "1",
            ".",
            " The",
            " only",
            " cor",
            "rob",
            "oration",
            " was",
            " a",
            " ",
            "193",
            "8",
            " clipping",
            " from",
            " the",
            " Los",
            " Angeles",
            " Examiner",
            ",",
            " in",
            " which",
            " Herr",
            "ick",
            " told",
            " a",
            " story",
            " of",
            " her",
            " and",
            " her",
            " husband",
            " joking",
            " with",
            " each",
            " other",
            " using",
            " the",
            " phrase",
            ",",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.457,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "icism",
            "Âł",
            "–",
            " from",
            " Religious",
            "T",
            "olerance",
            ".org",
            " What",
            " do",
            " Ag",
            "nost",
            "ics",
            " Believe",
            "?",
            "Âł",
            "–",
            " A",
            " Jewish",
            " perspective",
            " F",
            "ides",
            " et",
            " Ratio",
            " ",
            "Âł",
            "–",
            " the",
            " relationship",
            " between",
            " faith",
            " and",
            " reason",
            " Kar",
            "ol",
            " Woj",
            "ty",
            "la",
            " [",
            "199",
            "8",
            "]",
            " The",
            " Natural",
            " Religion",
            " by",
            " Brendan",
            " Conn",
            "olly",
            ",",
            " ",
            "200",
            "8",
            " ",
            " ",
            "Ep",
            "istem",
            "ological",
            " theories"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.457,
            -0.0,
            -0.0,
            -0.0,
            0.103,
            -0.0,
            -0.0,
            -0.0,
            0.128,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "icism",
            "French",
            " Nobel",
            " laure",
            "ates",
            "N",
            "obel",
            " laure",
            "ates",
            " in",
            " Physiology",
            " or",
            " Medicine",
            "French",
            " Roman",
            " Catholics",
            "French",
            " collaborators",
            " with",
            " Nazi",
            " Germany",
            "Kn",
            "ights",
            " of",
            " the",
            " Legion",
            " of",
            " Honour",
            "Members",
            " of",
            " the",
            " Pont",
            "if",
            "ical",
            " Academy",
            " of",
            " Sciences",
            "Cor",
            "respond",
            "ing",
            " Members",
            " of",
            " the",
            " Russian",
            " Academy",
            " of",
            " Sciences",
            " (",
            "191",
            "7",
            "–",
            "192",
            "5",
            ")",
            "Cor",
            "respond"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.062,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "eter",
            " cata",
            "lectic",
            " verses",
            ":",
            " These",
            " are",
            " long",
            " lines",
            " of",
            " an",
            "apest",
            "s",
            ",",
            " tro",
            "che",
            "es",
            " or",
            " iam",
            "bs",
            " (",
            "where",
            " each",
            " line",
            " is",
            " ideally",
            " measured",
            " in",
            " four",
            " dip",
            "odes",
            " or",
            " pairs",
            " of",
            " feet",
            "),",
            " used",
            " in",
            " various",
            " situations",
            " within",
            " each",
            " play",
            " such",
            " as",
            ":",
            " formal",
            " debates",
            " or",
            " ag",
            "ons",
            " between",
            " characters",
            " (",
            "typically",
            " in",
            " an",
            "apest",
            "ic",
            " rhythm",
            ");",
            " excited",
            " dialogue"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " On",
            " ",
            "22",
            "Âł",
            "January",
            " ",
            "201",
            "4",
            ",",
            " European",
            " Space",
            " Agency",
            " (",
            "ESA",
            ")",
            " scientists",
            " reported",
            " the",
            " detection",
            ",",
            " for",
            " the",
            " first",
            " definitive",
            " time",
            ",",
            " of",
            " water",
            " vapor",
            " on",
            " C",
            "eres",
            ",",
            " the",
            " largest",
            " object",
            " in",
            " the",
            " asteroid",
            " belt",
            ".",
            " The",
            " detection",
            " was",
            " made",
            " by",
            " using",
            " the",
            " far",
            "-in",
            "frared",
            " abilities",
            " of",
            " the",
            " Hers",
            "ch",
            "el",
            " Space",
            " Observatory",
            ".",
            " The",
            " finding",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.451,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "iÃ¨re",
            ",",
            " La",
            " RÃ©",
            "vÃ©",
            "lation",
            " d",
            "'H",
            "erm",
            "Ã¨s",
            " Tr",
            "ism",
            "Ã©g",
            "iste",
            ",",
            " Paris",
            ",",
            " Les",
            " Bel",
            "les",
            " Let",
            "tres",
            ",",
            " ",
            "201",
            "4",
            " (",
            ",",
            " O",
            "CL",
            "C",
            " ",
            "897",
            "235",
            "256",
            ").",
            " Robert",
            " H",
            "alle",
            "ux",
            " and",
            " Henri",
            "-D",
            "omin",
            "ique",
            " S",
            "aff",
            "rey",
            " (",
            "eds",
            ".),",
            " Les",
            " al",
            "ch",
            "im",
            "istes",
            " g",
            "rec",
            "s",
            ",",
            " t",
            ".",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "edes",
            " under",
            " Gust",
            "av",
            "us",
            " Ad",
            "olph",
            "us",
            " defeat",
            " the",
            " Holy",
            " Roman",
            " Empire",
            " during",
            " the",
            " Thirty",
            " Years",
            "'",
            " War",
            ".",
            "164",
            "2",
            " –",
            " Irish",
            " Confederate",
            " Wars",
            ":",
            " A",
            " Confederate",
            " Irish",
            " militia",
            " is",
            " routed",
            " in",
            " the",
            " Battle",
            " of",
            " Kil",
            "rush",
            " when",
            " it",
            " attempts",
            " to",
            " halt",
            " the",
            " progress",
            " of",
            " a",
            " Royal",
            "ist",
            " Army",
            ".",
            "171",
            "5",
            " –",
            " The",
            " P",
            "oc",
            "otal",
            "igo",
            " Mass",
            "acre",
            " triggers"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.418,
            -0.0,
            -0.0,
            0.075,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.068,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ative",
            " (",
            "the",
            "oret",
            "ik",
            "Äĵ",
            ")",
            " philosophy",
            " which",
            " is",
            " \"",
            "the",
            "ological",
            "\"",
            " and",
            " studies",
            " the",
            " divine",
            ".",
            " He",
            " wrote",
            " in",
            " his",
            " Met",
            "aph",
            "ysics",
            " (",
            "102",
            "6",
            "a",
            "16",
            "):",
            "Sub",
            "stance",
            " ",
            "A",
            "rist",
            "otle",
            " examines",
            " the",
            " concepts",
            " of",
            " substance",
            " (",
            "ous",
            "ia",
            ")",
            " and",
            " essence",
            " (",
            "to",
            " ti",
            " Ãª",
            "n",
            " ein",
            "ai",
            ",",
            " \"",
            "the",
            " what",
            " it",
            " was",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " environment",
            "\"",
            " in",
            " the",
            " European",
            " Union",
            " under",
            " directive",
            " ",
            "67",
            "/",
            "548",
            "/",
            "EE",
            "C",
            ".",
            "The",
            " International",
            " Agency",
            " for",
            " Research",
            " on",
            " Cancer",
            " (",
            "I",
            "ARC",
            ")",
            " recognizes",
            " arsen",
            "ic",
            " and",
            " in",
            "organic",
            " arsen",
            "ic",
            " compounds",
            " as",
            " group",
            " ",
            "1",
            " carcin",
            "ogens",
            ",",
            " and",
            " the",
            " EU",
            " lists",
            " arsen",
            "ic",
            " tri",
            "oxide",
            ",",
            " arsen",
            "ic",
            " pent",
            "oxide",
            ",",
            " and",
            " arsen",
            "ate",
            " salts",
            " as",
            " category"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "acial",
            " zone",
            ",",
            " which",
            " covers",
            " the",
            " glac",
            "iated",
            " areas",
            " of",
            " the",
            " mountain",
            ".",
            " Clim",
            "atic",
            " conditions",
            " show",
            " var",
            "iances",
            " within",
            " the",
            " same",
            " zones",
            ";",
            " for",
            " example",
            ",",
            " weather",
            " conditions",
            " at",
            " the",
            " head",
            " of",
            " a",
            " mountain",
            " valley",
            ",",
            " extending",
            " directly",
            " from",
            " the",
            " peaks",
            ",",
            " are",
            " colder",
            " and",
            " more",
            " severe",
            " than",
            " those",
            " at",
            " the",
            " mouth",
            " of",
            " a",
            " valley",
            " which",
            " tend",
            " to",
            " be",
            " less",
            " severe",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " During",
            " ",
            "196",
            "2",
            "–",
            "196",
            "3",
            ",",
            " Gins",
            "berg",
            " and",
            " Or",
            "lov",
            "sky",
            " travelled",
            " extensively",
            " across",
            " India",
            ",",
            " living",
            " half",
            " a",
            " year",
            " at",
            " a",
            " time",
            " in",
            " Cal",
            "cut",
            "ta",
            " (",
            "now",
            " Kolkata",
            ")",
            " and",
            " Ben",
            "ares",
            " (",
            "Var",
            "an",
            "asi",
            ").",
            " On",
            " his",
            " road",
            " to",
            " India",
            " he",
            " stayed",
            " two",
            " months",
            " in",
            " Athens",
            " (",
            " August",
            " ",
            "29",
            ",",
            " ",
            "196",
            "1",
            " -"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "urring",
            " as",
            " \"",
            "face",
            "-saving",
            " fabrication",
            "\",",
            " and",
            " that",
            " Armstrong",
            " himself",
            " later",
            " admitted",
            " to",
            " miss",
            "pe",
            "aking",
            " the",
            " line",
            ".",
            "About",
            " seven",
            " minutes",
            " after",
            " stepping",
            " onto",
            " the",
            " Moon",
            "'s",
            " surface",
            ",",
            " Armstrong",
            " collected",
            " a",
            " contingency",
            " soil",
            " sample",
            " using",
            " a",
            " sample",
            " bag",
            " on",
            " a",
            " stick",
            ".",
            " He",
            " then",
            " folded",
            " the",
            " bag",
            " and",
            " tucked",
            " it",
            " into",
            " a",
            " pocket",
            " on",
            " his",
            " right",
            " thigh",
            ".",
            " This",
            " was"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.029,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "esy",
            ".",
            " In",
            " anger",
            ",",
            " Her",
            "acles",
            " sn",
            "atched",
            " the",
            " sacred",
            " tripod",
            " and",
            " started",
            " walking",
            " away",
            ",",
            " intending",
            " to",
            " start",
            " his",
            " own",
            " oracle",
            ".",
            " However",
            ",",
            " Apollo",
            " did",
            " not",
            " tolerate",
            " this",
            " and",
            " stopped",
            " Her",
            "acles",
            ";",
            " a",
            " duel",
            " ensued",
            " between",
            " them",
            ".",
            " Artem",
            "is",
            " rushed",
            " to",
            " support",
            " Apollo",
            ",",
            " while",
            " Athena",
            " supported",
            " Her",
            "acles",
            ".",
            " Soon",
            ",",
            " Zeus",
            " threw",
            " his",
            " thunder",
            "bolt",
            " between"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.293,
            0.13,
            -0.0,
            -0.0,
            -0.0,
            0.277,
            0.163,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " over",
            "est",
            "imates",
            " himself",
            " and",
            " under",
            "est",
            "imates",
            " his",
            " aunt",
            "'s",
            " mental",
            " ac",
            "uity",
            ".",
            " Miss",
            " Mar",
            "ple",
            " employs",
            " young",
            " women",
            " (",
            "including",
            " Clara",
            ",",
            " Emily",
            ",",
            " Alice",
            ",",
            " Esther",
            ",",
            " G",
            "w",
            "enda",
            ",",
            " and",
            " Amy",
            ")",
            " from",
            " a",
            " nearby",
            " orphan",
            "age",
            ",",
            " whom",
            " she",
            " trains",
            " for",
            " service",
            " as",
            " general",
            " house",
            "maids",
            " after",
            " the",
            " retirement",
            " of",
            " her",
            " long",
            "-time",
            " maid",
            "-house",
            "keeper"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.212,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ials",
            " to",
            " Abraham",
            " Lincoln",
            " Removal",
            " of",
            " Confederate",
            " monuments",
            " and",
            " memor",
            "ials",
            "Other",
            " civil",
            " wars",
            " in",
            " modern",
            " history",
            " Box",
            "er",
            " Rebellion",
            " Chinese",
            " Civil",
            " War",
            " Finnish",
            " Civil",
            " War",
            " Mexican",
            " Revolution",
            " Russian",
            " Civil",
            " War",
            " Spanish",
            " Civil",
            " War",
            " Ta",
            "iping",
            " Rebellion",
            "References",
            "Notes",
            "C",
            "itations",
            "B",
            "ibli",
            "ography",
            "  ",
            " ",
            " B",
            "ering",
            "er"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " considered",
            " \"",
            "ego",
            "ists",
            "\"",
            " sometimes",
            " gave",
            " more",
            " than",
            " expected",
            " because",
            " that",
            " would",
            " help",
            " others",
            ",",
            " leading",
            " to",
            " the",
            " conclusion",
            " that",
            " there",
            " are",
            " other",
            " factors",
            " in",
            " charity",
            ",",
            " such",
            " as",
            " a",
            " person",
            "'s",
            " environment",
            " and",
            " values",
            ".",
            "Psych",
            "ology",
            "The",
            " International",
            " Encyclopedia",
            " of",
            " the",
            " Social",
            " Sciences",
            " defines",
            " psychological",
            " altru",
            "ism",
            " as",
            " \"",
            "a",
            " motivational",
            " state",
            " to",
            " increase",
            " another",
            "'s",
            " welfare"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " North",
            " America",
            "Countries",
            " and",
            " territories",
            " where",
            " English",
            " is",
            " an",
            " official",
            " language",
            "Member",
            " states",
            " of",
            " the",
            " Caribbean",
            " Community",
            "Member",
            " states",
            " of",
            " the",
            " Commonwealth",
            " of",
            " Nations",
            "Member",
            " states",
            " of",
            " the",
            " Organisation",
            " of",
            " Eastern",
            " Caribbean",
            " States",
            "Member",
            " states",
            " of",
            " the",
            " United",
            " Nations",
            "Small",
            " Island",
            " Developing",
            " States",
            "British",
            " Le",
            "eward",
            " Islands",
            "Former",
            " British",
            " colonies",
            " and",
            " protector",
            "ates",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.067,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.222,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "-m",
            "ountain",
            "-",
            "aval",
            "anche",
            " ",
            "195",
            "0",
            "–",
            "195",
            "1",
            " winter",
            "-of",
            "-",
            "terror",
            " aval",
            "anches",
            " February",
            " ",
            "10",
            ",",
            " ",
            "197",
            "0",
            " Val",
            " d",
            "'",
            "Is",
            "Ã¨re",
            " avalanche",
            " February",
            " ",
            "9",
            ",",
            " ",
            "199",
            "9",
            " Mont",
            "roc",
            " avalanche",
            " February",
            " ",
            "21",
            ",",
            " ",
            "199",
            "9",
            " Ev",
            "ol",
            "Ã¨ne",
            " avalanche",
            " February",
            " ",
            "23",
            ",",
            " ",
            "199",
            "9"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.181,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ovsky",
            " films",
            ",",
            " and",
            " that",
            " Fell",
            "ini",
            " began",
            " to",
            " make",
            " Fell",
            "ini",
            " films",
            " [...]",
            " Bu",
            "Ã±",
            "uel",
            " nearly",
            " always",
            " made",
            " Bu",
            "Ã±",
            "uel",
            " films",
            ".\"",
            " This",
            " past",
            "iche",
            " of",
            " one",
            "'s",
            " own",
            " work",
            " has",
            " been",
            " derog",
            "ator",
            "ily",
            " termed",
            " as",
            " \"",
            "self",
            "-k",
            "ar",
            "aoke",
            "\".",
            "V",
            "ad",
            "im",
            " Yus",
            "ov",
            "T",
            "ark",
            "ovsky",
            " worked",
            " in",
            " close",
            " collaboration",
            " with",
            " cinemat",
            "ographer",
            " Vad"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.17,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " need",
            " to",
            " keep",
            " their",
            " skin",
            " damp",
            ".",
            "Modern",
            " amphib",
            "ians",
            " have",
            " a",
            " simplified",
            " anatomy",
            " compared",
            " to",
            " their",
            " ancestors",
            " due",
            " to",
            " pa",
            "edom",
            "orph",
            "osis",
            ",",
            " caused",
            " by",
            " two",
            " evolutionary",
            " trends",
            ":",
            " mini",
            "atur",
            "ization",
            " and",
            " an",
            " unusually",
            " large",
            " genome",
            ",",
            " which",
            " result",
            " in",
            " a",
            " slower",
            " growth",
            " and",
            " development",
            " rate",
            " compared",
            " to",
            " other",
            " verte",
            "brates",
            ".",
            " Another",
            " reason",
            " for",
            " their",
            " size",
            " is",
            " associated"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.131,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.051,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.031,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ification",
            " by",
            " J",
            ".",
            " Marshall",
            " U",
            "nger",
            ".",
            " (",
            "199",
            "4",
            ")",
            " ",
            "Ag",
            "gl",
            "utin",
            "ative",
            " languages",
            "Central",
            " Asia",
            "Prop",
            "osed",
            " language",
            " families",
            "<|begin_of_text|>",
            "A",
            "ust",
            "rian",
            " German",
            " (),",
            " Austrian",
            " Standard",
            " German",
            " (",
            "AS",
            "G",
            "),",
            " Standard",
            " Austrian",
            " German",
            " (),",
            " Austrian",
            " High",
            " German",
            " (),",
            " or",
            " simply",
            " just",
            " Austrian",
            " (),",
            " is",
            " an",
            " official",
            " and",
            " standard",
            " variety",
            " of",
            " Standard",
            " High",
            " German",
            " written"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.047,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.123,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " is",
            " unw",
            "ritten",
            ",",
            " it",
            " also",
            " has",
            " an",
            " inherent",
            " onset",
            " .",
            " For",
            " the",
            " syll",
            "able",
            " ,",
            " which",
            " requires",
            " one",
            " or",
            " the",
            " other",
            " of",
            " the",
            " inherent",
            " sounds",
            " to",
            " be",
            " overt",
            ",",
            " it",
            " is",
            " ",
            " that",
            " is",
            " written",
            ".",
            " Thus",
            " it",
            " is",
            " the",
            " r",
            "ime",
            " (",
            "v",
            "owel",
            ")",
            " that",
            " is",
            " basic",
            " to",
            " the",
            " system",
            ".",
            "M",
            "ero",
            "itic",
            "It",
            " is",
            " difficult",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.084,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            "off",
            " argues",
            " that",
            " proto",
            "-A",
            "f",
            "ro",
            "asi",
            "atic",
            " syll",
            "ables",
            " dis",
            "allowed",
            " conson",
            "ant",
            " clusters",
            " or",
            " vowels",
            " at",
            " the",
            " end",
            " of",
            " a",
            " syll",
            "able",
            ".",
            "S",
            "yll",
            "able",
            " weight",
            " plays",
            " an",
            " important",
            " role",
            " in",
            " AA",
            ",",
            " especially",
            " in",
            " Ch",
            "adic",
            ";",
            " it",
            " can",
            " affect",
            " the",
            " form",
            " of",
            " aff",
            "ix",
            "es",
            " attached",
            " to",
            " a",
            " word",
            ".",
            "Con",
            "son",
            "ant",
            " systems",
            "Several"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " Square",
            " (",
            "Tur",
            "kish",
            ":",
            " Za",
            "fer",
            " Mey",
            "dan",
            "Ä±",
            "),",
            " the",
            " marble",
            " and",
            " bronze",
            " statue",
            " was",
            " crafted",
            " by",
            " the",
            " renowned",
            " Italian",
            " sculpt",
            "or",
            " Piet",
            "ro",
            " Canon",
            "ica",
            " in",
            " ",
            "192",
            "7",
            " and",
            " depicts",
            " a",
            " standing",
            " AtatÃ¼rk",
            " who",
            " wears",
            " a",
            " Republic",
            " era",
            " modern",
            " military",
            " uniform",
            ",",
            " with",
            " the",
            " rank",
            " Field",
            " Marshal",
            ".",
            "Mon",
            "ument",
            " to",
            " a",
            " Secure",
            ",",
            " Conf",
            "ident",
            " Future"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " running",
            " east",
            " passed",
            " through",
            " Ankara",
            " and",
            " a",
            " succession",
            " of",
            " em",
            "per",
            "ors",
            " and",
            " their",
            " armies",
            " came",
            " this",
            " way",
            ".",
            " They",
            " were",
            " not",
            " the",
            " only",
            " ones",
            " to",
            " use",
            " the",
            " Roman",
            " highway",
            " network",
            ",",
            " which",
            " was",
            " equally",
            " convenient",
            " for",
            " invaders",
            ".",
            " In",
            " the",
            " second",
            " half",
            " of",
            " the",
            " ",
            "3",
            "rd",
            " century",
            ",",
            " A",
            "ncy",
            "ra",
            " was",
            " invaded",
            " in",
            " rapid",
            " succession",
            " by",
            " the",
            " Go",
            "ths",
            " coming"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " how",
            " many",
            " caution",
            " runs",
            " require",
            " more",
            " braking",
            ".",
            " There",
            " are",
            " three",
            " variables",
            " to",
            " consider",
            " in",
            " racing",
            ":",
            " brake",
            " pedal",
            " displacement",
            ",",
            " brake",
            " pedal",
            " force",
            ",",
            " and",
            " vehicle",
            " dec",
            "eler",
            "ation",
            ".",
            " Various",
            " combinations",
            " of",
            " these",
            " variables",
            " work",
            " together",
            " to",
            " determine",
            " the",
            " stiffness",
            ",",
            " sensitivity",
            ",",
            " and",
            " pedal",
            " force",
            " of",
            " the",
            " brakes",
            ".",
            " When",
            " using",
            " the",
            " brakes",
            " effectively",
            ",",
            " the",
            " driver",
            " must",
            " go",
            " through"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " with",
            " nitrogen",
            "L",
            "ith",
            "ium",
            " is",
            " the",
            " only",
            " metal",
            " that",
            " combines",
            " directly",
            " with",
            " nitrogen",
            " at",
            " room",
            " temperature",
            ".",
            "3",
            "Li",
            " +",
            " ",
            "1",
            "/",
            "2",
            "N",
            "2",
            " âĨĴ",
            " Li",
            "3",
            "N",
            "Li",
            "3",
            "N",
            " can",
            " react",
            " with",
            " water",
            " to",
            " liber",
            "ate",
            " ammonia",
            ".",
            "Li",
            "3",
            "N",
            " +",
            " ",
            "3",
            "H",
            "2",
            "O",
            " âĨĴ",
            " ",
            "3",
            "Li",
            "OH",
            " +",
            " NH",
            "3"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "Ã¸r",
    ".scalablytyped",
    "_ASSUME",
    "INET",
    " KÃ¼r"
  ],
  "bottom_logits": [
    " Super",
    "argar",
    " Arg",
    "ensagem",
    "ovit"
  ],
  "act_min": -0.0,
  "act_max": 0.586
}