{
  "index": 59375,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            1.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " contain",
            " some",
            " sort",
            " of",
            " an",
            "aph",
            "ora",
            ",",
            " repetition",
            " of",
            " a",
            " \"",
            "fixed",
            " base",
            "\"",
            " (",
            "for",
            " example",
            " \"",
            "who",
            "\"",
            " in",
            " How",
            "l",
            ",",
            " \"",
            "America",
            "\"",
            " in",
            " America",
            ")",
            " and",
            " this",
            " has",
            " become",
            " a",
            " recognizable",
            " feature",
            " of",
            " Gins",
            "berg",
            "'s",
            " style",
            ".",
            " He",
            " said",
            " later",
            " this",
            " was",
            " a",
            " cr",
            "utch",
            " because",
            " he",
            " lacked",
            " confidence",
            ";",
            " he",
            " did",
            " not",
            " yet",
            " trust",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " contain",
            " trees",
            ",",
            " the",
            " ancestral",
            " member",
            " is",
            " most",
            " likely",
            " to",
            " have",
            " been",
            " a",
            " tree",
            " or",
            " shr",
            "ub",
            ".",
            "Because",
            " all",
            " cl",
            "ades",
            " are",
            " represented",
            " in",
            " the",
            " Southern",
            " Hemisphere",
            " but",
            " many",
            " not",
            " in",
            " the",
            " Northern",
            " Hemisphere",
            ",",
            " it",
            " is",
            " natural",
            " to",
            " conject",
            "ure",
            " that",
            " there",
            " is",
            " a",
            " common",
            " southern",
            " origin",
            " to",
            " them",
            ".",
            " Ast",
            "era",
            "les",
            " are",
            " ang",
            "ios",
            "perms",
            ",",
            " flowering",
            " plants",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.336,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " which",
            " contain",
            " alk",
            "ali",
            " metals",
            " in",
            " a",
            " âĪĴ",
            "1",
            " oxidation",
            " state",
            ",",
            " which",
            " is",
            " very",
            " unusual",
            " as",
            " before",
            " the",
            " discovery",
            " of",
            " the",
            " alkal",
            "ides",
            ",",
            " the",
            " alk",
            "ali",
            " metals",
            " were",
            " not",
            " expected",
            " to",
            " be",
            " able",
            " to",
            " form",
            " an",
            "ions",
            " and",
            " were",
            " thought",
            " to",
            " be",
            " able",
            " to",
            " appear",
            " in",
            " salts",
            " only",
            " as",
            " c",
            "ations",
            ".",
            " The",
            " alkal",
            "ide",
            " an",
            "ions",
            " have",
            " filled",
            " s",
            "-sub"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.289,
            0.107,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "6",
            "/",
            "183",
            "7",
            " contain",
            " the",
            " first",
            " programs",
            " for",
            " the",
            " engine",
            ".",
            " She",
            " also",
            " developed",
            " a",
            " vision",
            " of",
            " the",
            " capability",
            " of",
            " computers",
            " to",
            " go",
            " beyond",
            " mere",
            " calculating",
            " or",
            " number",
            "-cr",
            "unch",
            "ing",
            ",",
            " while",
            " many",
            " others",
            ",",
            " including",
            " B",
            "abbage",
            " himself",
            ",",
            " focused",
            " only",
            " on",
            " those",
            " capabilities",
            ".",
            " Her",
            " mindset",
            " of",
            " \"",
            "po",
            "etical",
            " science",
            "\"",
            " led",
            " her",
            " to",
            " ask",
            " questions",
            " about",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.289,
            0.107,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "6",
            "/",
            "183",
            "7",
            " contain",
            " the",
            " first",
            " programs",
            " for",
            " the",
            " engine",
            ".",
            " She",
            " also",
            " developed",
            " a",
            " vision",
            " of",
            " the",
            " capability",
            " of",
            " computers",
            " to",
            " go",
            " beyond",
            " mere",
            " calculating",
            " or",
            " number",
            "-cr",
            "unch",
            "ing",
            ",",
            " while",
            " many",
            " others",
            ",",
            " including",
            " B",
            "abbage",
            " himself",
            ",",
            " focused",
            " only",
            " on",
            " those",
            " capabilities",
            ".",
            " Her",
            " mindset",
            " of",
            " \"",
            "po",
            "etical",
            " science",
            "\"",
            " led",
            " her",
            " to",
            " ask",
            " questions",
            " about",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.258,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "izza",
            " compared",
            " with",
            " petroleum",
            " bit",
            "umen",
            " has",
            " shown",
            " that",
            " the",
            " environmental",
            " impact",
            " of",
            " the",
            " s",
            "elen",
            "izza",
            " is",
            " about",
            " half",
            " the",
            " impact",
            " of",
            " the",
            " road",
            " asphalt",
            " produced",
            " in",
            " oil",
            " refin",
            "eries",
            " in",
            " terms",
            " of",
            " carbon",
            " dioxide",
            " emission",
            ".",
            "Rec",
            "ycling",
            " ",
            "Bit",
            "umen",
            " is",
            " a",
            " commonly",
            " recycled",
            " material",
            " in",
            " the",
            " construction",
            " industry",
            ".",
            " The",
            " two",
            " most",
            " common",
            " recycled",
            " materials",
            " that",
            " contain",
            " bit",
            "umen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.25,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " crust",
            " contain",
            " aluminium",
            ".",
            " In",
            " contrast",
            ",",
            " the",
            " Earth",
            "'s",
            " mantle",
            " is",
            " only",
            " ",
            "2",
            ".",
            "38",
            "%",
            " aluminium",
            " by",
            " mass",
            ".",
            " Aluminium",
            " also",
            " occurs",
            " in",
            " seaw",
            "ater",
            " at",
            " a",
            " concentration",
            " of",
            " ",
            "2",
            " Î¼",
            "g",
            "/kg",
            ".",
            "Because",
            " of",
            " its",
            " strong",
            " affinity",
            " for",
            " oxygen",
            ",",
            " aluminium",
            " is",
            " almost",
            " never",
            " found",
            " in",
            " the",
            " elemental",
            " state",
            ";",
            " instead",
            " it",
            " is",
            " found",
            " in",
            " ox",
            "ides"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            1.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " containing",
            " highly",
            " radi",
            "otoxic",
            " alpha",
            "-em",
            "itting",
            " trans",
            "uran",
            "ic",
            " elements",
            " from",
            " nuclear",
            " re",
            "processing",
            " plants",
            " have",
            " been",
            " produced",
            " at",
            " industrial",
            " scale",
            " in",
            " France",
            ",",
            " Belgium",
            " and",
            " Japan",
            ",",
            " but",
            " this",
            " type",
            " of",
            " waste",
            " conditioning",
            " has",
            " been",
            " abandoned",
            " because",
            " operational",
            " safety",
            " issues",
            " (",
            "ris",
            "ks",
            " of",
            " fire",
            ",",
            " as",
            " occurred",
            " in",
            " a",
            " bit",
            "umin",
            "isation",
            " plant",
            " at",
            " Tok",
            "ai",
            " Works",
            " in",
            " Japan",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            1.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " these",
            " contain",
            " only",
            " rub",
            "id",
            "ium",
            " and",
            " no",
            " other",
            " alk",
            "ali",
            " metals",
            ".",
            " Ca",
            "esium",
            " is",
            " more",
            " abundant",
            " than",
            " some",
            " commonly",
            " known",
            " elements",
            ",",
            " such",
            " as",
            " ant",
            "imony",
            ",",
            " cad",
            "mium",
            ",",
            " tin",
            ",",
            " and",
            " tung",
            "sten",
            ",",
            " but",
            " is",
            " much",
            " less",
            " abundant",
            " than",
            " rub",
            "id",
            "ium",
            ".",
            "Franc",
            "ium",
            "-",
            "223",
            ",",
            " the",
            " only",
            " naturally",
            " occurring",
            " is",
            "otope",
            " of",
            " franc",
            "ium",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " ",
            " As",
            " with",
            " go",
            "-to",
            " telesc",
            "opes",
            ",",
            " digital",
            " setting",
            " circle",
            " computers",
            " (",
            "commercial",
            " names",
            " include",
            " Ar",
            "go",
            " Nav",
            "is",
            ",",
            " Sky",
            " Commander",
            ",",
            " and",
            " NGC",
            " Max",
            ")",
            " contain",
            " databases",
            " of",
            " tens",
            " of",
            " thousands",
            " of",
            " celestial",
            " objects",
            " and",
            " projections",
            " of",
            " planet",
            " positions",
            ".",
            "To",
            " find",
            " a",
            " celestial",
            " object",
            " in",
            " a",
            " telescope",
            " equipped",
            " with",
            " a",
            " D",
            "SC",
            " computer",
            ",",
            " one",
            " does",
            " not",
            " need"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            1.242,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " containing",
            " fundamental",
            " al",
            "chemical",
            " information",
            ".",
            " He",
            " also",
            " described",
            " al",
            "chemy",
            ",",
            " along",
            " with",
            " meditation",
            ",",
            " as",
            " the",
            " sole",
            " spiritual",
            " practices",
            " that",
            " could",
            " allow",
            " one",
            " to",
            " gain",
            " imm",
            "ortality",
            " or",
            " to",
            " transcend",
            ".",
            " In",
            " his",
            " work",
            " Inner",
            " Chapters",
            " of",
            " the",
            " Book",
            " of",
            " the",
            " Master",
            " Who",
            " Em",
            "br",
            "aces",
            " Sp",
            "ont",
            "aneous",
            " Nature",
            " (",
            "317",
            " AD",
            "),",
            " Hong",
            " argued",
            " that",
            " al",
            "chemical",
            " solutions",
            " such"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            1.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " Chlor",
            "oplast",
            "s",
            " contain",
            " circular",
            " DNA",
            " like",
            " that",
            " in",
            " cyan",
            "ob",
            "acteria",
            " and",
            " are",
            " interpreted",
            " as",
            " representing",
            " reduced",
            " end",
            "os",
            "ymb",
            "iotic",
            " cyan",
            "ob",
            "acteria",
            ".",
            " However",
            ",",
            " the",
            " exact",
            " origin",
            " of",
            " the",
            " chlor",
            "oplast",
            "s",
            " is",
            " different",
            " among",
            " separate",
            " line",
            "ages",
            " of",
            " algae",
            ",",
            " reflecting",
            " their",
            " acquisition",
            " during",
            " different",
            " end",
            "os",
            "ymb",
            "iotic",
            " events",
            ".",
            " The",
            " table",
            " below",
            " describes",
            " the",
            " composition",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.234,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "v",
            " ÃĹ",
            " ",
            "0",
            ".",
            "337",
            "%),",
            " comparable",
            " with",
            " that",
            " of",
            " neon",
            " (",
            "18",
            ".",
            "18",
            " ppm",
            "v",
            ")",
            " on",
            " Earth",
            " and",
            " with",
            " inter",
            "plan",
            "etary",
            " g",
            "asses",
            ",",
            " measured",
            " by",
            " probes",
            ".",
            "The",
            " atmos",
            "pheres",
            " of",
            " Mars",
            ",",
            " Mercury",
            " and",
            " Titan",
            " (",
            "the",
            " largest",
            " moon",
            " of",
            " Saturn",
            ")",
            " contain",
            " arg",
            "on",
            ",",
            " predominantly",
            " as",
            " ,",
            " and",
            " its",
            " content",
            " may",
            " be",
            " as",
            " high"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.219,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "3",
            ")",
            "3",
            ".",
            " They",
            " all",
            " contain",
            " act",
            "inium",
            " in",
            " the",
            " oxidation",
            " state",
            " +",
            "3",
            ".",
            " In",
            " particular",
            ",",
            " the",
            " lattice",
            " constants",
            " of",
            " the",
            " analogous",
            " lan",
            "than",
            "um",
            " and",
            " act",
            "inium",
            " compounds",
            " differ",
            " by",
            " only",
            " a",
            " few",
            " percent",
            ".",
            "Here",
            " a",
            ",",
            " b",
            " and",
            " c",
            " are",
            " lattice",
            " constants",
            ",",
            " No",
            " is",
            " space",
            " group",
            " number",
            " and",
            " Z",
            " is",
            " the",
            " number",
            " of",
            " formula",
            " units",
            " per"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.219,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Natural",
            " fruits",
            " and",
            " vegetables",
            " also",
            " contain",
            " acids",
            ".",
            " Cit",
            "ric",
            " acid",
            " is",
            " present",
            " in",
            " oranges",
            ",",
            " lemon",
            " and",
            " other",
            " citrus",
            " fruits",
            ".",
            " Ox",
            "alic",
            " acid",
            " is",
            " present",
            " in",
            " tomatoes",
            ",",
            " spinach",
            ",",
            " and",
            " especially",
            " in",
            " c",
            "aram",
            "b",
            "ola",
            " and",
            " rh",
            "ubar",
            "b",
            ";",
            " rh",
            "ubar",
            "b",
            " leaves",
            " and",
            " un",
            "ripe",
            " c",
            "aram",
            "b",
            "olas",
            " are",
            " toxic",
            " because",
            " of",
            " high",
            " concentrations",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.219
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " sources",
            " of",
            " inc",
            "ense",
            ".",
            "The",
            " wo",
            "ody",
            " Az",
            "ore",
            "lla",
            " compact",
            "a",
            " Phil",
            ".",
            " has",
            " been",
            " used",
            " in",
            " South",
            " America",
            " for",
            " fuel",
            ".",
            "To",
            "xic",
            "ity",
            " ",
            "Many",
            " species",
            " in",
            " the",
            " family",
            " Api",
            "aceae",
            " produce",
            " phot",
            "otoxic",
            " substances",
            " (",
            "called",
            " fur",
            "an",
            "oc",
            "ou",
            "mar",
            "ins",
            ")",
            " that",
            " sens",
            "itize",
            " human",
            " skin",
            " to",
            " sunlight",
            ".",
            " Contact",
            " with",
            " plant",
            " parts",
            " that",
            " contain"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " stellar",
            " absorption",
            " spectra",
            ".",
            " More",
            " thoroughly",
            " investigated",
            " are",
            " compounds",
            " of",
            " the",
            " formula",
            " R",
            "4",
            "Al",
            "2",
            " which",
            " contain",
            " an",
            " Al",
            "–",
            "Al",
            " bond",
            " and",
            " where",
            " R",
            " is",
            " a",
            " large",
            " organic",
            " lig",
            "and",
            ".",
            "Org",
            "ano",
            "al",
            "uminium",
            " compounds",
            " and",
            " related",
            " hy",
            "d",
            "rides",
            " ",
            "A",
            " variety",
            " of",
            " compounds",
            " of",
            " empirical",
            " formula",
            " Al",
            "R",
            "3",
            " and",
            " Al",
            "R",
            "1",
            ".",
            "5",
            "Cl",
            "1",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            0.053,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " were",
            " published",
            " in",
            " ",
            "191",
            "2",
            "–",
            "14",
            ".",
            " Three",
            " more",
            ",",
            " to",
            " contain",
            " the",
            " Ch",
            "or",
            "ale",
            " Prel",
            "udes",
            " with",
            " Schwe",
            "itzer",
            "'s",
            " analyses",
            ",",
            " were",
            " to",
            " be",
            " worked",
            " on",
            " in",
            " Africa",
            ",",
            " but",
            " these",
            " were",
            " never",
            " completed",
            ",",
            " perhaps",
            " because",
            " for",
            " him",
            " they",
            " were",
            " inse",
            "parable",
            " from",
            " his",
            " evolving",
            " theological",
            " thought",
            ".",
            "On",
            " departure",
            " for",
            " Lam",
            "bar",
            "Ã©nÃ©",
            " in",
            " ",
            "191"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            1.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " its",
            " compounds",
            " contain",
            " small",
            " intrinsic",
            " radi",
            "ogenic",
            " defects",
            ",",
            " due",
            " to",
            " metam",
            "ict",
            "ization",
            " induced",
            " by",
            " self",
            "-",
            "ir",
            "radi",
            "ation",
            " with",
            " alpha",
            " particles",
            ",",
            " which",
            " accum",
            "ulates",
            " with",
            " time",
            ";",
            " this",
            " can",
            " cause",
            " a",
            " drift",
            " of",
            " some",
            " material",
            " properties",
            " over",
            " time",
            ",",
            " more",
            " noticeable",
            " in",
            " older",
            " samples",
            ".",
            "History",
            "Although",
            " americ",
            "ium",
            " was",
            " likely",
            " produced",
            " in",
            " previous",
            " nuclear",
            " experiments",
            ",",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " some",
            " otherwise",
            " idle",
            " time",
            " to",
            " do",
            " some",
            " navig",
            "ational",
            " sightings",
            ",",
            " maneuver",
            "ing",
            " the",
            " module",
            " to",
            " view",
            " various",
            " stars",
            " by",
            " using",
            " the",
            " computer",
            " keyboard",
            ".",
            " He",
            " accidentally",
            " erased",
            " some",
            " of",
            " the",
            " computer",
            "'s",
            " memory",
            ",",
            " which",
            " caused",
            " the",
            " inert",
            "ial",
            " measurement",
            " unit",
            " (",
            "IM",
            "U",
            ")",
            " to",
            " contain",
            " data",
            " indicating",
            " that",
            " the",
            " module",
            " was",
            " in",
            " the",
            " same",
            " relative",
            " orientation",
            " it",
            " had",
            " been",
            " in"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " called",
            " a",
            " lam",
            "ell",
            "ip",
            "odium",
            " which",
            " contain",
            " protr",
            "usions",
            " called",
            " fil",
            "op",
            "odia",
            ".",
            " The",
            " fil",
            "op",
            "odia",
            " are",
            " the",
            " mechanism",
            " by",
            " which",
            " the",
            " entire",
            " process",
            " adher",
            "es",
            " to",
            " surfaces",
            " and",
            " explores",
            " the",
            " surrounding",
            " environment",
            ".",
            " Act",
            "in",
            " plays",
            " a",
            " major",
            " role",
            " in",
            " the",
            " mobility",
            " of",
            " this",
            " system",
            ".",
            " En",
            "vironments",
            " with",
            " high",
            " levels",
            " of",
            " cell",
            " ad",
            "hesion",
            " molecules",
            " (",
            "CAM",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            0.033,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " with",
            " a",
            " central",
            " brain",
            ",",
            " a",
            " spinal",
            " cord",
            ",",
            " and",
            " nerves",
            " throughout",
            " the",
            " body",
            ".",
            " The",
            " amphib",
            "ian",
            " brain",
            " is",
            " relatively",
            " simple",
            " but",
            " broadly",
            " the",
            " same",
            " struct",
            "urally",
            " as",
            " in",
            " rept",
            "iles",
            ",",
            " birds",
            " and",
            " mammals",
            ".",
            " Their",
            " brains",
            " are",
            " elong",
            "ated",
            ",",
            " except",
            " in",
            " ca",
            "ec",
            "ilians",
            ",",
            " and",
            " contain",
            " the",
            " usual",
            " motor",
            " and",
            " sensory",
            " areas",
            " of",
            " tet",
            "rap",
            "ods",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " ca",
            "esium",
            ".",
            " Poll",
            "uc",
            "ite",
            ",",
            " carn",
            "all",
            "ite",
            ",",
            " le",
            "uc",
            "ite",
            ",",
            " and",
            " le",
            "pid",
            "ol",
            "ite",
            " are",
            " all",
            " minerals",
            " that",
            " contain",
            " rub",
            "id",
            "ium",
            ".",
            " As",
            " a",
            " by",
            "-product",
            " of",
            " lithium",
            " extraction",
            ",",
            " it",
            " is",
            " commercially",
            " obtained",
            " from",
            " le",
            "pid",
            "ol",
            "ite",
            ".",
            " Rub",
            "id",
            "ium",
            " is",
            " also",
            " found",
            " in",
            " potassium",
            " rocks",
            " and",
            " br",
            "ines",
            ",",
            " which",
            " is",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " synthesis",
            "Act",
            "inium",
            " is",
            " found",
            " only",
            " in",
            " traces",
            " in",
            " uranium",
            " ores",
            "Âł",
            "–",
            " one",
            " ton",
            "ne",
            " of",
            " uranium",
            " in",
            " ore",
            " contains",
            " about",
            " ",
            "0",
            ".",
            "2",
            " mill",
            "igrams",
            " of",
            " ",
            "227",
            "Ac",
            " –",
            " and",
            " in",
            " thor",
            "ium",
            " ores",
            ",",
            " which",
            " contain",
            " about",
            " ",
            "5",
            " nan",
            "ograms",
            " of",
            " ",
            "228",
            "Ac",
            " per",
            " one",
            " ton",
            "ne",
            " of",
            " thor",
            "ium",
            ".",
            " The",
            " act",
            "inium",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.211,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "180",
            "0",
            "s",
            ",",
            " the",
            " English",
            " chem",
            "ist",
            " John",
            " Dalton",
            " compiled",
            " experimental",
            " data",
            " gathered",
            " by",
            " him",
            " and",
            " other",
            " scientists",
            " and",
            " discovered",
            " a",
            " pattern",
            " now",
            " known",
            " as",
            " the",
            " \"",
            "law",
            " of",
            " multiple",
            " proportions",
            "\".",
            " He",
            " noticed",
            " that",
            " in",
            " chemical",
            " compounds",
            " which",
            " contain",
            " a",
            " particular",
            " chemical",
            " element",
            ",",
            " the",
            " content",
            " of",
            " that",
            " element",
            " in",
            " these",
            " compounds",
            " will",
            " differ",
            " in",
            " weight",
            " by",
            " ratios",
            " of",
            " small"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "sv",
            ")",
            " operated",
            " in",
            " the",
            " building",
            " from",
            " ",
            "191",
            "7",
            " to",
            " ",
            "196",
            "2",
            ".",
            "The",
            " museum",
            " documents",
            " work",
            " and",
            " everyday",
            " life",
            " by",
            " collecting",
            " personal",
            " stories",
            " about",
            " people",
            "'s",
            " professional",
            " lives",
            " from",
            " both",
            " the",
            " past",
            " and",
            " the",
            " present",
            ".",
            " The",
            " museum",
            "'s",
            " archive",
            " contain",
            " material",
            " from",
            " memory",
            " collections",
            " and",
            " documentation",
            " projects",
            ".",
            "Since",
            " ",
            "200",
            "9",
            ",",
            " the",
            " museum",
            " also",
            " houses",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.203,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " vowel",
            " di",
            "ac",
            "rit",
            "ics",
            ",",
            " or",
            " both",
            ".",
            " The",
            " term",
            " pure",
            " ab",
            "jad",
            " refers",
            " to",
            " scripts",
            " entirely",
            " lacking",
            " in",
            " vowel",
            " indicators",
            ".",
            " However",
            ",",
            " most",
            " modern",
            " ab",
            "j",
            "ads",
            ",",
            " such",
            " as",
            " Arabic",
            ",",
            " Hebrew",
            ",",
            " Ar",
            "ama",
            "ic",
            ",",
            " and",
            " P",
            "ahl",
            "avi",
            ",",
            " are",
            " \"",
            "imp",
            "ure",
            "\"",
            " ab",
            "jad",
            "st",
            "hat",
            " is",
            ",",
            " they",
            " also",
            " contain",
            " symbols",
            " for",
            " some"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "ok",
            "len",
            "ic",
            " languages",
            " contain",
            " many",
            " Aust",
            "ro",
            "asi",
            "atic",
            " loan",
            "words",
            ",",
            " some",
            " of",
            " which",
            " are",
            " similar",
            " to",
            " the",
            " ones",
            " found",
            " in",
            " Ch",
            "amic",
            ".",
            " Ace",
            "hn",
            "ese",
            " substr",
            "atum",
            " (",
            "Sid",
            "well",
            " ",
            "200",
            "6",
            ").",
            " Ace",
            "hn",
            "ese",
            " has",
            " many",
            " basic",
            " words",
            " that",
            " are",
            " of",
            " Aust",
            "ro",
            "asi",
            "atic",
            " origin",
            ",",
            " suggesting",
            " that",
            " either",
            " Austr",
            "ones",
            "ian",
            " speakers",
            " have",
            " absorbed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " Archive",
            " has",
            " preserved",
            " many",
            " of",
            " his",
            " home",
            " movies",
            ".",
            " The",
            " Alfred",
            " Hitch",
            "cock",
            " Papers",
            " are",
            " housed",
            " at",
            " the",
            " Academy",
            "'s",
            " Margaret",
            " Herr",
            "ick",
            " Library",
            ".",
            " The",
            " David",
            " O",
            ".",
            " Sel",
            "zn",
            "ick",
            " and",
            " the",
            " Ernest",
            " Leh",
            "man",
            " collections",
            " housed",
            " at",
            " the",
            " Harry",
            " R",
            "ansom",
            " Humanities",
            " Research",
            " Center",
            " in",
            " Austin",
            ",",
            " Texas",
            ",",
            " contain",
            " material",
            " related",
            " to",
            " Hitch",
            "cock",
            "'s",
            " work",
            " on",
            " the",
            " production"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.047,
            1.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "J",
            "164",
            "9",
            "+",
            "263",
            "5",
            " (",
            "201",
            "4",
            ")",
            " is",
            " a",
            " spiral",
            " galaxy",
            " containing",
            " contain",
            " DR",
            "AG",
            "Ns",
            " (",
            "Double",
            " Radio",
            "-source",
            " Associated",
            " with",
            " Galactic",
            " N",
            "ucle",
            "us",
            ").",
            " Yellow",
            "balls",
            " (",
            "201",
            "5",
            ")",
            " are",
            " a",
            " type",
            " of",
            " compact",
            " star",
            "-form",
            "ing",
            " region",
            ".",
            "9",
            "Sp",
            "itch",
            " (",
            "201",
            "5",
            ")",
            " is",
            " a",
            " distant",
            " grav",
            "itation",
            "ally",
            " lens",
            "ed",
            " galaxy",
            " with"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " highest",
            " professional",
            " level",
            " of",
            " American",
            " football",
            " in",
            " the",
            " United",
            " States",
            ".",
            " The",
            " AFC",
            " and",
            " its",
            " counterpart",
            ",",
            " the",
            " National",
            " Football",
            " Conference",
            " (",
            "N",
            "FC",
            "),",
            " each",
            " contain",
            " ",
            "16",
            " teams",
            " with",
            " ",
            "4",
            " divisions",
            ".",
            " Both",
            " conferences",
            " were",
            " created",
            " as",
            " part",
            " of",
            " the",
            " ",
            "197",
            "0",
            " merger",
            " between",
            " the",
            " National",
            " Football",
            " League",
            ",",
            " and",
            " the",
            " American",
            " Football",
            " League",
            " (",
            "A",
            "FL",
            ").",
            " All"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.195,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            ";",
            " ",
            "10",
            "%",
            " hydrogen",
            ";",
            " up",
            " to",
            " ",
            "6",
            "%",
            " sulfur",
            ";",
            " and",
            " molecular",
            "ly",
            ",",
            " between",
            " ",
            "5",
            " and",
            " ",
            "25",
            "%",
            " by",
            " weight",
            " of",
            " as",
            "ph",
            "alten",
            "es",
            " dispersed",
            " in",
            " ",
            "90",
            "%",
            " to",
            " ",
            "65",
            "%",
            " mal",
            "ten",
            "es",
            ".",
            " ",
            " Most",
            " natural",
            " bit",
            "um",
            "ens",
            " also",
            " contain",
            " organ",
            "os",
            "ulf",
            "ur",
            " compounds",
            ",",
            " ",
            " Nickel",
            " and",
            " van",
            "adium"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.938,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.18,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " reclaimed",
            " asphalt",
            " pavement",
            " (",
            "RAP",
            ")",
            " and",
            " reclaimed",
            " asphalt",
            " sh",
            "ingles",
            " (",
            "R",
            "AS",
            ").",
            " R",
            "AP",
            " is",
            " recycled",
            " at",
            " a",
            " greater",
            " rate",
            " than",
            " any",
            " other",
            " material",
            " in",
            " the",
            " United",
            " States",
            ",",
            " and",
            " typically",
            " contains",
            " approximately",
            " ",
            "5",
            "–",
            "6",
            "%",
            " bit",
            "umen",
            " binder",
            ".",
            " Asphalt",
            " sh",
            "ingles",
            " typically",
            " contain",
            " ",
            "20",
            "–",
            "40",
            "%",
            " bit",
            "umen",
            " binder",
            ".",
            "Bit",
            "umen",
            " naturally"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.172,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " continental",
            " climate",
            ",",
            " has",
            " decid",
            "uous",
            " forests",
            " and",
            " forest",
            " ste",
            "pp",
            "es",
            ".",
            " Western",
            " and",
            " southern",
            " Anat",
            "olia",
            ",",
            " which",
            " have",
            " a",
            " Mediterranean",
            " climate",
            ",",
            " contain",
            " Mediterranean",
            " forests",
            ",",
            " wood",
            "lands",
            ",",
            " and",
            " scrub",
            " e",
            "core",
            "g",
            "ions",
            ".",
            " E",
            "ux",
            "ine",
            "-Col",
            "ch",
            "ic",
            " decid",
            "uous",
            " forests",
            ":",
            " These",
            " temper",
            "ate",
            " broad",
            "leaf",
            " and",
            " mixed",
            " forests",
            " extend",
            " across",
            " northern",
            " Anat",
            "olia",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.156,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " favorite",
            " directors",
            " were",
            " Bu",
            "Ã±",
            "uel",
            ",",
            " Miz",
            "og",
            "uchi",
            ",",
            " Berg",
            "man",
            ",",
            " B",
            "ress",
            "on",
            ",",
            " K",
            "uros",
            "awa",
            ",",
            " Michel",
            "angelo",
            " Anton",
            "ioni",
            ",",
            " Jean",
            " V",
            "igo",
            ",",
            " and",
            " Carl",
            " The",
            "odor",
            " D",
            "rey",
            "er",
            ".",
            "With",
            " the",
            " exception",
            " of",
            " City",
            " Lights",
            ",",
            " the",
            " list",
            " does",
            " not",
            " contain",
            " any",
            " films",
            " of",
            " the",
            " early",
            " silent",
            " era",
            ".",
            " The",
            " reason",
            " is",
            " that"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " sure",
            " sign",
            " of",
            " its",
            " actual",
            " existence",
            " has",
            " yet",
            " emerged",
            ".\"",
            " Additionally",
            " ML",
            " H",
            "j",
            "Ã¤",
            "lm",
            " in",
            " her",
            " most",
            " recent",
            " research",
            " (",
            "201",
            "7",
            ")",
            " inserts",
            " that",
            " \"",
            "man",
            "us",
            "cripts",
            " containing",
            " translations",
            " of",
            " the",
            " g",
            "ospels",
            " are",
            " encountered",
            " no",
            " earlier",
            " than",
            " the",
            " year",
            " ",
            "873",
            "\"",
            "I",
            "rf",
            "an",
            " Shah",
            "Ã®",
            "d",
            " quoting",
            " the",
            " ",
            "10",
            "th",
            "-century",
            " enc",
            "yc",
            "lo",
            "ped"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " positive",
            " feedback",
            " loops",
            " have",
            " long",
            " been",
            " recognized",
            " as",
            " important",
            " for",
            " global",
            " warming",
            ".",
            "C",
            "ryo",
            "con",
            "ite",
            ",",
            " pow",
            "d",
            "ery",
            " wind",
            "bl",
            "own",
            " dust",
            " containing",
            " so",
            "ot",
            ",",
            " sometimes",
            " reduces",
            " al",
            "bedo",
            " on",
            " glaciers",
            " and",
            " ice",
            " sheets",
            ".",
            "The",
            " dynam",
            "ical",
            " nature",
            " of",
            " al",
            "bedo",
            " in",
            " response",
            " to",
            " positive",
            " feedback",
            ",",
            " together",
            " with",
            " the",
            " effects",
            " of",
            " small",
            " errors",
            " in",
            " the",
            " measurement",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.93,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            1.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " risk",
            " for",
            " more",
            " serious",
            " illness",
            " or",
            " death",
            " from",
            " the",
            " virus",
            ".",
            "Some",
            " Canadians",
            " are",
            " drinking",
            " water",
            " that",
            " contains",
            " in",
            "organic",
            " arsen",
            "ic",
            ".",
            " Private",
            "-d",
            "ug",
            "–",
            "well",
            " waters",
            " are",
            " most",
            " at",
            " risk",
            " for",
            " containing",
            " in",
            "organic",
            " arsen",
            "ic",
            ".",
            " Prel",
            "iminary",
            " well",
            " water",
            " analysis",
            " typically",
            " does",
            " not",
            " test",
            " for",
            " arsen",
            "ic",
            ".",
            " Researchers",
            " at",
            " the",
            " Geological",
            " Survey",
            " of",
            " Canada",
            " have",
            " modeled",
            " relative"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.988,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " order",
            " also",
            " contains",
            " many",
            " ge",
            "ophy",
            "tes",
            " (",
            "bul",
            "bs",
            ",",
            " c",
            "orm",
            "s",
            ",",
            " and",
            " various",
            " kinds",
            " of",
            " tub",
            "er",
            ").",
            " According",
            " to",
            " tel",
            "om",
            "ere",
            " sequence",
            ",",
            " at",
            " least",
            " two",
            " evolutionary",
            " switch",
            "-points",
            " happened",
            " within",
            " the",
            " order",
            ".",
            " The",
            " basal",
            " sequence",
            " is",
            " formed",
            " by",
            " T",
            "TT",
            "AG",
            "GG",
            " like",
            " in",
            " the",
            " majority",
            " of",
            " higher",
            " plants",
            ".",
            " Bas",
            "al",
            " motif",
            " was",
            " changed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            0.984,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " War",
            "hol",
            " \"",
            "w",
            "rote",
            "\"",
            " several",
            " books",
            " that",
            " were",
            " commercially",
            " published",
            ":",
            " a",
            ",",
            " A",
            " Novel",
            " (",
            "196",
            "8",
            ",",
            " )",
            " is",
            " a",
            " literal",
            " transcription",
            "—",
            "cont",
            "aining",
            " spelling",
            " errors",
            " and",
            " phon",
            "etically",
            " written",
            " background",
            " noise",
            " and",
            " mum",
            "bling",
            "—",
            "of",
            " audio",
            " recordings",
            " of",
            " O",
            "nd",
            "ine",
            " and",
            " several",
            " of",
            " Andy",
            " War",
            "hol",
            "'s",
            " friends",
            " hanging",
            " out",
            " at",
            " the",
            " Factory",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " designed",
            " to",
            " remain",
            " cool",
            " in",
            " the",
            " heat",
            " of",
            " the",
            " day",
            ".",
            " Each",
            " home",
            " had",
            " a",
            " kitchen",
            " with",
            " an",
            " open",
            " roof",
            ",",
            " which",
            " contained",
            " a",
            " grind",
            "stone",
            " for",
            " milling",
            " grain",
            " and",
            " a",
            " small",
            " oven",
            " for",
            " baking",
            " the",
            " bread",
            ".",
            " Cer",
            "amics",
            " served",
            " as",
            " household",
            " w",
            "ares",
            " for",
            " the",
            " storage",
            ",",
            " preparation",
            ",",
            " transport",
            ",",
            " and",
            " consumption",
            " of",
            " food",
            ",",
            " drink",
            ",",
            " and",
            " raw",
            " materials"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "ua",
            ",",
            " made",
            " perf",
            "umes",
            " herself",
            " to",
            " serve",
            " as",
            " gifts",
            ".",
            " In",
            " this",
            " period",
            ",",
            " the",
            " only",
            " book",
            " of",
            " secrets",
            " as",
            "cribed",
            " to",
            " a",
            " woman",
            " was",
            " ",
            " ('",
            "The",
            " Secrets",
            " of",
            " Sign",
            "ora",
            " Is",
            "abella",
            " Cort",
            "ese",
            "').",
            " This",
            " book",
            " contained",
            " information",
            " on",
            " how",
            " to",
            " turn",
            " base",
            " metals",
            " into",
            " gold",
            ",",
            " medicine",
            ",",
            " and",
            " cosmetics",
            ".",
            " However",
            ",",
            " it",
            " is",
            " rumored",
            " that",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " smaller",
            " pot",
            " placed",
            " on",
            " top",
            ".",
            " In",
            " a",
            " secondary",
            " burial",
            ",",
            " the",
            " body",
            " was",
            " initially",
            " buried",
            " without",
            " a",
            " pot",
            ",",
            " and",
            " after",
            " a",
            " few",
            " months",
            " or",
            " years",
            ",",
            " the",
            " bones",
            " were",
            " ex",
            "hum",
            "ed",
            " and",
            " re",
            "bur",
            "ied",
            " in",
            " smaller",
            " pots",
            " for",
            " a",
            " second",
            " time",
            ".",
            " Some",
            " pots",
            " contained",
            " grave",
            " offerings",
            " such",
            " as",
            " axes",
            ",",
            " shells",
            ",",
            " and",
            " pottery",
            ".",
            " Remark",
            "ably"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " United",
            " States",
            ".",
            " The",
            " convention",
            " unanimously",
            " voted",
            " to",
            " se",
            "cede",
            " on",
            " December",
            " ",
            "20",
            ",",
            " ",
            "186",
            "0",
            ",",
            " and",
            " adopted",
            " a",
            " se",
            "cession",
            " declaration",
            ".",
            " It",
            " argued",
            " for",
            " states",
            "'",
            " rights",
            " for",
            " slave",
            " owners",
            " in",
            " the",
            " South",
            ",",
            " but",
            " contained",
            " a",
            " complaint",
            " about",
            " states",
            "'",
            " rights",
            " in",
            " the",
            " North",
            " in",
            " the",
            " form",
            " of",
            " opposition",
            " to",
            " the",
            " F",
            "ug",
            "itive",
            " Slave",
            " Act"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " when",
            " it",
            " interacts",
            " with",
            " a",
            " portion",
            " of",
            " a",
            " sample",
            ".",
            "S",
            "pect",
            "ra",
            " of",
            " excited",
            " states",
            " can",
            " be",
            " used",
            " to",
            " analyze",
            " the",
            " atomic",
            " composition",
            " of",
            " distant",
            " stars",
            ".",
            " Specific",
            " light",
            " wavelengths",
            " contained",
            " in",
            " the",
            " observed",
            " light",
            " from",
            " stars",
            " can",
            " be",
            " separated",
            " out",
            " and",
            " related",
            " to",
            " the",
            " quant",
            "ized",
            " transitions",
            " in",
            " free",
            " gas",
            " atoms",
            ".",
            " These",
            " colors",
            " can",
            " be",
            " replicated",
            " using",
            " a",
            " gas",
            "-dis"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " based",
            " on",
            " racial",
            " and",
            " anthrop",
            "ological",
            " data",
            ".",
            " Instead",
            ",",
            " Green",
            "berg",
            " proposed",
            " an",
            " Afro",
            "asi",
            "atic",
            " family",
            " consisting",
            " of",
            " five",
            " branches",
            ":",
            " Ber",
            "ber",
            ",",
            " Ch",
            "adic",
            ",",
            " Cush",
            "itic",
            ",",
            " Egyptian",
            ",",
            " and",
            " Sem",
            "itic",
            ".",
            " Rel",
            "uct",
            "ance",
            " among",
            " some",
            " scholars",
            " to",
            " recognize",
            " Ch",
            "adic",
            " as",
            " a",
            " branch",
            " of",
            " Afro",
            "asi",
            "atic",
            " persisted",
            " as",
            " late",
            " as",
            " the",
            " ",
            "198",
            "0"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " regions",
            " where",
            " ter",
            "mites",
            " of",
            " the",
            " family",
            " Hod",
            "oter",
            "mit",
            "idae",
            " occur",
            ".",
            " Ter",
            "mites",
            " of",
            " this",
            " family",
            " depend",
            " on",
            " dead",
            " and",
            " with",
            "ered",
            " grass",
            " and",
            " are",
            " most",
            " populous",
            " in",
            " heavily",
            " graz",
            "ed",
            " grass",
            "lands",
            " and",
            " sav",
            "annah",
            "s",
            ",",
            " including",
            " far",
            "mland",
            ".",
            " For",
            " most",
            " of",
            " the",
            " year",
            ",",
            " a",
            "ard",
            "w",
            "olves",
            " spend",
            " time",
            " in",
            " shared",
            " territories",
            " consisting",
            " of",
            " up",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.059,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " into",
            " tar",
            ",",
            " not",
            " the",
            " other",
            " way",
            " around",
            ".",
            "Composition",
            "Normal",
            " composition",
            "The",
            " components",
            " of",
            " bit",
            "umen",
            " include",
            " four",
            " main",
            " classes",
            " of",
            " compounds",
            ":",
            " N",
            "aph",
            "th",
            "ene",
            " arom",
            "atics",
            " (",
            "n",
            "aph",
            "thal",
            "ene",
            "),",
            " consisting",
            " of",
            " partially",
            " hydrogen",
            "ated",
            " pol",
            "yc",
            "yclic",
            " aromatic",
            " compounds",
            " Polar",
            " arom",
            "atics",
            ",",
            " consisting",
            " of",
            " high",
            " molecular",
            " weight",
            " phen",
            "ols",
            " and",
            " car",
            "box"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " students",
            ",",
            " creative",
            " professionals",
            ",",
            " and",
            " software",
            " engineers",
            ".",
            " The",
            " current",
            " lineup",
            " consists",
            " of",
            " the",
            " MacBook",
            " Air",
            " and",
            " MacBook",
            " Pro",
            " laptops",
            ",",
            " and",
            " the",
            " i",
            "Mac",
            ",",
            " Mac",
            " mini",
            ",",
            " Mac",
            " Studio",
            " and",
            " Mac",
            " Pro",
            " desktop",
            " computers",
            ".",
            "Often",
            " described",
            " as",
            " a",
            " w",
            "alled",
            " garden",
            ",",
            " Mac",
            "s",
            " use",
            " Apple",
            " silicon",
            " chips",
            ",",
            " run",
            " the",
            " macOS",
            " operating",
            " system",
            ",",
            " and",
            " include",
            " Apple",
            " software"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            0.028,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " al",
            "gal",
            " turf",
            " is",
            " commonly",
            " used",
            " but",
            " poorly",
            " defined",
            ".",
            " Al",
            "gal",
            " tur",
            "fs",
            " are",
            " thick",
            ",",
            " carpet",
            "-like",
            " beds",
            " of",
            " seaw",
            "eed",
            " that",
            " retain",
            " sediment",
            " and",
            " compete",
            " with",
            " foundation",
            " species",
            " like",
            " cor",
            "als",
            " and",
            " k",
            "elps",
            ",",
            " and",
            " they",
            " are",
            " usually",
            " less",
            " than",
            " ",
            "15",
            "Âł",
            "cm",
            " tall",
            ".",
            " Such",
            " a",
            " turf",
            " may",
            " consist",
            " of",
            " one",
            " or",
            " more",
            " species",
            ",",
            " and",
            " will"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.163,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "id",
            " period",
            " to",
            " the",
            " Saf",
            "avid",
            " period",
            "The",
            " S",
            "asan",
            "ian",
            " Empire",
            " turned",
            " Caucasian",
            " Albania",
            " into",
            " a",
            " v",
            "ass",
            "al",
            " state",
            " in",
            " ",
            "252",
            ",",
            " while",
            " King",
            " Ur",
            "n",
            "ay",
            "r",
            " officially",
            " adopted",
            " Christianity",
            " as",
            " the",
            " state",
            " religion",
            " in",
            " the",
            " ",
            "4",
            "th",
            " century",
            ".",
            " Despite",
            " Sass",
            "an",
            "id",
            " rule",
            ",",
            " Albania",
            " remained",
            " an",
            " entity",
            " in",
            " the",
            " region",
            " until",
            " the",
            " ",
            "9"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " diameter",
            " (",
            "typically",
            " about",
            " one",
            " mic",
            "rometer",
            " (",
            "Âµ",
            "m",
            ")",
            " across",
            ").",
            " The",
            " largest",
            " mamm",
            "alian",
            " ax",
            "ons",
            " can",
            " reach",
            " a",
            " diameter",
            " of",
            " up",
            " to",
            " ",
            "20",
            "Âł",
            "Âµ",
            "m",
            ".",
            " The",
            " squid",
            " giant",
            " ax",
            "on",
            ",",
            " which",
            " is",
            " specialized",
            " to",
            " conduct",
            " signals",
            " very",
            " rapidly",
            ",",
            " is",
            " close",
            " to",
            " ",
            "1",
            " mill",
            "imeter",
            " in",
            " diameter",
            ",",
            " the",
            " size",
            " of",
            " a",
            " small",
            " pencil"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " F",
            " are",
            " increasing",
            " the",
            " sample",
            " size",
            " and",
            " reducing",
            " the",
            " error",
            " variance",
            " by",
            " tight",
            " experimental",
            " controls",
            ".",
            "There",
            " are",
            " two",
            " methods",
            " of",
            " concluding",
            " the",
            " AN",
            "O",
            "VA",
            " hypothesis",
            " test",
            ",",
            " both",
            " of",
            " which",
            " produce",
            " the",
            " same",
            " result",
            ":",
            " The",
            " textbook",
            " method",
            " is",
            " to",
            " compare",
            " the",
            " observed",
            " value",
            " of",
            " F",
            " with",
            " the",
            " critical",
            " value",
            " of",
            " F",
            " determined",
            " from",
            " tables",
            ".",
            " The",
            " critical",
            " value",
            " of",
            " F"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " they",
            " both",
            " have",
            " developed",
            " anal",
            " glands",
            " from",
            " which",
            " they",
            " extr",
            "ude",
            " a",
            " black",
            " substance",
            " that",
            " is",
            " sme",
            "ared",
            " on",
            " rocks",
            " or",
            " grass",
            " stalk",
            "s",
            " in",
            " -",
            "long",
            " streak",
            "s",
            ".",
            " A",
            "ard",
            "w",
            "olves",
            " also",
            " have",
            " scent",
            " glands",
            " on",
            " the",
            " fore",
            "foot",
            " and",
            " pen",
            "ile",
            " pad",
            ".",
            " They",
            " often",
            " mark",
            " near",
            " term",
            "ite",
            " m",
            "ounds",
            " within",
            " their",
            " territory",
            " every",
            " ",
            "20",
            " minutes",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "CH",
            "3",
            "Li",
            ")x",
            "),",
            " which",
            " exists",
            " in",
            " tet",
            "ram",
            "eric",
            " (",
            "x",
            " =",
            " ",
            "4",
            ",",
            " tet",
            "rah",
            "edral",
            ")",
            " and",
            " hex",
            "am",
            "eric",
            " (",
            "x",
            " =",
            " ",
            "6",
            ",",
            " oct",
            "ah",
            "edral",
            ")",
            " forms",
            ".",
            " Organ",
            "olith",
            "ium",
            " compounds",
            ",",
            " especially",
            " n",
            "-b",
            "uty",
            "ll",
            "ith",
            "ium",
            ",",
            " are",
            " useful",
            " re",
            "agents",
            " in",
            " organic",
            " synthesis",
            ",",
            " as",
            " might",
            " be",
            " expected",
            " given"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "«ĺ",
    "NetMessage",
    "GuidId",
    "habi",
    "ICODE"
  ],
  "bottom_logits": [
    " Im",
    "mere",
    "1",
    " B",
    " Robinson"
  ],
  "act_min": -0.0,
  "act_max": 1.406
}