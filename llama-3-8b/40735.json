{
  "index": 40735,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.672,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " normal",
            ",",
            " or",
            " can",
            "'t",
            " get",
            " it",
            " out",
            " of",
            " your",
            " mind",
            ".\"",
            "The",
            " physiological",
            " symptoms",
            " of",
            " anxiety",
            " may",
            " include",
            ":",
            "Ne",
            "uro",
            "logical",
            ",",
            " as",
            " headache",
            ",",
            " p",
            "arest",
            "hes",
            "ias",
            ",",
            " fasc",
            "ic",
            "ulations",
            ",",
            " vert",
            "igo",
            ",",
            " or",
            " pres",
            "yn",
            "cope",
            ".",
            "Digest",
            "ive",
            ",",
            " as",
            " abdominal",
            " pain",
            ",",
            " nausea",
            ",",
            " diarrhea",
            ",",
            " ind",
            "igest",
            "ion",
            ",",
            " dry",
            " mouth",
            ",",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.056,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Effects",
            " may",
            " be",
            " similar",
            " to",
            " arsen",
            "ic",
            " poisoning",
            ".",
            " Occupational",
            " exposure",
            " may",
            " cause",
            " respiratory",
            " irritation",
            ",",
            " pneum",
            "o",
            "con",
            "iosis",
            ",",
            " ant",
            "imony",
            " spots",
            " on",
            " the",
            " skin",
            ",",
            " gastrointestinal",
            " symptoms",
            ",",
            " and",
            " cardiac",
            " arr",
            "hythm",
            "ias",
            ".",
            " In",
            " addition",
            ",",
            " ant",
            "imony",
            " tri",
            "oxide",
            " is",
            " potentially",
            " carcin",
            "ogenic",
            " to",
            " humans",
            ".",
            "Ad",
            "verse",
            " health",
            " effects",
            " have",
            " been",
            " observed",
            " in",
            " humans",
            " and",
            " animals"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            "aged",
            " ",
            "15",
            "â€“",
            "49",
            ")",
            " are",
            " living",
            " with",
            " HIV",
            "/AIDS",
            " (",
            "as",
            " of",
            " ",
            "200",
            "9",
            ").",
            " The",
            " risk",
            " of",
            " contracting",
            " disease",
            " is",
            " very",
            " high",
            ".",
            " There",
            " are",
            " food",
            " and",
            " water",
            "borne",
            " diseases",
            ",",
            " bacterial",
            " and",
            " proto",
            "zo",
            "al",
            " diarrhea",
            ",",
            " hepatitis",
            " A",
            ",",
            " and",
            " ty",
            "ph",
            "oid",
            " fever",
            ";",
            " vector",
            "borne",
            " diseases",
            ",",
            " malaria",
            ",",
            " African",
            " try",
            "pan",
            "os",
            "om",
            "ias"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " Deng",
            "ue",
            ",",
            " fil",
            "ari",
            "asis",
            ",",
            " le",
            "ish",
            "man",
            "ias",
            "is",
            " and",
            " on",
            "ch",
            "oc",
            "erc",
            "ias",
            "is",
            " (",
            "river",
            " blindness",
            ")",
            " are",
            " other",
            " diseases",
            " carried",
            " by",
            " insects",
            " that",
            " also",
            " occur",
            " in",
            " the",
            " region",
            ".",
            " Angola",
            " has",
            " one",
            " of",
            " the",
            " highest",
            " infant",
            " mortality",
            " rates",
            " in",
            " the",
            " world",
            " and",
            " one",
            " of",
            " the",
            " world",
            "'s",
            " lowest",
            " life",
            " expect",
            "ancies",
            ".",
            " A",
            " ",
            "200",
            "7"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "master",
            "ed",
            " integral",
            " and",
            " differential",
            " calculus",
            "\"",
            " while",
            " still",
            " just",
            " fourteen",
            ".",
            " His",
            " love",
            " of",
            " algebra",
            " and",
            " geometry",
            " was",
            " so",
            " great",
            " that",
            " at",
            " twelve",
            ",",
            " he",
            " was",
            " already",
            " confident",
            " that",
            " nature",
            " could",
            " be",
            " understood",
            " as",
            " a",
            " \"",
            "math",
            "ematic",
            "al",
            " structure",
            "\".",
            "At",
            " thirteen",
            ",",
            " when",
            " his",
            " range",
            " of",
            " enthus",
            "ias",
            "ms",
            " had",
            " broad",
            "ened",
            " to",
            " include",
            " music",
            " and",
            " philosophy",
            ",",
            " Einstein",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "-pl",
            "aque",
            " action",
            ",",
            " but",
            " less",
            " than",
            " chlor",
            "hex",
            "idine",
            ".",
            " M",
            "ethyl",
            " sal",
            "icy",
            "late",
            " does",
            " not",
            " stain",
            " teeth",
            ".",
            "N",
            "yst",
            "atin",
            "N",
            "yst",
            "atin",
            " suspension",
            " is",
            " an",
            " ant",
            "if",
            "ungal",
            " ingredient",
            " used",
            " for",
            " the",
            " treatment",
            " of",
            " oral",
            " candid",
            "ias",
            "is",
            ".",
            "Pot",
            "assium",
            " ox",
            "al",
            "ate",
            "A",
            " randomized",
            " clinical",
            " trial",
            " found",
            " promising",
            " results",
            " in",
            " controlling",
            " and",
            " reducing",
            " dent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " involved",
            ".",
            "Rare",
            "ly",
            " parasites",
            " can",
            " cause",
            " abs",
            "cess",
            "es",
            " and",
            " this",
            " is",
            " more",
            " common",
            " in",
            " the",
            " developing",
            " world",
            ".",
            " Specific",
            " parasites",
            " known",
            " to",
            " do",
            " this",
            " include",
            " dr",
            "ac",
            "unc",
            "ul",
            "ias",
            "is",
            " and",
            " my",
            "ias",
            "is",
            ".",
            "Per",
            "ian",
            "al",
            " abs",
            "cess",
            "S",
            "urgery",
            " of",
            " the",
            " anal",
            " fist",
            "ula",
            " to",
            " drain",
            " an",
            " abs",
            "cess",
            " treats",
            " the",
            " fist",
            "ula",
            " and",
            " reduces",
            " likelihood"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " involved",
            ".",
            "Rare",
            "ly",
            " parasites",
            " can",
            " cause",
            " abs",
            "cess",
            "es",
            " and",
            " this",
            " is",
            " more",
            " common",
            " in",
            " the",
            " developing",
            " world",
            ".",
            " Specific",
            " parasites",
            " known",
            " to",
            " do",
            " this",
            " include",
            " dr",
            "ac",
            "unc",
            "ul",
            "ias",
            "is",
            " and",
            " my",
            "ias",
            "is",
            ".",
            "Per",
            "ian",
            "al",
            " abs",
            "cess",
            "S",
            "urgery",
            " of",
            " the",
            " anal",
            " fist",
            "ula",
            " to",
            " drain",
            " an",
            " abs",
            "cess",
            " treats",
            " the",
            " fist",
            "ula",
            " and",
            " reduces",
            " likelihood"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " her",
            "pet",
            "iform",
            "e",
            " ulcer",
            "ation",
            " (",
            "an",
            " uncommon",
            " type",
            " of",
            " aph",
            "th",
            "ous",
            " stom",
            "atitis",
            "),",
            " but",
            " prolonged",
            " use",
            " may",
            " lead",
            " to",
            " oral",
            " candid",
            "ias",
            "is",
            ",",
            " as",
            " the",
            " fungal",
            " population",
            " of",
            " the",
            " mouth",
            " over",
            "g",
            "rows",
            " in",
            " the",
            " absence",
            " of",
            " enough",
            " competing",
            " bacteria",
            ".",
            " Similarly",
            ",",
            " min",
            "oc",
            "ycl",
            "ine",
            " mouth",
            "w",
            "ashes",
            " of",
            " ",
            "0",
            ".",
            "5",
            "%",
            " concentrations"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " le",
            "ish",
            "man",
            "ias",
            "is",
            " in",
            " domestic",
            " animals",
            ".",
            " Besides",
            " having",
            " low",
            " therapeutic",
            " indices",
            ",",
            " the",
            " drugs",
            " have",
            " minimal",
            " penetration",
            " of",
            " the",
            " bone",
            " marrow",
            ",",
            " where",
            " some",
            " of",
            " the",
            " Le",
            "ish",
            "mania",
            " am",
            "ast",
            "ig",
            "otes",
            " reside",
            ",",
            " and",
            " curing",
            " the",
            " disease",
            " â€“",
            " especially",
            " the",
            " visceral",
            " form",
            " â€“",
            " is",
            " very",
            " difficult",
            ".",
            " Elemental",
            " ant",
            "imony",
            " as",
            " an",
            " ant",
            "imony",
            " pill",
            " was",
            " once"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "is",
            " (",
            "sleep",
            "ing",
            " sickness",
            ");",
            " respiratory",
            " disease",
            ":",
            " mening",
            "oc",
            "oc",
            "cal",
            " mening",
            "itis",
            ",",
            " and",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            ",",
            " a",
            " water",
            " contact",
            " disease",
            ",",
            " as",
            " of",
            " ",
            "200",
            "5",
            ".",
            "Eth",
            "nic",
            " groups",
            "R",
            "ough",
            "ly",
            " ",
            "37",
            "%",
            " of",
            " Ang",
            "ol",
            "ans",
            " are",
            " Ov",
            "imb",
            "und",
            "u",
            ",",
            " ",
            "25",
            "%",
            " are",
            " Amb",
            "und",
            "u",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " poisoning",
            " by",
            " Olymp",
            "ias",
            ".",
            "News",
            " of",
            " Philip",
            "'s",
            " death",
            " r",
            "oused",
            " many",
            " states",
            " into",
            " revolt",
            ",",
            " including",
            " The",
            "bes",
            ",",
            " Athens",
            ",",
            " Th",
            "ess",
            "aly",
            ",",
            " and",
            " the",
            " Th",
            "rac",
            "ian",
            " tribes",
            " north",
            " of",
            " Maced",
            "on",
            ".",
            " When",
            " news",
            " of",
            " the",
            " rev",
            "ol",
            "ts",
            " reached",
            " Alexander",
            ",",
            " he",
            " responded",
            " quickly",
            ".",
            " Though",
            " advised",
            " to",
            " use",
            " diplomacy",
            ",",
            " Alexander",
            " must",
            "ered",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.19,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            "idine",
            " mouth",
            "wash",
            " before",
            " and",
            " after",
            " a",
            " tooth",
            " extraction",
            " may",
            " reduce",
            " the",
            " risk",
            " of",
            " a",
            " dry",
            " socket",
            ".",
            " Other",
            " uses",
            " of",
            " chlor",
            "hex",
            "idine",
            " mouth",
            "wash",
            " include",
            " prevention",
            " of",
            " oral",
            " candid",
            "ias",
            "is",
            " in",
            " immun",
            "ocom",
            "prom",
            "ised",
            " persons",
            ",",
            " treatment",
            " of",
            " dent",
            "ure",
            "-related",
            " stom",
            "atitis",
            ",",
            " muc",
            "osal",
            " ulcer",
            "ation",
            "/",
            "eros",
            "ions",
            " and",
            " oral",
            " muc",
            "osal",
            " lesions",
            ",",
            " general",
            " burning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "S",
            "Mar",
            "shall",
            " S",
            "ahl",
            "ins",
            "No",
            "el",
            " B",
            ".",
            " Sal",
            "azar",
            "Roger",
            " Sand",
            "all",
            "Edward",
            " Sap",
            "ir",
            "Pat",
            "ricia",
            " Saw",
            "in",
            "N",
            "ancy",
            " Sche",
            "per",
            "-H",
            "ugh",
            "es",
            "Wil",
            "helm",
            " Schmidt",
            "T",
            "ob",
            "ias",
            " Sch",
            "ne",
            "eba",
            "um",
            "James",
            " C",
            ".",
            " Scott",
            "Th",
            "ayer",
            " Sc",
            "udder",
            "El",
            "man",
            " Service",
            "A"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " arsen",
            "ic",
            " tri",
            "oxide",
            " (",
            "by",
            " Thomas",
            " Fowler",
            ").",
            " Ars",
            "phen",
            "amine",
            ",",
            " as",
            " well",
            " as",
            " ne",
            "osal",
            "vars",
            "an",
            ",",
            " was",
            " indicated",
            " for",
            " sy",
            "phil",
            "is",
            ",",
            " but",
            " has",
            " been",
            " supers",
            "eded",
            " by",
            " modern",
            " antibiotics",
            ".",
            " However",
            ",",
            " arsen",
            "icals",
            " such",
            " as",
            " mel",
            "ars",
            "op",
            "rol",
            " are",
            " still",
            " used",
            " for",
            " the",
            " treatment",
            " of",
            " try",
            "pan",
            "os",
            "om",
            "ias",
            "is",
            ",",
            " since",
            " although"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "sy",
            ",",
            " fe",
            "vers",
            ",",
            " strang",
            "ulated",
            " hern",
            "ias",
            ",",
            " nec",
            "rosis",
            ",",
            " abdominal",
            " tum",
            "ours",
            " and",
            " chronic",
            " const",
            "ipation",
            " and",
            " nicotine",
            " poisoning",
            ",",
            " while",
            " also",
            " attempting",
            " to",
            " deal",
            " with",
            " deliberate",
            " poison",
            "ings",
            ",",
            " fetish",
            "ism",
            " and",
            " fear",
            " of",
            " cann",
            "ibal",
            "ism",
            " among",
            " the",
            " Mb",
            "ah",
            "ou",
            "in",
            ".",
            "Sch",
            "we",
            "itzer",
            "'s",
            " wife",
            ",",
            " Hel",
            "ene",
            " Schwe",
            "itzer",
            ",",
            " served",
            " as",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " family",
            " friend",
            ",",
            " Dem",
            "ar",
            "atus",
            ",",
            " who",
            " mediated",
            " between",
            " the",
            " two",
            " parties",
            ".",
            "In",
            " the",
            " following",
            " year",
            ",",
            " the",
            " Persian",
            " sat",
            "rap",
            " (",
            "g",
            "overn",
            "or",
            ")",
            " of",
            " Car",
            "ia",
            ",",
            " Pix",
            "od",
            "arus",
            ",",
            " offered",
            " his",
            " eldest",
            " daughter",
            " to",
            " Alexander",
            "'s",
            " half",
            "-b",
            "ro",
            "ther",
            ",",
            " Philip",
            " Arr",
            "h",
            "ida",
            "eus",
            ".",
            " Olymp",
            "ias",
            " and",
            " several",
            " of",
            " Alexander",
            "'s",
            " friends",
            " suggested"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            ",",
            " Philip",
            " II",
            ",",
            " and",
            " his",
            " fourth",
            " wife",
            ",",
            " Olymp",
            "ias",
            " (",
            "daughter",
            " of",
            " Ne",
            "opt",
            "ole",
            "mus",
            " I",
            ",",
            " king",
            " of",
            " E",
            "pir",
            "us",
            ").",
            " Although",
            " Philip",
            " had",
            " seven",
            " or",
            " eight",
            " wives",
            ",",
            " Olymp",
            "ias",
            " was",
            " his",
            " principal",
            " wife",
            " for",
            " some",
            " time",
            ",",
            " likely",
            " because",
            " she",
            " gave",
            " birth",
            " to",
            " Alexander",
            ".",
            "Several",
            " legends",
            " surround",
            " Alexander",
            "'s",
            " birth",
            " and",
            " childhood",
            ".",
            " According"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            ",",
            " Philip",
            " II",
            ",",
            " and",
            " his",
            " fourth",
            " wife",
            ",",
            " Olymp",
            "ias",
            " (",
            "daughter",
            " of",
            " Ne",
            "opt",
            "ole",
            "mus",
            " I",
            ",",
            " king",
            " of",
            " E",
            "pir",
            "us",
            ").",
            " Although",
            " Philip",
            " had",
            " seven",
            " or",
            " eight",
            " wives",
            ",",
            " Olymp",
            "ias",
            " was",
            " his",
            " principal",
            " wife",
            " for",
            " some",
            " time",
            ",",
            " likely",
            " because",
            " she",
            " gave",
            " birth",
            " to",
            " Alexander",
            ".",
            "Several",
            " legends",
            " surround",
            " Alexander",
            "'s",
            " birth",
            " and",
            " childhood",
            ".",
            " According"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            "on",
            ",",
            " Philip",
            " II",
            ",",
            " and",
            " his",
            " fourth",
            " wife",
            ",",
            " Olymp",
            "ias",
            " (",
            "daughter",
            " of",
            " Ne",
            "opt",
            "ole",
            "mus",
            " I",
            ",",
            " king",
            " of",
            " E",
            "pir",
            "us",
            ").",
            " Although",
            " Philip",
            " had",
            " seven",
            " or",
            " eight",
            " wives",
            ",",
            " Olymp",
            "ias",
            " was",
            " his",
            " principal",
            " wife",
            " for",
            " some",
            " time",
            ",",
            " likely",
            " because",
            " she",
            " gave",
            " birth",
            " to",
            " Alexander",
            ".",
            "Several",
            " legends",
            " surround",
            " Alexander",
            "'s",
            " birth",
            " and",
            " childhood",
            ".",
            " According"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Spart",
            "ans",
            "'",
            " punishment",
            " to",
            " the",
            " League",
            " of",
            " Corinth",
            ",",
            " which",
            " then",
            " deferred",
            " to",
            " Alexander",
            ",",
            " who",
            " chose",
            " to",
            " pardon",
            " them",
            ".",
            " There",
            " was",
            " also",
            " considerable",
            " friction",
            " between",
            " Ant",
            "ip",
            "ater",
            " and",
            " Olymp",
            "ias",
            ",",
            " and",
            " each",
            " complained",
            " to",
            " Alexander",
            " about",
            " the",
            " other",
            ".",
            "In",
            " general",
            ",",
            " Greece",
            " enjoyed",
            " a",
            " period",
            " of",
            " peace",
            " and",
            " prosperity",
            " during",
            " Alexander",
            "'s",
            " campaign",
            " in",
            " Asia",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Corn",
            "hill",
            " Publishing",
            " Co",
            ".",
            " ",
            "192",
            "2",
            ".",
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " ",
            "10",
            ".",
            " ",
            "162",
            "â€“",
            "219",
            " (",
            "1",
            "â€“",
            "8",
            " CE",
            ")",
            " P",
            "aus",
            "an",
            "ias",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            " Description",
            " of",
            " Greece",
            " with",
            " an",
            " English",
            " Translation",
            " by",
            " W",
            ".H",
            ".S",
            ".",
            " Jones",
            ",",
            " L",
            "itt",
            ".D",
            ".,",
            " and",
            " H",
            ".A",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "ral",
            "ic",
            "\"",
            " referred",
            " to",
            " the",
            " U",
            "ral",
            " Mountains",
            ".",
            "While",
            " the",
            " U",
            "ral",
            "-Al",
            "ta",
            "ic",
            " family",
            " hypothesis",
            " can",
            " still",
            " be",
            " found",
            " in",
            " some",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            ",",
            " atl",
            "ases",
            ",",
            " and",
            " similar",
            " general",
            " references",
            ",",
            " since",
            " the",
            " ",
            "196",
            "0",
            "s",
            " it",
            " has",
            " been",
            " heavily",
            " criticized",
            ".",
            " Even",
            " lingu",
            "ists",
            " who",
            " accept",
            " the",
            " basic",
            " Alta",
            "ic",
            " family",
            ",",
            " such"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " carried",
            " the",
            " I",
            "li",
            "ad",
            " with",
            " him",
            ",",
            " but",
            " his",
            " court",
            " bi",
            "ographers",
            " do",
            " not",
            " mention",
            " the",
            " spear",
            ";",
            " however",
            ",",
            " it",
            " was",
            " shown",
            " in",
            " the",
            " time",
            " of",
            " P",
            "aus",
            "an",
            "ias",
            " in",
            " the",
            " ",
            "2",
            "nd",
            " century",
            " CE",
            ".",
            "A",
            "ch",
            "illes",
            ",",
            " Ajax",
            " and",
            " a",
            " game",
            " of",
            " p",
            "ette",
            "ia",
            " ",
            "Numer",
            "ous",
            " paintings",
            " on",
            " pottery",
            " have",
            " suggested",
            " a",
            " tale",
            " not"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Corn",
            "hill",
            " Publishing",
            " Co",
            ".",
            " ",
            "192",
            "2",
            ".",
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " ",
            "10",
            ".",
            " ",
            "162",
            "â€“",
            "219",
            " (",
            "1",
            "â€“",
            "8",
            " CE",
            ")",
            " P",
            "aus",
            "an",
            "ias",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            " Description",
            " of",
            " Greece",
            " with",
            " an",
            " English",
            " Translation",
            " by",
            " W",
            ".H",
            ".S",
            ".",
            " Jones",
            ",",
            " L",
            "itt",
            ".D",
            ".,",
            " and",
            " H",
            ".A",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Corn",
            "hill",
            " Publishing",
            " Co",
            ".",
            " ",
            "192",
            "2",
            ".",
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " ",
            "10",
            ".",
            " ",
            "162",
            "â€“",
            "219",
            " (",
            "1",
            "â€“",
            "8",
            " CE",
            ")",
            " P",
            "aus",
            "an",
            "ias",
            ",",
            " P",
            "aus",
            "an",
            "ias",
            " Description",
            " of",
            " Greece",
            " with",
            " an",
            " English",
            " Translation",
            " by",
            " W",
            ".H",
            ".S",
            ".",
            " Jones",
            ",",
            " L",
            "itt",
            ".D",
            ".,",
            " and",
            " H",
            ".A",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            "Field",
            "notes",
            " and",
            " memoir",
            "s",
            "Hist",
            "ories",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " .",
            "Text",
            "books",
            " and",
            " key",
            " theoretical",
            " works",
            "External",
            " links",
            " Open",
            " Encyclopedia",
            " of",
            " Anthrop",
            "ology",
            ".",
            "Organ",
            "isations",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " ",
            " (",
            "A",
            "IO",
            ")",
            " ",
            "Behaviour",
            "al",
            " sciences",
            "Humans",
            "<|begin_of_text|>",
            "A",
            "gricult",
            "ural",
            " science"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.151,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " stemmed",
            " directly",
            " from",
            " their",
            " environment",
            ".",
            " Living",
            " and",
            " working",
            " close",
            " to",
            " the",
            " Nile",
            " brought",
            " hazards",
            " from",
            " malaria",
            " and",
            " debilitating",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            " parasites",
            ",",
            " which",
            " caused",
            " liver",
            " and",
            " intestinal",
            " damage",
            ".",
            " Dangerous",
            " wildlife",
            " such",
            " as",
            " cro",
            "cod",
            "iles",
            " and",
            " hip",
            "pos",
            " were",
            " also",
            " a",
            " common",
            " threat",
            ".",
            " The",
            " lifelong",
            " lab",
            "ors",
            " of",
            " farming",
            " and",
            " building",
            " put",
            " stress",
            " on",
            " the",
            " spine"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "He",
            " appears",
            " to",
            " have",
            " believed",
            " himself",
            " a",
            " deity",
            ",",
            " or",
            " at",
            " least",
            " sought",
            " to",
            " de",
            "ify",
            " himself",
            ".",
            " Olymp",
            "ias",
            " always",
            " insisted",
            " to",
            " him",
            " that",
            " he",
            " was",
            " the",
            " son",
            " of",
            " Zeus",
            ",",
            " a",
            " theory",
            " apparently",
            " confirmed",
            " to",
            " him",
            " by",
            " the",
            " oracle",
            " of",
            " Am",
            "un",
            " at",
            " Si",
            "wa",
            ".",
            " He",
            " began",
            " to",
            " identify",
            " himself",
            " as",
            " the",
            " son",
            " of",
            " Zeus",
            "-Am",
            "mon",
            ".",
            " Alexander"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.079,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            "Field",
            "notes",
            " and",
            " memoir",
            "s",
            "Hist",
            "ories",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " .",
            "Text",
            "books",
            " and",
            " key",
            " theoretical",
            " works",
            "External",
            " links",
            " Open",
            " Encyclopedia",
            " of",
            " Anthrop",
            "ology",
            ".",
            "Organ",
            "isations",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " ",
            " (",
            "A",
            "IO",
            ")",
            " ",
            "Behaviour",
            "al",
            " sciences",
            "Humans",
            "<|begin_of_text|>",
            "A",
            "gricult",
            "ural",
            " science"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " widely",
            " accepted",
            " until",
            " the",
            " ",
            "196",
            "0",
            "s",
            " and",
            " is",
            " still",
            " listed",
            " in",
            " many",
            " enc",
            "yc",
            "lo",
            "ped",
            "ias",
            " and",
            " hand",
            "books",
            ",",
            " and",
            " references",
            " to",
            " Alta",
            "ic",
            " as",
            " a",
            " language",
            " family",
            " continue",
            " to",
            " per",
            "col",
            "ate",
            " to",
            " modern",
            " sources",
            " through",
            " these",
            " older",
            " sources",
            ".",
            " Since",
            " the",
            " ",
            "195",
            "0",
            "s",
            ",",
            " most",
            " comparative",
            " lingu",
            "ists",
            " have",
            " rejected",
            " the",
            " proposal",
            ",",
            " after",
            " supposed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " reign",
            " by",
            " eliminating",
            " potential",
            " rivals",
            " to",
            " the",
            " throne",
            ".",
            " He",
            " had",
            " his",
            " cousin",
            ",",
            " the",
            " former",
            " Amy",
            "nt",
            "as",
            " IV",
            ",",
            " executed",
            ".",
            " He",
            " also",
            " had",
            " two",
            " Maced",
            "onian",
            " princes",
            " from",
            " the",
            " region",
            " of",
            " Lyn",
            "cest",
            "is",
            " killed",
            " for",
            " having",
            " been",
            " involved",
            " in",
            " his",
            " father",
            "'s",
            " assassination",
            ",",
            " but",
            " spared",
            " a",
            " third",
            ",",
            " Alexander",
            " Lyn",
            "cest",
            "es",
            ".",
            " Olymp",
            "ias",
            " had",
            " Cle",
            "op"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " the",
            " ancient",
            " Greek",
            " bi",
            "ographer",
            " Pl",
            "ut",
            "arch",
            ",",
            " on",
            " the",
            " eve",
            " of",
            " the",
            " consum",
            "m",
            "ation",
            " of",
            " her",
            " marriage",
            " to",
            " Philip",
            ",",
            " Olymp",
            "ias",
            " dreamed",
            " that",
            " her",
            " womb",
            " was",
            " struck",
            " by",
            " a",
            " thunder",
            "bolt",
            " that",
            " caused",
            " a",
            " flame",
            " to",
            " spread",
            " \"",
            "far",
            " and",
            " wide",
            "\"",
            " before",
            " dying",
            " away",
            ".",
            " Som",
            "etime",
            " after",
            " the",
            " wedding",
            ",",
            " Philip",
            " is",
            " said",
            " to",
            " have",
            " seen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "26",
            " October",
            " ",
            "201",
            "7",
            " and",
            " by",
            " J",
            "oz",
            "ias",
            " van",
            " A",
            "arts",
            "en",
            " on",
            " ",
            "4",
            " December",
            " ",
            "201",
            "7",
            ".",
            "Unlike",
            " most",
            " other",
            " Dutch",
            " municipalities",
            ",",
            " Amsterdam",
            " is",
            " subdiv",
            "ided",
            " into",
            " eight",
            " borough",
            "s",
            ",",
            " called",
            " st",
            "ads",
            "d",
            "elen",
            " or",
            " '",
            "district",
            "s",
            "',",
            " and",
            " the",
            " urban",
            " area",
            " of",
            " We",
            "esp",
            ",",
            " a",
            " system",
            " that",
            " was",
            " implemented",
            " gradually",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.024,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " the",
            " effects",
            " are",
            ":",
            " Aeros",
            "ol",
            " direct",
            " effect",
            ".",
            " Aeros",
            "ols",
            " directly",
            " scatter",
            " and",
            " absorb",
            " radiation",
            ".",
            " The",
            " scattering",
            " of",
            " radiation",
            " causes",
            " atmospheric",
            " cooling",
            ",",
            " whereas",
            " absorption",
            " can",
            " cause",
            " atmospheric",
            " warming",
            ".",
            " Aeros",
            "ol",
            " indirect",
            " effect",
            ".",
            " Aeros",
            "ols",
            " modify",
            " the",
            " properties",
            " of",
            " clouds",
            " through",
            " a",
            " subset",
            " of",
            " the",
            " aeros",
            "ol",
            " population",
            " called",
            " cloud",
            " cond",
            "ensation",
            " nuclei",
            ".",
            " Increased",
            " nuclei",
            " concentrations",
            " lead"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " first",
            " structure",
            " burned",
            " down",
            " in",
            " ",
            "184",
            "9",
            ",",
            " but",
            " was",
            " rebuilt",
            " on",
            " the",
            " same",
            " site",
            " in",
            " ",
            "185",
            "1",
            ".",
            " This",
            " second",
            " cap",
            "itol",
            " building",
            " in",
            " Montgomery",
            " remains",
            " to",
            " the",
            " present",
            " day",
            ".",
            " It",
            " was",
            " designed",
            " by",
            " Bar",
            "ach",
            "ias",
            " Holt",
            " of",
            " Ex",
            "eter",
            ",",
            " Maine",
            ".",
            "Civil",
            " War",
            " and",
            " Reconstruction",
            " ",
            "By",
            " ",
            "186",
            "0",
            ",",
            " the",
            " population",
            " had",
            " increased",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " optimistic",
            ".",
            " Gins",
            "berg",
            " continued",
            " to",
            " write",
            " through",
            " his",
            " final",
            " illness",
            ",",
            " with",
            " his",
            " last",
            " poem",
            ",",
            " \"",
            "Things",
            " I",
            "'ll",
            " Not",
            " Do",
            " (",
            "N",
            "ost",
            "alg",
            "ias",
            ")\",",
            " written",
            " on",
            " March",
            " ",
            "30",
            ".",
            "He",
            " died",
            " on",
            " April",
            " ",
            "5",
            ",",
            " ",
            "199",
            "7",
            ",",
            " surrounded",
            " by",
            " family",
            " and",
            " friends",
            " in",
            " his",
            " East",
            " Village",
            " loft",
            " in",
            " Manhattan",
            ",",
            " succ",
            "umbing",
            " to",
            " liver"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.099,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " lubric",
            "ants",
            " and",
            " fuels",
            " to",
            " operate",
            " farm",
            " vehicles",
            " and",
            " machinery",
            ".",
            "Ind",
            "irect",
            " consumption",
            " includes",
            " the",
            " manufacture",
            " of",
            " fertil",
            "izers",
            ",",
            " pesticides",
            ",",
            " and",
            " farm",
            " machinery",
            ".",
            " In",
            " particular",
            ",",
            " the",
            " production",
            " of",
            " nitrogen",
            " fertilizer",
            " can",
            " account",
            " for",
            " over",
            " half",
            " of",
            " agricultural",
            " energy",
            " usage",
            ".",
            " Together",
            ",",
            " direct",
            " and",
            " indirect",
            " consumption",
            " by",
            " US",
            " farms",
            " accounts",
            " for",
            " about",
            " ",
            "2",
            "%",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " less",
            " than",
            " ",
            "1",
            "Ã‚Å‚",
            "cm",
            " in",
            " diameter",
            " due",
            " to",
            " diffusion",
            " problems",
            ",",
            " a",
            " size",
            " which",
            " puts",
            " a",
            " limit",
            " on",
            " the",
            " amount",
            " of",
            " post",
            "h",
            "atching",
            " growth",
            ".",
            "The",
            " smallest",
            " amphib",
            "ian",
            " (",
            "and",
            " verte",
            "brate",
            ")",
            " in",
            " the",
            " world",
            " is",
            " a",
            " micro",
            "h",
            "yl",
            "id",
            " frog",
            " from",
            " New",
            " Guinea",
            " (",
            "Pa",
            "ed",
            "oph",
            "ry",
            "ne",
            " am",
            "au",
            "ensis",
            ")",
            " first",
            " discovered",
            " in"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " general",
            " theory",
            " of",
            " rel",
            "ativity",
            " revealed",
            " that",
            " it",
            " does",
            " not",
            " necessarily",
            " express",
            " a",
            " property",
            " of",
            " physical",
            " space",
            "â€”",
            "Sch",
            "openh",
            "auer",
            " criticized",
            " mathematic",
            "ians",
            " for",
            " trying",
            " to",
            " use",
            " indirect",
            " concepts",
            " to",
            " prove",
            " what",
            " he",
            " held",
            " was",
            " directly",
            " evident",
            " from",
            " intuitive",
            " perception",
            ".",
            "Throughout",
            " his",
            " writings",
            ",",
            " Sch",
            "openh",
            "auer",
            " criticized",
            " the",
            " logical",
            " derivation",
            " of",
            " philosoph",
            "ies",
            " and",
            " mathematics",
            " from",
            " mere",
            " concepts",
            ",",
            " instead"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "ana",
            " Ar",
            "sen",
            "ic",
            " and",
            " Old",
            " Lace",
            " Ar",
            "sen",
            "ic",
            " bio",
            "chemistry",
            " Ar",
            "sen",
            "ic",
            " compounds",
            " Ar",
            "sen",
            "ic",
            " poisoning",
            " Ar",
            "sen",
            "ic",
            " toxicity",
            " Ar",
            "sen",
            "ic",
            " tri",
            "oxide",
            " Fowler",
            "'s",
            " solution",
            " G",
            "FA",
            "J",
            "-",
            "1",
            " Gra",
            "inger",
            " challenge",
            " Hyp",
            "oth",
            "etical",
            " types",
            " of",
            " bio",
            "chemistry",
            " Org",
            "ano",
            "ars",
            "enic",
            " chemistry"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.058,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.34,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            "anced",
            " co",
            "ag",
            "ulation",
            "\",",
            " a",
            " form",
            " of",
            " arg",
            "on",
            " plasma",
            " beam",
            " elect",
            "ros",
            "urgery",
            ".",
            " The",
            " procedure",
            " carries",
            " a",
            " risk",
            " of",
            " producing",
            " gas",
            " emb",
            "ol",
            "ism",
            " and",
            " has",
            " resulted",
            " in",
            " the",
            " death",
            " of",
            " at",
            " least",
            " one",
            " patient",
            ".",
            "Blue",
            " arg",
            "on",
            " lasers",
            " are",
            " used",
            " in",
            " surgery",
            " to",
            " weld",
            " arteries",
            ",",
            " destroy",
            " tumors",
            ",",
            " and",
            " correct",
            " eye",
            " defects",
            ".",
            "Arg",
            "on",
            " has",
            " also"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.034,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " motor",
            " nerves",
            " induced",
            " paralysis",
            ".",
            " Her",
            "ophil",
            "us",
            " named",
            " the",
            " men",
            "inges",
            " and",
            " vent",
            "ric",
            "les",
            " in",
            " the",
            " brain",
            ",",
            " appreciated",
            " the",
            " division",
            " between",
            " cere",
            "bell",
            "um",
            " and",
            " cere",
            "br",
            "um",
            " and",
            " recognized",
            " that",
            " the",
            " brain",
            " was",
            " the",
            " \"",
            "seat",
            " of",
            " intellect",
            "\"",
            " and",
            " not",
            " a",
            " \"",
            "cool",
            "ing",
            " chamber",
            "\"",
            " as",
            " prop",
            "ounded",
            " by",
            " Aristotle",
            " Her",
            "ophil",
            "us",
            " is",
            " also",
            " credited"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.303,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            "10",
            "%,",
            " but",
            " is",
            " higher",
            " if",
            " the",
            " abs",
            "cess",
            " rupt",
            "ures",
            ".",
            "E",
            "pid",
            "emi",
            "ology",
            "Skin",
            " abs",
            "cess",
            "es",
            " are",
            " common",
            " and",
            " have",
            " become",
            " more",
            " common",
            " in",
            " recent",
            " years",
            ".",
            " Risk",
            " factors",
            " include",
            " intr",
            "avenous",
            " drug",
            " use",
            ",",
            " with",
            " rates",
            " reported",
            " as",
            " high",
            " as",
            " ",
            "65",
            "%",
            " among",
            " users",
            ".",
            " In",
            " ",
            "200",
            "5",
            ",",
            " in",
            " the",
            " United",
            " States",
            " ",
            "3"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.291,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " or",
            " \"",
            "sp",
            "ont",
            "aneous",
            " abortion",
            "\";",
            " these",
            " occur",
            " in",
            " approximately",
            " ",
            "30",
            "%",
            " to",
            " ",
            "40",
            "%",
            " of",
            " all",
            " pregnancies",
            ".",
            " When",
            " deliberate",
            " steps",
            " are",
            " taken",
            " to",
            " end",
            " a",
            " pregnancy",
            ",",
            " it",
            " is",
            " called",
            " an",
            " induced",
            " abortion",
            ",",
            " or",
            " less",
            " frequently",
            " \"",
            "ind",
            "uced",
            " miscar",
            "riage",
            "\".",
            " The",
            " un",
            "modified",
            " word",
            " abortion",
            " generally",
            " refers",
            " to",
            " an",
            " induced",
            " abortion",
            ".",
            " The",
            " reasons",
            " why"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.249,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " od",
            "ors",
            " or",
            " tastes",
            ",",
            " PET",
            "-sc",
            "ans",
            " show",
            " increased",
            " blood",
            " flow",
            " in",
            " the",
            " amy",
            "gd",
            "ala",
            ".",
            " In",
            " these",
            " studies",
            ",",
            " the",
            " participants",
            " also",
            " reported",
            " moderate",
            " anxiety",
            ".",
            " This",
            " might",
            " indicate",
            " that",
            " anxiety",
            " is",
            " a",
            " protective",
            " mechanism",
            " designed",
            " to",
            " prevent",
            " the",
            " organism",
            " from",
            " engaging",
            " in",
            " potentially",
            " harmful",
            " behaviors",
            ".",
            "Social",
            "Social",
            " risk",
            " factors",
            " for",
            " anxiety",
            " include",
            " a",
            " history",
            " of",
            " trauma",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.231,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ã‚Â°F",
            "),",
            " were",
            " shown",
            " to",
            " produce",
            " a",
            " greater",
            " exposure",
            " risk",
            " than",
            " when",
            " bit",
            "umen",
            " was",
            " heated",
            " to",
            " lower",
            " temperatures",
            ",",
            " such",
            " as",
            " those",
            " typically",
            " used",
            " in",
            " asphalt",
            " pavement",
            " mix",
            " production",
            " and",
            " placement",
            ".",
            " I",
            "ARC",
            " has",
            " classified",
            " paving",
            " asphalt",
            " f",
            "umes",
            " as",
            " a",
            " Class",
            " ",
            "2",
            "B",
            " possible",
            " carcin",
            "ogen",
            ",",
            " indicating",
            " inadequate",
            " evidence",
            " of",
            " carcin",
            "ogenic",
            "ity",
            " in",
            " humans",
            ".",
            "In",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " were",
            " voted",
            " unanimously",
            " by",
            " j",
            "uries",
            ".",
            " This",
            " judicial",
            " authority",
            " was",
            " removed",
            " in",
            " April",
            " ",
            "201",
            "7",
            ".",
            "On",
            " May",
            " ",
            "14",
            ",",
            " ",
            "201",
            "9",
            ",",
            " Alabama",
            " passed",
            " the",
            " Human",
            " Life",
            " Protection",
            " Act",
            ",",
            " banning",
            " abortion",
            " at",
            " any",
            " stage",
            " of",
            " pregnancy",
            " unless",
            " there",
            " is",
            " a",
            " \"",
            "serious",
            " health",
            " risk",
            "\",",
            " with",
            " no",
            " exceptions",
            " for",
            " rape",
            " and",
            " incest",
            ".",
            " The",
            " law",
            " subjects",
            " doctors"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "Eth",
            "nom",
            "usic",
            "ology",
            " is",
            " an",
            " academic",
            " field",
            " encompass",
            "ing",
            " various",
            " approaches",
            " to",
            " the",
            " study",
            " of",
            " music",
            " (",
            "b",
            "road",
            "ly",
            " defined",
            "),",
            " that",
            " emphasize",
            " its",
            " cultural",
            ",",
            " social",
            ",",
            " material",
            ",",
            " cognitive",
            ",",
            " biological",
            ",",
            " and",
            " other",
            " dimensions",
            " or",
            " contexts",
            " instead",
            " of",
            " or",
            " in",
            " addition",
            " to",
            " its",
            " isolated",
            " sound",
            " component",
            " or",
            " any",
            " particular",
            " repertoire",
            ".",
            "Eth",
            "nom",
            "usic",
            "ology",
            " can",
            " be"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " his",
            " B",
            "act",
            "rian",
            " sat",
            "rap",
            " and",
            " k",
            "ins",
            "man",
            ".",
            " As",
            " Alexander",
            " approached",
            ",",
            " B",
            "ess",
            "us",
            " had",
            " his",
            " men",
            " fatally",
            " stab",
            " the",
            " Great",
            " King",
            " and",
            " then",
            " declared",
            " himself",
            " D",
            "arius",
            "'s",
            " successor",
            " as",
            " Art",
            "ax",
            "er",
            "xes",
            " V",
            ",",
            " before",
            " ret",
            "reating",
            " into",
            " Central",
            " Asia",
            " to",
            " launch",
            " a",
            " guerr",
            "illa",
            " campaign",
            " against",
            " Alexander",
            ".",
            " Alexander",
            " buried",
            " D",
            "arius",
            "'s",
            " remains"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " planet",
            " ",
            "6",
            ".",
            "6",
            " times",
            " the",
            " mass",
            " of",
            " Earth",
            ".",
            " The",
            " planets",
            " vary",
            " in",
            " orbital",
            " period",
            " from",
            " ",
            "2",
            " days",
            " to",
            " ",
            "124",
            " days",
            ".",
            " ",
            "91",
            " Aqu",
            "ari",
            "i",
            " is",
            " an",
            " orange",
            " giant",
            " star",
            " orb",
            "ited",
            " by",
            " one",
            " planet",
            ",",
            " ",
            "91",
            "Ã‚Å‚",
            "Aqu",
            "ari",
            "i",
            "Ã‚Å‚b",
            ".",
            " The",
            " planet",
            "'s",
            " mass",
            " is",
            " ",
            "2",
            ".",
            "9",
            " times",
            " the",
            " mass",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " it",
            ".",
            " I",
            " need",
            " wider",
            " powers",
            ".\"",
            "Secondary",
            " characters",
            "The",
            " following",
            " secondary",
            " characters",
            " also",
            " appear",
            " in",
            " the",
            " novel",
            ".",
            "Hugh",
            " Ak",
            "ston",
            " is",
            " identified",
            " as",
            " \"",
            "One",
            " of",
            " the",
            " last",
            " great",
            " advocates",
            " of",
            " reason",
            ".\"",
            " He",
            " was",
            " a",
            " renowned",
            " philosopher",
            " and",
            " the",
            " head",
            " of",
            " the",
            " Department",
            " of",
            " Philosophy",
            " at",
            " Patrick",
            " Henry",
            " University",
            ",",
            " where",
            " he",
            " taught",
            " Francisco",
            " d",
            "'",
            "An",
            "con",
            "ia"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " ",
            "21",
            "st",
            " century",
            ",",
            " Audi",
            " set",
            " forth",
            " on",
            " a",
            " German",
            " rac",
            "etr",
            "ack",
            " to",
            " claim",
            " and",
            " maintain",
            " several",
            " world",
            " records",
            ",",
            " such",
            " as",
            " top",
            " speed",
            " endurance",
            ".",
            " This",
            " effort",
            " was",
            " in",
            "-line",
            " with",
            " the",
            " company",
            "'s",
            " heritage",
            " from",
            " the",
            " ",
            "193",
            "0",
            "s",
            " racing",
            " era",
            " Silver",
            " Ar",
            "rows",
            ".",
            "Through",
            " the",
            " early",
            " ",
            "199",
            "0",
            "s",
            ",",
            " Audi",
            " began",
            " to",
            " shift"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "antas",
            " Airways",
            " Limited",
            " (",
            "b",
            ".",
            " ",
            "189",
            "5",
            ")",
            "197",
            "7",
            " â€“",
            " K",
            "Ã…Ä¯",
            "ichi",
            " K",
            "ido",
            ",",
            " Japanese",
            " politician",
            ",",
            " ",
            "13",
            "th",
            " Lord",
            " Keeper",
            " of",
            " the",
            " Priv",
            "y",
            " Seal",
            " of",
            " Japan",
            " (",
            "b",
            ".",
            " ",
            "188",
            "9",
            ")",
            "197",
            "9",
            " â€“",
            " Ivan",
            " V",
            "asily",
            "ov",
            ",",
            " Bulgarian",
            " architect",
            ",",
            " designed",
            " the",
            " SS",
            ".",
            " Cyril",
            " and",
            " Method",
            "ius",
            " National",
            " Library",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    ".scalablytyped",
    "elerik",
    "enthal",
    "inois",
    "avia"
  ],
  "bottom_logits": [
    " CONTEXT",
    "Disposition",
    " bolt",
    "Ã¥Ä¯Äµ",
    " Lehr"
  ],
  "act_min": -0.0,
  "act_max": 0.672
}