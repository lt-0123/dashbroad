{
  "index": 25646,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Natural",
            " fruits",
            " and",
            " vegetables",
            " also",
            " contain",
            " acids",
            ".",
            " Cit",
            "ric",
            " acid",
            " is",
            " present",
            " in",
            " oranges",
            ",",
            " lemon",
            " and",
            " other",
            " citrus",
            " fruits",
            ".",
            " Ox",
            "alic",
            " acid",
            " is",
            " present",
            " in",
            " tomatoes",
            ",",
            " spinach",
            ",",
            " and",
            " especially",
            " in",
            " c",
            "aram",
            "b",
            "ola",
            " and",
            " rh",
            "ubar",
            "b",
            ";",
            " rh",
            "ubar",
            "b",
            " leaves",
            " and",
            " un",
            "ripe",
            " c",
            "aram",
            "b",
            "olas",
            " are",
            " toxic",
            " because",
            " of",
            " high",
            " concentrations",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " solution",
            " of",
            " hydrogen",
            " chloride",
            " that",
            " is",
            " found",
            " in",
            " gastric",
            " acid",
            " in",
            " the",
            " stomach",
            " and",
            " activates",
            " digestive",
            " enzymes",
            "),",
            " ac",
            "etic",
            " acid",
            " (",
            "vine",
            "gar",
            " is",
            " a",
            " dil",
            "ute",
            " aque",
            "ous",
            " solution",
            " of",
            " this",
            " liquid",
            "),",
            " sulfur",
            "ic",
            " acid",
            " (",
            "used",
            " in",
            " car",
            " batteries",
            "),",
            " and",
            " cit",
            "ric",
            " acid",
            " (",
            "found",
            " in",
            " citrus",
            " fruits",
            ").",
            " As",
            " these",
            " examples",
            " show",
            ",",
            " acids",
            " (",
            "in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " solution",
            " of",
            " hydrogen",
            " chloride",
            " that",
            " is",
            " found",
            " in",
            " gastric",
            " acid",
            " in",
            " the",
            " stomach",
            " and",
            " activates",
            " digestive",
            " enzymes",
            "),",
            " ac",
            "etic",
            " acid",
            " (",
            "vine",
            "gar",
            " is",
            " a",
            " dil",
            "ute",
            " aque",
            "ous",
            " solution",
            " of",
            " this",
            " liquid",
            "),",
            " sulfur",
            "ic",
            " acid",
            " (",
            "used",
            " in",
            " car",
            " batteries",
            "),",
            " and",
            " cit",
            "ric",
            " acid",
            " (",
            "found",
            " in",
            " citrus",
            " fruits",
            ").",
            " As",
            " these",
            " examples",
            " show",
            ",",
            " acids",
            " (",
            "in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " ox",
            "alic",
            " acid",
            ".",
            " As",
            "cor",
            "bic",
            " acid",
            " (",
            "V",
            "itamin",
            " C",
            ")",
            " is",
            " an",
            " essential",
            " vitamin",
            " for",
            " the",
            " human",
            " body",
            " and",
            " is",
            " present",
            " in",
            " such",
            " foods",
            " as",
            " a",
            "ml",
            "a",
            " (",
            "Indian",
            " goose",
            "berry",
            "),",
            " lemon",
            ",",
            " citrus",
            " fruits",
            ",",
            " and",
            " gu",
            "ava",
            ".",
            "Many",
            " acids",
            " can",
            " be",
            " found",
            " in",
            " various",
            " kinds",
            " of",
            " food",
            " as",
            " additives",
            ",",
            " as",
            " they",
            " alter",
            " their",
            " taste"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " was",
            " found",
            ",",
            " and",
            " the",
            " rational",
            ",",
            " life",
            "-aff",
            "irm",
            "ing",
            " optimism",
            " of",
            " the",
            " Age",
            " of",
            " Enlightenment",
            " began",
            " to",
            " evapor",
            "ate",
            ".",
            " A",
            " rift",
            " opened",
            " between",
            " this",
            " world",
            "-view",
            ",",
            " as",
            " material",
            " knowledge",
            ",",
            " and",
            " the",
            " life",
            "-view",
            ",",
            " understood",
            " as",
            " Will",
            ",",
            " expressed",
            " in",
            " the",
            " pessim",
            "ist",
            " philosoph",
            "ies",
            " from",
            " Sch",
            "openh",
            "auer",
            " onward",
            ".",
            " Scientific",
            " material",
            "ism",
            " (",
            "advanced",
            " by",
            " Herbert"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            "ably",
            " young",
            " population",
            " initiated",
            " a",
            " period",
            " of",
            " creativity",
            " and",
            " optimism",
            ";",
            " G",
            "aff",
            "a",
            " and",
            " the",
            " Ka",
            "os",
            "P",
            "ilot",
            " school",
            " were",
            " founded",
            " in",
            " ",
            "198",
            "3",
            " and",
            " ",
            "199",
            "1",
            " respectively",
            ",",
            " and",
            " A",
            "arhus",
            " was",
            " at",
            " the",
            " centre",
            " of",
            " a",
            " re",
            "naissance",
            " in",
            " Danish",
            " rock",
            " and",
            " pop",
            " music",
            " launching",
            " bands",
            " and",
            " musicians",
            " such",
            " as",
            " TV",
            "2",
            ",",
            " Gn",
            "ags",
            ",",
            " Thomas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " constituent",
            " of",
            " Deb",
            "ier",
            "ne",
            "'s",
            " ",
            "189",
            "9",
            " and",
            " ",
            "190",
            "0",
            " results",
            ";",
            " in",
            " fact",
            ",",
            " the",
            " chemical",
            " properties",
            " he",
            " reported",
            " make",
            " it",
            " likely",
            " that",
            " he",
            " had",
            ",",
            " instead",
            ",",
            " accidentally",
            " identified",
            " prot",
            "act",
            "inium",
            ",",
            " which",
            " would",
            " not",
            " be",
            " discovered",
            " for",
            " another",
            " fourteen",
            " years",
            ",",
            " only",
            " to",
            " have",
            " it",
            " disappear",
            " due",
            " to",
            " its",
            " hydro",
            "ly",
            "sis",
            " and",
            " ads",
            "orption",
            " onto"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "¯",
            "Âł",
            "(x",
            "Âł",
            "âĪĴ",
            "Âł",
            "an",
            ")",
            "Âł",
            "+",
            "Âł",
            "1",
            "has",
            " no",
            " zero",
            " in",
            " F",
            ".",
            " However",
            ",",
            " the",
            " union",
            " of",
            " all",
            " finite",
            " fields",
            " of",
            " a",
            " fixed",
            " characteristic",
            " p",
            " is",
            " an",
            " algebra",
            "ically",
            " closed",
            " field",
            ",",
            " which",
            " is",
            ",",
            " in",
            " fact",
            ",",
            " the",
            " algebra",
            "ic",
            " closure",
            " of",
            " the",
            " field",
            " ",
            " with",
            " p",
            " elements",
            ".",
            "Equivalent",
            " properties",
            "Given",
            " a",
            " field"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " I",
            " (",
            "r",
            ".",
            " ",
            "802",
            "–",
            "811",
            ")",
            " strengthened",
            " its",
            " fort",
            "ifications",
            ",",
            " a",
            " fact",
            " which",
            " probably",
            " saved",
            " it",
            " from",
            " sack",
            " during",
            " the",
            " large",
            "-scale",
            " invasion",
            " of",
            " Anat",
            "olia",
            " by",
            " Cal",
            "iph",
            " Har",
            "un",
            " al",
            "-R",
            "ash",
            "id",
            " in",
            " the",
            " next",
            " year",
            ".",
            " Arab",
            " sources",
            " report",
            " that",
            " Har",
            "un",
            " and",
            " his",
            " successor",
            " al",
            "-M",
            "a",
            "'m",
            "un",
            " (",
            "r",
            ".",
            " ",
            "813"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "–",
            "G",
            "Ã¶",
            "del",
            " set",
            " theory",
            ",",
            " a",
            " conservative",
            " extension",
            " of",
            " Z",
            "FC",
            ".",
            " Sometimes",
            " slightly",
            " stronger",
            " theories",
            " such",
            " as",
            " Morse",
            "–",
            "Kel",
            "ley",
            " set",
            " theory",
            " or",
            " set",
            " theory",
            " with",
            " a",
            " strongly",
            " inaccessible",
            " cardinal",
            " allowing",
            " the",
            " use",
            " of",
            " a",
            " Gro",
            "th",
            "end",
            "ie",
            "ck",
            " universe",
            " is",
            " used",
            ",",
            " but",
            " in",
            " fact",
            ",",
            " most",
            " mathematic",
            "ians",
            " can",
            " actually",
            " prove",
            " all",
            " they",
            " need",
            " in",
            " systems"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " hunting",
            ".",
            "In",
            " fact",
            ",",
            " tad",
            "po",
            "les",
            " developing",
            " in",
            " ponds",
            " and",
            " streams",
            " are",
            " typically",
            " herb",
            "iv",
            "orous",
            ".",
            " Pond",
            " tad",
            "po",
            "les",
            " tend",
            " to",
            " have",
            " deep",
            " bodies",
            ",",
            " large",
            " ca",
            "ud",
            "al",
            " fins",
            " and",
            " small",
            " mouths",
            ";",
            " they",
            " swim",
            " in",
            " the",
            " quiet",
            " waters",
            " feeding",
            " on",
            " growing",
            " or",
            " loose",
            " fragments",
            " of",
            " vegetation",
            ".",
            " Stream",
            " dwell",
            "ers",
            " mostly",
            " have",
            " larger",
            " mouths",
            ",",
            " shallow",
            " bodies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " axiom",
            " of",
            " choice",
            ".",
            "Equ",
            "ival",
            "ents",
            "There",
            " are",
            " important",
            " statements",
            " that",
            ",",
            " assuming",
            " the",
            " ax",
            "ioms",
            " of",
            " Z",
            "F",
            " but",
            " neither",
            " AC",
            " nor",
            " Â¬",
            "AC",
            ",",
            " are",
            " equivalent",
            " to",
            " the",
            " axiom",
            " of",
            " choice",
            ".",
            " The",
            " most",
            " important",
            " among",
            " them",
            " are",
            " Z",
            "orn",
            "'s",
            " lemma",
            " and",
            " the",
            " well",
            "-order",
            "ing",
            " theorem",
            ".",
            " In",
            " fact",
            ",",
            " Z",
            "erm",
            "elo",
            " initially",
            " introduced",
            " the",
            " axiom"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.5,
            0.012,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " ",
            "1",
            ".",
            "93",
            "%",
            " (",
            "M",
            "ars",
            ").",
            "The",
            " predomin",
            "ance",
            " of",
            " radi",
            "ogenic",
            " ",
            " is",
            " the",
            " reason",
            " the",
            " standard",
            " atomic",
            " weight",
            " of",
            " terrestrial",
            " arg",
            "on",
            " is",
            " greater",
            " than",
            " that",
            " of",
            " the",
            " next",
            " element",
            ",",
            " potassium",
            ",",
            " a",
            " fact",
            " that",
            " was",
            " puzz",
            "ling",
            " when",
            " arg",
            "on",
            " was",
            " discovered",
            ".",
            " Mend",
            "ele",
            "ev",
            " positioned",
            " the",
            " elements",
            " on",
            " his",
            " periodic",
            " table",
            " in",
            " order"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " sodium",
            ",",
            " and",
            " potassium",
            " are",
            " the",
            " only",
            " three",
            " metals",
            " in",
            " the",
            " periodic",
            " table",
            " that",
            " are",
            " less",
            " dense",
            " than",
            " water",
            ":",
            " in",
            " fact",
            ",",
            " lithium",
            " is",
            " the",
            " least",
            " dense",
            " known",
            " solid",
            " at",
            " room",
            " temperature",
            ".",
            "Comp",
            "ounds",
            " ",
            "The",
            " alk",
            "ali",
            " metals",
            " form",
            " complete",
            " series",
            " of",
            " compounds",
            " with",
            " all",
            " usually",
            " encountered",
            " an",
            "ions",
            ",",
            " which",
            " well",
            " illustrate",
            " group",
            " trends",
            ".",
            " These",
            " compounds",
            " can"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " this",
            " reason",
            ".",
            "E",
            "instein",
            " argued",
            " that",
            " this",
            " is",
            " true",
            " for",
            " a",
            " fundamental",
            " reason",
            ":",
            " the",
            " gravitational",
            " field",
            " could",
            " be",
            " made",
            " to",
            " vanish",
            " by",
            " a",
            " choice",
            " of",
            " coordinates",
            ".",
            " He",
            " maintained",
            " that",
            " the",
            " non",
            "-c",
            "ov",
            "ariant",
            " energy",
            " momentum",
            " pseud",
            "ot",
            "ensor",
            " was",
            ",",
            " in",
            " fact",
            ",",
            " the",
            " best",
            " description",
            " of",
            " the",
            " energy",
            " momentum",
            " distribution",
            " in",
            " a",
            " gravitational",
            " field",
            ".",
            " While",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " norm",
            ",",
            " and",
            " is",
            " the",
            " -",
            "norm",
            " (",
            "see",
            " L",
            "p",
            " space",
            ")",
            " for",
            " any",
            "Âł",
            ".",
            " In",
            " fact",
            " the",
            " absolute",
            " value",
            " is",
            " the",
            " \"",
            "only",
            "\"",
            " norm",
            " on",
            " ,",
            " in",
            " the",
            " sense",
            " that",
            ",",
            " for",
            " every",
            " norm",
            " ",
            " on",
            " ,",
            " .",
            "The",
            " complex",
            " absolute",
            " value",
            " is",
            " a",
            " special",
            " case",
            " of",
            " the",
            " norm",
            " in",
            " an",
            " inner",
            " product",
            " space",
            ",",
            " which",
            " is",
            " identical",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            " least",
            " once",
            " at",
            " the",
            " City",
            " Dion",
            "ys",
            "ia",
            ",",
            " with",
            " Babylon",
            "ians",
            " in",
            " ",
            "427",
            ",",
            " and",
            " at",
            " least",
            " three",
            " times",
            " at",
            " the",
            " Lena",
            "ia",
            ",",
            " with",
            " The",
            " A",
            "char",
            "n",
            "ians",
            " in",
            " ",
            "425",
            ",",
            " Knights",
            " in",
            " ",
            "424",
            ",",
            " and",
            " Fro",
            "gs",
            " in",
            " ",
            "405",
            ".",
            " Fro",
            "gs",
            " in",
            " fact",
            " won",
            " the",
            " unique",
            " distinction",
            " of",
            " a",
            " repeat",
            " performance",
            " at",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            " exists",
            ",",
            " is",
            " probably",
            " more",
            " complex",
            " and",
            " distant",
            " than",
            " we",
            " can",
            " imagine",
            " on",
            " the",
            " basis",
            " of",
            " our",
            " present",
            " state",
            " of",
            " knowledge",
            "\".",
            "Support",
            "ers",
            " of",
            " the",
            " Alta",
            "ic",
            " hypothesis",
            " formerly",
            " set",
            " the",
            " date",
            " of",
            " the",
            " Proto",
            "-Al",
            "ta",
            "ic",
            " language",
            " at",
            " around",
            " ",
            "400",
            "0",
            " BC",
            ",",
            " but",
            " today",
            " at",
            " around",
            " ",
            "500",
            "0",
            " BC",
            " or",
            " ",
            "600",
            "0",
            " BC",
            ".",
            " This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " her",
            " brother",
            "'s",
            " policies",
            " which",
            " directly",
            " threaten",
            " his",
            " business",
            ".",
            " When",
            " the",
            " government",
            " passes",
            " laws",
            " and",
            " decre",
            "es",
            " which",
            " make",
            " it",
            " impossible",
            " for",
            " him",
            " to",
            " continue",
            ",",
            " he",
            " sets",
            " all",
            " his",
            " oil",
            " wells",
            " on",
            " fire",
            ",",
            " leaving",
            " a",
            " single",
            " note",
            ":",
            " \"",
            "I",
            " am",
            " leaving",
            " it",
            " as",
            " I",
            " found",
            " it",
            ".",
            " Take",
            " over",
            ".",
            " It",
            "'s",
            " yours",
            ".\"",
            " One",
            " particular",
            " burning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " assumes",
            " some",
            " degree",
            " of",
            " violence",
            " will",
            " occur",
            ",",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " as",
            " formulated",
            " by",
            " Roth",
            "bard",
            " and",
            " others",
            " holds",
            " strongly",
            " to",
            " the",
            " central",
            " libertarian",
            " non",
            "ag",
            "gression",
            " axiom",
            ",",
            " sometimes",
            " non",
            "-ag",
            "gression",
            " principle",
            ".",
            " Roth",
            "bard",
            " wrote",
            ":",
            "R",
            "oth",
            "bard",
            "'s",
            " defense",
            " of",
            " the",
            " self",
            "-",
            "ownership",
            " principle",
            " stems",
            " from",
            " what",
            " he",
            " believed",
            " to",
            " be",
            " his",
            " fals"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.494,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " dog",
            " weights",
            " (",
            "meaning",
            " the",
            " group",
            " is",
            " relatively",
            " homogeneous",
            ")",
            " and",
            " (",
            "b",
            ")",
            " the",
            " mean",
            " of",
            " each",
            " group",
            " is",
            " distinct",
            " (",
            "if",
            " two",
            " groups",
            " have",
            " the",
            " same",
            " mean",
            ",",
            " then",
            " it",
            " isn",
            "'t",
            " reasonable",
            " to",
            " conclude",
            " that",
            " the",
            " groups",
            " are",
            ",",
            " in",
            " fact",
            ",",
            " separate",
            " in",
            " any",
            " meaningful",
            " way",
            ").",
            "In",
            " the",
            " illustrations",
            " to",
            " the",
            " right",
            ",",
            " groups",
            " are",
            " identified",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            " is",
            " in",
            " fact",
            " based",
            " on",
            " a",
            " reading",
            " of",
            " the",
            " plays",
            ".",
            " For",
            " example",
            ",",
            " conversation",
            " among",
            " the",
            " guests",
            " turns",
            " to",
            " the",
            " subject",
            " of",
            " Love",
            " and",
            " Arist",
            "oph",
            "anes",
            " explains",
            " his",
            " notion",
            " of",
            " it",
            " in",
            " terms",
            " of",
            " an",
            " amusing",
            " alleg",
            "ory",
            ",",
            " a",
            " device",
            " he",
            " often",
            " uses",
            " in",
            " his",
            " plays",
            ".",
            " He",
            " is",
            " represented",
            " as",
            " suffering",
            " an",
            " attack",
            " of",
            " h",
            "icc",
            "ups",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "anes",
            " is",
            " in",
            " fact",
            " based",
            " on",
            " a",
            " reading",
            " of",
            " the",
            " plays",
            ".",
            " For",
            " example",
            ",",
            " conversation",
            " among",
            " the",
            " guests",
            " turns",
            " to",
            " the",
            " subject",
            " of",
            " Love",
            " and",
            " Arist",
            "oph",
            "anes",
            " explains",
            " his",
            " notion",
            " of",
            " it",
            " in",
            " terms",
            " of",
            " an",
            " amusing",
            " alleg",
            "ory",
            ",",
            " a",
            " device",
            " he",
            " often",
            " uses",
            " in",
            " his",
            " plays",
            ".",
            " He",
            " is",
            " represented",
            " as",
            " suffering",
            " an",
            " attack",
            " of",
            " h",
            "icc",
            "ups",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.49,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            "Business",
            " Art",
            "\"—",
            "he",
            ",",
            " in",
            " fact",
            ",",
            " wrote",
            " about",
            " his",
            " interest",
            " in",
            " thinking",
            " about",
            " art",
            " as",
            " business",
            " in",
            " The",
            " Philosophy",
            " of",
            " Andy",
            " War",
            "hol",
            " from",
            " A",
            " to",
            " B",
            " and",
            " Back",
            " Again",
            ".",
            "Fil",
            "ms",
            "War",
            "hol",
            " appeared",
            " as",
            " himself",
            " in",
            " the",
            " film",
            " Coc",
            "aine",
            " Cowboys",
            " (",
            "197",
            "9",
            ")",
            " and",
            " in",
            " the",
            " film",
            " T",
            "oot",
            "sie",
            " (",
            "198",
            "2",
            ").",
            "After"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "Steve",
            " and",
            " Sim",
            ",",
            " the",
            " major",
            " characters",
            ",",
            " are",
            " twentieth",
            "-century",
            " cousins",
            " of",
            " Huck",
            " Finn",
            " and",
            " Tom",
            " Sawyer",
            ";",
            " Der",
            "le",
            "th",
            "'s",
            " minor",
            " characters",
            ",",
            " little",
            " gems",
            " of",
            " comic",
            " drawing",
            ".\"",
            " The",
            " first",
            " novel",
            " in",
            " the",
            " series",
            ",",
            " The",
            " Moon",
            " T",
            "enders",
            ",",
            " does",
            ",",
            " in",
            " fact",
            ",",
            " involve",
            " a",
            " raft",
            "ing",
            " adventure",
            " down",
            " the",
            " Wisconsin",
            " River",
            ",",
            " which",
            " led",
            " regional"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " refusal",
            " of",
            " that",
            " knowledge",
            " which",
            " is",
            " in",
            " fact",
            " offered",
            " to",
            " man",
            "Âł",
            "...",
            " The",
            " knowledge",
            " of",
            " God",
            " has",
            " always",
            " existed",
            "\".",
            " He",
            " asserted",
            " that",
            " ag",
            "nost",
            "icism",
            " is",
            " a",
            " choice",
            " of",
            " comfort",
            ",",
            " pride",
            ",",
            " domin",
            "ion",
            ",",
            " and",
            " utility",
            " over",
            " truth",
            ",",
            " and",
            " is",
            " opposed",
            " by",
            " the",
            " following",
            " attitudes",
            ":",
            " the",
            " keen",
            "est",
            " self",
            "-c",
            "rit",
            "icism",
            ",",
            " humble",
            " listening",
            " to",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " refusal",
            " of",
            " that",
            " knowledge",
            " which",
            " is",
            " in",
            " fact",
            " offered",
            " to",
            " man",
            "Âł",
            "...",
            " The",
            " knowledge",
            " of",
            " God",
            " has",
            " always",
            " existed",
            "\".",
            " He",
            " asserted",
            " that",
            " ag",
            "nost",
            "icism",
            " is",
            " a",
            " choice",
            " of",
            " comfort",
            ",",
            " pride",
            ",",
            " domin",
            "ion",
            ",",
            " and",
            " utility",
            " over",
            " truth",
            ",",
            " and",
            " is",
            " opposed",
            " by",
            " the",
            " following",
            " attitudes",
            ":",
            " the",
            " keen",
            "est",
            " self",
            "-c",
            "rit",
            "icism",
            ",",
            " humble",
            " listening",
            " to",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " In",
            " fact",
            " none",
            " were",
            " tested",
            ".",
            "In",
            " ",
            "201",
            "5",
            ",",
            " Audi",
            " admitted",
            " that",
            " at",
            " least",
            " ",
            "2",
            ".",
            "1",
            " million",
            " Audi",
            " cars",
            " had",
            " been",
            " involved",
            " in",
            " the",
            " Volkswagen",
            " emissions",
            " testing",
            " scandal",
            " in",
            " which",
            " software",
            " installed",
            " in",
            " the",
            " cars",
            " manipulated",
            " emissions",
            " data",
            " to",
            " fool",
            " regulators",
            " and",
            " allow",
            " the",
            " cars",
            " to",
            " poll",
            "ute",
            " at",
            " higher",
            " than",
            " government",
            "-m",
            "and",
            "ated",
            " levels",
            ".",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " \"",
            "anc",
            "ient",
            " wisdom",
            "\".",
            " Although",
            " the",
            " seeds",
            " of",
            " these",
            " events",
            " were",
            " planted",
            " as",
            " early",
            " as",
            " the",
            " ",
            "17",
            "th",
            " century",
            ",",
            " al",
            "chemy",
            " still",
            " flour",
            "ished",
            " for",
            " some",
            " two",
            " hundred",
            " years",
            ",",
            " and",
            " in",
            " fact",
            " may",
            " have",
            " reached",
            " its",
            " peak",
            " in",
            " the",
            " ",
            "18",
            "th",
            " century",
            ".",
            " As",
            " late",
            " as",
            " ",
            "178",
            "1",
            " James",
            " Price",
            " claimed",
            " to",
            " have",
            " produced",
            " a",
            " powder"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " the",
            " two",
            " lengths",
            " must",
            " not",
            " be",
            " prime",
            " to",
            " one",
            " another",
            ".",
            " Eu",
            "clid",
            " stip",
            "ulated",
            " this",
            " so",
            " that",
            " he",
            " could",
            " construct",
            " a",
            " re",
            "duct",
            "io",
            " ad",
            " absurd",
            "um",
            " proof",
            " that",
            " the",
            " two",
            " numbers",
            "'",
            " common",
            " measure",
            " is",
            " in",
            " fact",
            " the",
            " greatest",
            ".",
            " While",
            " Nic",
            "om",
            "ach",
            "us",
            "'",
            " algorithm",
            " is",
            " the",
            " same",
            " as",
            " Eu",
            "clid",
            "'s",
            ",",
            " when",
            " the",
            " numbers",
            " are",
            " prime"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " was",
            " The",
            " Id",
            "iot",
            ",",
            " an",
            " adaptation",
            " of",
            " the",
            " novel",
            " by",
            " the",
            " director",
            "'s",
            " favorite",
            " writer",
            ",",
            " F",
            "y",
            "odor",
            " Dost",
            "oe",
            "v",
            "sky",
            ".",
            " The",
            " story",
            " is",
            " relocated",
            " from",
            " Russia",
            " to",
            " Hok",
            "k",
            "aid",
            "o",
            ",",
            " but",
            " otherwise",
            " adher",
            "es",
            " closely",
            " to",
            " the",
            " original",
            ",",
            " a",
            " fact",
            " seen",
            " by",
            " many",
            " critics",
            " as",
            " detrimental",
            " to",
            " the",
            " work",
            ".",
            " A",
            " studio",
            "-m",
            "and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "ay",
            "ev",
            "na",
            " Vish",
            "ny",
            "ak",
            "ova",
            " (",
            "n",
            "Ã©e",
            " Dub",
            "as",
            "ova",
            ")",
            " belonged",
            " to",
            " an",
            " old",
            " Dub",
            "as",
            "ov",
            " family",
            " of",
            " Russian",
            " nob",
            "ility",
            " that",
            " traces",
            " its",
            " history",
            " back",
            " to",
            " the",
            " ",
            "17",
            "th",
            " century",
            ";",
            " among",
            " her",
            " relatives",
            " was",
            " Admiral",
            " F",
            "y",
            "odor",
            " Dub",
            "as",
            "ov",
            ",",
            " a",
            " fact",
            " she",
            " had",
            " to",
            " conceal",
            " during",
            " the",
            " Soviet",
            " days",
            ".",
            " She",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " ja",
            "un",
            "ty",
            " (",
            "as",
            " in",
            " many",
            " l",
            "imer",
            "icks",
            ")",
            " and",
            " tro",
            "cha",
            "ic",
            " meter",
            " is",
            " suited",
            " to",
            " rapid",
            " delivery",
            " (",
            "the",
            " word",
            " \"",
            "tro",
            "che",
            "e",
            "\"",
            " is",
            " in",
            " fact",
            " derived",
            " from",
            " tre",
            "chein",
            ",",
            " \"",
            "to",
            " run",
            "\",",
            " as",
            " demonstrated",
            " for",
            " example",
            " by",
            " chor",
            "uses",
            " who",
            " enter",
            " at",
            " speed",
            ",",
            " often",
            " in",
            " aggressive",
            " mood",
            ")",
            " However",
            ",",
            " even",
            " though",
            " both"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " In",
            " fact",
            " none",
            " were",
            " tested",
            ".",
            "In",
            " ",
            "201",
            "5",
            ",",
            " Audi",
            " admitted",
            " that",
            " at",
            " least",
            " ",
            "2",
            ".",
            "1",
            " million",
            " Audi",
            " cars",
            " had",
            " been",
            " involved",
            " in",
            " the",
            " Volkswagen",
            " emissions",
            " testing",
            " scandal",
            " in",
            " which",
            " software",
            " installed",
            " in",
            " the",
            " cars",
            " manipulated",
            " emissions",
            " data",
            " to",
            " fool",
            " regulators",
            " and",
            " allow",
            " the",
            " cars",
            " to",
            " poll",
            "ute",
            " at",
            " higher",
            " than",
            " government",
            "-m",
            "and",
            "ated",
            " levels",
            ".",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " ja",
            "un",
            "ty",
            " (",
            "as",
            " in",
            " many",
            " l",
            "imer",
            "icks",
            ")",
            " and",
            " tro",
            "cha",
            "ic",
            " meter",
            " is",
            " suited",
            " to",
            " rapid",
            " delivery",
            " (",
            "the",
            " word",
            " \"",
            "tro",
            "che",
            "e",
            "\"",
            " is",
            " in",
            " fact",
            " derived",
            " from",
            " tre",
            "chein",
            ",",
            " \"",
            "to",
            " run",
            "\",",
            " as",
            " demonstrated",
            " for",
            " example",
            " by",
            " chor",
            "uses",
            " who",
            " enter",
            " at",
            " speed",
            ",",
            " often",
            " in",
            " aggressive",
            " mood",
            ")",
            " However",
            ",",
            " even",
            " though",
            " both"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " thriller",
            " fiction",
            ".",
            " Of",
            " the",
            " first",
            ",",
            " Giant",
            "'s",
            " Bread",
            " published",
            " in",
            " ",
            "193",
            "0",
            ",",
            " a",
            " reviewer",
            " for",
            " The",
            " New",
            " York",
            " Times",
            " wrote",
            ",",
            " \"...",
            "her",
            " book",
            " is",
            " far",
            " above",
            " the",
            " average",
            " of",
            " current",
            " fiction",
            ",",
            " in",
            " fact",
            ",",
            " comes",
            " well",
            " under",
            " the",
            " classification",
            " of",
            " a",
            " '",
            "good",
            " book",
            "'.",
            " And",
            " it",
            " is",
            " only",
            " a",
            " satisfying",
            " novel",
            " that",
            " can",
            " claim",
            " that",
            " app"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " congen",
            "ers",
            ".",
            " As",
            " such",
            ",",
            " aluminium",
            " is",
            " the",
            " most",
            " elect",
            "rop",
            "os",
            "itive",
            " metal",
            " in",
            " its",
            " group",
            ",",
            " and",
            " its",
            " hydro",
            "x",
            "ide",
            " is",
            " in",
            " fact",
            " more",
            " basic",
            " than",
            " that",
            " of",
            " gall",
            "ium",
            ".",
            " Aluminium",
            " also",
            " bears",
            " minor",
            " similarities",
            " to",
            " the",
            " metal",
            "loid",
            " bor",
            "on",
            " in",
            " the",
            " same",
            " group",
            ":",
            " Al",
            "X",
            "3",
            " compounds",
            " are",
            " val",
            "ence",
            " is",
            "oe",
            "lect",
            "ronic",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.032,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " must",
            " be",
            " able",
            " to",
            " give",
            " a",
            " \"",
            "proof",
            "\"",
            " of",
            " this",
            " fact",
            ",",
            " or",
            " more",
            " properly",
            " speaking",
            ",",
            " a",
            " met",
            "apro",
            "of",
            ".",
            " ",
            " These",
            " examples",
            " are",
            " met",
            "athe",
            "ore",
            "ms",
            " of",
            " our",
            " theory",
            " of",
            " mathematical",
            " logic",
            " since",
            " we",
            " are",
            " dealing",
            " with",
            " the",
            " very",
            " concept",
            " of",
            " proof",
            " itself",
            ".",
            " Aside",
            " from",
            " this",
            ",",
            " we",
            " can",
            " also",
            " have",
            " Exist",
            "ential",
            " General",
            "ization",
            ":",
            "A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " can",
            " say",
            " is",
            " that",
            " conditions",
            " were",
            " bad",
            " and",
            " that",
            " Swift",
            "'s",
            " irony",
            " brilliantly",
            " underscore",
            "d",
            " this",
            " fact",
            "\".",
            "\"People",
            " are",
            " the",
            " riches",
            " of",
            " a",
            " nation",
            "\"",
            "At",
            " the",
            " start",
            " of",
            " a",
            " new",
            " industrial",
            " age",
            " in",
            " the",
            " ",
            "18",
            "th",
            " century",
            ",",
            " it",
            " was",
            " believed",
            " that",
            " \"",
            "people",
            " are",
            " the",
            " riches",
            " of",
            " the",
            " nation",
            "\",",
            " and",
            " there",
            " was",
            " a",
            " general",
            " faith",
            " in",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "agnet",
            "ism",
            ",",
            " Einstein",
            "'s",
            " equation",
            " in",
            " general",
            " rel",
            "ativity",
            ",",
            " Mend",
            "el",
            "'s",
            " laws",
            " of",
            " genetics",
            ",",
            " Darwin",
            "'s",
            " Natural",
            " selection",
            " law",
            ",",
            " etc",
            ".",
            " These",
            " founding",
            " assertions",
            " are",
            " usually",
            " called",
            " principles",
            " or",
            " post",
            "ulates",
            " so",
            " as",
            " to",
            " distinguish",
            " from",
            " mathematical",
            " ax",
            "ioms",
            ".",
            "As",
            " a",
            " matter",
            " of",
            " facts",
            ",",
            " the",
            " role",
            " of",
            " ax",
            "ioms",
            " in",
            " mathematics",
            " and",
            " post",
            "ulates",
            " in",
            " experimental"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Sy",
            "ri",
            "ac",
            " Christianity",
            " (",
            "Church",
            " of",
            " the",
            " East",
            ")",
            " and",
            " Oriental",
            " Orth",
            "odoxy",
            " are",
            " prevalent",
            " minority",
            " denomin",
            "ations",
            ",",
            " which",
            " are",
            " both",
            " Eastern",
            " Christian",
            " sect",
            "s",
            " mainly",
            " adher",
            "ed",
            " to",
            " Assy",
            "rian",
            " people",
            " or",
            " Sy",
            "ri",
            "ac",
            " Christians",
            ".",
            " Vibr",
            "ant",
            " indigenous",
            " minorities",
            " in",
            " West",
            " Asia",
            " are",
            " adher",
            "ing",
            " to",
            " the",
            " Eastern",
            " Catholic",
            " Churches",
            " and",
            " Eastern",
            " Orth",
            "odoxy",
            ".",
            " Saint",
            " Thomas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            "-third",
            " of",
            " all",
            " tax",
            " revenue",
            " to",
            " the",
            " state",
            ",",
            " but",
            " did",
            " not",
            " receive",
            " a",
            " proportional",
            " amount",
            " in",
            " services",
            ".",
            " Urban",
            " interests",
            " were",
            " consistently",
            " under",
            "represented",
            " in",
            " the",
            " legislature",
            ".",
            " A",
            " ",
            "196",
            "0",
            " study",
            " noted",
            " that",
            " because",
            " of",
            " rural",
            " domination",
            ",",
            " \"",
            "a",
            " minority",
            " of",
            " about",
            " ",
            "25",
            "%",
            " of",
            " the",
            " total",
            " state",
            " population",
            " is",
            " in",
            " majority",
            " control",
            " of",
            " the",
            " Alabama",
            " legislature"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " towards",
            " which",
            " the",
            " action",
            " of",
            " the",
            " sea",
            " is",
            " directed",
            "\".",
            "History",
            "Pre",
            "-col",
            "on",
            "ization",
            "Numer",
            "ous",
            " indigenous",
            " peoples",
            " occupied",
            " Alaska",
            " for",
            " thousands",
            " of",
            " years",
            " before",
            " the",
            " arrival",
            " of",
            " European",
            " peoples",
            " to",
            " the",
            " area",
            ".",
            " Lingu",
            "istic",
            " and",
            " DNA",
            " studies",
            " done",
            " here",
            " have",
            " provided",
            " evidence",
            " for",
            " the",
            " settlement",
            " of",
            " North",
            " America",
            " by",
            " way",
            " of",
            " the",
            " B",
            "ering",
            " land",
            " bridge",
            ".",
            " At"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Angola",
            " had",
            " a",
            " community",
            " of",
            " approximately",
            " ",
            "350",
            ",",
            "000",
            " Portuguese",
            ",",
            " but",
            " the",
            " vast",
            " majority",
            " left",
            " after",
            " independence",
            " and",
            " the",
            " ensuing",
            " civil",
            " war",
            ".",
            " However",
            ",",
            " Angola",
            " has",
            " recovered",
            " its",
            " Portuguese",
            " minority",
            " in",
            " recent",
            " years",
            ";",
            " currently",
            ",",
            " there",
            " are",
            " about",
            " ",
            "200",
            ",",
            "000",
            " registered",
            " with",
            " the",
            " cons",
            "ulates",
            ",",
            " and",
            " increasing",
            " due",
            " to",
            " the",
            " debt",
            " crisis",
            " in",
            " Portugal",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.252,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.16,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " airborne",
            " or",
            " ground",
            " vibrations",
            ".",
            " They",
            " have",
            " muscular",
            " tongues",
            ",",
            " which",
            " in",
            " many",
            " species",
            " can",
            " be",
            " protr",
            "uded",
            ".",
            " Modern",
            " amphib",
            "ians",
            " have",
            " fully",
            " oss",
            "ified",
            " vert",
            "ebra",
            "e",
            " with",
            " art",
            "icular",
            " processes",
            ".",
            " Their",
            " ribs",
            " are",
            " usually",
            " short",
            " and",
            " may",
            " be",
            " fused",
            " to",
            " the",
            " vert",
            "ebra",
            "e",
            ".",
            " Their",
            " skulls",
            " are",
            " mostly",
            " broad",
            " and",
            " short",
            ",",
            " and",
            " are",
            " often",
            " incom",
            "pletely",
            " oss"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.212,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " exploit",
            " divisions",
            " between",
            " the",
            " merc",
            "ant",
            "ile",
            " north",
            " and",
            " slave",
            "-",
            "own",
            "ing",
            " south",
            ",",
            " but",
            " after",
            " the",
            " defeat",
            " of",
            " York",
            "town",
            ",",
            " he",
            " was",
            " forced",
            " to",
            " accept",
            " the",
            " fact",
            " that",
            " this",
            " policy",
            " had",
            " failed",
            ".",
            " It",
            " was",
            " clear",
            " the",
            " war",
            " was",
            " lost",
            ",",
            " although",
            " the",
            " Royal",
            " Navy",
            " forced",
            " the",
            " French",
            " to",
            " relocate",
            " their",
            " fleet",
            " to",
            " the",
            " Caribbean",
            " in",
            " November",
            " ",
            "178"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.155,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " fully",
            " oss",
            "ified",
            " and",
            " the",
            " vert",
            "ebra",
            "e",
            " inter",
            "lock",
            " with",
            " each",
            " other",
            " and",
            " have",
            " art",
            "icular",
            " processes",
            ".",
            " Their",
            " ribs",
            " are",
            " usually",
            " short",
            " and",
            " may",
            " be",
            " fused",
            " to",
            " the",
            " vert",
            "ebra",
            "e",
            ".",
            " Their",
            " skulls",
            " are",
            " mostly",
            " broad",
            " and",
            " short",
            ",",
            " and",
            " are",
            " often",
            " incom",
            "pletely",
            " oss",
            "ified",
            ".",
            " Their",
            " skin",
            " contains",
            " little",
            " ker",
            "atin",
            " and",
            " lacks",
            " scales",
            ",",
            " but",
            " contains"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " sword",
            " are",
            " elevated",
            " to",
            " something",
            " special",
            " and",
            " put",
            " into",
            " the",
            " lim",
            "elight",
            ".",
            " T",
            "ark",
            "ovsky",
            " has",
            " also",
            " expressed",
            " interest",
            " in",
            " the",
            " art",
            " of",
            " Ha",
            "iku",
            " and",
            " its",
            " ability",
            " to",
            " create",
            " \"",
            "images",
            " in",
            " such",
            " a",
            " way",
            " that",
            " they",
            " mean",
            " nothing",
            " beyond",
            " themselves",
            "\".",
            "T",
            "ark",
            "ovsky",
            " was",
            " also",
            " a",
            " deeply",
            " religious",
            " Orthodox",
            " Christian",
            ",",
            " who",
            " believed",
            " great",
            " art",
            " should",
            " have",
            " a",
            " higher"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.143,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " a",
            " Doub",
            "t",
            " (",
            "194",
            "3",
            ";",
            " in",
            "ducted",
            " ",
            "199",
            "1",
            "),",
            " Not",
            "orious",
            " (",
            "194",
            "6",
            ";",
            " in",
            "ducted",
            " ",
            "200",
            "6",
            "),",
            " Str",
            "angers",
            " on",
            " a",
            " Train",
            " (",
            "195",
            "1",
            ";",
            " in",
            "ducted",
            " ",
            "202",
            "1",
            "),",
            " Rear",
            " Window",
            " (",
            "195",
            "4",
            ";",
            " in",
            "ducted",
            " ",
            "199",
            "7",
            "),",
            " Vert",
            "igo",
            " (",
            "195",
            "8",
            ";",
            " in",
            "ducted",
            " ",
            "198"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.14,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " nature",
            " of",
            " Christ",
            ",",
            " and",
            " a",
            " form",
            " of",
            " A",
            "rian",
            "ism",
            " seems",
            " to",
            " have",
            " originated",
            " there",
            ".",
            "In",
            " ",
            "362",
            "–",
            "363",
            ",",
            " Emperor",
            " Julian",
            " passed",
            " through",
            " A",
            "ncy",
            "ra",
            " on",
            " his",
            " way",
            " to",
            " an",
            " ill",
            "-f",
            "ated",
            " campaign",
            " against",
            " the",
            " Pers",
            "ians",
            ",",
            " and",
            " according",
            " to",
            " Christian",
            " sources",
            ",",
            " engaged",
            " in",
            " a",
            " persecution",
            " of",
            " various",
            " holy",
            " men",
            ".",
            " The",
            " stone",
            " base",
            " for"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "–",
            "present",
            "190",
            "8",
            " –",
            " The",
            " Qing",
            " dynasty",
            " prom",
            "ulg",
            "ates",
            " the",
            " Q",
            "inding",
            " X",
            "ian",
            "fa",
            " Dag",
            "ang",
            ",",
            " the",
            " first",
            " constitutional",
            " document",
            " in",
            " the",
            " history",
            " of",
            " China",
            ",",
            " transforming",
            " the",
            " Qing",
            " empire",
            " into",
            " a",
            " constitutional",
            " monarchy",
            ".",
            "191",
            "4",
            " –",
            " World",
            " War",
            " I",
            ":",
            " Battle",
            " of",
            " Ãī",
            "tre",
            "ux",
            ":",
            " A",
            " British",
            " re",
            "arg",
            "uard",
            " action",
            " by",
            " the",
            " Royal",
            " Mun"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " essential",
            " for",
            " data",
            " transmission",
            " were",
            " the",
            " start",
            " of",
            " message",
            " (",
            "S",
            "OM",
            "),",
            " end",
            " of",
            " address",
            " (",
            "EO",
            "A",
            "),",
            " end",
            " of",
            " message",
            " (",
            "E",
            "OM",
            "),",
            " end",
            " of",
            " transmission",
            " (",
            "E",
            "OT",
            "),",
            " \"",
            "who",
            " are",
            " you",
            "?\"",
            " (",
            "WR",
            "U",
            "),",
            " \"",
            "are",
            " you",
            "?\"",
            " (",
            "RU",
            "),",
            " a",
            " reserved",
            " device",
            " control",
            " (",
            "DC",
            "0",
            "),",
            " synchronous",
            " idle",
            " (",
            "SYNC",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " quantitative",
            " law",
            ",",
            " that",
            " the",
            " speed",
            ",",
            " v",
            ",",
            " of",
            " a",
            " falling",
            " body",
            " is",
            " proportional",
            " (",
            "say",
            ",",
            " with",
            " constant",
            " c",
            ")",
            " to",
            " its",
            " weight",
            ",",
            " W",
            ",",
            " and",
            " invers",
            "ely",
            " proportional",
            " to",
            " the",
            " density",
            ",",
            " Ï",
            "ģ",
            ",",
            " of",
            " the",
            " fluid",
            " in",
            " which",
            " it",
            " is",
            " falling",
            ":",
            ";",
            " ",
            "A",
            "rist",
            "otle",
            " implies",
            " that",
            " in",
            " a",
            " vacuum",
            " the",
            " speed",
            " of",
            " fall"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "\".",
            " For",
            " this",
            " to",
            " happen",
            ",",
            " the",
            " deceased",
            " had",
            " to",
            " be",
            " judged",
            " worthy",
            " in",
            " a",
            " trial",
            ",",
            " in",
            " which",
            " the",
            " heart",
            " was",
            " weighed",
            " against",
            " a",
            " \"",
            "fe",
            "ather",
            " of",
            " truth",
            ".\"",
            " If",
            " deemed",
            " worthy",
            ",",
            " the",
            " deceased",
            " could",
            " continue",
            " their",
            " existence",
            " on",
            " earth",
            " in",
            " spiritual",
            " form",
            ".",
            " If",
            " they",
            " were",
            " not",
            " deemed",
            " worthy",
            ",",
            " their",
            " heart",
            " was",
            " eaten",
            " by",
            " Am",
            "mit",
            " the",
            " Dev"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Arts",
            " et",
            " Let",
            "tres",
            ",",
            " and",
            " appointments",
            ",",
            " including",
            " Fellow",
            " of",
            " the",
            " American",
            " Association",
            " for",
            " the",
            " Adv",
            "ancement",
            " of",
            " Science",
            " and",
            " the",
            " International",
            " Institute",
            " for",
            " Strategic",
            " Studies",
            ".",
            "In",
            " ",
            "200",
            "6",
            ",",
            " Al",
            "vin",
            " and",
            " Heidi",
            " T",
            "off",
            "ler",
            " were",
            " recipients",
            " of",
            " Brown",
            " University",
            "'s",
            " Independent",
            " Award",
            ".",
            "Personal",
            " life",
            "To",
            "ff",
            "ler",
            " was",
            " married",
            " to",
            " Heidi",
            " T",
            "off",
            "ler",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "/xhtml",
    "onda",
    "ahoma",
    "achuset",
    ".LayoutStyle"
  ],
  "bottom_logits": [
    " Brow",
    " conduct",
    " radical",
    "\"text",
    " aff"
  ],
  "act_min": -0.0,
  "act_max": 0.602
}