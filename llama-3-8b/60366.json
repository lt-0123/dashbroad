{
  "index": 60366,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Private",
            " property",
            " ",
            "An",
            "ar",
            "cho",
            "-capital",
            "ists",
            " post",
            "ulate",
            " the",
            " privat",
            "ization",
            " of",
            " everything",
            ",",
            " including",
            " cities",
            " with",
            " all",
            " their",
            " infra",
            "structures",
            ",",
            " public",
            " spaces",
            ",",
            " streets",
            " and",
            " urban",
            " management",
            " systems",
            ".",
            "Central",
            " to",
            " Roth",
            "bard",
            "ian",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " are",
            " the",
            " concepts",
            " of",
            " self",
            "-",
            "ownership",
            " and",
            " original",
            " appropriation",
            " that",
            " combines",
            " personal",
            " and",
            " private",
            " property",
            ".",
            " Hans",
            "-H",
            "ermann"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.023,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Private",
            " property",
            " ",
            "An",
            "ar",
            "cho",
            "-capital",
            "ists",
            " post",
            "ulate",
            " the",
            " privat",
            "ization",
            " of",
            " everything",
            ",",
            " including",
            " cities",
            " with",
            " all",
            " their",
            " infra",
            "structures",
            ",",
            " public",
            " spaces",
            ",",
            " streets",
            " and",
            " urban",
            " management",
            " systems",
            ".",
            "Central",
            " to",
            " Roth",
            "bard",
            "ian",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " are",
            " the",
            " concepts",
            " of",
            " self",
            "-",
            "ownership",
            " and",
            " original",
            " appropriation",
            " that",
            " combines",
            " personal",
            " and",
            " private",
            " property",
            ".",
            " Hans",
            "-H",
            "ermann"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.637,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Reaction",
            " with",
            " hydrogen",
            "With",
            " hydrogen",
            ",",
            " alk",
            "ali",
            " metals",
            " form",
            " saline",
            " hy",
            "d",
            "rides",
            " that",
            " hydro",
            "ly",
            "se",
            " in",
            " water",
            ".",
            " ",
            "2",
            " Na",
            " \\",
            " +",
            " H",
            "2",
            " \\",
            " ->",
            "[\\",
            "ce",
            "{\\",
            "Delta",
            "}]",
            " \\",
            " ",
            "2",
            " Na",
            "H",
            "2",
            " Na",
            "H",
            " \\",
            " +",
            " \\",
            " ",
            "2",
            " H",
            "2",
            "O",
            " \\",
            " \\",
            "longrightarrow",
            " \\",
            " ",
            "2",
            " Na",
            "OH",
            " \\",
            " +"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Semantic",
            " ambiguity",
            " occurs",
            " when",
            " a",
            " word",
            ",",
            " phrase",
            " or",
            " sentence",
            ",",
            " taken",
            " out",
            " of",
            " context",
            ",",
            " has",
            " more",
            " than",
            " one",
            " interpretation",
            ".",
            " In",
            " \"",
            "We",
            " saw",
            " her",
            " duck",
            "\"",
            " (",
            "example",
            " due",
            " to",
            " Richard",
            " Nord",
            "quist",
            "),",
            " the",
            " words",
            " \"",
            "her",
            " duck",
            "\"",
            " can",
            " refer",
            " either",
            " to",
            " the",
            " person",
            "'s",
            " bird",
            " (",
            "the",
            " noun",
            " \"",
            "duck",
            "\",",
            " modified",
            " by",
            " the",
            " possess",
            "ive"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "External",
            " links",
            " ",
            "  ",
            " ",
            " Albert",
            " Einstein",
            " on",
            " Religion",
            " Sh",
            "ap",
            "ell",
            " Manus",
            "cript",
            " Foundation",
            " Why",
            " I",
            " Am",
            " An",
            " Ag",
            "nostic",
            " by",
            " Robert",
            " G",
            ".",
            " In",
            "gers",
            "oll",
            ",",
            " [",
            "189",
            "6",
            "].",
            " Dictionary",
            " of",
            " the",
            " History",
            " of",
            " Ideas",
            ":",
            " Ag",
            "nost",
            "icism",
            " Ag",
            "nost",
            "icism",
            " from",
            " INTER",
            "S",
            "Âł",
            "–",
            " Inter",
            "disciplinary",
            " Encyclopedia",
            " of",
            " Religion",
            " and",
            " Science",
            " Ag",
            "nost"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "External",
            " links",
            " Curt",
            "ip",
            "ot",
            ":",
            " Acid",
            "–",
            "Base",
            " equ",
            "ilib",
            "ria",
            " diagrams",
            ",",
            " pH",
            " calculation",
            " and",
            " tit",
            "ration",
            " curves",
            " simulation",
            " and",
            " analysis",
            " –",
            " fre",
            "eware",
            " ",
            "Ac",
            "id",
            "–",
            "base",
            " chemistry",
            "<|begin_of_text|>",
            "Bit",
            "umen",
            " (",
            ",",
            " )",
            " is",
            " an",
            " immensely",
            " visc",
            "ous",
            " constituent",
            " of",
            " petroleum",
            ".",
            " Depending",
            " on",
            " its",
            " exact",
            " composition",
            " it",
            " can",
            " be",
            " a",
            " sticky",
            ",",
            " black",
            " liquid",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "External",
            " links",
            " Curt",
            "ip",
            "ot",
            ":",
            " Acid",
            "–",
            "Base",
            " equ",
            "ilib",
            "ria",
            " diagrams",
            ",",
            " pH",
            " calculation",
            " and",
            " tit",
            "ration",
            " curves",
            " simulation",
            " and",
            " analysis",
            " –",
            " fre",
            "eware",
            " ",
            "Ac",
            "id",
            "–",
            "base",
            " chemistry",
            "<|begin_of_text|>",
            "Bit",
            "umen",
            " (",
            ",",
            " )",
            " is",
            " an",
            " immensely",
            " visc",
            "ous",
            " constituent",
            " of",
            " petroleum",
            ".",
            " Depending",
            " on",
            " its",
            " exact",
            " composition",
            " it",
            " can",
            " be",
            " a",
            " sticky",
            ",",
            " black",
            " liquid",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "External",
            " links",
            " ",
            "  ",
            " ",
            " Albert",
            " Einstein",
            " on",
            " Religion",
            " Sh",
            "ap",
            "ell",
            " Manus",
            "cript",
            " Foundation",
            " Why",
            " I",
            " Am",
            " An",
            " Ag",
            "nostic",
            " by",
            " Robert",
            " G",
            ".",
            " In",
            "gers",
            "oll",
            ",",
            " [",
            "189",
            "6",
            "].",
            " Dictionary",
            " of",
            " the",
            " History",
            " of",
            " Ideas",
            ":",
            " Ag",
            "nost",
            "icism",
            " Ag",
            "nost",
            "icism",
            " from",
            " INTER",
            "S",
            "Âł",
            "–",
            " Inter",
            "disciplinary",
            " Encyclopedia",
            " of",
            " Religion",
            " and",
            " Science",
            " Ag",
            "nost"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "More",
            " recently",
            ",",
            " several",
            " North",
            " American",
            " hospitals",
            " have",
            " opted",
            " for",
            " less",
            "-in",
            "vasive",
            " loop",
            " drainage",
            " over",
            " standard",
            " drainage",
            " and",
            " wound",
            " packing",
            ".",
            " In",
            " one",
            " study",
            " of",
            " ",
            "143",
            " pediatric",
            " outcomes",
            ",",
            " a",
            " failure",
            " rate",
            " of",
            " ",
            "1",
            ".",
            "4",
            "%",
            " was",
            " reported",
            " in",
            " the",
            " loop",
            " group",
            " versus",
            " ",
            "10",
            ".",
            "5",
            "%",
            " in",
            " the",
            " packing",
            " group",
            " (",
            "P",
            "<",
            ".",
            "030",
            "),",
            " while"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.606,
            0.219,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Description",
            "Members",
            " of",
            " the",
            " Aster",
            "aceae",
            " are",
            " mostly",
            " herb",
            "aceous",
            " plants",
            ",",
            " but",
            " some",
            " shr",
            "ubs",
            ",",
            " vines",
            ",",
            " and",
            " trees",
            " (",
            "such",
            " as",
            " L",
            "ach",
            "an",
            "odes",
            " arb",
            "orea",
            ")",
            " do",
            " exist",
            ".",
            " Aster",
            "aceae",
            " species",
            " are",
            " generally",
            " easy",
            " to",
            " distinguish",
            " from",
            " other",
            " plants",
            " because",
            " of",
            " their",
            " unique",
            " inf",
            "lo",
            "res",
            "cence",
            " and",
            " other",
            " shared",
            " characteristics",
            ",",
            " such",
            " as",
            " the",
            " joined"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Many",
            " local",
            " offices",
            " (",
            "county",
            " commissioners",
            ",",
            " boards",
            " of",
            " education",
            ",",
            " tax",
            " assess",
            "ors",
            ",",
            " may",
            "ors",
            ",",
            " etc",
            ".)",
            " in",
            " the",
            " state",
            " are",
            " still",
            " held",
            " by",
            " Democrats",
            ".",
            " Many",
            " metropolitan",
            " and",
            " suburban",
            " counties",
            " have",
            " voters",
            " who",
            " are",
            " majority",
            " Democrats",
            ",",
            " resulting",
            " in",
            " local",
            " elections",
            " being",
            " decided",
            " in",
            " the",
            " Democratic",
            " primary",
            ".",
            " Similarly",
            " most",
            " rural",
            " counties",
            " are",
            " majority",
            "-",
            "Republican",
            " and",
            " elections",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.06,
            -0.0,
            -0.0,
            -0.0,
            0.003,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Many",
            " illnesses",
            " can",
            " be",
            " cured",
            " by",
            " the",
            " one",
            " medicine",
            " of",
            " love",
            " and",
            " compassion",
            ".",
            " These",
            " qualities",
            " are",
            " the",
            " ultimate",
            " source",
            " of",
            " human",
            " happiness",
            ",",
            " and",
            " the",
            " need",
            " for",
            " them",
            " lies",
            " at",
            " the",
            " very",
            " core",
            " of",
            " our",
            " being",
            "\"",
            " (",
            "Dal",
            "ai",
            " Lama",
            ").",
            "The",
            " notion",
            " of",
            " altru",
            "ism",
            " is",
            " modified",
            " in",
            " such",
            " a",
            " world",
            "-view",
            ",",
            " since",
            " the",
            " belief",
            " is",
            " that",
            " such",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.598,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Rotor",
            "craft",
            " ",
            "Rotor",
            "craft",
            ",",
            " or",
            " rotary",
            "-wing",
            " aircraft",
            ",",
            " use",
            " a",
            " spinning",
            " rotor",
            " with",
            " a",
            "ero",
            "foil",
            " cross",
            "-section",
            " blades",
            " (",
            "a",
            " rotary",
            " wing",
            ")",
            " to",
            " provide",
            " lift",
            ".",
            " Types",
            " include",
            " helicopters",
            ",",
            " aut",
            "ogy",
            "ros",
            ",",
            " and",
            " various",
            " hybrids",
            " such",
            " as",
            " gy",
            "rod",
            "ynes",
            " and",
            " compound",
            " rotor",
            "craft",
            ".",
            "Hel",
            "icopt",
            "ers",
            " have",
            " a",
            " rotor",
            " turned",
            " by",
            " an",
            " engine",
            "-driven"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "From",
            " the",
            " late",
            " ",
            "196",
            "0",
            "s",
            " onwards",
            " many",
            " buildings",
            " in",
            " Amsterdam",
            " have",
            " been",
            " squ",
            "atted",
            " both",
            " for",
            " housing",
            " and",
            " for",
            " using",
            " as",
            " social",
            " centres",
            ".",
            " A",
            " number",
            " of",
            " these",
            " squ",
            "ats",
            " have",
            " legal",
            "ised",
            " and",
            " become",
            " well",
            " known",
            ",",
            " such",
            " as",
            " OCC",
            "II",
            ",",
            " OT",
            "301",
            ",",
            " Parad",
            "iso",
            " and",
            " V",
            "rank",
            "rijk",
            ".",
            "S",
            "ister",
            " cities",
            " ",
            " Manchester",
            ",",
            " Greater"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.087,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "From",
            " the",
            " ",
            "10",
            "th",
            " to",
            " late",
            " ",
            "7",
            "th",
            " centuries",
            " BCE",
            ",",
            " much",
            " of",
            " Anat",
            "olia",
            " (",
            "particularly",
            " the",
            " southeastern",
            " regions",
            ")",
            " fell",
            " to",
            " the",
            " Neo",
            "-Ass",
            "y",
            "rian",
            " Empire",
            ",",
            " including",
            " all",
            " of",
            " the",
            " Sy",
            "ro",
            "-H",
            "itt",
            "ite",
            " states",
            ",",
            " Tab",
            "al",
            ",",
            " Comm",
            "ag",
            "ene",
            ",",
            " the",
            " C",
            "immer",
            "ians",
            " and",
            " Sc",
            "yth",
            "ians",
            ",",
            " and",
            " swath",
            "es",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Jets",
            "-P",
            "atri",
            "ots",
            " rivalry",
            "A",
            "FC",
            " North",
            "B",
            "eng",
            "als",
            "-R",
            "av",
            "ens",
            " rivalry",
            "B",
            "eng",
            "als",
            "-B",
            "row",
            "ns",
            " rivalry",
            "B",
            "eng",
            "als",
            "-",
            "Steel",
            "ers",
            " rivalry",
            "B",
            "row",
            "ns",
            "-R",
            "av",
            "ens",
            " rivalry",
            "B",
            "row",
            "ns",
            "-",
            "Steel",
            "ers",
            " rivalry",
            "R",
            "av",
            "ens",
            "-",
            "Steel",
            "ers",
            " rivalry",
            "A",
            "FC",
            " South",
            " R",
            "ivals"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Other",
            " uses",
            " List",
            " of",
            " tropical",
            " storms",
            " named",
            " Ada",
            " Ada",
            " (",
            "food",
            "),",
            " a",
            " traditional",
            " Kerala",
            " delic",
            "acy",
            " Ada",
            ",",
            " the",
            " cryptocurrency",
            " of",
            " the",
            " Card",
            "ano",
            " blockchain",
            " platform",
            " Ada",
            " Bridge",
            ",",
            " Bel",
            "grade",
            ",",
            " Serbia",
            " ,",
            " a",
            " cargo",
            " vessel",
            " built",
            " for",
            " the",
            " London",
            " and",
            " South",
            " Western",
            " Railway",
            " Ada",
            " (",
            "ship",
            "),",
            " a",
            " wooden",
            " k",
            "etch",
            ",",
            " wreck",
            "ed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Other",
            " places",
            " Atlas",
            " Cinema",
            ",",
            " a",
            " historic",
            " movie",
            " theatre",
            " in",
            " Istanbul",
            ",",
            " Turkey",
            " Atlas",
            " Mountains",
            ",",
            " a",
            " set",
            " of",
            " mountain",
            " ranges",
            " in",
            " north",
            "western",
            " Africa",
            " Atlas",
            ",",
            " Nil",
            "Ã¼",
            "fer",
            ",",
            " a",
            " village",
            " in",
            " B",
            "urs",
            "a",
            " Province",
            ",",
            " Turkey",
            "People",
            " with",
            " the",
            " name",
            " Atlas",
            " (",
            "given",
            " name",
            ")",
            " Atlas",
            " (",
            "gr",
            "aff",
            "iti",
            " artist",
            ")",
            " ",
            " Atlas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Other",
            " GMO",
            " crops",
            " used",
            " by",
            " growers",
            " include",
            " insect",
            "-resistant",
            " crops",
            ",",
            " which",
            " have",
            " a",
            " gene",
            " from",
            " the",
            " soil",
            " bacter",
            "ium",
            " Bac",
            "illus",
            " th",
            "uring",
            "iens",
            "is",
            " (",
            "B",
            "t",
            "),",
            " which",
            " produces",
            " a",
            " toxin",
            " specific",
            " to",
            " insects",
            ".",
            " These",
            " crops",
            " resist",
            " damage",
            " by",
            " insects",
            ".",
            " Some",
            " believe",
            " that",
            " similar",
            " or",
            " better",
            " pest",
            "-res",
            "istance",
            " traits",
            " can",
            " be",
            " acquired",
            " through",
            " traditional",
            " breeding",
            " practices",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.262,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Other",
            " places",
            " Atlas",
            " Cinema",
            ",",
            " a",
            " historic",
            " movie",
            " theatre",
            " in",
            " Istanbul",
            ",",
            " Turkey",
            " Atlas",
            " Mountains",
            ",",
            " a",
            " set",
            " of",
            " mountain",
            " ranges",
            " in",
            " north",
            "western",
            " Africa",
            " Atlas",
            ",",
            " Nil",
            "Ã¼",
            "fer",
            ",",
            " a",
            " village",
            " in",
            " B",
            "urs",
            "a",
            " Province",
            ",",
            " Turkey",
            "People",
            " with",
            " the",
            " name",
            " Atlas",
            " (",
            "given",
            " name",
            ")",
            " Atlas",
            " (",
            "gr",
            "aff",
            "iti",
            " artist",
            ")",
            " ",
            " Atlas"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Entre",
            "preneur",
            "ial",
            " opportunities",
            " were",
            " common",
            " for",
            " the",
            " al",
            "chem",
            "ists",
            " of",
            " Renaissance",
            " Europe",
            ".",
            " Al",
            "chem",
            "ists",
            " were",
            " contracted",
            " by",
            " the",
            " elite",
            " for",
            " practical",
            " purposes",
            " related",
            " to",
            " mining",
            ",",
            " medical",
            " services",
            ",",
            " and",
            " the",
            " production",
            " of",
            " chemicals",
            ",",
            " medicines",
            ",",
            " metals",
            ",",
            " and",
            " gem",
            "stones",
            ".",
            " Rud",
            "olf",
            " II",
            ",",
            " Holy",
            " Roman",
            " Emperor",
            ",",
            " in",
            " the",
            " late",
            " ",
            "16",
            "th",
            " century",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Meteor",
            " showers",
            " ",
            "A",
            "ries",
            " is",
            " home",
            " to",
            " several",
            " meteor",
            " showers",
            ".",
            " The",
            " Day",
            "time",
            " Ari",
            "et",
            "id",
            " meteor",
            " shower",
            " is",
            " one",
            " of",
            " the",
            " strongest",
            " meteor",
            " showers",
            " that",
            " occurs",
            " during",
            " the",
            " day",
            ",",
            " lasting",
            " from",
            " ",
            "22",
            " May",
            " to",
            " ",
            "2",
            " July",
            ".",
            " It",
            " is",
            " an",
            " annual",
            " shower",
            " associated",
            " with",
            " the",
            " Mars",
            "den",
            " group",
            " of",
            " com",
            "ets",
            " that",
            " peaks",
            " on",
            " ",
            "7",
            " June"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.551,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Public",
            " reaction",
            " ",
            "Swift",
            "'s",
            " essay",
            " created",
            " a",
            " backlash",
            " within",
            " the",
            " community",
            " after",
            " its",
            " publication",
            ".",
            " The",
            " work",
            " was",
            " aimed",
            " at",
            " the",
            " arist",
            "ocracy",
            ",",
            " and",
            " they",
            " responded",
            " in",
            " turn",
            ".",
            " Several",
            " members",
            " of",
            " society",
            " wrote",
            " to",
            " Swift",
            " regarding",
            " the",
            " work",
            ".",
            " Lord",
            " Bath",
            "urst",
            "'s",
            " letter",
            " (",
            "12",
            " February",
            " ",
            "172",
            "9",
            "–",
            "30",
            ")",
            " int",
            "imated",
            " that",
            " he",
            " certainly",
            " understood",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Online",
            " sources",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            "SIM",
            "BAD",
            "External",
            " links",
            " ",
            " The",
            " Deep",
            " Phot",
            "ographic",
            " Guide",
            " to",
            " the",
            " Const",
            "ell",
            "ations",
            ":",
            " A",
            "ries",
            " The",
            " clickable",
            " A",
            "ries",
            " Star",
            " Tales",
            " –",
            " A",
            "ries",
            " War",
            "burg",
            " Institute",
            " Icon",
            "ographic",
            " Database",
            " (",
            "med",
            "ieval",
            " and",
            " early",
            " modern",
            " images",
            " of",
            " A",
            "ries",
            ")",
            " ",
            "Const",
            "ell",
            "ations",
            "Const"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Today",
            "'s",
            " Japanese",
            " ab",
            "acus",
            " is",
            " a",
            " ",
            "1",
            ":",
            "4",
            " type",
            ",",
            " four",
            "-be",
            "ad",
            " ab",
            "acus",
            ",",
            " introduced",
            " from",
            " China",
            " in",
            " the",
            " Mu",
            "rom",
            "achi",
            " era",
            ".",
            " It",
            " adopts",
            " the",
            " form",
            " of",
            " the",
            " upper",
            " deck",
            " one",
            " bead",
            " and",
            " the",
            " bottom",
            " four",
            " beads",
            ".",
            " The",
            " top",
            " bead",
            " on",
            " the",
            " upper",
            " deck",
            " was",
            " equal",
            " to",
            " five",
            " and",
            " the",
            " bottom",
            " one",
            " is",
            " similar",
            " to"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.547,
            -0.0,
            0.03,
            -0.0,
            -0.0,
            -0.0,
            0.015,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Functional",
            " analysis",
            "The",
            " H",
            "ahn",
            "–",
            "Ban",
            "ach",
            " theorem",
            " in",
            " functional",
            " analysis",
            ",",
            " allowing",
            " the",
            " extension",
            " of",
            " linear",
            " function",
            "als",
            "The",
            " theorem",
            " that",
            " every",
            " Hil",
            "bert",
            " space",
            " has",
            " an",
            " or",
            "thon",
            "ormal",
            " basis",
            ".",
            "The",
            " Ban",
            "ach",
            "–",
            "Al",
            "a",
            "og",
            "lu",
            " theorem",
            " about",
            " compact",
            "ness",
            " of",
            " sets",
            " of",
            " function",
            "als",
            ".",
            "The",
            " B",
            "aire",
            " category",
            " theorem",
            " about",
            " complete",
            " metric",
            " spaces"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.543,
            0.116,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "History",
            "Anc",
            "ient",
            " Near",
            " Eastern",
            " alph",
            "ab",
            "ets",
            "The",
            " Ancient",
            " Egyptian",
            " writing",
            " system",
            " had",
            " a",
            " set",
            " of",
            " some",
            " ",
            "24",
            " hier",
            "og",
            "lyph",
            "s",
            " that",
            " are",
            " called",
            " un",
            "il",
            "iterals",
            ",",
            " which",
            " are",
            " glyphs",
            " that",
            " provide",
            " one",
            " sound",
            ".",
            " These",
            " glyphs",
            " were",
            " used",
            " as",
            " pronunciation",
            " guides",
            " for",
            " log",
            "ograms",
            ",",
            " to",
            " write",
            " gramm",
            "atical",
            " inf",
            "lections",
            ",",
            " and",
            ",",
            " later",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.543,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            0.249,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.297,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.117,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.121,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "History",
            " of",
            " the",
            " Mediterranean",
            "<|begin_of_text|>",
            "An",
            "alog",
            " Brothers",
            " were",
            " an",
            " experimental",
            " hip",
            " hop",
            " band",
            " featuring",
            " Tracy",
            " \"",
            "Ice",
            "-T",
            "\"",
            " Mar",
            "row",
            " (",
            "Ice",
            " Osc",
            "illator",
            ")",
            " on",
            " keyboards",
            ",",
            " drums",
            " and",
            " vocals",
            ",",
            " Keith",
            " \"",
            "K",
            "ool",
            " Keith",
            "\"",
            " Thornton",
            " (",
            "Keith",
            " K",
            "org",
            ")",
            " on",
            " bass",
            ",",
            " strings",
            " and",
            " vocals",
            ",",
            " Marc",
            " Live",
            " (",
            "Marc",
            " Mo",
            "og",
            ")",
            " on",
            " drums",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.543,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Critical",
            " assessment",
            " ",
            "Accent",
            "ure",
            ",",
            " the",
            " management",
            " consultancy",
            " firm",
            ",",
            " identified",
            " T",
            "off",
            "ler",
            " in",
            " ",
            "200",
            "2",
            " as",
            " being",
            " among",
            " the",
            " most",
            " influential",
            " voices",
            " in",
            " business",
            " leaders",
            ",",
            " along",
            " with",
            " Bill",
            " Gates",
            " and",
            " Peter",
            " Dr",
            "ucker",
            ".",
            " T",
            "off",
            "ler",
            " has",
            " also",
            " been",
            " described",
            " in",
            " a",
            " Financial",
            " Times",
            " interview",
            " as",
            " the",
            " \"",
            "world",
            "'s",
            " most",
            " famous",
            " futuro",
            "log",
            "ist",
            "\".",
            " In"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Relationship",
            " with",
            " his",
            " parents",
            "G",
            "ins",
            "berg",
            " referred",
            " to",
            " his",
            " parents",
            " in",
            " a",
            " ",
            "198",
            "5",
            " interview",
            " as",
            " \"",
            "old",
            "-fashioned",
            " delic",
            "at",
            "essen",
            " philosophers",
            "\".",
            " His",
            " mother",
            " was",
            " also",
            " an",
            " active",
            " member",
            " of",
            " the",
            " Communist",
            " Party",
            " and",
            " took",
            " Gins",
            "berg",
            " and",
            " his",
            " brother",
            " Eugene",
            " to",
            " party",
            " meetings",
            ".",
            " Gins",
            "berg",
            " later",
            " said",
            " that",
            " his",
            " mother",
            " \"",
            "made",
            " up",
            " bedtime",
            " stories",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.005
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Students",
            " at",
            " colleges",
            " and",
            " universities",
            " can",
            " also",
            " take",
            " part",
            " in",
            " single",
            "-se",
            "ater",
            " racing",
            " through",
            " the",
            " Formula",
            " S",
            "AE",
            " competition",
            ",",
            " which",
            " involves",
            " designing",
            " and",
            " building",
            " a",
            " single",
            "-se",
            "ater",
            " car",
            " in",
            " a",
            " multid",
            "isc",
            "iplinary",
            " team",
            " and",
            " racing",
            " it",
            " at",
            " the",
            " competition",
            ".",
            " This",
            " also",
            " develops",
            " other",
            " soft",
            " skills",
            ",",
            " such",
            " as",
            " teamwork",
            ",",
            " while",
            " promoting",
            " motors",
            "port",
            " and",
            " engineering",
            ".",
            "The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.539,
            0.005,
            -0.0,
            -0.0,
            0.146,
            -0.0,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            0.142,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.136,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.13,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.126,
            -0.0,
            0.132,
            -0.0,
            -0.0,
            0.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Business",
            "people",
            " from",
            " Boston",
            "Canadian",
            " activists",
            "Canadian",
            " ag",
            "nost",
            "ics",
            "Canadian",
            " Aviation",
            " Hall",
            " of",
            " Fame",
            " in",
            "duct",
            "ees",
            "Canadian",
            " educational",
            " theorists",
            "Canadian",
            " em",
            "igrants",
            " to",
            " the",
            " United",
            " States",
            "Canadian",
            " e",
            "ug",
            "enic",
            "ists",
            "Canadian",
            " physicists",
            "Canadian",
            " Unit",
            "arians",
            "Deaths",
            " from",
            " diabetes",
            "F",
            "ell",
            "ows",
            " of",
            " the",
            " American",
            " Academy",
            " of",
            " Arts",
            " and",
            " Sciences"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.095,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Business",
            " Art",
            "\"—",
            "he",
            ",",
            " in",
            " fact",
            ",",
            " wrote",
            " about",
            " his",
            " interest",
            " in",
            " thinking",
            " about",
            " art",
            " as",
            " business",
            " in",
            " The",
            " Philosophy",
            " of",
            " Andy",
            " War",
            "hol",
            " from",
            " A",
            " to",
            " B",
            " and",
            " Back",
            " Again",
            ".",
            "Fil",
            "ms",
            "War",
            "hol",
            " appeared",
            " as",
            " himself",
            " in",
            " the",
            " film",
            " Coc",
            "aine",
            " Cowboys",
            " (",
            "197",
            "9",
            ")",
            " and",
            " in",
            " the",
            " film",
            " T",
            "oot",
            "sie",
            " (",
            "198",
            "2",
            ").",
            "After"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Sid",
            "well",
            " ",
            "201",
            "5",
            "b",
            ")",
            " suggests",
            " that",
            " Aust",
            "ro",
            "asi",
            "atic",
            " branches",
            " may",
            " have",
            " a",
            " loosely",
            " nested",
            " structure",
            " rather",
            " than",
            " a",
            " completely",
            " rake",
            "-like",
            " structure",
            ",",
            " with",
            " an",
            " east",
            "–",
            "west",
            " division",
            " (",
            "cons",
            "isting",
            " of",
            " M",
            "unda",
            ",",
            " K",
            "has",
            "ic",
            ",",
            " P",
            "ala",
            "ung",
            "ic",
            ",",
            " and",
            " Kh",
            "mu",
            "ic",
            " forming",
            " a",
            " western",
            " group",
            " as",
            " opposed",
            " to",
            " all",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Each",
            " particle",
            " of",
            " matter",
            " has",
            " a",
            " corresponding",
            " antim",
            "atter",
            " particle",
            " with",
            " the",
            " opposite",
            " electrical",
            " charge",
            ".",
            " Thus",
            ",",
            " the",
            " posit",
            "ron",
            " is",
            " a",
            " positively",
            " charged",
            " ant",
            "ie",
            "lectron",
            " and",
            " the",
            " ant",
            "ipro",
            "ton",
            " is",
            " a",
            " negatively",
            " charged",
            " equivalent",
            " of",
            " a",
            " proton",
            ".",
            " When",
            " a",
            " matter",
            " and",
            " corresponding",
            " antim",
            "atter",
            " particle",
            " meet",
            ",",
            " they",
            " annihil",
            "ate",
            " each",
            " other",
            ".",
            " Because",
            " of",
            " this",
            ",",
            " along"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.5,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "All",
            " of",
            " the",
            " discovered",
            " alk",
            "ali",
            " metals",
            " occur",
            " in",
            " nature",
            " as",
            " their",
            " compounds",
            ":",
            " in",
            " order",
            " of",
            " abundance",
            ",",
            " sodium",
            " is",
            " the",
            " most",
            " abundant",
            ",",
            " followed",
            " by",
            " potassium",
            ",",
            " lithium",
            ",",
            " rub",
            "id",
            "ium",
            ",",
            " ca",
            "esium",
            ",",
            " and",
            " finally",
            " franc",
            "ium",
            ",",
            " which",
            " is",
            " very",
            " rare",
            " due",
            " to",
            " its",
            " extremely",
            " high",
            " radio",
            "activity",
            ";",
            " franc",
            "ium",
            " occurs",
            " only",
            " in",
            " minute",
            " traces",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "The",
            " Ghost",
            "s",
            " of",
            " Sau",
            "k",
            " County",
            "\".",
            " All",
            " Hall",
            "ows",
            " ",
            "18",
            " (",
            "199",
            "8",
            ");",
            " in",
            " Howard",
            "'s",
            " Touch",
            "stones",
            ":",
            " Essays",
            " on",
            " the",
            " Fantastic",
            ".",
            " Stafford",
            "shire",
            " UK",
            ":",
            " Al",
            "chemy",
            " Press",
            ",",
            " ",
            "201",
            "4",
            ".",
            " David",
            " E",
            ".",
            " Schultz",
            " and",
            " S",
            ".T",
            ".",
            " Jos",
            "hi",
            " (",
            "eds",
            ").",
            " Ecc",
            "entric",
            ",",
            " Im",
            "pr",
            "actical",
            " Devils",
            ":",
            " The",
            " Letters"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.072,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "The",
            " Dis",
            "appearance",
            " of",
            " Mr",
            ".",
            " D",
            "aven",
            "heim",
            "\")",
            " Martin",
            " G",
            "abel",
            ",",
            " General",
            " Electric",
            " Theater",
            " (",
            "4",
            "/",
            "1",
            "/",
            "196",
            "2",
            ";",
            " adaptation",
            " of",
            " \"",
            "The",
            " Dis",
            "appearance",
            " of",
            " Mr",
            ".",
            " D",
            "aven",
            "heim",
            "\")",
            " Hor",
            "st",
            " B",
            "oll",
            "mann",
            ",",
            " Black",
            " Coffee",
            " ",
            "197",
            "3",
            " Ian",
            " Hol",
            "m",
            ",",
            " Murder",
            " by",
            " the",
            " Book",
            ",",
            " ",
            "198",
            "6"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "The",
            " clad",
            "ogram",
            " below",
            " shows",
            " the",
            " main",
            " cl",
            "ades",
            " of",
            " living",
            " act",
            "in",
            "opt",
            "ery",
            "g",
            "ians",
            " and",
            " their",
            " evolutionary",
            " relationships",
            " to",
            " other",
            " ext",
            "ant",
            " groups",
            " of",
            " fishes",
            " and",
            " the",
            " four",
            "-l",
            "im",
            "bed",
            " verte",
            "brates",
            " (",
            "tet",
            "rap",
            "ods",
            ").",
            " The",
            " latter",
            " include",
            " mostly",
            " terrestrial",
            " species",
            " but",
            " also",
            " groups",
            " that",
            " became",
            " second",
            "arily",
            " aquatic",
            " (",
            "e",
            ".g",
            ".",
            " Wh",
            "ales",
            " and",
            " Dolphins"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.396,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " gallon",
            " of",
            " gas",
            " in",
            " urban",
            " Alaska",
            " today",
            " is",
            " usually",
            " thirty",
            " to",
            " sixty",
            " cents",
            " higher",
            " than",
            " the",
            " national",
            " average",
            ";",
            " prices",
            " in",
            " rural",
            " areas",
            " are",
            " generally",
            " significantly",
            " higher",
            " but",
            " vary",
            " widely",
            " depending",
            " on",
            " transportation",
            " costs",
            ",",
            " seasonal",
            " usage",
            " peaks",
            ",",
            " nearby",
            " petroleum",
            " development",
            " infrastructure",
            " and",
            " many",
            " other",
            " factors",
            ".",
            "Permanent",
            " Fund",
            "The",
            " Alaska",
            " Permanent",
            " Fund",
            " is",
            " a",
            " constitution",
            "ally",
            " authorized",
            " appropriation",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.027,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.33,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.064,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.318,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "ary",
            " Islands",
            "Mor",
            "occo",
            "South",
            " Africa",
            "North",
            " America",
            "External",
            " links",
            " ",
            " –",
            " a",
            " database",
            " of",
            " all",
            " al",
            "gal",
            " names",
            " including",
            " images",
            ",",
            " n",
            "omencl",
            "ature",
            ",",
            " taxonomy",
            ",",
            " distribution",
            ",",
            " bibliography",
            ",",
            " uses",
            ",",
            " extracts",
            " ",
            " ",
            " ",
            " ",
            "End",
            "os",
            "ymb",
            "iotic",
            " events",
            "Poly",
            "ph",
            "yle",
            "tic",
            " groups",
            "Common",
            " names",
            " of",
            " organisms",
            "<|begin_of_text|>",
            "Analysis",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.077,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.318,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.106
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " AydÄ±n",
            ",",
            " who",
            " have",
            " ",
            "49",
            " and",
            " ",
            "30",
            " beaches",
            " awarded",
            " respectively",
            ".",
            "See",
            " also",
            " Exclusive",
            " economic",
            " zone",
            " of",
            " Greece",
            " Geography",
            " of",
            " Turkey",
            " List",
            " of",
            " Greek",
            " place",
            " names",
            " Ae",
            "ge",
            "an",
            " Boat",
            " Report",
            "References",
            "External",
            " links",
            " ",
            " ",
            "Se",
            "as",
            " of",
            " Greece",
            "Se",
            "as",
            " of",
            " Turkey",
            "Marg",
            "inal",
            " seas",
            " of",
            " the",
            " Mediterranean",
            "European"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.043,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.293,
            -0.0,
            0.034,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Kosovo",
            "'s",
            " sovereignty",
            " in",
            " ",
            "200",
            "8",
            ".",
            " Furthermore",
            ",",
            " both",
            " governments",
            " hold",
            " annual",
            " joint",
            " meetings",
            ",",
            " displayed",
            " by",
            " the",
            " inaugural",
            " meeting",
            " in",
            " ",
            "201",
            "4",
            ",",
            " which",
            " serves",
            " as",
            " an",
            " official",
            " platform",
            " to",
            " enhance",
            " bilateral",
            " cooperation",
            " and",
            " reinforce",
            " their",
            " joint",
            " commitment",
            " to",
            " policies",
            " that",
            " promote",
            " the",
            " stability",
            " and",
            " prosperity",
            " of",
            " the",
            " broader",
            " Alban",
            "ian",
            " region",
            ".",
            "Military",
            " ",
            "The",
            " Alban",
            "ian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " very",
            " significant",
            " moment",
            " in",
            " the",
            " history",
            " of",
            " England",
            "—or",
            " at",
            " least",
            " in",
            " the",
            " history",
            " of",
            " English",
            " Poetry",
            ".\"",
            "Soon",
            " after",
            " the",
            " book",
            "shop",
            " reading",
            ",",
            " plans",
            " were",
            " h",
            "atched",
            " for",
            " the",
            " International",
            " Poetry",
            " Inc",
            "arnation",
            ",",
            " which",
            " was",
            " held",
            " at",
            " the",
            " Royal",
            " Albert",
            " Hall",
            " in",
            " London",
            " on",
            " June",
            " ",
            "11",
            ",",
            " ",
            "196",
            "5",
            ".",
            " The",
            " event",
            " attracted",
            " an",
            " audience",
            " of",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " resonance",
            " imaging",
            ",",
            " computed",
            " tom",
            "ography",
            ",",
            " and",
            " ultrasound",
            " imaging",
            " have",
            " all",
            " enabled",
            " examination",
            " of",
            " internal",
            " structures",
            " in",
            " unprecedented",
            " detail",
            " to",
            " a",
            " degree",
            " far",
            " beyond",
            " the",
            " imagination",
            " of",
            " earlier",
            " generations",
            ".",
            "See",
            " also",
            " ",
            " An",
            "atom",
            "ical",
            " model",
            " ",
            " Outline",
            " of",
            " human",
            " anatomy",
            " Pl",
            "ast",
            "ination",
            "References",
            "External",
            " links",
            " ",
            " ",
            " Anatomy",
            ",",
            " In",
            " Our",
            " Time",
            ".",
            " BBC",
            " Radio",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.193,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.035,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            0.154
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " My",
            "Anime",
            "List",
            " to",
            " discuss",
            " anime",
            ",",
            " manga",
            " and",
            " track",
            " their",
            " progress",
            " watching",
            " respective",
            " series",
            " as",
            " well",
            " as",
            " using",
            " news",
            " outlets",
            " such",
            " as",
            " Anime",
            " News",
            " Network",
            ".",
            "Due",
            " to",
            " anime",
            "'s",
            " increased",
            " popularity",
            " in",
            " recent",
            " years",
            ",",
            " a",
            " large",
            " number",
            " of",
            " celebrities",
            " such",
            " as",
            " Elon",
            " Musk",
            ",",
            " BTS",
            " and",
            " Ari",
            "ana",
            " Grande",
            " have",
            " come",
            " out",
            " as",
            " anime",
            " fans",
            ".",
            "Anime",
            " style",
            " ",
            "One"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.223,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "seconds",
            ",",
            " allowing",
            " the",
            " first",
            " ignition",
            " of",
            " the",
            " S",
            "-",
            "IV",
            "B",
            " engine",
            " a",
            " few",
            " seconds",
            " later",
            ".",
            "Apollo",
            " ",
            "11",
            " entered",
            " a",
            " near",
            "-c",
            "ircular",
            " Earth",
            " orbit",
            " at",
            " an",
            " altitude",
            " of",
            " ",
            " by",
            " ,",
            " twelve",
            " minutes",
            " into",
            " its",
            " flight",
            ".",
            " After",
            " one",
            " and",
            " a",
            " half",
            " orbits",
            ",",
            " a",
            " second",
            " ignition",
            " of",
            " the",
            " S",
            "-",
            "IV",
            "B",
            " engine",
            " pushed",
            " the",
            " spacecraft",
            " onto",
            " its"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.106,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.18,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.241,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " Buddh",
            "ists",
            " from",
            " the",
            " United",
            " States",
            "Trans",
            "atl",
            "antic",
            " Records",
            " artists",
            "United",
            " States",
            " Merchant",
            " Mariners",
            " of",
            " World",
            " War",
            " II",
            "W",
            "riters",
            " from",
            " Boulder",
            ",",
            " Colorado",
            "W",
            "riters",
            " from",
            " Newark",
            ",",
            " New",
            " Jersey",
            "W",
            "riters",
            " from",
            " Pat",
            "erson",
            ",",
            " New",
            " Jersey",
            "Y",
            "ipp",
            "ies",
            "National",
            " Arts",
            " Club",
            " Medal",
            " of",
            " Honor",
            " Rec",
            "ipients",
            "<|begin_of_text|>",
            "In",
            " mathematics",
            ",",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.183,
            0.232,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " saw",
            " both",
            " cars",
            " retire",
            " from",
            " the",
            " race",
            ",",
            " marking",
            " the",
            " first",
            " time",
            " that",
            " an",
            " Audi",
            " car",
            " has",
            " failed",
            " to",
            " score",
            " a",
            " podium",
            " in",
            " a",
            " World",
            " End",
            "urance",
            " Championship",
            " race",
            ".",
            "Results",
            "Formula",
            " E",
            "A",
            "udi",
            " provide",
            " factory",
            " support",
            " to",
            " Ab",
            "t",
            " Sports",
            "line",
            " in",
            " the",
            " F",
            "IA",
            " Formula",
            " E",
            " Championship",
            ",",
            " The",
            " team",
            " competed",
            " under",
            " the",
            " title",
            " of",
            " Audi",
            " Sport",
            " Ab"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.155,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.134,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " used",
            " alone",
            " as",
            " an",
            " agent",
            " for",
            " radiation",
            " therapy",
            ",",
            " in",
            " particular",
            " targeted",
            " alpha",
            " therapy",
            " (",
            "T",
            "AT",
            ").",
            " This",
            " is",
            "otope",
            " has",
            " a",
            " half",
            "-life",
            " of",
            " ",
            "10",
            " days",
            ",",
            " making",
            " it",
            " much",
            " more",
            " suitable",
            " for",
            " radiation",
            " therapy",
            " than",
            " ",
            "213",
            "Bi",
            " (",
            "half",
            "-life",
            " ",
            "46",
            " minutes",
            ").",
            " Additionally",
            ",",
            " ",
            "225",
            "Ac",
            " dec",
            "ays",
            " to",
            " n",
            "ont",
            "oxic",
            " ",
            "209"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            "itz",
            ",",
            " Simon",
            " V",
            "ink",
            "eno",
            "og",
            ",",
            " Spike",
            " Hawkins",
            " and",
            " Tom",
            " McGr",
            "ath",
            ".",
            " The",
            " event",
            " was",
            " organized",
            " by",
            " Gins",
            "berg",
            "'s",
            " friend",
            ",",
            " the",
            " filmmaker",
            " Barbara",
            " Rubin",
            ".",
            "Peter",
            " White",
            "head",
            " documented",
            " the",
            " event",
            " on",
            " film",
            " and",
            " released",
            " it",
            " as",
            " Wh",
            "olly",
            " Comm",
            "union",
            ".",
            " A",
            " book",
            " featuring",
            " images",
            " from",
            " the",
            " film",
            " and",
            " some",
            " of",
            " the",
            " poems",
            " that",
            " were",
            " performed",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.109,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " room",
            ".",
            " On",
            " the",
            " day",
            " the",
            " city",
            " gates",
            " are",
            " reopened",
            ",",
            " he",
            " shoots",
            " at",
            " random",
            " at",
            " people",
            " on",
            " the",
            " street",
            ",",
            " w",
            "ounding",
            " some",
            " and",
            " killing",
            " a",
            " dog",
            ".",
            " The",
            " police",
            " arrest",
            " him",
            ".",
            "Father",
            " Panel",
            "oux",
            ":",
            " Father",
            " Panel",
            "oux",
            " is",
            " a",
            " learned",
            ",",
            " well",
            "-res",
            "pected",
            " Jes",
            "uit",
            " priest",
            ".",
            " He",
            " is",
            " well",
            " known",
            " for",
            " having",
            " given",
            " a",
            " series",
            " of",
            " lectures"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.078,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.02,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " de",
            " Gr",
            "asse",
            " remained",
            " in",
            " Ches",
            "apeake",
            " Bay",
            " and",
            " intercepted",
            " him",
            " on",
            " September",
            " ",
            "5",
            ";",
            " although",
            " the",
            " Battle",
            " of",
            " the",
            " Ches",
            "apeake",
            " was",
            " inde",
            "cis",
            "ive",
            " in",
            " terms",
            " of",
            " losses",
            ",",
            " Graves",
            " was",
            " forced",
            " to",
            " retreat",
            ",",
            " leaving",
            " Cornwall",
            "is",
            " isolated",
            ".",
            " An",
            " attempted",
            " breakout",
            " over",
            " York",
            " River",
            " at",
            " Glouce",
            "ster",
            " Point",
            " failed",
            " due",
            " to",
            " bad",
            " weather",
            ".",
            " Under",
            " heavy",
            " bombard"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.043,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " its",
            " recurrence",
            " and",
            " the",
            " need",
            " for",
            " repeated",
            " surgery",
            ".",
            " There",
            " is",
            " no",
            " evidence",
            " that",
            " fec",
            "al",
            " in",
            "contin",
            "ence",
            " is",
            " a",
            " consequence",
            " of",
            " this",
            " surgery",
            " for",
            " abs",
            "cess",
            " drainage",
            ".",
            "Per",
            "ian",
            "al",
            " abs",
            "cess",
            "es",
            " can",
            " be",
            " seen",
            " in",
            " people",
            " with",
            ",",
            " for",
            " example",
            ",",
            " inflammatory",
            " bowel",
            " disease",
            " (",
            "such",
            " as",
            " Cro",
            "hn",
            "'s",
            " disease",
            ")",
            " or",
            " diabetes",
            ".",
            " ",
            " Often"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "7",
            ",",
            " which",
            " Tr",
            "uff",
            "aut",
            " nicknamed",
            " the",
            " \"",
            "H",
            "itch",
            "book",
            "\".",
            " The",
            " audio",
            " tapes",
            " were",
            " used",
            " as",
            " the",
            " basis",
            " of",
            " a",
            " documentary",
            " in",
            " ",
            "201",
            "5",
            ".",
            " Tr",
            "uff",
            "aut",
            " sought",
            " the",
            " interview",
            " because",
            " it",
            " was",
            " clear",
            " to",
            " him",
            " that",
            " Hitch",
            "cock",
            " was",
            " not",
            " simply",
            " the",
            " mass",
            "-market",
            " entert",
            "ainer",
            " the",
            " American",
            " media",
            " made",
            " him",
            " out",
            " to",
            " be",
            ".",
            " It",
            " was"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "achuset",
    "avatar",
    "ÅĻÃŃz",
    "emiah",
    "ToProps"
  ],
  "bottom_logits": [
    " Mystic",
    "hold",
    "band",
    "mere",
    " Perc"
  ],
  "act_min": -0.0,
  "act_max": 0.644
}