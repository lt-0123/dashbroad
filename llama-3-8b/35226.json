{
  "index": 35226,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            "N",
            "arc",
            "iss",
            "ales",
            ").",
            " Of",
            " these",
            ",",
            " L",
            "ili",
            "aceae",
            " was",
            " divided",
            " into",
            " eleven",
            " tribes",
            " (",
            "with",
            " ",
            "133",
            " genera",
            ")",
            " and",
            " A",
            "mary",
            "ll",
            "id",
            "aceae",
            " into",
            " four",
            " tribes",
            " (",
            "with",
            " ",
            "68",
            " genera",
            "),",
            " yet",
            " both",
            " contained",
            " many",
            " genera",
            " that",
            " would",
            " eventually",
            " segreg",
            "ate",
            " to",
            " each",
            " other",
            "'s",
            " contemporary",
            " orders",
            " (",
            "L",
            "ilia",
            "les",
            " and",
            " As",
            "par",
            "ag",
            "ales",
            " respectively"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.727,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " a",
            " very",
            " large",
            " order",
            " containing",
            " almost",
            " all",
            " monoc",
            "ots",
            " with",
            " colorful",
            " tep",
            "als",
            " and",
            " lacking",
            " starch",
            " in",
            " their",
            " end",
            "os",
            "perm",
            ".",
            " DNA",
            " sequence",
            " analysis",
            " indicated",
            " that",
            " many",
            " of",
            " the",
            " taxa",
            " previously",
            " included",
            " in",
            " L",
            "ilia",
            "les",
            " should",
            " actually",
            " be",
            " redistributed",
            " over",
            " three",
            " orders",
            ",",
            " L",
            "ilia",
            "les",
            ",",
            " As",
            "par",
            "ag",
            "ales",
            ",",
            " and",
            " Dios",
            "core",
            "ales",
            ".",
            " The",
            " boundaries",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.727,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " a",
            " very",
            " large",
            " order",
            " containing",
            " almost",
            " all",
            " monoc",
            "ots",
            " with",
            " colorful",
            " tep",
            "als",
            " and",
            " lacking",
            " starch",
            " in",
            " their",
            " end",
            "os",
            "perm",
            ".",
            " DNA",
            " sequence",
            " analysis",
            " indicated",
            " that",
            " many",
            " of",
            " the",
            " taxa",
            " previously",
            " included",
            " in",
            " L",
            "ilia",
            "les",
            " should",
            " actually",
            " be",
            " redistributed",
            " over",
            " three",
            " orders",
            ",",
            " L",
            "ilia",
            "les",
            ",",
            " As",
            "par",
            "ag",
            "ales",
            ",",
            " and",
            " Dios",
            "core",
            "ales",
            ".",
            " The",
            " boundaries",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " cells",
            " after",
            " each",
            " division",
            ".",
            " In",
            " simultaneous",
            " micro",
            "spor",
            "ogenesis",
            ",",
            " there",
            " is",
            " no",
            " wall",
            " formation",
            " until",
            " all",
            " four",
            " cell",
            " nuclei",
            " are",
            " present",
            ".",
            " L",
            "ilia",
            "les",
            " all",
            " have",
            " successive",
            " micro",
            "spor",
            "ogenesis",
            ",",
            " which",
            " is",
            " thought",
            " to",
            " be",
            " the",
            " primitive",
            " condition",
            " in",
            " monoc",
            "ots",
            ".",
            " It",
            " seems",
            " that",
            " when",
            " the",
            " As",
            "par",
            "ag",
            "ales",
            " first",
            " diver",
            "ged",
            " they",
            " developed",
            " simultaneous",
            " micro",
            "spor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " data",
            ".",
            " This",
            " complicated",
            " the",
            " discussion",
            " about",
            " plant",
            " evolution",
            " and",
            " necess",
            "itated",
            " a",
            " major",
            " restructuring",
            ".",
            " r",
            "bc",
            "L",
            " gene",
            " sequencing",
            " and",
            " clad",
            "istic",
            " analysis",
            " of",
            " monoc",
            "ots",
            " had",
            " re",
            "defined",
            " the",
            " L",
            "ilia",
            "les",
            " in",
            " ",
            "199",
            "5",
            ".",
            " from",
            " four",
            " morph",
            "ological",
            " orders",
            " sens",
            "u",
            " Dah",
            "lg",
            "ren",
            ".",
            " The",
            " largest",
            " cl",
            "ade",
            " representing",
            " the",
            " L",
            "ili",
            "aceae",
            ",",
            " all",
            " previously"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            "yt",
            "om",
            "elan",
            " is",
            " not",
            " unique",
            " to",
            " As",
            "par",
            "ag",
            "ales",
            " (",
            "i",
            ".e",
            ".",
            " it",
            " is",
            " not",
            " a",
            " syn",
            "ap",
            "omor",
            "phy",
            ")",
            " but",
            " it",
            " is",
            " common",
            " within",
            " the",
            " order",
            " and",
            " rare",
            " outside",
            " it",
            ".",
            " The",
            " inner",
            " portion",
            " of",
            " the",
            " seed",
            " coat",
            " is",
            " usually",
            " completely",
            " collapsed",
            ".",
            " In",
            " contrast",
            ",",
            " the",
            " morph",
            "ologically",
            " similar",
            " seeds",
            " of",
            " L",
            "ilia",
            "les",
            " have",
            " no",
            " ph"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.065,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " Lil",
            "ian",
            "ae",
            " sens",
            "u",
            " Chase",
            " &",
            " Re",
            "veal",
            " (",
            "mon",
            "oc",
            "ots",
            ")",
            " based",
            " on",
            " molecular",
            " phy",
            "logen",
            "etic",
            " evidence",
            ".",
            " The",
            " l",
            "ili",
            "oid",
            " monoc",
            "ot",
            " orders",
            " are",
            " bracket",
            "ed",
            ",",
            " namely",
            " Pet",
            "ros",
            "av",
            "iales",
            ",",
            " Dios",
            "core",
            "ales",
            ",",
            " Pand",
            "ana",
            "les",
            ",",
            " L",
            "ilia",
            "les",
            " and",
            " As",
            "par",
            "ag",
            "ales",
            ".",
            " These",
            " constitute",
            " a",
            " paraph",
            "yle",
            "tic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "yled",
            "ons",
            " with",
            " colourful",
            " tep",
            "als",
            " and",
            " without",
            " starch",
            " in",
            " their",
            " end",
            "os",
            "perm",
            " (",
            "the",
            " l",
            "ili",
            "oid",
            " monoc",
            "ots",
            ").",
            " The",
            " L",
            "ilia",
            "les",
            " was",
            " difficult",
            " to",
            " divide",
            " into",
            " families",
            " because",
            " morph",
            "ological",
            " characters",
            " were",
            " not",
            " present",
            " in",
            " patterns",
            " that",
            " clearly",
            " dem",
            "arc",
            "ated",
            " groups",
            ".",
            " This",
            " kept",
            " the",
            " L",
            "ili",
            "aceae",
            " separate",
            " from",
            " the",
            " A",
            "mary",
            "ll",
            "id",
            "aceae",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "yt",
            "om",
            "elan",
            "in",
            " in",
            " species",
            " with",
            " dry",
            " fruits",
            " (",
            "nuts",
            ").",
            " The",
            " inner",
            " part",
            " of",
            " the",
            " seed",
            " coat",
            " is",
            " generally",
            " collapsed",
            ",",
            " in",
            " contrast",
            " to",
            " L",
            "ilia",
            "les",
            " whose",
            " seeds",
            " have",
            " a",
            " well",
            " developed",
            " outer",
            " ep",
            "ider",
            "mis",
            ",",
            " lack",
            " ph",
            "yt",
            "om",
            "elan",
            "in",
            ",",
            " and",
            " usually",
            " display",
            " a",
            " cellular",
            " inner",
            " layer",
            ".",
            "The",
            " orders",
            " which",
            " have",
            " been",
            " separated",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " old",
            " L",
            "ilia",
            "les",
            " are",
            " difficult",
            " to",
            " characterize",
            ".",
            " No",
            " single",
            " morph",
            "ological",
            " character",
            " appears",
            " to",
            " be",
            " diagnostic",
            " of",
            " the",
            " order",
            " As",
            "par",
            "ag",
            "ales",
            ".",
            " The",
            " flowers",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            " are",
            " of",
            " a",
            " general",
            " type",
            " among",
            " the",
            " l",
            "ili",
            "oid",
            " monoc",
            "ots",
            ".",
            " Compared",
            " to",
            " L",
            "ilia",
            "les",
            ",",
            " they",
            " usually",
            " have",
            " plain",
            " tep",
            "als",
            " without",
            " markings",
            " in",
            " the",
            " form"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " described",
            " both",
            " the",
            " L",
            "ili",
            "aceae",
            " and",
            " the",
            " type",
            " family",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            ",",
            " the",
            " As",
            "par",
            "ag",
            "aceae",
            ",",
            " as",
            " L",
            "ilia",
            " and",
            " As",
            "par",
            "agi",
            ",",
            " respectively",
            ",",
            " in",
            " ",
            "178",
            "9",
            ".",
            " J",
            "uss",
            "ieu",
            " established",
            " the",
            " hierarchical",
            " system",
            " of",
            " taxonomy",
            " (",
            "phy",
            "log",
            "eny",
            "),",
            " placing",
            " As",
            "paragus",
            " and",
            " related",
            " genera",
            " within",
            " a",
            " division",
            " of",
            " Mon",
            "oc"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " old",
            " L",
            "ilia",
            "les",
            " are",
            " difficult",
            " to",
            " characterize",
            ".",
            " No",
            " single",
            " morph",
            "ological",
            " character",
            " appears",
            " to",
            " be",
            " diagnostic",
            " of",
            " the",
            " order",
            " As",
            "par",
            "ag",
            "ales",
            ".",
            " The",
            " flowers",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            " are",
            " of",
            " a",
            " general",
            " type",
            " among",
            " the",
            " l",
            "ili",
            "oid",
            " monoc",
            "ots",
            ".",
            " Compared",
            " to",
            " L",
            "ilia",
            "les",
            ",",
            " they",
            " usually",
            " have",
            " plain",
            " tep",
            "als",
            " without",
            " markings",
            " in",
            " the",
            " form"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " They",
            " are",
            " frequently",
            " clustered",
            " at",
            " the",
            " end",
            " of",
            " the",
            " plant",
            " stem",
            ".",
            "The",
            " As",
            "par",
            "ag",
            "ales",
            " are",
            " generally",
            " distinguished",
            " from",
            " the",
            " L",
            "ilia",
            "les",
            " by",
            " the",
            " lack",
            " of",
            " markings",
            " on",
            " the",
            " tep",
            "als",
            ",",
            " the",
            " presence",
            " of",
            " sept",
            "al",
            " n",
            "ect",
            "aries",
            " in",
            " the",
            " ovar",
            "ies",
            ",",
            " rather",
            " than",
            " the",
            " bases",
            " of",
            " the",
            " tep",
            "als",
            " or",
            " st",
            "amen",
            " fil",
            "aments",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ili",
            "aceae",
            " within",
            " the",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " saw",
            " it",
            " as",
            " a",
            " paraph",
            "yle",
            "tic",
            " (\"",
            "catch",
            "-all",
            "\")",
            " family",
            ",",
            " being",
            " all",
            " L",
            "ilia",
            "les",
            " not",
            " included",
            " in",
            " the",
            " other",
            " orders",
            ",",
            " but",
            " hoped",
            " that",
            " the",
            " future",
            " would",
            " reveal",
            " some",
            " characteristic",
            " that",
            " would",
            " group",
            " them",
            " better",
            ".",
            " The",
            " order",
            " L",
            "ilia",
            "les",
            " was",
            " very",
            " large",
            " and",
            " included",
            " almost",
            " all",
            " monoc",
            "ot"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ili",
            "aceae",
            " within",
            " the",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " saw",
            " it",
            " as",
            " a",
            " paraph",
            "yle",
            "tic",
            " (\"",
            "catch",
            "-all",
            "\")",
            " family",
            ",",
            " being",
            " all",
            " L",
            "ilia",
            "les",
            " not",
            " included",
            " in",
            " the",
            " other",
            " orders",
            ",",
            " but",
            " hoped",
            " that",
            " the",
            " future",
            " would",
            " reveal",
            " some",
            " characteristic",
            " that",
            " would",
            " group",
            " them",
            " better",
            ".",
            " The",
            " order",
            " L",
            "ilia",
            "les",
            " was",
            " very",
            " large",
            " and",
            " included",
            " almost",
            " all",
            " monoc",
            "ot"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.094,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            "id",
            "aceae",
            ",",
            " the",
            " Ag",
            "ave",
            "ae",
            " would",
            " be",
            " part",
            " of",
            " As",
            "par",
            "ag",
            "aceae",
            " but",
            " the",
            " Al",
            "stro",
            "emer",
            "iae",
            " would",
            " become",
            " a",
            " family",
            " within",
            " the",
            " L",
            "ilia",
            "les",
            ".",
            "The",
            " number",
            " of",
            " known",
            " genera",
            " (",
            "and",
            " species",
            ")",
            " continued",
            " to",
            " grow",
            " and",
            " by",
            " the",
            " time",
            " of",
            " the",
            " next",
            " major",
            " British",
            " classification",
            ",",
            " that",
            " of",
            " the",
            " Bent",
            "ham",
            " &",
            " Hook",
            "er",
            " system"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ili",
            "aceae",
            " within",
            " the",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " saw",
            " it",
            " as",
            " a",
            " paraph",
            "yle",
            "tic",
            " (\"",
            "catch",
            "-all",
            "\")",
            " family",
            ",",
            " being",
            " all",
            " L",
            "ilia",
            "les",
            " not",
            " included",
            " in",
            " the",
            " other",
            " orders",
            ",",
            " but",
            " hoped",
            " that",
            " the",
            " future",
            " would",
            " reveal",
            " some",
            " characteristic",
            " that",
            " would",
            " group",
            " them",
            " better",
            ".",
            " The",
            " order",
            " L",
            "ilia",
            "les",
            " was",
            " very",
            " large",
            " and",
            " included",
            " almost",
            " all",
            " monoc",
            "ot"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " included",
            " in",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " including",
            " both",
            " the",
            " Cal",
            "och",
            "ort",
            "aceae",
            " and",
            " L",
            "ili",
            "aceae",
            " sens",
            "u",
            " Tam",
            "ura",
            ".",
            " This",
            " re",
            "defined",
            " family",
            ",",
            " that",
            " became",
            " referred",
            " to",
            " as",
            " core",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " correspond",
            "ed",
            " to",
            " the",
            " emerging",
            " circ",
            "ums",
            "cription",
            " of",
            " the",
            " Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Group",
            " (",
            "199",
            "8",
            ").",
            "Ph",
            "y",
            "log"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " included",
            " in",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " including",
            " both",
            " the",
            " Cal",
            "och",
            "ort",
            "aceae",
            " and",
            " L",
            "ili",
            "aceae",
            " sens",
            "u",
            " Tam",
            "ura",
            ".",
            " This",
            " re",
            "defined",
            " family",
            ",",
            " that",
            " became",
            " referred",
            " to",
            " as",
            " core",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " correspond",
            "ed",
            " to",
            " the",
            " emerging",
            " circ",
            "ums",
            "cription",
            " of",
            " the",
            " Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Group",
            " (",
            "199",
            "8",
            ").",
            "Ph",
            "y",
            "log"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " included",
            " in",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " including",
            " both",
            " the",
            " Cal",
            "och",
            "ort",
            "aceae",
            " and",
            " L",
            "ili",
            "aceae",
            " sens",
            "u",
            " Tam",
            "ura",
            ".",
            " This",
            " re",
            "defined",
            " family",
            ",",
            " that",
            " became",
            " referred",
            " to",
            " as",
            " core",
            " L",
            "ilia",
            "les",
            ",",
            " but",
            " correspond",
            "ed",
            " to",
            " the",
            " emerging",
            " circ",
            "ums",
            "cription",
            " of",
            " the",
            " Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Group",
            " (",
            "199",
            "8",
            ").",
            "Ph",
            "y",
            "log"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " classification",
            " systems",
            ".",
            " It",
            " was",
            " first",
            " put",
            " forward",
            " by",
            " Hub",
            "er",
            " in",
            " ",
            "197",
            "7",
            " and",
            " later",
            " taken",
            " up",
            " in",
            " the",
            " Dah",
            "lg",
            "ren",
            " system",
            " of",
            " ",
            "198",
            "5",
            " and",
            " then",
            " the",
            " AP",
            "G",
            " in",
            " ",
            "199",
            "8",
            ",",
            " ",
            "200",
            "3",
            " and",
            " ",
            "200",
            "9",
            ".",
            " Before",
            " this",
            ",",
            " many",
            " of",
            " its",
            " families",
            " were",
            " assigned",
            " to",
            " the",
            " old",
            " order",
            " L",
            "ilia",
            "les"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " group",
            ",",
            " but",
            " had",
            " also",
            " included",
            " groups",
            " now",
            " located",
            " in",
            " L",
            "ilia",
            "les",
            ",",
            " Pand",
            "ana",
            "les",
            " and",
            " Z",
            "ing",
            "ib",
            "era",
            "les",
            ".",
            " Research",
            " in",
            " the",
            " ",
            "21",
            "st",
            " century",
            " has",
            " supported",
            " the",
            " mon",
            "ophy",
            "ly",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            ",",
            " based",
            " on",
            " morphology",
            ",",
            " ",
            "18",
            "S",
            " r",
            "DNA",
            ",",
            " and",
            " other",
            " DNA",
            " sequences",
            ",",
            " although",
            " some",
            " phy",
            "logen",
            "etic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " four",
            " groups",
            " including",
            " the",
            " ",
            " '",
            "As",
            "par",
            "ag",
            "oid",
            "'",
            " L",
            "ili",
            "if",
            "lor",
            "ae",
            ".",
            "The",
            " widely",
            " used",
            " Cron",
            "quist",
            " system",
            " (",
            "196",
            "8",
            "",
            "198",
            "8",
            ")",
            " used",
            " the",
            " very",
            " broadly",
            " defined",
            " order",
            " L",
            "ilia",
            "les",
            ".",
            "These",
            " various",
            " proposals",
            " to",
            " separate",
            " small",
            " groups",
            " of",
            " genera",
            " into",
            " more",
            " homogeneous",
            " families",
            " made",
            " little",
            " impact",
            " till",
            " that",
            " of",
            " Dah",
            "lg",
            "ren",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            "ishes",
            " some",
            " members",
            " of",
            " As",
            "par",
            "ag",
            "ales",
            " from",
            " L",
            "ilia",
            "les",
            ".",
            " Micro",
            "spor",
            "ogenesis",
            " involves",
            " a",
            " cell",
            " dividing",
            " twice",
            " (",
            "me",
            "iot",
            "ically",
            ")",
            " to",
            " form",
            " four",
            " daughter",
            " cells",
            ".",
            " There",
            " are",
            " two",
            " kinds",
            " of",
            " micro",
            "spor",
            "ogenesis",
            ":",
            " successive",
            " and",
            " simultaneous",
            " (",
            "although",
            " intermedi",
            "ates",
            " exist",
            ").",
            " In",
            " successive",
            " micro",
            "spor",
            "ogenesis",
            ",",
            " walls",
            " are",
            " laid",
            " down",
            " separating",
            " the",
            " daughter"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.215,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            "ae",
            " were",
            " a",
            " sub",
            "order",
            " which",
            " included",
            " both",
            " families",
            " L",
            "ili",
            "aceae",
            " and",
            " A",
            "mary",
            "ll",
            "id",
            "aceae",
            ".",
            " The",
            " L",
            "ili",
            "aceae",
            " had",
            " eight",
            " sub",
            "f",
            "amilies",
            " and",
            " the",
            " A",
            "mary",
            "ll",
            "id",
            "aceae",
            " four",
            ".",
            " In",
            " this",
            " rearr",
            "angement",
            " of",
            " L",
            "ili",
            "aceae",
            ",",
            " with",
            " fewer",
            " subdivisions",
            ",",
            " the",
            " core",
            " L",
            "ilia",
            "les",
            " were",
            " represented",
            " as",
            " sub",
            "family",
            " L",
            "ilio",
            "ide"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " called",
            " dia",
            "o",
            " who",
            " had",
            " both",
            " secular",
            " and",
            " religious",
            " authority",
            " in",
            " modern",
            " terms",
            ".",
            " He",
            " was",
            " endowed",
            " with",
            " powers",
            " that",
            " could",
            " influence",
            " nature",
            ":",
            " a",
            " sh",
            "aman",
            ".",
            " The",
            " dia",
            "o",
            " position",
            " was",
            " her",
            "editary",
            ".",
            " By",
            " being",
            " allowed",
            " to",
            " marry",
            " multiple",
            " wives",
            ",",
            " the",
            " dia",
            "o",
            " was",
            " able",
            " to",
            " establish",
            " and",
            " maintain",
            " political",
            " alliances",
            " with",
            " other",
            " groups",
            ",",
            " tribes",
            ",",
            " or",
            " villages"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " shown",
            " on",
            " the",
            " right",
            ":",
            "The",
            " hydro",
            "b",
            "oration",
            "-",
            "oxid",
            "ation",
            " and",
            " oxy",
            "merc",
            "uration",
            "-re",
            "duction",
            " of",
            " al",
            "ken",
            "es",
            " are",
            " more",
            " reliable",
            " in",
            " organic",
            " synthesis",
            ".",
            " Al",
            "ken",
            "es",
            " react",
            " with",
            " N",
            "-b",
            "rom",
            "os",
            "ucc",
            "in",
            "im",
            "ide",
            " and",
            " water",
            " in",
            " hal",
            "oh",
            "y",
            "dr",
            "in",
            " formation",
            " reaction",
            ".",
            " A",
            "min",
            "es",
            " can",
            " be",
            " converted",
            " to",
            " dia",
            "zon",
            "ium"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " called",
            " dia",
            "o",
            " who",
            " had",
            " both",
            " secular",
            " and",
            " religious",
            " authority",
            " in",
            " modern",
            " terms",
            ".",
            " He",
            " was",
            " endowed",
            " with",
            " powers",
            " that",
            " could",
            " influence",
            " nature",
            ":",
            " a",
            " sh",
            "aman",
            ".",
            " The",
            " dia",
            "o",
            " position",
            " was",
            " her",
            "editary",
            ".",
            " By",
            " being",
            " allowed",
            " to",
            " marry",
            " multiple",
            " wives",
            ",",
            " the",
            " dia",
            "o",
            " was",
            " able",
            " to",
            " establish",
            " and",
            " maintain",
            " political",
            " alliances",
            " with",
            " other",
            " groups",
            ",",
            " tribes",
            ",",
            " or",
            " villages"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.668,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.652,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " called",
            " dia",
            "o",
            " who",
            " had",
            " both",
            " secular",
            " and",
            " religious",
            " authority",
            " in",
            " modern",
            " terms",
            ".",
            " He",
            " was",
            " endowed",
            " with",
            " powers",
            " that",
            " could",
            " influence",
            " nature",
            ":",
            " a",
            " sh",
            "aman",
            ".",
            " The",
            " dia",
            "o",
            " position",
            " was",
            " her",
            "editary",
            ".",
            " By",
            " being",
            " allowed",
            " to",
            " marry",
            " multiple",
            " wives",
            ",",
            " the",
            " dia",
            "o",
            " was",
            " able",
            " to",
            " establish",
            " and",
            " maintain",
            " political",
            " alliances",
            " with",
            " other",
            " groups",
            ",",
            " tribes",
            ",",
            " or",
            " villages"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " CIA",
            " in",
            " the",
            " ",
            "197",
            "0",
            "s",
            ",",
            " about",
            " the",
            " matter",
            ",",
            " but",
            " Hel",
            "ms",
            " denied",
            " that",
            " the",
            " CIA",
            " had",
            " anything",
            " to",
            " do",
            " with",
            " selling",
            " illegal",
            " drugs",
            ".",
            " Gins",
            "berg",
            " wrote",
            " many",
            " essays",
            " and",
            " articles",
            ",",
            " researching",
            " and",
            " compiling",
            " evidence",
            " of",
            " the",
            " CIA",
            "'s",
            " alleged",
            " involvement",
            " in",
            " drug",
            " trafficking",
            ",",
            " but",
            " it",
            " took",
            " ten",
            " years",
            ",",
            " and",
            " the",
            " publication",
            " of",
            " McCoy",
            "'s",
            " book"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            "if",
            "lor",
            "ae",
            ".",
            " Where",
            " Cron",
            "quist",
            " saw",
            " one",
            " family",
            ",",
            " Dah",
            "lg",
            "ren",
            " saw",
            " forty",
            " distributed",
            " over",
            " three",
            " orders",
            " (",
            "pred",
            "omin",
            "antly",
            " L",
            "ilia",
            "les",
            " and",
            " As",
            "par",
            "ag",
            "ales",
            ").",
            " Over",
            " the",
            " ",
            "198",
            "0",
            "s",
            ",",
            " in",
            " the",
            " context",
            " of",
            " a",
            " more",
            " general",
            " review",
            " of",
            " the",
            " classification",
            " of",
            " ang",
            "ios",
            "perms",
            ",",
            " the",
            " L",
            "ili",
            "aceae",
            " were",
            " subjected",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Afghanistan",
            ".",
            "Af",
            "ghan",
            " Christians",
            ",",
            " who",
            " number",
            " ",
            "500",
            "",
            "8",
            ",",
            "000",
            ",",
            " practice",
            " their",
            " faith",
            " secretly",
            " due",
            " to",
            " intense",
            " societal",
            " opposition",
            ",",
            " and",
            " there",
            " are",
            " no",
            " public",
            " churches",
            ".",
            "Urban",
            "ization",
            "As",
            " estimated",
            " by",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "26",
            "%",
            " of",
            " the",
            " population",
            " was",
            " urban",
            "ized",
            " as",
            " of",
            " ",
            "202",
            "0",
            ".",
            " This",
            " is",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " fairy",
            " story",
            ";",
            " today",
            " it",
            " is",
            " a",
            " political",
            " satire",
            " with",
            " a",
            " good",
            " deal",
            " of",
            " point",
            "\".",
            " Animal",
            " Farm",
            " has",
            " been",
            " subject",
            " to",
            " much",
            " comment",
            " in",
            " the",
            " decades",
            " since",
            " these",
            " early",
            " remarks",
            ".",
            "Between",
            " ",
            "195",
            "2",
            " and",
            " ",
            "195",
            "7",
            ",",
            " the",
            " CIA",
            ",",
            " in",
            " an",
            " operation",
            " cod",
            "en",
            "amed",
            " A",
            "ed",
            "inosaur",
            ",",
            " sent",
            " millions",
            " of",
            " balloons",
            " carrying",
            " copies",
            " of",
            " the",
            " novel"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " Imagine",
            "ers",
            " created",
            " Great",
            " Moments",
            " with",
            " Mr",
            ".",
            " Lincoln",
            " that",
            " debuted",
            " at",
            " the",
            " ",
            "196",
            "4",
            " New",
            " York",
            " World",
            "'s",
            " Fair",
            ".",
            "Dr",
            ".",
            " William",
            " Barry",
            ",",
            " an",
            " Education",
            " Fut",
            "ur",
            "ist",
            " and",
            " former",
            " visiting",
            " West",
            " Point",
            " Professor",
            " of",
            " Philosophy",
            " and",
            " Eth",
            "ical",
            " Reason",
            "ing",
            " at",
            " the",
            " United",
            " States",
            " Military",
            " Academy",
            ",",
            " created",
            " an",
            " AI",
            " android",
            " character",
            " named",
            " \"",
            "Maria",
            " Bot",
            "\".",
            " This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " name",
            " of",
            " a",
            " \"",
            "N",
            "ymph",
            " or",
            " Titan",
            " goddess",
            " of",
            " Lydia",
            "\".",
            " The",
            " I",
            "li",
            "ad",
            " (",
            "attrib",
            "uted",
            " by",
            " the",
            " ancient",
            " Greeks",
            " to",
            " Homer",
            ")",
            " mentions",
            " two",
            " Ph",
            "ry",
            "g",
            "ians",
            " in",
            " the",
            " Trojan",
            " War",
            " named",
            " As",
            "ios",
            " (",
            "an",
            " adjective",
            " meaning",
            " \"",
            "Asian",
            "\");",
            " and",
            " also",
            " a",
            " marsh",
            " or",
            " low",
            "land",
            " containing",
            " a",
            " marsh",
            " in",
            " Lydia",
            " as",
            " .",
            " According",
            " to",
            " many"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " fighting",
            " brothers",
            " and",
            " separated",
            " them",
            ".",
            " He",
            " re",
            "prim",
            "anded",
            " Her",
            "acles",
            " for",
            " this",
            " act",
            " of",
            " violation",
            " and",
            " asked",
            " Apollo",
            " to",
            " give",
            " a",
            " solution",
            " to",
            " Her",
            "acles",
            ".",
            " Apollo",
            " then",
            " ordered",
            " the",
            " hero",
            " to",
            " serve",
            " under",
            " Om",
            "ph",
            "ale",
            ",",
            " queen",
            " of",
            " Lydia",
            " for",
            " one",
            " year",
            " in",
            " order",
            " to",
            " pur",
            "ify",
            " himself",
            ".",
            "After",
            " their",
            " reconciliation",
            ",",
            " Apollo",
            " and",
            " Her",
            "acles",
            " together"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.218,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.273,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " varying",
            ".",
            " Other",
            " ancient",
            " Roman",
            " observ",
            "ances",
            " include",
            " V",
            "eneral",
            "ia",
            " (",
            "April",
            " ",
            "1",
            "),",
            " Meg",
            "ales",
            "ia",
            " (",
            "April",
            " ",
            "10",
            "",
            "16",
            "),",
            " Ford",
            "ic",
            "idia",
            " (",
            "April",
            " ",
            "15",
            "),",
            " Par",
            "ilia",
            " (",
            "April",
            " ",
            "21",
            "),",
            " V",
            "inal",
            "ia",
            " Urb",
            "ana",
            " (",
            "April",
            " ",
            "23",
            "),",
            " Rob",
            "ig",
            "alia",
            " (",
            "April",
            " ",
            "25",
            "),",
            " and",
            " Ser",
            "apia",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " therefore",
            " a",
            " poorer",
            " substitute",
            ".",
            " Ex",
            "cess",
            " ca",
            "esium",
            " can",
            " lead",
            " to",
            " hyp",
            "ok",
            "alem",
            "ia",
            ",",
            " arr",
            "yth",
            "mia",
            ",",
            " and",
            " acute",
            " cardiac",
            " arrest",
            ",",
            " but",
            " such",
            " amounts",
            " would",
            " not",
            " ordinarily",
            " be",
            " encountered",
            " in",
            " natural",
            " sources",
            ".",
            " As",
            " such",
            ",",
            " ca",
            "esium",
            " is",
            " not",
            " a",
            " major",
            " chemical",
            " environmental",
            " pollut",
            "ant",
            ".",
            " The",
            " median",
            " lethal",
            " dose",
            " (",
            "LD",
            "50",
            ")",
            " value",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.492,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " others",
            ".",
            " Among",
            " them",
            " was",
            " the",
            " actress",
            " Mara",
            " Cas",
            "ares",
            ",",
            " who",
            " would",
            " later",
            " have",
            " an",
            " affair",
            " with",
            " Cam",
            "us",
            ".",
            "Cam",
            "us",
            " took",
            " an",
            " active",
            " role",
            " in",
            " the",
            " underground",
            " resistance",
            " movement",
            " against",
            " the",
            " Germans",
            " during",
            " the",
            " French",
            " Occupation",
            ".",
            " Upon",
            " his",
            " arrival",
            " in",
            " Paris",
            ",",
            " he",
            " started",
            " working",
            " as",
            " a",
            " journalist",
            " and",
            " editor",
            " of",
            " the",
            " banned",
            " newspaper",
            " Combat",
            ".",
            " He",
            " continued"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            "ux",
            " de",
            " voyage",
            ")",
            " (",
            "197",
            "8",
            ")",
            " Note",
            "books",
            " ",
            "195",
            "1",
            "",
            "195",
            "9",
            " (",
            "200",
            "8",
            ").",
            " Published",
            " as",
            " Carn",
            "ets",
            " Tome",
            " III",
            ":",
            " Mars",
            " ",
            "195",
            "1",
            " ",
            " December",
            " ",
            "195",
            "9",
            " (",
            "198",
            "9",
            ")",
            " Correspond",
            "ence",
            " (",
            "194",
            "4",
            "",
            "195",
            "9",
            ")",
            " The",
            " correspondence",
            " of",
            " Albert",
            " Cam",
            "us",
            " and",
            " Mara",
            " Cas",
            "ares",
            ",",
            " with",
            " a",
            " pre"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "",
            "m",
            "a",
            " (),",
            " with",
            " al",
            "-",
            " being",
            " the",
            " Arabic",
            " definite",
            " article",
            " '",
            "the",
            "'.",
            " Together",
            " this",
            " association",
            " can",
            " be",
            " interpreted",
            " as",
            " '",
            "the",
            " process",
            " of",
            " trans",
            "mutation",
            " by",
            " which",
            " to",
            " fuse",
            " or",
            " reun",
            "ite",
            " with",
            " the",
            " divine",
            " or",
            " original",
            " form",
            "'.",
            " Several",
            " et",
            "ym",
            "ologies",
            " have",
            " been",
            " proposed",
            " for",
            " the",
            " Greek",
            " term",
            ".",
            " The",
            " first",
            " was",
            " proposed",
            " by",
            " Z",
            "os",
            "imos",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "III",
            ",",
            " ",
            "5",
            "v",
            "|",
            " colspan",
            "=\"",
            "2",
            "\"",
            " |",
            "2",
            " PC",
            "MC",
            "IA",
            "-slot",
            " II",
            ",",
            " ",
            "5",
            "v",
            " or",
            " ",
            "12",
            "v",
            "|",
            " colspan",
            "=\"",
            "2",
            "\"",
            " |",
            "1",
            " PC",
            "MC",
            "IA",
            "-slot",
            " II",
            ",",
            " ",
            "5",
            "v",
            " or",
            " ",
            "12",
            "v",
            "|",
            "1",
            " PC",
            "MC",
            "IA",
            "-slot",
            " II",
            ",",
            " ",
            "5",
            "v",
            " or",
            " ",
            "12",
            "v"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.066,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            "200",
            "7",
            ",",
            " the",
            " Port",
            " of",
            " Th",
            "ess",
            "alon",
            "iki",
            " handled",
            " ",
            "14",
            ",",
            "373",
            ",",
            "245",
            " tonnes",
            " of",
            " cargo",
            " and",
            " ",
            "222",
            ",",
            "824",
            " TE",
            "U",
            "'s",
            ".",
            " Pal",
            "ou",
            "k",
            "ia",
            ",",
            " on",
            " the",
            " island",
            " of",
            " Sal",
            "am",
            "is",
            ",",
            " is",
            " a",
            " major",
            " passenger",
            " port",
            ".",
            "F",
            "ishing",
            "Fish",
            " are",
            " Greece",
            "'s",
            " second",
            "-largest",
            " agricultural",
            " export",
            ",",
            " and",
            " Greece",
            " has"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.236,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "rap",
            "od",
            " verte",
            "brates",
            " that",
            " are",
            " not",
            " am",
            "ni",
            "otes",
            ".",
            " Amph",
            "ibia",
            " in",
            " its",
            " widest",
            " sense",
            " ()",
            " was",
            " divided",
            " into",
            " three",
            " subclasses",
            ",",
            " two",
            " of",
            " which",
            " are",
            " extinct",
            ":",
            "These",
            " three",
            " subclasses",
            " do",
            " not",
            " include",
            " all",
            " extinct",
            " amphib",
            "ians",
            ".",
            " Other",
            " extinct",
            " amphib",
            "ian",
            " groups",
            " include",
            " Emb",
            "ol",
            "omer",
            "i",
            " (",
            "Late",
            " Pale",
            "ozo",
            "ic",
            " large",
            " aquatic",
            " predators",
            "),",
            " Seymour",
            "iam",
            "or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "201",
            "6",
            ",",
            " all",
            " four",
            " members",
            " of",
            " AB",
            "BA",
            " made",
            " a",
            " public",
            " appearance",
            " at",
            " M",
            "amma",
            " Mia",
            "!",
            " The",
            " Party",
            " in",
            " Stockholm",
            ".",
            " On",
            " ",
            "6",
            " June",
            " ",
            "201",
            "6",
            ",",
            " the",
            " quart",
            "et",
            " appeared",
            " together",
            " at",
            " a",
            " private",
            " party",
            " at",
            " Bern",
            "s",
            " Sal",
            "ong",
            "er",
            " in",
            " Stockholm",
            ",",
            " which",
            " was",
            " held",
            " to",
            " celebrate",
            " the",
            " ",
            "50",
            "th",
            " anniversary",
            " of",
            " Anders",
            "son"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " at",
            "rophy",
            " and",
            " fasc",
            "ic",
            "ulations",
            ",",
            " and",
            " upper",
            " motor",
            " neuron",
            " (",
            "UM",
            "N",
            ")",
            " findings",
            " include",
            " hyper",
            "ref",
            "lex",
            "ia",
            ",",
            " sp",
            "astic",
            "ity",
            ",",
            " muscle",
            " sp",
            "asm",
            ",",
            " and",
            " abnormal",
            " reflex",
            "es",
            ".",
            "Pure",
            " upper",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just",
            " U",
            "MN",
            " findings",
            ",",
            " include",
            " P",
            "LS",
            ".",
            "Pure",
            " lower",
            " motor",
            " neuron",
            " diseases",
            ",",
            " or",
            " those",
            " with",
            " just"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.324,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " related",
            " to",
            " am",
            "ni",
            "otes",
            " than",
            " to",
            " l",
            "iss",
            "am",
            "ph",
            "ib",
            "ians",
            ".",
            "Sub",
            "class",
            " Lep",
            "os",
            "pond",
            "y",
            "li",
            "",
            " (",
            "A",
            " potentially",
            " poly",
            "ph",
            "yle",
            "tic",
            " Late",
            " Pale",
            "ozo",
            "ic",
            " group",
            " of",
            " small",
            " forms",
            ",",
            " likely",
            " more",
            " closely",
            " related",
            " to",
            " am",
            "ni",
            "otes",
            " than",
            " L",
            "iss",
            "am",
            "ph",
            "ibia",
            ")",
            " Sub",
            "class",
            " Tem",
            "nos",
            "pond",
            "y",
            "li",
            "",
            " (",
            "div"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.258,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "ium",
            " (",
            "and",
            " to",
            " a",
            " much",
            " lesser",
            " extent",
            " ca",
            "esium",
            ")",
            " can",
            " function",
            " as",
            " temporary",
            " c",
            "ures",
            " for",
            " hyp",
            "ok",
            "alem",
            "ia",
            ";",
            " while",
            " rub",
            "id",
            "ium",
            " can",
            " adequately",
            " phys",
            "i",
            "ologically",
            " substitute",
            " potassium",
            " in",
            " some",
            " systems",
            ",",
            " ca",
            "esium",
            " is",
            " never",
            " able",
            " to",
            " do",
            " so",
            ".",
            " There",
            " is",
            " only",
            " very",
            " limited",
            " evidence",
            " in",
            " the",
            " form",
            " of",
            " deficiency",
            " symptoms",
            " for",
            " rub",
            "id",
            "ium"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " were",
            " then",
            " left",
            " to",
            " battle",
            " over",
            " control",
            " of",
            " eastern",
            " and",
            " southern",
            " Anat",
            "olia",
            " and",
            " colonial",
            " territories",
            " in",
            " Syria",
            ".",
            " The",
            " Assy",
            "rians",
            " had",
            " better",
            " success",
            " than",
            " the",
            " Egyptians",
            ",",
            " annex",
            "ing",
            " much",
            " H",
            "itt",
            "ite",
            " (",
            "and",
            " Hur",
            "rian",
            ")",
            " territory",
            " in",
            " these",
            " regions",
            ".",
            "Post",
            "-H",
            "itt",
            "ite",
            " Anat",
            "olia",
            " (",
            "12",
            "th",
            "",
            "6",
            "th",
            " century",
            " BCE",
            ")",
            "After",
            " ",
            "118"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.012,
            -0.0,
            -0.0,
            0.122,
            0.216,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "da",
            ".",
            "The",
            " Z",
            "ay",
            "yan",
            "ids",
            " retained",
            " their",
            " control",
            " over",
            " Algeria",
            " for",
            " ",
            "3",
            " centuries",
            ".",
            " Much",
            " of",
            " the",
            " eastern",
            " territories",
            " of",
            " Algeria",
            " were",
            " under",
            " the",
            " authority",
            " of",
            " the",
            " H",
            "af",
            "sid",
            " dynasty",
            ",",
            " although",
            " the",
            " Em",
            "irate",
            " of",
            " Be",
            "ja",
            "ia",
            " encompass",
            "ing",
            " the",
            " Alger",
            "ian",
            " territories",
            " of",
            " the",
            " H",
            "afs",
            "ids",
            " would",
            " occasionally",
            " be",
            " independent",
            " from",
            " central",
            " Tunis",
            "ian",
            " control"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ism",
            ",",
            " spiritual",
            "ism",
            ",",
            " Ros",
            "ic",
            "ruc",
            "ian",
            "ism",
            ",",
            " and",
            " other",
            " myst",
            "ic",
            " movements",
            ".",
            " Institutions",
            " involved",
            " in",
            " this",
            " research",
            " include",
            " The",
            " Ch",
            "ym",
            "istry",
            " of",
            " Isaac",
            " Newton",
            " project",
            " at",
            " Indiana",
            " University",
            ",",
            " the",
            " University",
            " of",
            " Ex",
            "eter",
            " Centre",
            " for",
            " the",
            " Study",
            " of",
            " Es",
            "oteric",
            "ism",
            " (",
            "EX",
            "E",
            "SE",
            "SO",
            "),",
            " the",
            " European",
            " Society",
            " for",
            " the",
            " Study",
            " of",
            " Western",
            " Es"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "sky",
            ",",
            " his",
            " lifelong",
            " partner",
            ".",
            " Judge",
            " Clayton",
            " W",
            ".",
            " Horn",
            " ruled",
            " that",
            " \"",
            "How",
            "l",
            "\"",
            " was",
            " not",
            " obscene",
            ",",
            " asking",
            ":",
            " \"",
            "Would",
            " there",
            " be",
            " any",
            " freedom",
            " of",
            " press",
            " or",
            " speech",
            " if",
            " one",
            " must",
            " reduce",
            " his",
            " vocabulary",
            " to",
            " v",
            "apid",
            " innoc",
            "uous",
            " eup",
            "hem",
            "isms",
            "?\"",
            "G",
            "ins",
            "berg",
            " was",
            " a",
            " Buddhist",
            " who",
            " extensively",
            " studied",
            " Eastern",
            " religious",
            " disciplines",
            ".",
            " He",
            " lived"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " boom",
            " of",
            " the",
            " oil",
            " industry",
            " and",
            " the",
            " tourist",
            " sector",
            " in",
            " the",
            " ",
            "20",
            "th",
            " century",
            " the",
            " architectural",
            " style",
            " of",
            " the",
            " island",
            " incorporated",
            " a",
            " more",
            " American",
            " and",
            " international",
            " influence",
            ".",
            " In",
            " addition",
            ",",
            " elements",
            " of",
            " the",
            " Art",
            " Dec",
            "o",
            " style",
            " can",
            " still",
            " be",
            " seen",
            " in",
            " several",
            " buildings",
            " in",
            " San",
            " Nicolas",
            ".",
            " Therefore",
            ",",
            " it",
            " can",
            " be",
            " said",
            " that",
            " the",
            " island",
            "'s",
            " architecture",
            " is",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ards",
            ",",
            " and",
            " base",
            " things",
            " sire",
            " base",
            "\"",
            " (",
            "IV",
            ",",
            " ",
            "2",
            ")",
            " to",
            " reinforce",
            " his",
            " her",
            "edit",
            "arian",
            " argument",
            ".",
            "Mech",
            "an",
            "istically",
            ",",
            " Sch",
            "openh",
            "auer",
            " believed",
            " that",
            " a",
            " person",
            " inherits",
            " his",
            " intellect",
            " through",
            " his",
            " mother",
            ",",
            " and",
            " personal",
            " character",
            " through",
            " the",
            " father",
            ".",
            " This",
            " belief",
            " in",
            " her",
            "it",
            "ability",
            " of",
            " traits",
            " informed",
            " Sch",
            "openh",
            "auer",
            "'s",
            " view",
            " of",
            " love"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            ")",
            " was",
            " the",
            " American",
            " space",
            "flight",
            " that",
            " first",
            " landed",
            " humans",
            " on",
            " the",
            " Moon",
            ".",
            " Commander",
            " Neil",
            " Armstrong",
            " and",
            " Lunar",
            " Module",
            " Pilot",
            " Buzz",
            " Ald",
            "rin",
            " landed",
            " the",
            " Apollo",
            " Lunar",
            " Module",
            " Eagle",
            " on",
            " July",
            " ",
            "20",
            ",",
            " ",
            "196",
            "9",
            ",",
            " at",
            " ",
            "20",
            ":",
            "17",
            " UTC",
            ",",
            " and",
            " Armstrong",
            " became",
            " the",
            " first",
            " person",
            " to",
            " step",
            " onto",
            " the",
            " Moon",
            "'s",
            " surface",
            " six",
            " hours",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "alem",
    ".fhir",
    "",
    "t",
    "kup"
  ],
  "bottom_logits": [
    "IAS",
    " Selected",
    " friend",
    " divisive",
    " lief"
  ],
  "act_min": -0.0,
  "act_max": 0.73
}