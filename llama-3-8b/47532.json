{
  "index": 47532,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "iet",
            " bias",
            " was",
            " unacceptable",
            ",",
            " and",
            " the",
            " choice",
            " of",
            " pigs",
            " as",
            " the",
            " dominant",
            " class",
            " was",
            " thought",
            " to",
            " be",
            " especially",
            " offensive",
            ".",
            " It",
            " may",
            " reasonably",
            " be",
            " assumed",
            " that",
            " the",
            " \"",
            "important",
            " official",
            "\"",
            " was",
            " a",
            " man",
            " named",
            " Peter",
            " Sm",
            "ol",
            "lett",
            ",",
            " who",
            " was",
            " later",
            " un",
            "masked",
            " as",
            " a",
            " Soviet",
            " agent",
            ".",
            " Orwell",
            " was",
            " suspicious",
            " of",
            " Sm",
            "ol",
            "lett",
            "/",
            "Sm",
            "ol",
            "ka",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.816,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " be",
            " compassionate",
            " and",
            " humble",
            ",",
            " not",
            " small",
            "-minded",
            " or",
            " arrogant",
            ".",
            " No",
            " truth",
            " should",
            " be",
            " denied",
            " outright",
            ",",
            " but",
            " all",
            " should",
            " be",
            " questioned",
            ".",
            " Science",
            " reveals",
            " an",
            " ever",
            "-growing",
            " vision",
            " of",
            " our",
            " universe",
            " that",
            " should",
            " not",
            " be",
            " discounted",
            " due",
            " to",
            " bias",
            " toward",
            " older",
            " understand",
            "ings",
            ".",
            " Reason",
            " is",
            " to",
            " be",
            " trusted",
            " and",
            " cultivated",
            ".",
            " To",
            " believe",
            " in",
            " God",
            " is",
            " not",
            " to",
            " fore",
            "go"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.812,
            0.324,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " the",
            " M",
            "-type",
            ".",
            " There",
            " are",
            " also",
            " several",
            " smaller",
            " classes",
            ".",
            "The",
            " proportion",
            " of",
            " known",
            " asteroids",
            " falling",
            " into",
            " the",
            " various",
            " spectral",
            " types",
            " does",
            " not",
            " necessarily",
            " reflect",
            " the",
            " proportion",
            " of",
            " all",
            " asteroids",
            " that",
            " are",
            " of",
            " that",
            " type",
            ";",
            " some",
            " types",
            " are",
            " easier",
            " to",
            " detect",
            " than",
            " others",
            ",",
            " bias",
            "ing",
            " the",
            " totals",
            ".",
            "Pro",
            "blems",
            " ",
            "Originally",
            ",",
            " spectral",
            " design",
            "ations",
            " were",
            " based",
            " on"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "-S",
            "i",
            " cells",
            " installed",
            " above",
            " healthy",
            " vegetation",
            ".",
            " An",
            " analysis",
            " on",
            " the",
            " bias",
            " due",
            " to",
            " the",
            " specular",
            " reflect",
            "ivity",
            " of",
            " ",
            "22",
            " commonly",
            " occurring",
            " surface",
            " materials",
            " (",
            "both",
            " human",
            "-made",
            " and",
            " natural",
            ")",
            " provided",
            " effective",
            " al",
            "bedo",
            " values",
            " for",
            " sim",
            "ulating",
            " the",
            " performance",
            " of",
            " seven",
            " phot",
            "ovolta",
            "ic",
            " materials",
            " mounted",
            " on",
            " three",
            " common",
            " phot",
            "ovolta",
            "ic",
            " system",
            " top",
            "ologies",
            ":",
            " industrial",
            " (",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " seeks",
            " to",
            " reduce",
            " male",
            " bias",
            " in",
            " research",
            " findings",
            ",",
            " anthrop",
            "ological",
            " hiring",
            " practices",
            ",",
            " and",
            " the",
            " scholarly",
            " production",
            " of",
            " knowledge",
            ".",
            " Anthrop",
            "ology",
            " engages",
            " often",
            " with",
            " feminists",
            " from",
            " non",
            "-West",
            "ern",
            " traditions",
            ",",
            " whose",
            " perspectives",
            " and",
            " experiences",
            " can",
            " differ",
            " from",
            " those",
            " of",
            " white",
            " feminists",
            " of",
            " Europe",
            ",",
            " America",
            ",",
            " and",
            " elsewhere",
            ".",
            " From",
            " the",
            " perspective",
            " of",
            " the",
            " Western",
            " world",
            ",",
            " historically",
            " such"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " significance",
            ".",
            " So",
            " AN",
            "O",
            "VA",
            " statistical",
            " significance",
            " result",
            " is",
            " independent",
            " of",
            " constant",
            " bias",
            " and",
            " scaling",
            " errors",
            " as",
            " well",
            " as",
            " the",
            " units",
            " used",
            " in",
            " expressing",
            " observations",
            ".",
            " In",
            " the",
            " era",
            " of",
            " mechanical",
            " calculation",
            " it",
            " was",
            " common",
            " to",
            " subtract",
            " a",
            " constant",
            " from",
            " all",
            " observations",
            " (",
            "when",
            " equivalent",
            " to",
            " dropping",
            " leading",
            " digits",
            ")",
            " to",
            " simplify",
            " data",
            " entry",
            ".",
            " This",
            " is",
            " an",
            " example",
            " of",
            " data",
            " coding"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.801,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.001,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " seeks",
            " to",
            " reduce",
            " male",
            " bias",
            " in",
            " research",
            " findings",
            ",",
            " anthrop",
            "ological",
            " hiring",
            " practices",
            ",",
            " and",
            " the",
            " scholarly",
            " production",
            " of",
            " knowledge",
            ".",
            " Anthrop",
            "ology",
            " engages",
            " often",
            " with",
            " feminists",
            " from",
            " non",
            "-West",
            "ern",
            " traditions",
            ",",
            " whose",
            " perspectives",
            " and",
            " experiences",
            " can",
            " differ",
            " from",
            " those",
            " of",
            " white",
            " feminists",
            " of",
            " Europe",
            ",",
            " America",
            ",",
            " and",
            " elsewhere",
            ".",
            " From",
            " the",
            " perspective",
            " of",
            " the",
            " Western",
            " world",
            ",",
            " historically",
            " such"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " hashtag",
            " #",
            "O",
            "sc",
            "ars",
            "So",
            "White",
            ",",
            " based",
            " on",
            " activists",
            "'",
            " perception",
            " that",
            " its",
            " all",
            "-white",
            " acting",
            " nominee",
            " list",
            " reflected",
            " bias",
            ".",
            " In",
            " response",
            ",",
            " the",
            " Academy",
            " initiated",
            " \"",
            "historic",
            "\"",
            " changes",
            " in",
            " membership",
            " by",
            " ",
            "202",
            "0",
            ".",
            " Some",
            " media",
            " critics",
            " claim",
            " the",
            " Academy",
            "'s",
            " efforts",
            " to",
            " address",
            " its",
            " purported",
            " racial",
            ",",
            " gender",
            " and",
            " national",
            " biases",
            " are",
            " merely",
            " distractions",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.061,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " Py",
            "rr",
            "hus",
            ",",
            " after",
            " his",
            " father",
            "'s",
            " possible",
            " alias",
            ")",
            " and",
            " One",
            "iros",
            ".",
            " According",
            " to",
            " this",
            " story",
            ",",
            " Od",
            "ys",
            "se",
            "us",
            " learned",
            " from",
            " the",
            " prophet",
            " Cal",
            "chas",
            " that",
            " the",
            " Ach",
            "ae",
            "ans",
            " would",
            " be",
            " unable",
            " to",
            " capture",
            " Troy",
            " without",
            " Achilles",
            "'",
            " aid",
            ".",
            " Od",
            "ys",
            "se",
            "us",
            " went",
            " to",
            " Sky",
            "ros",
            " in",
            " the",
            " guise",
            " of",
            " a",
            " ped",
            "d",
            "ler",
            " selling"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " believe",
            " in",
            " God",
            " but",
            " they",
            " are",
            " not",
            " sufficient",
            " for",
            " an",
            " ag",
            "nostic",
            " to",
            " become",
            " a",
            " the",
            "ist",
            ".",
            " It",
            " is",
            " not",
            " enough",
            " to",
            " believe",
            " in",
            " an",
            " ancient",
            " holy",
            " book",
            ",",
            " even",
            " though",
            " when",
            " it",
            " is",
            " accurately",
            " analyzed",
            " without",
            " bias",
            ",",
            " it",
            " proves",
            " to",
            " be",
            " more",
            " trustworthy",
            " and",
            " admirable",
            " than",
            " what",
            " we",
            " are",
            " taught",
            " in",
            " school",
            ".",
            " Neither",
            " is",
            " it",
            " enough",
            " to",
            " realize"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.797,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " hashtag",
            " #",
            "O",
            "sc",
            "ars",
            "So",
            "White",
            ",",
            " based",
            " on",
            " activists",
            "'",
            " perception",
            " that",
            " its",
            " all",
            "-white",
            " acting",
            " nominee",
            " list",
            " reflected",
            " bias",
            ".",
            " In",
            " response",
            ",",
            " the",
            " Academy",
            " initiated",
            " \"",
            "historic",
            "\"",
            " changes",
            " in",
            " membership",
            " by",
            " ",
            "202",
            "0",
            ".",
            " Some",
            " media",
            " critics",
            " claim",
            " the",
            " Academy",
            "'s",
            " efforts",
            " to",
            " address",
            " its",
            " purported",
            " racial",
            ",",
            " gender",
            " and",
            " national",
            " biases",
            " are",
            " merely",
            " distractions",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.793,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.232,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " consistent",
            " with",
            " related",
            " work",
            " on",
            " attention",
            "al",
            " bias",
            " in",
            " implicit",
            " memory",
            ".",
            " Additionally",
            " recent",
            " research",
            " has",
            " found",
            " that",
            " implicit",
            " racial",
            " evaluations",
            " (",
            "i",
            ".e",
            ".",
            " automatic",
            " prejud",
            "iced",
            " attitudes",
            ")",
            " can",
            " be",
            " amplified",
            " during",
            " inter",
            "group",
            " interaction",
            ".",
            " Negative",
            " experiences",
            " have",
            " been",
            " illustrated",
            " in",
            " producing",
            " not",
            " only",
            " negative",
            " expectations",
            ",",
            " but",
            " also",
            " avoid",
            "ant",
            ",",
            " or",
            " antagon",
            "istic",
            ",",
            " behavior",
            " such",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.793,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " believed",
            " to",
            " be",
            " perpetrated",
            " by",
            " those",
            " that",
            " were",
            " involved",
            " in",
            " creating",
            " the",
            " movie",
            ".",
            " Examples",
            " of",
            " whisper",
            " campaigns",
            " include",
            " the",
            " allegations",
            " against",
            " Zero",
            " Dark",
            " Thirty",
            " suggesting",
            " that",
            " it",
            " just",
            "ifies",
            " torture",
            " and",
            " the",
            " claim",
            " that",
            " Lincoln",
            " dist",
            "orts",
            " history",
            ".",
            "Acc",
            "us",
            "ations",
            " of",
            " bias",
            " ",
            "Typ",
            "ical",
            " criticism",
            " of",
            " the",
            " Academy",
            " Awards",
            " for",
            " Best",
            " Picture",
            " is",
            " that",
            " among",
            " the",
            " winners",
            " and",
            " nominees"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.793,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Some",
            " examples",
            " of",
            " reversible",
            " error",
            " would",
            " be",
            " errone",
            "ously",
            " instruct",
            "ing",
            " the",
            " jury",
            " on",
            " the",
            " law",
            " applicable",
            " to",
            " the",
            " case",
            ",",
            " permitting",
            " seriously",
            " improper",
            " argument",
            " by",
            " an",
            " attorney",
            ",",
            " admitting",
            " or",
            " excluding",
            " evidence",
            " improperly",
            ",",
            " acting",
            " outside",
            " the",
            " court",
            "'s",
            " jurisdiction",
            ",",
            " injecting",
            " bias",
            " into",
            " the",
            " proceeding",
            " or",
            " appearing",
            " to",
            " do",
            " so",
            ",",
            " jur",
            "or",
            " misconduct",
            ",",
            " etc",
            ".",
            " ",
            " The",
            " failure"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.793,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            " Some",
            " examples",
            " of",
            " reversible",
            " error",
            " would",
            " be",
            " errone",
            "ously",
            " instruct",
            "ing",
            " the",
            " jury",
            " on",
            " the",
            " law",
            " applicable",
            " to",
            " the",
            " case",
            ",",
            " permitting",
            " seriously",
            " improper",
            " argument",
            " by",
            " an",
            " attorney",
            ",",
            " admitting",
            " or",
            " excluding",
            " evidence",
            " improperly",
            ",",
            " acting",
            " outside",
            " the",
            " court",
            "'s",
            " jurisdiction",
            ",",
            " injecting",
            " bias",
            " into",
            " the",
            " proceeding",
            " or",
            " appearing",
            " to",
            " do",
            " so",
            ",",
            " jur",
            "or",
            " misconduct",
            ",",
            " etc",
            ".",
            " ",
            " The",
            " failure"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " their",
            " publications",
            " have",
            " contributed",
            " to",
            " anthropology",
            ",",
            " along",
            " the",
            " way",
            " correcting",
            " against",
            " the",
            " systemic",
            " biases",
            " beginning",
            " with",
            " the",
            " \"",
            "p",
            "atri",
            "arch",
            "al",
            " origins",
            " of",
            " anthropology",
            " (",
            "and",
            " (",
            "acad",
            "emia",
            ")\"",
            " and",
            " note",
            " that",
            " from",
            " ",
            "189",
            "1",
            " to",
            " ",
            "193",
            "0",
            " doctor",
            "ates",
            " in",
            " anthropology",
            " went",
            " to",
            " males",
            " more",
            " than",
            " ",
            "85",
            "%,",
            " more",
            " than",
            " ",
            "81",
            "%",
            " were",
            " under"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.785,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " their",
            " publications",
            " have",
            " contributed",
            " to",
            " anthropology",
            ",",
            " along",
            " the",
            " way",
            " correcting",
            " against",
            " the",
            " systemic",
            " biases",
            " beginning",
            " with",
            " the",
            " \"",
            "p",
            "atri",
            "arch",
            "al",
            " origins",
            " of",
            " anthropology",
            " (",
            "and",
            " (",
            "acad",
            "emia",
            ")\"",
            " and",
            " note",
            " that",
            " from",
            " ",
            "189",
            "1",
            " to",
            " ",
            "193",
            "0",
            " doctor",
            "ates",
            " in",
            " anthropology",
            " went",
            " to",
            " males",
            " more",
            " than",
            " ",
            "85",
            "%,",
            " more",
            " than",
            " ",
            "81",
            "%",
            " were",
            " under"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.781,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " '",
            "per",
            "ipheral",
            "'",
            " perspectives",
            " have",
            " been",
            " ignored",
            ",",
            " observed",
            " only",
            " from",
            " an",
            " outsider",
            " perspective",
            ",",
            " and",
            " regarded",
            " as",
            " less",
            "-valid",
            " or",
            " less",
            "-important",
            " than",
            " knowledge",
            " from",
            " the",
            " Western",
            " world",
            ".",
            " Expl",
            "oring",
            " and",
            " addressing",
            " that",
            " double",
            " bias",
            " against",
            " women",
            " from",
            " marginalized",
            " racial",
            " or",
            " ethnic",
            " groups",
            " is",
            " of",
            " particular",
            " interest",
            " in",
            " intersection",
            "al",
            " feminist",
            " anthropology",
            ".",
            "F",
            "emin",
            "ist",
            " anthrop",
            "ologists",
            " have",
            " stated"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " character",
            "izations",
            ".",
            "See",
            " also",
            " ",
            " Abstract",
            " machine",
            " AL",
            "G",
            "OL",
            " Algorithm",
            " engineering",
            " Algorithm",
            " character",
            "izations",
            " Algorithm",
            "ic",
            " bias",
            " Algorithm",
            "ic",
            " composition",
            " Algorithm",
            "ic",
            " entities",
            " Algorithm",
            "ic",
            " synthesis",
            " Algorithm",
            "ic",
            " technique",
            " Algorithm",
            "ic",
            " topology",
            " Gar",
            "bage",
            " in",
            ",",
            " garbage",
            " out",
            " Introduction",
            " to",
            " Algorithms",
            " (",
            "text",
            "book",
            ")",
            " Government",
            " by",
            " algorithm",
            " List"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.777,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " keep",
            " the",
            " tunnel",
            "ing",
            " current",
            " at",
            " a",
            " set",
            " value",
            ".",
            " How",
            " much",
            " the",
            " tip",
            " moves",
            " to",
            " and",
            " away",
            " from",
            " the",
            " surface",
            " is",
            " interpreted",
            " as",
            " the",
            " height",
            " profile",
            ".",
            " For",
            " low",
            " bias",
            ",",
            " the",
            " microscope",
            " images",
            " the",
            " averaged",
            " electron",
            " orbit",
            "als",
            " across",
            " closely",
            " packed",
            " energy",
            " levels",
            "the",
            " local",
            " density",
            " of",
            " the",
            " electronic",
            " states",
            " near",
            " the",
            " Ferm",
            "i",
            " level",
            ".",
            " Because",
            " of",
            " the",
            " distances"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.77,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "35",
            ",",
            " and",
            " only",
            " ",
            "7",
            ".",
            "2",
            "%",
            " to",
            " anyone",
            " over",
            " ",
            "40",
            " years",
            " old",
            ",",
            " thus",
            " reflecting",
            " an",
            " age",
            " gap",
            " in",
            " the",
            " pursuit",
            " of",
            " anthropology",
            " by",
            " first",
            "-wave",
            " feminists",
            " until",
            " later",
            " in",
            " life",
            ".",
            " This",
            " correction",
            " of",
            " systemic",
            " bias",
            " may",
            " include",
            " mainstream",
            " feminist",
            " theory",
            ",",
            " history",
            ",",
            " lingu",
            "istics",
            ",",
            " archae",
            "ology",
            ",",
            " and",
            " anthropology",
            ".",
            " Femin",
            "ist",
            " anthrop",
            "ologists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.766,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " science",
            ",",
            " technology",
            ",",
            " engineering",
            ",",
            " and",
            " maths",
            ",\"",
            " and",
            " to",
            " \"",
            "create",
            " new",
            " role",
            " models",
            " for",
            " girls",
            " and",
            " women",
            "\"",
            " in",
            " these",
            " fields",
            ".",
            " Events",
            " have",
            " included",
            " Wikipedia",
            " edit",
            "-a",
            "-th",
            "ons",
            " with",
            " the",
            " aim",
            " of",
            " improving",
            " the",
            " representation",
            " of",
            " women",
            " on",
            " Wikipedia",
            " in",
            " terms",
            " of",
            " articles",
            " and",
            " editors",
            " to",
            " reduce",
            " unintended",
            " gender",
            " bias",
            " on",
            " Wikipedia",
            ".",
            "The",
            " Ada",
            " Initiative",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.738,
            0.15,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.203,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " leading",
            " to",
            " under",
            "-re",
            "presentation",
            " in",
            " the",
            " state",
            " senate",
            " for",
            " more",
            " urban",
            "ized",
            ",",
            " populous",
            " counties",
            ".",
            " The",
            " rural",
            " bias",
            " of",
            " the",
            " state",
            " legislature",
            ",",
            " which",
            " had",
            " also",
            " failed",
            " to",
            " red",
            "istrict",
            " seats",
            " in",
            " the",
            " state",
            " house",
            ",",
            " affected",
            " politics",
            " well",
            " into",
            " the",
            " ",
            "20",
            "th",
            " century",
            ",",
            " failing",
            " to",
            " recognize",
            " the",
            " rise",
            " of",
            " industrial",
            " cities",
            " and",
            " urban",
            "ized",
            " areas",
            ".",
            "\"The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " she",
            " argued",
            " had",
            " been",
            " mis",
            "used",
            " to",
            " legit",
            "imize",
            " racist",
            " con",
            "ceptions",
            " of",
            " Ancient",
            " Egypt",
            " with",
            " \"",
            "scientific",
            " evidence",
            "\".",
            " ",
            "In",
            " ",
            "202",
            "3",
            ",",
            " Christopher",
            " Eh",
            "ret",
            " criticised",
            " the",
            " conclusions",
            " of",
            " the",
            " ",
            "201",
            "7",
            " study",
            " which",
            " proposed",
            " the",
            " ancient",
            " Egyptians",
            " had",
            " a",
            " Lev",
            "antine",
            " background",
            " based",
            " on",
            " insufficient",
            " sampling",
            " and",
            " a",
            " biased",
            " interpretation",
            " of",
            " the",
            " genetic",
            " data",
            ".",
            " Eh",
            "ret"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " surrounding",
            " Air",
            " France",
            " Flight",
            " ",
            "896",
            "9",
            ",",
            " a",
            " hij",
            "acking",
            " perpetrated",
            " by",
            " the",
            " Armed",
            " Islamic",
            " Group",
            ".",
            " The",
            " Armed",
            " Islamic",
            " Group",
            " declared",
            " a",
            " ceasefire",
            " in",
            " October",
            " ",
            "199",
            "7",
            ".",
            "Al",
            "ger",
            "ia",
            " held",
            " elections",
            " in",
            " ",
            "199",
            "9",
            ",",
            " considered",
            " biased",
            " by",
            " international",
            " observers",
            " and",
            " most",
            " opposition",
            " groups",
            " which",
            " were",
            " won",
            " by",
            " President",
            " Abdel",
            "az",
            "iz",
            " Bout",
            "ef",
            "li",
            "ka",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.021,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " surrounding",
            " Air",
            " France",
            " Flight",
            " ",
            "896",
            "9",
            ",",
            " a",
            " hij",
            "acking",
            " perpetrated",
            " by",
            " the",
            " Armed",
            " Islamic",
            " Group",
            ".",
            " The",
            " Armed",
            " Islamic",
            " Group",
            " declared",
            " a",
            " ceasefire",
            " in",
            " October",
            " ",
            "199",
            "7",
            ".",
            "Al",
            "ger",
            "ia",
            " held",
            " elections",
            " in",
            " ",
            "199",
            "9",
            ",",
            " considered",
            " biased",
            " by",
            " international",
            " observers",
            " and",
            " most",
            " opposition",
            " groups",
            " which",
            " were",
            " won",
            " by",
            " President",
            " Abdel",
            "az",
            "iz",
            " Bout",
            "ef",
            "li",
            "ka",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " as",
            " a",
            " member",
            " of",
            " the",
            " embassy",
            " of",
            " The",
            "odos",
            "ius",
            "",
            "II",
            " at",
            " the",
            " Hun",
            "nic",
            " court",
            " in",
            " ",
            "449",
            ".",
            " He",
            " was",
            " obviously",
            " biased",
            " by",
            " his",
            " political",
            " position",
            ",",
            " but",
            " his",
            " writing",
            " is",
            " a",
            " major",
            " source",
            " for",
            " information",
            " on",
            " the",
            " life",
            " of",
            " At",
            "til",
            "a",
            ",",
            " and",
            " he",
            " is",
            " the",
            " only",
            " person",
            " known",
            " to",
            " have",
            " recorded",
            " a",
            " physical",
            " description",
            " of",
            " him"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " between",
            " quantities",
            " from",
            " which",
            " the",
            " required",
            " statistics",
            " can",
            " be",
            " calculated",
            " in",
            " a",
            " numer",
            "ically",
            " stable",
            " fashion",
            ".",
            "The",
            " following",
            " formulas",
            " can",
            " be",
            " used",
            " to",
            " update",
            " the",
            " mean",
            " and",
            " (",
            "estimated",
            ")",
            " variance",
            " of",
            " the",
            " sequence",
            ",",
            " for",
            " an",
            " additional",
            " element",
            " xn",
            ".",
            " Here",
            ",",
            " ",
            " denotes",
            " the",
            " sample",
            " mean",
            " of",
            " the",
            " first",
            " n",
            " samples",
            " ,",
            " ",
            " their",
            " biased",
            " sample",
            " variance",
            ",",
            " and",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " as",
            " a",
            " member",
            " of",
            " the",
            " embassy",
            " of",
            " The",
            "odos",
            "ius",
            "",
            "II",
            " at",
            " the",
            " Hun",
            "nic",
            " court",
            " in",
            " ",
            "449",
            ".",
            " He",
            " was",
            " obviously",
            " biased",
            " by",
            " his",
            " political",
            " position",
            ",",
            " but",
            " his",
            " writing",
            " is",
            " a",
            " major",
            " source",
            " for",
            " information",
            " on",
            " the",
            " life",
            " of",
            " At",
            "til",
            "a",
            ",",
            " and",
            " he",
            " is",
            " the",
            " only",
            " person",
            " known",
            " to",
            " have",
            " recorded",
            " a",
            " physical",
            " description",
            " of",
            " him"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " agricultural",
            " producers",
            " accounts",
            " for",
            " almost",
            " US",
            "$",
            "540",
            " billion",
            " a",
            " year",
            ".",
            " This",
            " amounts",
            " to",
            " ",
            "15",
            " percent",
            " of",
            " total",
            " agricultural",
            " production",
            " value",
            ",",
            " and",
            " is",
            " heavily",
            " biased",
            " towards",
            " measures",
            " that",
            " are",
            " leading",
            " to",
            " ineff",
            "iciency",
            ",",
            " as",
            " well",
            " as",
            " are",
            " une",
            "qu",
            "ally",
            " distributed",
            " and",
            " harmful",
            " for",
            " the",
            " environment",
            " and",
            " human",
            " health",
            ".",
            " ",
            "There",
            " are",
            " many",
            " influences",
            " on",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " solve",
            " the",
            " is",
            "",
            "ought",
            " problem",
            ".",
            " Critics",
            " have",
            " called",
            " her",
            " definitions",
            " of",
            " ego",
            "ism",
            " and",
            " altru",
            "ism",
            " biased",
            " and",
            " inconsistent",
            " with",
            " normal",
            " usage",
            ".",
            " Critics",
            " from",
            " religious",
            " traditions",
            " oppose",
            " her",
            " athe",
            "ism",
            " and",
            " her",
            " rejection",
            " of",
            " altru",
            "ism",
            ".",
            "Rand",
            "'s",
            " political",
            " philosophy",
            " emphasized",
            " individual",
            " rights",
            ",",
            " including",
            " property",
            " rights",
            ".",
            " She",
            " considered",
            " laisse",
            "z",
            "-f",
            "aire",
            " capitalism",
            " the",
            " only",
            " moral"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " ins",
            "ur",
            "mount",
            "able",
            " in",
            " the",
            " classical",
            " perspective",
            ".",
            " Elect",
            "rons",
            " tunnel",
            " through",
            " the",
            " vacuum",
            " between",
            " two",
            " biased",
            " electrodes",
            ",",
            " providing",
            " a",
            " tunnel",
            "ing",
            " current",
            " that",
            " is",
            " exponentially",
            " dependent",
            " on",
            " their",
            " separation",
            ".",
            " One",
            " electrode",
            " is",
            " a",
            " sharp",
            " tip",
            " ideally",
            " ending",
            " with",
            " a",
            " single",
            " atom",
            ".",
            " At",
            " each",
            " point",
            " of",
            " the",
            " scan",
            " of",
            " the",
            " surface",
            " the",
            " tip",
            "'s",
            " height",
            " is",
            " adjusted",
            " so",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " often",
            " sequential",
            ".",
            "Early",
            " experiments",
            " are",
            " often",
            " designed",
            " to",
            " provide",
            " mean",
            "-un",
            "biased",
            " estimates",
            " of",
            " treatment",
            " effects",
            " and",
            " of",
            " experimental",
            " error",
            ".",
            " Later",
            " experiments",
            " are",
            " often",
            " designed",
            " to",
            " test",
            " a",
            " hypothesis",
            " that",
            " a",
            " treatment",
            " effect",
            " has",
            " an",
            " important",
            " magnitude",
            ";",
            " in",
            " this",
            " case",
            ",",
            " the",
            " number",
            " of",
            " experimental",
            " units",
            " is",
            " chosen",
            " so",
            " that",
            " the",
            " experiment",
            " is",
            " within",
            " budget",
            " and",
            " has",
            " adequate",
            " power",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " often",
            " sequential",
            ".",
            "Early",
            " experiments",
            " are",
            " often",
            " designed",
            " to",
            " provide",
            " mean",
            "-un",
            "biased",
            " estimates",
            " of",
            " treatment",
            " effects",
            " and",
            " of",
            " experimental",
            " error",
            ".",
            " Later",
            " experiments",
            " are",
            " often",
            " designed",
            " to",
            " test",
            " a",
            " hypothesis",
            " that",
            " a",
            " treatment",
            " effect",
            " has",
            " an",
            " important",
            " magnitude",
            ";",
            " in",
            " this",
            " case",
            ",",
            " the",
            " number",
            " of",
            " experimental",
            " units",
            " is",
            " chosen",
            " so",
            " that",
            " the",
            " experiment",
            " is",
            " within",
            " budget",
            " and",
            " has",
            " adequate",
            " power",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.102,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 57,
          "is_repeated_datapoint": false,
          "tokens": [
            " standardized",
            " effect",
            " sizes",
            " are",
            " commonly",
            " used",
            " in",
            " much",
            " of",
            " the",
            " professional",
            " literature",
            ",",
            " a",
            " non",
            "-standard",
            "ized",
            " measure",
            " of",
            " effect",
            " size",
            " that",
            " has",
            " immediately",
            " \"",
            "meaning",
            "ful",
            "\"",
            " units",
            " may",
            " be",
            " preferable",
            " for",
            " reporting",
            " purposes",
            ".",
            "Model",
            " confirmation",
            "Sometimes",
            " tests",
            " are",
            " conducted",
            " to",
            " determine",
            " whether",
            " the",
            " assumptions",
            " of",
            " AN",
            "O",
            "VA",
            " appear",
            " to",
            " be",
            " violated",
            ".",
            " Res",
            "idual",
            "s",
            " are",
            " examined",
            " or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "Motor",
            " deficits",
            "Res",
            "idual",
            " motor",
            " deficits",
            " are",
            " estimated",
            " to",
            " remain",
            " in",
            " about",
            " ",
            "8",
            " to",
            " ",
            "30",
            "%",
            " of",
            " cases",
            ",",
            " the",
            " range",
            " in",
            " severity",
            " from",
            " mild",
            " clums",
            "iness",
            " to",
            " at",
            "ax",
            "ia",
            " and",
            " hem",
            "ip",
            "ares",
            "is",
            ".",
            "Ne",
            "uro",
            "c",
            "ognitive",
            "Patients",
            " with",
            " dem",
            "y",
            "el",
            "inating",
            " illnesses",
            ",",
            " such",
            " as",
            " MS",
            ",",
            " have",
            " shown",
            " cognitive",
            " deficits",
            " even",
            " when"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.406,
            0.099,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.022,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.017,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            0.19,
            -0.0,
            -0.0,
            0.357,
            0.093,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-fi",
            " television",
            " series",
            " \"",
            "Ali",
            "ens",
            "\"",
            " (",
            "Rose",
            "anne",
            "),",
            " a",
            " ",
            "199",
            "2",
            " television",
            " episode",
            "Other",
            " uses",
            " ",
            " Alien",
            " (",
            "shipping",
            " company",
            "),",
            " a",
            " Russian",
            " company",
            " Alien",
            " Sun",
            " (",
            "born",
            " ",
            "197",
            "4",
            "),",
            " Singapore",
            "an",
            " actress",
            " Alien",
            ",",
            " a",
            " perfume",
            " by",
            " Thi",
            "erry",
            " Mug",
            "ler",
            " Ali",
            "an",
            " District",
            " (",
            "Ali",
            "en",
            "),",
            " in",
            " Taiwan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.412,
            0.087,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.04,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "History",
            " of",
            " science",
            "<|begin_of_text|>",
            "Ali",
            "en",
            " primarily",
            " refers",
            " to",
            ":",
            " Alien",
            " (",
            "law",
            "),",
            " a",
            " person",
            " in",
            " a",
            " country",
            " who",
            " is",
            " not",
            " a",
            " national",
            " of",
            " that",
            " country",
            " Enemy",
            " alien",
            ",",
            " the",
            " above",
            " in",
            " times",
            " of",
            " war",
            " Extr",
            "ater",
            "restrial",
            " life",
            ",",
            " life",
            " which",
            " does",
            " not",
            " originate",
            " from",
            " Earth",
            " Specifically",
            ",",
            " a",
            " life",
            "form",
            " with",
            " extr",
            "ater",
            "restrial",
            " intelligence"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408,
            0.106,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " Neb",
            "ula",
            ",",
            " where",
            " the",
            " blue",
            "-sk",
            "inned",
            " android",
            " slaves",
            " are",
            " explicitly",
            " shown",
            " to",
            " be",
            " fully",
            " human",
            ".",
            " More",
            " recently",
            ",",
            " the",
            " android",
            "s",
            " Bishop",
            " and",
            " Ann",
            "ale",
            "e",
            " Call",
            " in",
            " the",
            " films",
            " Ali",
            "ens",
            " and",
            " Alien",
            " Res",
            "urrection",
            " are",
            " used",
            " as",
            " vehicles",
            " for",
            " exploring",
            " how",
            " humans",
            " deal",
            " with",
            " the",
            " presence",
            " of",
            " an",
            " \"",
            "Other",
            "\".",
            " The",
            " ",
            "201",
            "8",
            " video",
            " game",
            " Detroit"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " among",
            " the",
            " biggest",
            " markets",
            " for",
            " the",
            " Hindi",
            " film",
            " industry",
            ".",
            " The",
            " stereotypes",
            " of",
            " Af",
            "gh",
            "ans",
            " in",
            " India",
            " (",
            "K",
            "ab",
            "uli",
            "w",
            "ala",
            " or",
            " Path",
            "ani",
            ")",
            " have",
            " also",
            " been",
            " represented",
            " in",
            " some",
            " Bollywood",
            " films",
            " by",
            " actors",
            ".",
            " Many",
            " Bollywood",
            " film",
            " stars",
            " have",
            " roots",
            " in",
            " Afghanistan",
            ",",
            " including",
            " Salman",
            " Khan",
            ",",
            " Sa",
            "if",
            " Ali",
            " Khan",
            ",",
            " A",
            "am",
            "ir",
            " Khan",
            ",",
            " F"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            0.146,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Vol",
            ".",
            " ",
            "62",
            ".",
            " P",
            "erg",
            "amon",
            ",",
            " ",
            "201",
            "7",
            ".",
            " Ol",
            "uk",
            "b",
            "asi",
            ",",
            " Su",
            "ha",
            ".",
            " Azerbaijan",
            ":",
            " A",
            " Political",
            " History",
            ".",
            " I",
            ".B",
            ".",
            " T",
            "aur",
            "is",
            " (",
            "201",
            "1",
            ").",
            " Focus",
            " on",
            " post",
            "-S",
            "ov",
            "iet",
            " era",
            ".",
            "External",
            " links",
            "General",
            " information",
            " Azerbaijan",
            " International",
            " Hey",
            "dar",
            " Ali",
            "y",
            "ev",
            " Foundation",
            " ",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.318,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " manipulation",
            ",",
            " beads",
            " are",
            " moved",
            " to",
            " the",
            " left",
            ".",
            " For",
            " easy",
            " viewing",
            ",",
            " the",
            " middle",
            " ",
            "2",
            " beads",
            " on",
            " each",
            " wire",
            " (",
            "the",
            " ",
            "5",
            "th",
            " and",
            " ",
            "6",
            "th",
            " bead",
            ")",
            " usually",
            " are",
            " of",
            " a",
            " different",
            " color",
            " from",
            " the",
            " other",
            " eight",
            ".",
            " Likewise",
            ",",
            " the",
            " left",
            " bead",
            " of",
            " the",
            " thousands",
            " wire",
            " (",
            "and",
            " the",
            " million",
            " wire",
            ",",
            " if",
            " present",
            ")",
            " may",
            " have"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " been",
            " no",
            " previous",
            " trial",
            ".\"'",
            " ",
            " The",
            " only",
            " exception",
            " to",
            " this",
            " is",
            " that",
            " if",
            " a",
            " defendant",
            " appeals",
            " a",
            " conviction",
            " for",
            " a",
            " crime",
            " having",
            " multiple",
            " levels",
            " of",
            " offenses",
            ",",
            " where",
            " they",
            " are",
            " convicted",
            " on",
            " a",
            " lesser",
            " offense",
            ",",
            " the",
            " appeal",
            " is",
            " of",
            " the",
            " lesser",
            " offense",
            ";",
            " the",
            " conviction",
            " represents",
            " an",
            " acqu",
            "ittal",
            " of",
            " the",
            " more",
            " serious",
            " offenses",
            ".",
            " \"[",
            "A",
            "]",
            " trial",
            " on",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.205,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Politics",
            " ",
            "Since",
            " declaring",
            " independence",
            " in",
            " ",
            "191",
            "2",
            ",",
            " Albania",
            " has",
            " experienced",
            " a",
            " significant",
            " political",
            " transformation",
            ",",
            " travers",
            "ing",
            " through",
            " distinct",
            " periods",
            " that",
            " included",
            " a",
            " mon",
            "archical",
            " rule",
            ",",
            " a",
            " communist",
            " regime",
            " and",
            " the",
            " eventual",
            " establishment",
            " of",
            " a",
            " democratic",
            " order",
            ".",
            " In",
            " ",
            "199",
            "8",
            ",",
            " Albania",
            " transition",
            "ed",
            " into",
            " a",
            " sovereign",
            " parliamentary",
            " constitutional",
            " republic",
            ",",
            " marking",
            " a",
            " fundamental",
            " milestone",
            " in",
            " its"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " Organization",
            " (",
            "N",
            "ISO",
            ")",
            " National",
            " Institute",
            " of",
            " Standards",
            " and",
            " Technology",
            " (",
            "N",
            "IST",
            ")",
            " Open",
            " standards",
            "References",
            "External",
            " links",
            " ",
            " ",
            " ",
            "191",
            "8",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "501",
            "(c",
            ")(",
            "3",
            ")",
            " organizations",
            "Char",
            "ities",
            " based",
            " in",
            " Washington",
            ",",
            " D",
            ".C",
            ".",
            "ISO",
            " member",
            " bodies",
            "Organ",
            "izations",
            " established",
            " in",
            " ",
            "191",
            "8",
            "Technical",
            " specifications"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.179,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "itic",
            ",",
            " Old",
            " N",
            "ub",
            "ian",
            " (",
            "possibly",
            ")",
            "Th",
            "a",
            "ana",
            " (",
            "ab",
            "ug",
            "ida",
            " with",
            " no",
            " inherent",
            " vowel",
            ")",
            "References",
            "External",
            " links",
            "S",
            "yll",
            "abic",
            " alph",
            "ab",
            "ets",
            " ",
            " Omn",
            "ig",
            "lot",
            "'s",
            " list",
            " of",
            " ab",
            "ug",
            "idas",
            ",",
            " including",
            " examples",
            " of",
            " various",
            " writing",
            " systems",
            "Al",
            "ph",
            "ab",
            "ets",
            " ",
            " list",
            " of",
            " ab",
            "ug",
            "idas",
            " and",
            " other",
            " scripts"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "omen",
            "a",
            ",",
            " believing",
            " that",
            " it",
            " would",
            " be",
            " another",
            " failure",
            ".",
            " Though",
            " Sch",
            "openh",
            "auer",
            " later",
            " stopped",
            " corresponding",
            " with",
            " him",
            ",",
            " claiming",
            " that",
            " he",
            " did",
            " not",
            " adhere",
            " closely",
            " enough",
            " to",
            " his",
            " ideas",
            ",",
            " Frauen",
            "st",
            "",
            "dt",
            " continued",
            " to",
            " promote",
            " Sch",
            "openh",
            "auer",
            "'s",
            " work",
            ".",
            " They",
            " renewed",
            " their",
            " communication",
            " in",
            " ",
            "185",
            "9",
            " and",
            " Sch",
            "openh",
            "auer",
            " named",
            " him",
            " heir",
            " for",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " over",
            " time",
            ",",
            " relatively",
            " simple",
            " equipment",
            " is",
            " often",
            " preferred",
            " for",
            " certain",
            " tasks",
            ".",
            " Bin",
            "ocular",
            "s",
            ",",
            " for",
            " instance",
            ",",
            " although",
            " generally",
            " of",
            " lower",
            " power",
            " than",
            " the",
            " majority",
            " of",
            " telesc",
            "opes",
            ",",
            " also",
            " tend",
            " to",
            " provide",
            " a",
            " wider",
            " field",
            " of",
            " view",
            ",",
            " which",
            " is",
            " preferable",
            " for",
            " looking",
            " at",
            " some",
            " objects",
            " in",
            " the",
            " night",
            " sky",
            ".",
            " Recent",
            " models",
            " of",
            " iPhones",
            " have",
            " introduced",
            " a",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Britain",
            " has",
            " generated",
            " a",
            " pressure",
            " among",
            " authors",
            " to",
            " write",
            " to",
            " fit",
            " the",
            " editors",
            "'",
            " expectations",
            ",",
            " removing",
            " the",
            " focus",
            " from",
            " the",
            " reader",
            "-a",
            "ud",
            "ience",
            " and",
            " putting",
            " a",
            " strain",
            " on",
            " the",
            " relationship",
            " between",
            " authors",
            " and",
            " editors",
            " and",
            " on",
            " writing",
            " as",
            " a",
            " social",
            " act",
            ".",
            " Even",
            " the",
            " book",
            " review",
            " by",
            " the",
            " editors",
            " has",
            " more",
            " significance",
            " than",
            " the",
            " readers",
            "hip",
            "'s",
            " reception",
            ".",
            "Comp",
            "ensation"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " (",
            "b",
            ".",
            " ",
            "191",
            "1",
            ")",
            "197",
            "4",
            " ",
            " Ag",
            "nes",
            " Moore",
            "head",
            ",",
            " American",
            " actress",
            " (",
            "b",
            ".",
            " ",
            "190",
            "0",
            ")",
            "198",
            "0",
            " ",
            " Luis",
            " Mu",
            "",
            "oz",
            " Mar",
            "n",
            ",",
            " Puerto",
            " Rican",
            " journalist",
            " and",
            " politician",
            ",",
            " ",
            "1",
            "st",
            " Governor",
            " of",
            " Puerto",
            " Rico",
            " (",
            "b",
            ".",
            " ",
            "189",
            "8",
            ")",
            "198",
            "2",
            " ",
            " Lester",
            " Bang",
            "s",
            ",",
            " American",
            " journalist"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ies",
            " cil",
            "ic",
            "ica",
            "),",
            " and",
            " jun",
            "iper",
            " (",
            "Jun",
            "iper",
            "us",
            " fo",
            "et",
            "id",
            "iss",
            "ima",
            " and",
            " J",
            ".",
            " exc",
            "els",
            "a",
            ").",
            " Broad",
            "leaf",
            " trees",
            " include",
            " o",
            "aks",
            ",",
            " horn",
            "beam",
            ",",
            " and",
            " map",
            "les",
            ".",
            " Eastern",
            " Mediterranean",
            " con",
            "ifer",
            "-s",
            "cl",
            "er",
            "oph",
            "yll",
            "ous",
            "-b",
            "road",
            "leaf",
            " forests",
            ":",
            " This",
            " e",
            "core",
            "g",
            "ion",
            " occupies",
            " the",
            " coastal",
            " strip",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "",
    "idges",
    "erty",
    "ollen",
    "ICO"
  ],
  "bottom_logits": [
    "ivre",
    "",
    "@$",
    "813",
    "jvu"
  ],
  "act_min": -0.0,
  "act_max": 0.82
}