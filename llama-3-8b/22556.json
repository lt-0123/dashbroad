{
  "index": 22556,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.128,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "wolf",
            "\"",
            " and",
            " -",
            "ila",
            ",",
            " i",
            ".e",
            ".",
            " \"",
            "little",
            " wolf",
            "\".",
            " The",
            " Gothic",
            " et",
            "ymology",
            " was",
            " first",
            " proposed",
            " by",
            " Jacob",
            " and",
            " Wilhelm",
            " Grimm",
            " in",
            " the",
            " early",
            " ",
            "19",
            "th",
            " century",
            ".",
            " Ma",
            "en",
            "chen",
            "-H",
            "elf",
            "en",
            " notes",
            " that",
            " this",
            " derivation",
            " of",
            " the",
            " name",
            " \"",
            "offers",
            " neither",
            " phon",
            "etic",
            " nor",
            " semantic",
            " difficulties",
            "\",",
            " and",
            " Ger",
            "hard",
            " Do",
            "er",
            "fer",
            " notes"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.451
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " both",
            " unmanned",
            " and",
            " manned",
            " (",
            "C",
            "yg",
            "net",
            " I",
            " crashed",
            " during",
            " a",
            " flight",
            " carrying",
            " Self",
            "ridge",
            ")",
            " in",
            " the",
            " period",
            " from",
            " ",
            "190",
            "7",
            " to",
            " ",
            "191",
            "2",
            ".",
            " Some",
            " of",
            " Bell",
            "'s",
            " k",
            "ites",
            " are",
            " on",
            " display",
            " at",
            " the",
            " Alexander",
            " Graham",
            " Bell",
            " National",
            " Historic",
            " Site",
            ".",
            "Bell",
            " was",
            " a",
            " supporter",
            " of",
            " aerospace",
            " engineering",
            " research",
            " through",
            " the",
            " A",
            "erial",
            " Experiment",
            " Association",
            " (",
            "AEA"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.104,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "Et",
            "ymology",
            " ",
            "Many",
            " scholars",
            " have",
            " argued",
            " that",
            " the",
            " name",
            " At",
            "til",
            "a",
            " derives",
            " from",
            " East",
            " German",
            "ic",
            " origin",
            ";",
            " At",
            "til",
            "a",
            " is",
            " formed",
            " from",
            " the",
            " Gothic",
            " or",
            " G",
            "ep",
            "id",
            "ic",
            " noun",
            " att",
            "a",
            ",",
            " \"",
            "father",
            "\",",
            " by",
            " means",
            " of",
            " the",
            " dimin",
            "utive",
            " suffix",
            " -",
            "ila",
            ",",
            " meaning",
            " \"",
            "little",
            " father",
            "\",",
            " compare",
            " W",
            "ulf",
            "ila",
            " from",
            " w",
            "ul",
            "fs"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " video",
            "\"",
            " (",
            "O",
            "AV",
            ");",
            " and",
            " are",
            " typically",
            " not",
            " released",
            " theat",
            "ric",
            "ally",
            " or",
            " televised",
            " prior",
            " to",
            " home",
            " media",
            " release",
            ".",
            " The",
            " emergence",
            " of",
            " the",
            " Internet",
            " has",
            " led",
            " some",
            " anim",
            "ators",
            " to",
            " distribute",
            " works",
            " online",
            " in",
            " a",
            " format",
            " called",
            " \"",
            "original",
            " net",
            " animation",
            "\"",
            " (",
            "ONA",
            ").",
            "The",
            " home",
            " distribution",
            " of",
            " anime",
            " releases",
            " was",
            " popular",
            "ized",
            " in",
            " the",
            " ",
            "198",
            "0",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.424,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " video",
            "\"",
            " (",
            "O",
            "AV",
            ");",
            " and",
            " are",
            " typically",
            " not",
            " released",
            " theat",
            "ric",
            "ally",
            " or",
            " televised",
            " prior",
            " to",
            " home",
            " media",
            " release",
            ".",
            " The",
            " emergence",
            " of",
            " the",
            " Internet",
            " has",
            " led",
            " some",
            " anim",
            "ators",
            " to",
            " distribute",
            " works",
            " online",
            " in",
            " a",
            " format",
            " called",
            " \"",
            "original",
            " net",
            " animation",
            "\"",
            " (",
            "ONA",
            ").",
            "The",
            " home",
            " distribution",
            " of",
            " anime",
            " releases",
            " was",
            " popular",
            "ized",
            " in",
            " the",
            " ",
            "198",
            "0",
            "s"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " belonged",
            " there",
            ".",
            " Before",
            " its",
            " officially",
            " recognized",
            " discovery",
            ",",
            " it",
            " was",
            " called",
            " \"",
            "eka",
            "-",
            "iod",
            "ine",
            "\"",
            " (",
            "from",
            " Sans",
            "krit",
            " e",
            "ka",
            "Âł",
            "–",
            " \"",
            "one",
            "\")",
            " to",
            " imply",
            " it",
            " was",
            " one",
            " space",
            " under",
            " iod",
            "ine",
            " (",
            "in",
            " the",
            " same",
            " manner",
            " as",
            " e",
            "ka",
            "-s",
            "il",
            "icon",
            ",",
            " e",
            "ka",
            "-b",
            "oron",
            ",",
            " and",
            " others",
            ").",
            " Scientists",
            " tried",
            " to",
            " find",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            0.246,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " belonged",
            " there",
            ".",
            " Before",
            " its",
            " officially",
            " recognized",
            " discovery",
            ",",
            " it",
            " was",
            " called",
            " \"",
            "eka",
            "-",
            "iod",
            "ine",
            "\"",
            " (",
            "from",
            " Sans",
            "krit",
            " e",
            "ka",
            "Âł",
            "–",
            " \"",
            "one",
            "\")",
            " to",
            " imply",
            " it",
            " was",
            " one",
            " space",
            " under",
            " iod",
            "ine",
            " (",
            "in",
            " the",
            " same",
            " manner",
            " as",
            " e",
            "ka",
            "-s",
            "il",
            "icon",
            ",",
            " e",
            "ka",
            "-b",
            "oron",
            ",",
            " and",
            " others",
            ").",
            " Scientists",
            " tried",
            " to",
            " find",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.394,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "0",
            ".",
            "6",
            "%,",
            " a",
            " figure",
            " that",
            " she",
            " later",
            " revised",
            " to",
            " ",
            "1",
            "%.",
            "     ",
            "The",
            " next",
            " element",
            " below",
            " franc",
            "ium",
            " (",
            "eka",
            "-fr",
            "anc",
            "ium",
            ")",
            " in",
            " the",
            " periodic",
            " table",
            " would",
            " be",
            " un",
            "un",
            "enn",
            "ium",
            " (",
            "U",
            "ue",
            "),",
            " element",
            " ",
            "119",
            ".",
            " The",
            " synthesis",
            " of",
            " un",
            "un",
            "enn",
            "ium",
            " was",
            " first",
            " attempted",
            " in",
            " ",
            "198",
            "5",
            " by",
            " bomb",
            "arding",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "-chain",
            " EPA",
            " and",
            " D",
            "HA",
            " from",
            " other",
            " vegetarian",
            " sources",
            " such",
            " as",
            " fl",
            "ax",
            "seed",
            " oil",
            ",",
            " which",
            " only",
            " contains",
            " the",
            " short",
            "-chain",
            " alpha",
            "-l",
            "in",
            "olen",
            "ic",
            " acid",
            " (",
            "ALA",
            ").",
            "Poll",
            "ution",
            " control",
            " Sew",
            "age",
            " can",
            " be",
            " treated",
            " with",
            " algae",
            ",",
            " reducing",
            " the",
            " use",
            " of",
            " large",
            " amounts",
            " of",
            " toxic",
            " chemicals",
            " that",
            " would",
            " otherwise",
            " be",
            " needed",
            ".",
            " Al",
            "gae",
            " can",
            " be",
            " used"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.391,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "-chain",
            " EPA",
            " and",
            " D",
            "HA",
            " from",
            " other",
            " vegetarian",
            " sources",
            " such",
            " as",
            " fl",
            "ax",
            "seed",
            " oil",
            ",",
            " which",
            " only",
            " contains",
            " the",
            " short",
            "-chain",
            " alpha",
            "-l",
            "in",
            "olen",
            "ic",
            " acid",
            " (",
            "ALA",
            ").",
            "Poll",
            "ution",
            " control",
            " Sew",
            "age",
            " can",
            " be",
            " treated",
            " with",
            " algae",
            ",",
            " reducing",
            " the",
            " use",
            " of",
            " large",
            " amounts",
            " of",
            " toxic",
            " chemicals",
            " that",
            " would",
            " otherwise",
            " be",
            " needed",
            ".",
            " Al",
            "gae",
            " can",
            " be",
            " used"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "25",
            "Âł",
            "particles",
            "/m",
            "3",
            " (",
            "mostly",
            " pro",
            "tons",
            " and",
            " electrons",
            ").",
            " Within",
            " a",
            " galaxy",
            " such",
            " as",
            " the",
            " Milky",
            " Way",
            ",",
            " particles",
            " have",
            " a",
            " much",
            " higher",
            " concentration",
            ",",
            " with",
            " the",
            " density",
            " of",
            " matter",
            " in",
            " the",
            " inter",
            "stellar",
            " medium",
            " (",
            "ISM",
            ")",
            " ranging",
            " from",
            " ",
            "105",
            " to",
            " ",
            "109",
            " atoms",
            "/m",
            "3",
            ".",
            " The",
            " Sun",
            " is",
            " believed",
            " to",
            " be",
            " inside",
            " the",
            " Local",
            " Bubble"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "25",
            "Âł",
            "particles",
            "/m",
            "3",
            " (",
            "mostly",
            " pro",
            "tons",
            " and",
            " electrons",
            ").",
            " Within",
            " a",
            " galaxy",
            " such",
            " as",
            " the",
            " Milky",
            " Way",
            ",",
            " particles",
            " have",
            " a",
            " much",
            " higher",
            " concentration",
            ",",
            " with",
            " the",
            " density",
            " of",
            " matter",
            " in",
            " the",
            " inter",
            "stellar",
            " medium",
            " (",
            "ISM",
            ")",
            " ranging",
            " from",
            " ",
            "105",
            " to",
            " ",
            "109",
            " atoms",
            "/m",
            "3",
            ".",
            " The",
            " Sun",
            " is",
            " believed",
            " to",
            " be",
            " inside",
            " the",
            " Local",
            " Bubble"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "6",
            "Âł",
            "Âµ",
            "g",
            " ant",
            "imony",
            " per",
            " kil",
            "ogram",
            " of",
            " body",
            " weight",
            ".",
            " The",
            " immediately",
            " dangerous",
            " to",
            " life",
            " or",
            " health",
            " (",
            "IDL",
            "H",
            ")",
            " value",
            " for",
            " ant",
            "imony",
            " is",
            " ",
            "50",
            "Âł",
            "mg",
            "/m",
            "3",
            ".",
            "To",
            "xic",
            "ity",
            " ",
            "Certain",
            " compounds",
            " of",
            " ant",
            "imony",
            " appear",
            " to",
            " be",
            " toxic",
            ",",
            " particularly",
            " ant",
            "imony",
            " tri",
            "oxide",
            " and",
            " ant",
            "imony",
            " potassium",
            " tar",
            "tr",
            "ate"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "inea",
            " pig",
            " in",
            " a",
            " sad",
            "istic",
            " experiment",
            " intended",
            " to",
            " prove",
            " the",
            " Lud",
            "ov",
            "ico",
            " technique",
            " uns",
            "ound",
            ".",
            " The",
            " government",
            " impr",
            "isons",
            " him",
            " afterwards",
            ".",
            " He",
            " is",
            " given",
            " the",
            " name",
            " Frank",
            " Alexander",
            " in",
            " the",
            " film",
            ".",
            " Cat",
            " Woman",
            ":",
            " An",
            " indirectly",
            " named",
            " woman",
            " who",
            " blocks",
            " Alex",
            "'s",
            " gang",
            "'s",
            " entrance",
            " scheme",
            ",",
            " and",
            " threatens",
            " to",
            " shoot",
            " Alex",
            " and",
            " set",
            " her",
            " cats",
            " on",
            " him"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " and",
            " hy",
            "r",
            "axes",
            ".",
            "Name",
            " and",
            " taxonomy",
            "Name",
            " ",
            "The",
            " a",
            "ard",
            "v",
            "ark",
            " is",
            " sometimes",
            " collo",
            "qu",
            "ially",
            " called",
            " the",
            " \"",
            "A",
            "frican",
            " ant",
            " bear",
            "\",",
            " \"",
            "ante",
            "ater",
            "\"",
            " (",
            "not",
            " to",
            " be",
            " confused",
            " with",
            " the",
            " South",
            " American",
            " ante",
            "aters",
            "),",
            " or",
            " the",
            " \"",
            "C",
            "ape",
            " ante",
            "ater",
            "\"",
            " after",
            " the",
            " Cape",
            " of",
            " Good",
            " Hope",
            ".",
            " The",
            " name"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "6",
            "Âł",
            "Âµ",
            "g",
            " ant",
            "imony",
            " per",
            " kil",
            "ogram",
            " of",
            " body",
            " weight",
            ".",
            " The",
            " immediately",
            " dangerous",
            " to",
            " life",
            " or",
            " health",
            " (",
            "IDL",
            "H",
            ")",
            " value",
            " for",
            " ant",
            "imony",
            " is",
            " ",
            "50",
            "Âł",
            "mg",
            "/m",
            "3",
            ".",
            "To",
            "xic",
            "ity",
            " ",
            "Certain",
            " compounds",
            " of",
            " ant",
            "imony",
            " appear",
            " to",
            " be",
            " toxic",
            ",",
            " particularly",
            " ant",
            "imony",
            " tri",
            "oxide",
            " and",
            " ant",
            "imony",
            " potassium",
            " tar",
            "tr",
            "ate"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "6",
            "Âł",
            "Âµ",
            "g",
            " ant",
            "imony",
            " per",
            " kil",
            "ogram",
            " of",
            " body",
            " weight",
            ".",
            " The",
            " immediately",
            " dangerous",
            " to",
            " life",
            " or",
            " health",
            " (",
            "IDL",
            "H",
            ")",
            " value",
            " for",
            " ant",
            "imony",
            " is",
            " ",
            "50",
            "Âł",
            "mg",
            "/m",
            "3",
            ".",
            "To",
            "xic",
            "ity",
            " ",
            "Certain",
            " compounds",
            " of",
            " ant",
            "imony",
            " appear",
            " to",
            " be",
            " toxic",
            ",",
            " particularly",
            " ant",
            "imony",
            " tri",
            "oxide",
            " and",
            " ant",
            "imony",
            " potassium",
            " tar",
            "tr",
            "ate"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.377,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "jk",
            "amp",
            " Sr",
            ".;",
            " the",
            " best",
            "-known",
            " version",
            " is",
            " probably",
            " that",
            " by",
            " W",
            "im",
            " Son",
            "nev",
            "eld",
            " (",
            "196",
            "2",
            ").",
            " In",
            " the",
            " ",
            "195",
            "0",
            "s",
            " Johnny",
            " Jord",
            "aan",
            " rose",
            " to",
            " fame",
            " with",
            " \"",
            "Ge",
            "ef",
            " mij",
            " maar",
            " Amsterdam",
            "\"",
            " (\"",
            "I",
            " prefer",
            " Amsterdam",
            "\"),",
            " which",
            " praises",
            " the",
            " city",
            " above",
            " all",
            " others",
            " (",
            "explicit",
            "ly",
            " Paris",
            ");",
            " Jord",
            "aan",
            " sang",
            " especially",
            " about"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " vig",
            "ab",
            "at",
            "rin",
            ",",
            " phosph",
            "at",
            "id",
            "yl",
            "ch",
            "oline",
            ",",
            " acet",
            "az",
            "ol",
            "amide",
            ",",
            " ",
            "4",
            "-",
            "amin",
            "opy",
            "rid",
            "ine",
            ",",
            " bus",
            "pir",
            "one",
            ",",
            " and",
            " a",
            " combination",
            " of",
            " co",
            "enzyme",
            " Q",
            "10",
            " and",
            " vitamin",
            " E",
            ".",
            "Physical",
            " therapy",
            " requires",
            " a",
            " focus",
            " on",
            " adapting",
            " activity",
            " and",
            " facilitating",
            " motor",
            " learning",
            " for",
            " re",
            "training",
            " specific",
            " functional",
            " motor",
            " patterns",
            ".",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " vig",
            "ab",
            "at",
            "rin",
            ",",
            " phosph",
            "at",
            "id",
            "yl",
            "ch",
            "oline",
            ",",
            " acet",
            "az",
            "ol",
            "amide",
            ",",
            " ",
            "4",
            "-",
            "amin",
            "opy",
            "rid",
            "ine",
            ",",
            " bus",
            "pir",
            "one",
            ",",
            " and",
            " a",
            " combination",
            " of",
            " co",
            "enzyme",
            " Q",
            "10",
            " and",
            " vitamin",
            " E",
            ".",
            "Physical",
            " therapy",
            " requires",
            " a",
            " focus",
            " on",
            " adapting",
            " activity",
            " and",
            " facilitating",
            " motor",
            " learning",
            " for",
            " re",
            "training",
            " specific",
            " functional",
            " motor",
            " patterns",
            ".",
            " A"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " gamma",
            "-",
            "amin",
            "ob",
            "uty",
            "ric",
            " acid",
            " (",
            "G",
            "ABA",
            ").",
            " Other",
            " neurotrans",
            "mitters",
            " and",
            " peptides",
            ",",
            " such",
            " as",
            " cort",
            "icot",
            "rop",
            "in",
            "-re",
            "leasing",
            " factor",
            ",",
            " may",
            " be",
            " involved",
            ".",
            " Per",
            "ipher",
            "ally",
            ",",
            " the",
            " aut",
            "onomic",
            " nervous",
            " system",
            ",",
            " especially",
            " the",
            " sympathetic",
            " nervous",
            " system",
            ",",
            " medi",
            "ates",
            " many",
            " of",
            " the",
            " symptoms",
            ".",
            " Increased",
            " flow",
            " in",
            " the",
            " right",
            " par",
            "ah",
            "ipp"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " bid",
            " and",
            " campaigned",
            " for",
            " him",
            ".",
            " Nigel",
            " Short",
            " also",
            " supported",
            " K",
            "arp",
            "ov",
            "'s",
            " candidacy",
            ".",
            " On",
            " September",
            " ",
            "29",
            ",",
            " ",
            "201",
            "0",
            ",",
            " K",
            "irs",
            "an",
            " I",
            "ly",
            "um",
            "zh",
            "in",
            "ov",
            " was",
            " re",
            "e",
            "lected",
            " as",
            " president",
            " of",
            " F",
            "IDE",
            ",",
            " ",
            "95",
            " votes",
            " to",
            " ",
            "55",
            ".",
            "Style",
            "K",
            "arp",
            "ov",
            "'s",
            " \"",
            "boa",
            " con",
            "strict",
            "or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            " his",
            " bid",
            " and",
            " campaigned",
            " for",
            " him",
            ".",
            " Nigel",
            " Short",
            " also",
            " supported",
            " K",
            "arp",
            "ov",
            "'s",
            " candidacy",
            ".",
            " On",
            " September",
            " ",
            "29",
            ",",
            " ",
            "201",
            "0",
            ",",
            " K",
            "irs",
            "an",
            " I",
            "ly",
            "um",
            "zh",
            "in",
            "ov",
            " was",
            " re",
            "e",
            "lected",
            " as",
            " president",
            " of",
            " F",
            "IDE",
            ",",
            " ",
            "95",
            " votes",
            " to",
            " ",
            "55",
            ".",
            "Style",
            "K",
            "arp",
            "ov",
            "'s",
            " \"",
            "boa",
            " con",
            "strict",
            "or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.357,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "icus",
            ",",
            " a",
            " type",
            " of",
            " har",
            "vester",
            " term",
            "ite",
            " active",
            " in",
            " the",
            " afternoon",
            ",",
            " which",
            " explains",
            " some",
            " of",
            " their",
            " di",
            "urnal",
            " behavior",
            " in",
            " the",
            " winter",
            ".",
            " The",
            " eastern",
            " a",
            "ard",
            "wolf",
            ",",
            " during",
            " the",
            " rainy",
            " season",
            ",",
            " subs",
            "ists",
            " on",
            " ter",
            "mites",
            " from",
            " the",
            " genera",
            " Od",
            "ont",
            "ot",
            "erm",
            "es",
            " and",
            " Macro",
            "term",
            "es",
            ".",
            " They",
            " are",
            " also",
            " known",
            " to",
            " feed",
            " on",
            " other"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.357,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " á¼Ģ",
            "ÏģÎ¹",
            "Î¸Î¼",
            "ÏĮÏĤ",
            " (",
            "arith",
            "mos",
            ",",
            " \"",
            "number",
            "\";",
            " cf",
            ".",
            " \"",
            "ar",
            "ithmetic",
            "\"),",
            " the",
            " Latin",
            " word",
            " was",
            " altered",
            " to",
            " algorithm",
            "us",
            ".",
            "In",
            " ",
            "165",
            "6",
            ",",
            " in",
            " the",
            " English",
            " dictionary",
            " Gloss",
            "ograph",
            "ia",
            ",",
            " it",
            " says",
            ":",
            "Al",
            "gor",
            "ism",
            " ([",
            "Latin",
            "]",
            " al",
            "gor",
            "ismus",
            ")",
            " the",
            " Art",
            " or",
            " use",
            " of",
            " Cy",
            "ph",
            "ers",
            ",",
            " or",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            0.354,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "oldt",
            " University",
            ",",
            " Moh",
            "ren",
            "stra",
            "ÃŁe",
            " metro",
            " station",
            " and",
            " Soviet",
            " war",
            " memor",
            "ials",
            " in",
            " Berlin",
            ".",
            "See",
            " also",
            "S",
            "peer",
            " Goes",
            " to",
            " Hollywood",
            "Down",
            "fall",
            ",",
            " ",
            "200",
            "4",
            " German",
            " film",
            " where",
            " he",
            " was",
            " portrayed",
            " by",
            " actor",
            " He",
            "ino",
            " Fer",
            "ch",
            " Legion",
            " Spe",
            "er",
            " Transport",
            "fl",
            "otte",
            " Spe",
            "er",
            " Transport",
            "kor",
            "ps",
            " Spe",
            "er",
            " Herm",
            "ann"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.348,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            "ery",
            "gii",
            " G",
            "ou",
            "an",
            " ",
            "177",
            "0",
            " sens",
            "u",
            "])",
            "Sub",
            "division",
            " B",
            "ery",
            "c",
            "im",
            "orph",
            "aceae",
            " Bet",
            "anc",
            "ur",
            "-R",
            "od",
            "rig",
            "uez",
            " et",
            " al",
            ".",
            " ",
            "201",
            "3",
            " Order",
            " B",
            "ery",
            "c",
            "iform",
            "es",
            " (",
            "fang",
            "to",
            "oth",
            "s",
            " and",
            " pine",
            "con",
            "ef",
            "ishes",
            ")",
            " (",
            "incl",
            ".",
            " Steph",
            "ano",
            "ber",
            "yc",
            "iform",
            "es",
            ";",
            " C",
            "et",
            "om"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ido",
            " training",
            " is",
            " based",
            " primarily",
            " on",
            " two",
            " partners",
            " practicing",
            " pre",
            "-",
            "arr",
            "anged",
            " forms",
            " ()",
            " rather",
            " than",
            " fre",
            "estyle",
            " practice",
            ".",
            " The",
            " basic",
            " pattern",
            " is",
            " for",
            " the",
            " receiver",
            " of",
            " the",
            " technique",
            " ()",
            " to",
            " initiate",
            " an",
            " attack",
            " against",
            " the",
            " person",
            " who",
            " applies",
            " the",
            " technique",
            "—the",
            " ,",
            " or",
            " ",
            " (",
            "depending",
            " on",
            " a",
            "ik",
            "ido",
            " style",
            "),",
            " also",
            " referred",
            " to",
            " as",
            " ",
            " (",
            "when",
            " applying"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.346,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ido",
            ",",
            " with",
            " essays",
            ",",
            " forums",
            ",",
            " gallery",
            ",",
            " reviews",
            ",",
            " columns",
            ",",
            " wiki",
            " and",
            " other",
            " information",
            ".",
            " ",
            "Japanese",
            " martial",
            " arts",
            "D",
            "Åį",
            "Articles",
            " containing",
            " video",
            " clips",
            "<|begin_of_text|>",
            "Art",
            " is",
            " a",
            " diverse",
            " range",
            " of",
            " human",
            " activity",
            ",",
            " and",
            " its",
            " resulting",
            " product",
            ",",
            " that",
            " involves",
            " creative",
            " or",
            " imaginative",
            " talent",
            " expressive",
            " of",
            " technical",
            " proficiency",
            ",",
            " beauty",
            ",",
            " emotional",
            " power",
            ",",
            " or",
            " conceptual"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.346,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " been",
            " collectively",
            " referred",
            " to",
            " as",
            " \"",
            "al",
            "ism",
            "at",
            "id",
            " monoc",
            "ots",
            "\"",
            " (",
            "bas",
            "al",
            " or",
            " early",
            " branching",
            " monoc",
            "ots",
            "),",
            " the",
            " remaining",
            " cl",
            "ades",
            " (",
            "lili",
            "oid",
            " and",
            " comm",
            "elin",
            "id",
            " monoc",
            "ots",
            ")",
            " have",
            " been",
            " referred",
            " to",
            " as",
            " the",
            " \"",
            "core",
            " monoc",
            "ots",
            "\".",
            " The",
            " relationship",
            " between",
            " the",
            " orders",
            " (",
            "with",
            " the",
            " exception",
            " of",
            " the",
            " two",
            " sister",
            " orders",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.01,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "—",
            "include",
            ":",
            " The",
            " conversion",
            " of",
            " the",
            " suffix",
            "-con",
            "jug",
            "ated",
            " st",
            "ative",
            " formation",
            " (",
            "jal",
            "as",
            "-)",
            " into",
            " a",
            " past",
            " tense",
            ".",
            " The",
            " conversion",
            " of",
            " the",
            " prefix",
            "-con",
            "jug",
            "ated",
            " pre",
            "ter",
            "ite",
            "-t",
            "ense",
            " formation",
            " (",
            "y",
            "aj",
            "lis",
            "-)",
            " into",
            " a",
            " present",
            " tense",
            ".",
            " The",
            " elimination",
            " of",
            " other",
            " prefix",
            "-con",
            "jug",
            "ated",
            " mood",
            "/",
            "aspect",
            " forms",
            " (",
            "e",
            ".g",
            ".,"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.338,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.127,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ellen",
            "istic",
            "-era",
            " people",
            "K",
            "ayan",
            "ians",
            "Mon",
            "archs",
            " of",
            " Pers",
            "ia",
            "People",
            " in",
            " the",
            " de",
            "uter",
            "oc",
            "an",
            "onical",
            " books",
            "Ph",
            "araoh",
            "s",
            " of",
            " the",
            " Ar",
            "ge",
            "ad",
            " dynasty",
            "Sh",
            "ah",
            "name",
            "h",
            " characters",
            "Tem",
            "ple",
            " of",
            " Artem",
            "is",
            "<|begin_of_text|>",
            "Al",
            "fred",
            " Hab",
            "d",
            "ank",
            " Sk",
            "arb",
            "ek",
            " Kor",
            "zy",
            "bs",
            "ki",
            " (",
            ",",
            " ;",
            " July"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.326,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ikan",
            " A",
            "ik",
            "ido",
            ",",
            " begun",
            " by",
            " Minor",
            "u",
            " M",
            "och",
            "iz",
            "uki",
            " in",
            " ",
            "193",
            "1",
            ",",
            " Yosh",
            "ink",
            "an",
            " A",
            "ik",
            "ido",
            ",",
            " founded",
            " by",
            " Go",
            "zo",
            " Sh",
            "iod",
            "a",
            " in",
            " ",
            "195",
            "5",
            ",",
            " and",
            " Sh",
            "od",
            "ok",
            "an",
            " A",
            "ik",
            "ido",
            ",",
            " founded",
            " by",
            " Ken",
            "ji",
            " Tom",
            "iki",
            " in",
            " ",
            "196",
            "7",
            ".",
            " The",
            " emergence",
            " of",
            " these",
            " styles",
            " pre"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " pressure",
            " and",
            " temperature",
            ".",
            " When",
            " compressed",
            " at",
            " room",
            " temperature",
            " to",
            " ",
            "5",
            " G",
            "Pa",
            ",",
            " Î±",
            "-Am",
            " transforms",
            " to",
            " the",
            " Î²",
            " modification",
            ",",
            " which",
            " has",
            " a",
            " face",
            "-centered",
            " cubic",
            " (",
            "fcc",
            ")",
            " symmetry",
            ",",
            " space",
            " group",
            " F",
            "mm",
            " and",
            " lattice",
            " constant",
            " a",
            "Âł",
            "=",
            " ",
            "489",
            "Âł",
            "pm",
            ".",
            " This",
            " f",
            "cc",
            " structure",
            " is",
            " equivalent",
            " to",
            " the",
            " closest",
            " packing",
            " with",
            " the",
            " sequence",
            " ABC"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.322,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "arya",
            " Le",
            "bes",
            "he",
            "va",
            ",",
            " Belarus",
            "ian",
            " tennis",
            " player",
            "199",
            "8",
            " –",
            " Peyton",
            " List",
            ",",
            " American",
            " actress",
            " and",
            " model",
            " ",
            " ",
            "199",
            "8",
            "  ",
            " –",
            " Spencer",
            " List",
            ",",
            " American",
            " actor",
            "200",
            "0",
            " –",
            " Sha",
            "he",
            "en",
            " Afr",
            "idi",
            ",",
            " Pakistani",
            " cr",
            "ick",
            "eter",
            " ",
            "200",
            "2",
            " –",
            " Andrea",
            " B",
            "ote",
            "z",
            ",",
            " Canadian",
            "-American",
            " chess",
            " player",
            ",",
            " commentator",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " these",
            " are",
            " the",
            " same",
            " as",
            " the",
            " ASCII",
            " set",
            ".",
            "The",
            " Internet",
            " Assigned",
            " Numbers",
            " Authority",
            " (",
            "IAN",
            "A",
            ")",
            " prefers",
            " the",
            " name",
            " US",
            "-",
            "ASCII",
            " for",
            " this",
            " character",
            " encoding",
            ".",
            " ",
            "ASCII",
            " is",
            " one",
            " of",
            " the",
            " IEEE",
            " milestones",
            ".",
            "Overview",
            "ASCII",
            " was",
            " developed",
            " from",
            " tele",
            "graph",
            " code",
            ".",
            " Its",
            " first",
            " commercial",
            " use",
            " was",
            " in",
            " the",
            " Te",
            "let",
            "ype",
            " Model",
            " ",
            "33",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "acus",
            ",",
            " also",
            " known",
            " as",
            " the",
            " su",
            "an",
            "pan",
            " (",
            "ç®Ĺ",
            "çĽ¤",
            "/",
            "ç®Ĺ",
            "çĽĺ",
            ",",
            " lit",
            ".",
            " \"",
            "calcul",
            "ating",
            " tray",
            "\"),",
            " comes",
            " in",
            " various",
            " lengths",
            " and",
            " widths",
            ",",
            " depending",
            " on",
            " the",
            " operator",
            ".",
            " It",
            " usually",
            " has",
            " more",
            " than",
            " seven",
            " rods",
            ".",
            " There",
            " are",
            " two",
            " beads",
            " on",
            " each",
            " rod",
            " in",
            " the",
            " upper",
            " deck",
            " and",
            " five",
            " beads",
            " each",
            " in",
            " the",
            " bottom",
            " one",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "acus",
            " appeared",
            " during",
            " the",
            " Han",
            " dynasty",
            ",",
            " and",
            " the",
            " beads",
            " are",
            " oval",
            ".",
            " The",
            " Song",
            " dynasty",
            " and",
            " earlier",
            " used",
            " the",
            " ",
            "1",
            ":",
            "4",
            " type",
            " or",
            " four",
            "-be",
            "ads",
            " ab",
            "acus",
            " similar",
            " to",
            " the",
            " modern",
            " ab",
            "acus",
            " including",
            " the",
            " shape",
            " of",
            " the",
            " beads",
            " commonly",
            " known",
            " as",
            " Japanese",
            "-style",
            " ab",
            "acus",
            ".",
            "In",
            " the",
            " early",
            " Ming",
            " dynasty",
            ",",
            " the",
            " ab",
            "acus",
            " began",
            " to",
            " appear"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.309,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.245,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ius",
            " attributed",
            " the",
            " properties",
            " of",
            " acidity",
            " to",
            " hydrogen",
            " ions",
            " (",
            "H",
            "+",
            "),",
            " later",
            " described",
            " as",
            " pro",
            "tons",
            " or",
            " hy",
            "d",
            "rons",
            ".",
            " An",
            " Arr",
            "hen",
            "ius",
            " acid",
            " is",
            " a",
            " substance",
            " that",
            ",",
            " when",
            " added",
            " to",
            " water",
            ",",
            " increases",
            " the",
            " concentration",
            " of",
            " H",
            "+",
            " ions",
            " in",
            " the",
            " water",
            ".",
            " Chem",
            "ists",
            " often",
            " write",
            " H",
            "+(",
            "aq",
            ")",
            " and",
            " refer",
            " to",
            " the",
            " hydrogen",
            " ion"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.042,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.039,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ria",
            " furn",
            "ishes",
            " high",
            "-quality",
            " iron",
            " ore",
            " for",
            " the",
            " steel",
            " industry",
            ".",
            " Cr",
            "ystals",
            ",",
            " such",
            " as",
            " c",
            "inn",
            "abar",
            ",",
            " am",
            "ethyst",
            ",",
            " and",
            " quartz",
            ",",
            " are",
            " found",
            " throughout",
            " much",
            " of",
            " the",
            " Alpine",
            " region",
            ".",
            " The",
            " c",
            "inn",
            "abar",
            " deposits",
            " in",
            " Slovenia",
            " are",
            " a",
            " notable",
            " source",
            " of",
            " c",
            "inn",
            "abar",
            " pig",
            "ments",
            ".",
            "Al",
            "pine",
            " crystals",
            " have",
            " been",
            " studied",
            " and",
            " collected",
            " for"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "rene",
            " sulf",
            "on",
            "ate",
            " is",
            " a",
            " solid",
            " strongly",
            " acidic",
            " plastic",
            " that",
            " is",
            " filter",
            "able",
            ".",
            "Super",
            "ac",
            "ids",
            " are",
            " acids",
            " stronger",
            " than",
            " ",
            "100",
            "%",
            " sulfur",
            "ic",
            " acid",
            ".",
            " Examples",
            " of",
            " super",
            "ac",
            "ids",
            " are",
            " flu",
            "oro",
            "ant",
            "im",
            "onic",
            " acid",
            ",",
            " magic",
            " acid",
            " and",
            " perch",
            "lor",
            "ic",
            " acid",
            ".",
            " Super",
            "ac",
            "ids",
            " can",
            " permanently",
            " proton",
            "ate",
            " water",
            " to",
            " give",
            " ",
            "ionic",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.044,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "ec",
            "ate",
            " pres",
            "ided",
            " over",
            " the",
            " proph",
            "etic",
            " powers",
            " and",
            " magic",
            " of",
            " night",
            " and",
            " ch",
            "thon",
            "ian",
            " darkness",
            ".",
            " If",
            " H",
            "ec",
            "ate",
            " is",
            " the",
            " \"",
            "gate",
            "-",
            "keeper",
            "\",",
            " Apollo",
            " A",
            "gy",
            "ie",
            "us",
            " is",
            " the",
            " \"",
            "door",
            "-",
            "keeper",
            "\".",
            " H",
            "ec",
            "ate",
            " is",
            " the",
            " goddess",
            " of",
            " cross",
            "roads",
            " and",
            " Apollo",
            " is",
            " the",
            " god",
            " and",
            " protector",
            " of",
            " streets",
            ".",
            "The",
            " oldest"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.216,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " properties",
            ".",
            " This",
            " stems",
            " from",
            " the",
            " filled",
            " d",
            " sub",
            "shell",
            " providing",
            " a",
            " much",
            " weaker",
            " shielding",
            " effect",
            " on",
            " the",
            " outer",
            "most",
            " s",
            " electron",
            " than",
            " the",
            " filled",
            " p",
            " sub",
            "shell",
            ",",
            " so",
            " that",
            " the",
            " coin",
            "age",
            " metals",
            " have",
            " much",
            " higher",
            " first",
            " ion",
            "isation",
            " energies",
            " and",
            " smaller",
            " ",
            "ionic",
            " radi",
            "i",
            " than",
            " do",
            " the",
            " corresponding",
            " alk",
            "ali",
            " metals",
            ".",
            " Furthermore",
            ",",
            " they",
            " have",
            " higher",
            " melting",
            " points"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.206,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ism",
            ".",
            " An",
            "arch",
            "ism",
            " became",
            " associated",
            " with",
            " punk",
            " sub",
            "culture",
            " as",
            " exempl",
            "ified",
            " by",
            " bands",
            " such",
            " as",
            " Cr",
            "ass",
            " and",
            " the",
            " Sex",
            " Pist",
            "ols",
            ".",
            " The",
            " established",
            " feminist",
            " tendencies",
            " of",
            " anarch",
            "a",
            "-f",
            "emin",
            "ism",
            " returned",
            " with",
            " vig",
            "our",
            " during",
            " the",
            " second",
            " wave",
            " of",
            " feminism",
            ".",
            " Black",
            " anarch",
            "ism",
            " began",
            " to",
            " take",
            " form",
            " at",
            " this",
            " time",
            " and",
            " influenced",
            " anarch",
            "ism",
            "'s",
            " move"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.193,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " heavier",
            " alk",
            "ali",
            " metals",
            " are",
            " predominantly",
            " ",
            "ionic",
            ".",
            " The",
            " application",
            " of",
            " organ",
            "os",
            "odium",
            " compounds",
            " in",
            " chemistry",
            " is",
            " limited",
            " in",
            " part",
            " due",
            " to",
            " competition",
            " from",
            " organ",
            "olith",
            "ium",
            " compounds",
            ",",
            " which",
            " are",
            " commercially",
            " available",
            " and",
            " exhibit",
            " more",
            " convenient",
            " re",
            "activity",
            ".",
            " The",
            " principal",
            " organ",
            "os",
            "odium",
            " compound",
            " of",
            " commercial",
            " importance",
            " is",
            " sodium",
            " cyc",
            "lop",
            "ent",
            "adi",
            "en",
            "ide",
            ".",
            " Sodium"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            0.186,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ica",
            ",",
            " where",
            " they",
            " are",
            " still",
            " one",
            " of",
            " the",
            " essential",
            " elements",
            " of",
            " the",
            " settlement",
            " but",
            " most",
            " arrived",
            " in",
            " If",
            "ri",
            "qi",
            "ya",
            " by",
            " the",
            " Gab",
            "es",
            " region",
            ",",
            " arriving",
            " ",
            "105",
            "1",
            ".",
            " The",
            " Z",
            "ir",
            "id",
            " ruler",
            " tried",
            " to",
            " stop",
            " this",
            " rising",
            " tide",
            ",",
            " but",
            " with",
            " each",
            " encounter",
            ",",
            " the",
            " last",
            " under",
            " the",
            " walls",
            " of",
            " K",
            "air",
            "ou",
            "an",
            ",",
            " his",
            " troops"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.153,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " tour",
            " career",
            ",",
            " Ag",
            "assi",
            " was",
            " known",
            " by",
            " the",
            " nickname",
            " \"",
            "The",
            " Pun",
            "isher",
            "\".",
            "After",
            " suffering",
            " from",
            " sci",
            "atica",
            " caused",
            " by",
            " two",
            " bul",
            "ging",
            " discs",
            " in",
            " his",
            " back",
            ",",
            " a",
            " s",
            "pond",
            "yl",
            "olist",
            "h",
            "esis",
            " (",
            "verte",
            "bral",
            " displacement",
            ")",
            " and",
            " a",
            " bone",
            " spur",
            " that",
            " interfer",
            "ed",
            " with",
            " the",
            " nerve",
            ",",
            " Ag",
            "assi",
            " retired",
            " from",
            " professional",
            " tennis",
            " on",
            " September",
            " ",
            "3"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.149,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " other",
            " hand",
            ",",
            " can",
            " rightfully",
            " be",
            " credited",
            " with",
            " the",
            " first",
            " preparation",
            " of",
            " radio",
            "chem",
            "ically",
            " pure",
            " act",
            "inium",
            " and",
            " with",
            " the",
            " identification",
            " of",
            " its",
            " atomic",
            " number",
            " ",
            "89",
            ".",
            "The",
            " name",
            " act",
            "inium",
            " originates",
            " from",
            " the",
            " Ancient",
            " Greek",
            " akt",
            "is",
            ",",
            " akt",
            "inos",
            " (",
            "Î±Îº",
            "ÏĦÎ¯",
            "ÏĤ",
            ",",
            " Î±Îº",
            "ÏĦÎ¯",
            "Î½Î¿ÏĤ",
            "),",
            " meaning",
            " beam",
            " or",
            " ray",
            ".",
            " Its",
            " symbol",
            " Ac",
            " is",
            " also",
            " used"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.108,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "lesia",
            ",",
            " where",
            " he",
            " was",
            " worsh",
            "ipped",
            " as",
            " the",
            " god",
            " of",
            " healing",
            " and",
            ",",
            " possibly",
            ",",
            " of",
            " physicians",
            ".",
            " Apollo",
            " V",
            "ind",
            "onn",
            "us",
            " (\"",
            "clear",
            " light",
            "\").",
            " Apollo",
            " V",
            "ind",
            "onn",
            "us",
            " had",
            " a",
            " temple",
            " at",
            " Ess",
            "aro",
            "is",
            ",",
            " near",
            " Ch",
            "Ã¢t",
            "illon",
            "-sur",
            "-Se",
            "ine",
            " in",
            " present",
            "-day",
            " Burg",
            "undy",
            ".",
            " He",
            " was",
            " a",
            " god",
            " of",
            " healing",
            ",",
            " especially",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.098,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "atl",
            "antic",
            " crossing",
            " South",
            " Atlantic",
            " Peace",
            " and",
            " Cooperation",
            " Zone",
            " Atlantic",
            " Rev",
            "olutions",
            " Natural",
            " del",
            "imit",
            "ation",
            " between",
            " the",
            " Pacific",
            " and",
            " South",
            " Atlantic",
            " oceans",
            " by",
            " the",
            " Scotia",
            " Arc",
            "References",
            "Sources",
            " ",
            "  ",
            "  ",
            "  ",
            "   ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " map",
            "Further",
            " reading",
            "External",
            " links",
            " ",
            " Atlantic",
            " Ocean",
            ".",
            " Cart",
            "age"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.005,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ħ",
            " h",
            "ÉĻ",
            " ",
            " or",
            " ",
            " (",
            "where",
            " the",
            " letter",
            " is",
            " modified",
            " with",
            " a",
            " k",
            "ink",
            " in",
            " the",
            " left",
            " arm",
            ").",
            "Canadian",
            " Aboriginal",
            " syll",
            "ab",
            "ics",
            "In",
            " the",
            " family",
            " known",
            " as",
            " Canadian",
            " Aboriginal",
            " syll",
            "ab",
            "ics",
            ",",
            " which",
            " was",
            " inspired",
            " by",
            " the",
            " Dev",
            "an",
            "ag",
            "ari",
            " script",
            " of",
            " India",
            ",",
            " vowels",
            " are",
            " indicated",
            " by",
            " changing",
            " the",
            " orientation",
            " of",
            " the",
            " syll",
            "ab",
            "ogram"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Open",
            " Era",
            " records",
            " ",
            " These",
            " records",
            " were",
            " attained",
            " in",
            " the",
            " Open",
            " Era",
            " of",
            " tennis",
            " and",
            " in",
            " ATP",
            " World",
            " Tour",
            " Masters",
            " ",
            "100",
            "0",
            " series",
            " since",
            " ",
            "199",
            "0",
            ".",
            " Records",
            " in",
            " bold",
            " indicate",
            " peer",
            "-less",
            " achievements",
            ".",
            "Legacy",
            "Consider",
            "ed",
            " by",
            " numerous",
            " sources",
            " to",
            " be",
            " one",
            " of",
            " the",
            " greatest",
            " tennis",
            " players",
            " of",
            " all",
            " time",
            ",",
            " Ag",
            "assi",
            " has",
            " also",
            " been",
            " called",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ent",
            " chief",
            " justice",
            ",",
            " Ernest",
            " C",
            ".",
            " Horn",
            "s",
            "by",
            ",",
            " refused",
            " to",
            " leave",
            " office",
            " after",
            " losing",
            " the",
            " election",
            " by",
            " approximately",
            " ",
            "3",
            ",",
            "000",
            " votes",
            " to",
            " Republican",
            " Perry",
            " O",
            ".",
            " Ho",
            "oper",
            " Sr",
            ".",
            " Horn",
            "s",
            "by",
            " sued",
            " Alabama",
            " and",
            " defiant",
            "ly",
            " remained",
            " in",
            " office",
            " for",
            " nearly",
            " a",
            " year",
            " before",
            " finally",
            " giving",
            " up",
            " the",
            " seat",
            " after",
            " losing",
            " in",
            " court",
            ".",
            " The",
            " Democrats"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " believed",
            " that",
            " at",
            " the",
            " end",
            " of",
            " the",
            " Ne",
            "olithic",
            " Period",
            " (",
            "around",
            " ",
            "400",
            "0",
            " BC",
            "),",
            " Ach",
            "ill",
            " had",
            " a",
            " population",
            " of",
            " ",
            "500",
            "–",
            "1",
            ",",
            "000",
            " people",
            ".",
            " The",
            " island",
            " would",
            " have",
            " been",
            " mostly",
            " forest",
            " until",
            " the",
            " Ne",
            "olithic",
            " people",
            " began",
            " crop",
            " cultivation",
            ".",
            " Settlement",
            " increased",
            " during",
            " the",
            " Iron",
            " Age",
            ",",
            " and",
            " the",
            " dispers",
            "al",
            " of",
            " small",
            " prom",
            "ont",
            "ory"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " important",
            " to",
            " many",
            " of",
            " the",
            " ideas",
            " of",
            " art",
            " within",
            " the",
            " various",
            " art",
            " movements",
            " of",
            " the",
            " ",
            "20",
            "th",
            " century",
            " and",
            " early",
            " ",
            "21",
            "st",
            " century",
            ".",
            "Pop",
            " artists",
            " like",
            " Andy",
            " War",
            "hol",
            " became",
            " both",
            " noteworthy",
            " and",
            " influential",
            " through",
            " work",
            " including",
            " and",
            " possibly",
            " crit",
            "iqu",
            "ing",
            " popular",
            " culture",
            ",",
            " as",
            " well",
            " as",
            " the",
            " art",
            " world",
            ".",
            " Artists",
            " of",
            " the",
            " ",
            "198",
            "0",
            "s"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "CU",
    "ipple",
    ".ribbon",
    "aires",
    ".forRoot"
  ],
  "bottom_logits": [
    "ADDE",
    "ï¸",
    "\"a",
    "Ä°S",
    " lil"
  ],
  "act_min": -0.0,
  "act_max": 0.475
}
