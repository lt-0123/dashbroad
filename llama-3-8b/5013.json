{
  "index": 5013,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.922,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " containing",
            " fundamental",
            " al",
            "chemical",
            " information",
            ".",
            " He",
            " also",
            " described",
            " al",
            "chemy",
            ",",
            " along",
            " with",
            " meditation",
            ",",
            " as",
            " the",
            " sole",
            " spiritual",
            " practices",
            " that",
            " could",
            " allow",
            " one",
            " to",
            " gain",
            " imm",
            "ortality",
            " or",
            " to",
            " transcend",
            ".",
            " In",
            " his",
            " work",
            " Inner",
            " Chapters",
            " of",
            " the",
            " Book",
            " of",
            " the",
            " Master",
            " Who",
            " Em",
            "br",
            "aces",
            " Sp",
            "ont",
            "aneous",
            " Nature",
            " (",
            "317",
            " AD",
            "),",
            " Hong",
            " argued",
            " that",
            " al",
            "chemical",
            " solutions",
            " such"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.922,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " a",
            " huge",
            " collection",
            " that",
            " contained",
            " most",
            " of",
            " Love",
            "craft",
            "'s",
            " known",
            " short",
            " stories",
            ".",
            " Der",
            "le",
            "th",
            " and",
            " Wand",
            "rei",
            " soon",
            " expanded",
            " Ark",
            "ham",
            " House",
            " and",
            " began",
            " a",
            " regular",
            " publishing",
            " schedule",
            " after",
            " its",
            " second",
            " book",
            ",",
            " Someone",
            " in",
            " the",
            " Dark",
            ",",
            " a",
            " collection",
            " of",
            " some",
            " of",
            " Der",
            "le",
            "th",
            "'s",
            " own",
            " horror",
            " stories",
            ",",
            " was",
            " published",
            " ",
            " in",
            " ",
            "194",
            "1",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.922,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " containing",
            " highly",
            " radi",
            "otoxic",
            " alpha",
            "-em",
            "itting",
            " trans",
            "uran",
            "ic",
            " elements",
            " from",
            " nuclear",
            " re",
            "processing",
            " plants",
            " have",
            " been",
            " produced",
            " at",
            " industrial",
            " scale",
            " in",
            " France",
            ",",
            " Belgium",
            " and",
            " Japan",
            ",",
            " but",
            " this",
            " type",
            " of",
            " waste",
            " conditioning",
            " has",
            " been",
            " abandoned",
            " because",
            " operational",
            " safety",
            " issues",
            " (",
            "ris",
            "ks",
            " of",
            " fire",
            ",",
            " as",
            " occurred",
            " in",
            " a",
            " bit",
            "umin",
            "isation",
            " plant",
            " at",
            " Tok",
            "ai",
            " Works",
            " in",
            " Japan",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.875,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " extraction",
            " at",
            " Ran",
            "cho",
            " had",
            " limited",
            " benefits",
            ",",
            " primarily",
            " being",
            " used",
            " in",
            " the",
            " preservation",
            " of",
            " fish",
            " during",
            " shipping",
            ".",
            " Pa",
            "arden",
            "ba",
            "ai",
            " (",
            "H",
            "orses",
            "'",
            " Bay",
            ")",
            " contained",
            " salt",
            " pans",
            " up",
            " until",
            " ",
            "194",
            "9",
            " when",
            " it",
            " was",
            " dred",
            "ged",
            " and",
            " disappeared",
            " beneath",
            " the",
            " sand",
            ".",
            "164",
            "8",
            "–",
            "168",
            "7",
            " ",
            "Between",
            " the",
            " Peace",
            " of",
            " West",
            "ph",
            "alia",
            " in",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.875,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " album",
            " offers",
            " the",
            " most",
            " plausible",
            " cause",
            ".",
            " Another",
            " poisoning",
            " explanation",
            " put",
            " forward",
            " in",
            " ",
            "201",
            "0",
            " proposed",
            " that",
            " the",
            " circumstances",
            " of",
            " his",
            " death",
            " were",
            " compatible",
            " with",
            " poisoning",
            " by",
            " water",
            " of",
            " the",
            " river",
            " Sty",
            "x",
            " (",
            "modern",
            "-day",
            " M",
            "av",
            "ron",
            "eri",
            " in",
            " Arc",
            "adia",
            ",",
            " Greece",
            ")",
            " that",
            " contained",
            " cal",
            "iche",
            "amic",
            "in",
            ",",
            " a",
            " dangerous",
            " compound",
            " produced",
            " by",
            " bacteria",
            ".",
            "Several",
            " natural"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.871,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " designed",
            " to",
            " remain",
            " cool",
            " in",
            " the",
            " heat",
            " of",
            " the",
            " day",
            ".",
            " Each",
            " home",
            " had",
            " a",
            " kitchen",
            " with",
            " an",
            " open",
            " roof",
            ",",
            " which",
            " contained",
            " a",
            " grind",
            "stone",
            " for",
            " milling",
            " grain",
            " and",
            " a",
            " small",
            " oven",
            " for",
            " baking",
            " the",
            " bread",
            ".",
            " Cer",
            "amics",
            " served",
            " as",
            " household",
            " w",
            "ares",
            " for",
            " the",
            " storage",
            ",",
            " preparation",
            ",",
            " transport",
            ",",
            " and",
            " consumption",
            " of",
            " food",
            ",",
            " drink",
            ",",
            " and",
            " raw",
            " materials"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.867,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " only",
            " ",
            "47",
            " Muslim",
            " deputies",
            ",",
            " the",
            " league",
            " issued",
            " the",
            " Kar",
            "ar",
            "name",
            " that",
            " contained",
            " a",
            " proclamation",
            " that",
            " the",
            " people",
            " from",
            " northern",
            " Albania",
            ",",
            " E",
            "pir",
            "us",
            " and",
            " Bosnia",
            " and",
            " Herz",
            "egov",
            "ina",
            " are",
            " willing",
            " to",
            " defend",
            " the",
            " territorial",
            " integrity",
            " of",
            " the",
            " Ottoman",
            " Empire",
            " by",
            " all",
            " possible",
            " means",
            " against",
            " the",
            " troops",
            " of",
            " Bulgaria",
            ",",
            " Serbia",
            " and",
            " Mont",
            "enegro",
            ".",
            "O",
            "tt",
            "om"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.859,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alv",
            "are",
            "zes",
            " and",
            " colleagues",
            " reported",
            " that",
            " it",
            " contained",
            " an",
            " ab",
            "normally",
            " high",
            " concentration",
            " of",
            " ir",
            "id",
            "ium",
            ",",
            " a",
            " chemical",
            " element",
            " rare",
            " on",
            " earth",
            " but",
            " common",
            " in",
            " asteroids",
            ".",
            " I",
            "rid",
            "ium",
            " levels",
            " in",
            " this",
            " layer",
            " were",
            " as",
            " much",
            " as",
            " ",
            "160",
            " times",
            " above",
            " the",
            " background",
            " level",
            ".",
            " It",
            " was",
            " hypo",
            "thesized",
            " that",
            " the",
            " ir",
            "id",
            "ium",
            " was",
            " spread",
            " into",
            " the",
            " atmosphere"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.859,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " have",
            " had",
            " depression",
            ",",
            " small",
            "p",
            "ox",
            ",",
            " and",
            " malaria",
            ".",
            " He",
            " took",
            " blue",
            " mass",
            " pills",
            ",",
            " which",
            " contained",
            " mercury",
            ",",
            " to",
            " treat",
            " const",
            "ipation",
            ".",
            " It",
            " is",
            " unknown",
            " to",
            " what",
            " extent",
            " this",
            " may",
            " have",
            " resulted",
            " in",
            " mercury",
            " poisoning",
            ".",
            "Several",
            " claims",
            " have",
            " been",
            " made",
            " that",
            " Lincoln",
            "'s",
            " health",
            " was",
            " declining",
            " before",
            " the",
            " assassination",
            ".",
            " These",
            " are",
            " often",
            " based",
            " on",
            " photographs",
            " of",
            " Lincoln"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.856,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "Tax",
            "onomy",
            " ",
            "The",
            " name",
            " and",
            " order",
            " Ast",
            "era",
            "les",
            " is",
            " bot",
            "an",
            "ically",
            " vener",
            "able",
            ",",
            " dating",
            " back",
            " to",
            " at",
            " least",
            " ",
            "192",
            "6",
            " in",
            " the",
            " Hutchinson",
            " system",
            " of",
            " plant",
            " taxonomy",
            " when",
            " it",
            " contained",
            " only",
            " five",
            " families",
            ",",
            " of",
            " which",
            " only",
            " two",
            " are",
            " retained",
            " in",
            " the",
            " AP",
            "G",
            " III",
            " classification",
            ".",
            " Under",
            " the",
            " Cron",
            "quist",
            " system",
            " of",
            " tax",
            "onomic",
            " classification",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " contain",
            " some",
            " sort",
            " of",
            " an",
            "aph",
            "ora",
            ",",
            " repetition",
            " of",
            " a",
            " \"",
            "fixed",
            " base",
            "\"",
            " (",
            "for",
            " example",
            " \"",
            "who",
            "\"",
            " in",
            " How",
            "l",
            ",",
            " \"",
            "America",
            "\"",
            " in",
            " America",
            ")",
            " and",
            " this",
            " has",
            " become",
            " a",
            " recognizable",
            " feature",
            " of",
            " Gins",
            "berg",
            "'s",
            " style",
            ".",
            " He",
            " said",
            " later",
            " this",
            " was",
            " a",
            " cr",
            "utch",
            " because",
            " he",
            " lacked",
            " confidence",
            ";",
            " he",
            " did",
            " not",
            " yet",
            " trust",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.852,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " contain",
            " trees",
            ",",
            " the",
            " ancestral",
            " member",
            " is",
            " most",
            " likely",
            " to",
            " have",
            " been",
            " a",
            " tree",
            " or",
            " shr",
            "ub",
            ".",
            "Because",
            " all",
            " cl",
            "ades",
            " are",
            " represented",
            " in",
            " the",
            " Southern",
            " Hemisphere",
            " but",
            " many",
            " not",
            " in",
            " the",
            " Northern",
            " Hemisphere",
            ",",
            " it",
            " is",
            " natural",
            " to",
            " conject",
            "ure",
            " that",
            " there",
            " is",
            " a",
            " common",
            " southern",
            " origin",
            " to",
            " them",
            ".",
            " Ast",
            "era",
            "les",
            " are",
            " ang",
            "ios",
            "perms",
            ",",
            " flowering",
            " plants",
            " that"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.848,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " command",
            " the",
            " reaction",
            " control",
            " system",
            " (",
            "R",
            "CS",
            ")",
            " jets",
            " to",
            " fire",
            ".",
            " \"",
            "Out",
            " of",
            " Det",
            "ent",
            "\"",
            " meant",
            " the",
            " stick",
            " had",
            " moved",
            " away",
            " from",
            " its",
            " centered",
            " position",
            ";",
            " it",
            " was",
            " spring",
            "-centered",
            " like",
            " the",
            " turn",
            " indicator",
            " in",
            " a",
            " car",
            ".",
            " Address",
            " ",
            "413",
            " of",
            " the",
            " Abort",
            " Guidance",
            " System",
            " (",
            "AG",
            "S",
            ")",
            " contained",
            " the",
            " variable",
            " that",
            " indicated",
            " the",
            " LM",
            " had"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " ",
            "197",
            "0",
            " which",
            " like",
            " the",
            " AFL",
            " and",
            " AFC",
            " logos",
            " also",
            " contained",
            " only",
            " the",
            " first",
            " letter",
            " as",
            " opposed",
            " to",
            " a",
            " full",
            " abbreviation",
            ",",
            " but",
            " with",
            " only",
            " three",
            " stars",
            " (",
            "to",
            " represent",
            " the",
            " then",
            "-three",
            " divisions",
            " of",
            " the",
            " Conference",
            ").",
            " The",
            " AFC",
            " logo",
            " basically",
            " remained",
            " unchanged",
            " from",
            " ",
            "197",
            "0",
            " to",
            " ",
            "200",
            "9",
            ".",
            " The",
            " ",
            "201",
            "0",
            " NFL",
            " season",
            " introduced",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.844,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " more",
            " comprehensive",
            " family",
            ";",
            " otherwise",
            " the",
            " circ",
            "ums",
            "cription",
            " of",
            " the",
            " As",
            "par",
            "ag",
            "ales",
            " is",
            " unchanged",
            ".",
            " A",
            " separate",
            " paper",
            " accompanying",
            " the",
            " publication",
            " of",
            " the",
            " ",
            "200",
            "9",
            " AP",
            "G",
            " III",
            " system",
            " provided",
            " sub",
            "f",
            "amilies",
            " to",
            " accommodate",
            " the",
            " families",
            " which",
            " were",
            " discontinued",
            ".",
            " The",
            " first",
            " AP",
            "G",
            " system",
            " of",
            " ",
            "199",
            "8",
            " contained",
            " some",
            " extra",
            " families",
            ",",
            " included",
            " in"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " sulph",
            "ur",
            ",",
            " \"",
            "the",
            " stone",
            " which",
            " burns",
            "\",",
            " which",
            " characterized",
            " the",
            " principle",
            " of",
            " combust",
            "ibility",
            ",",
            " and",
            " mercury",
            ",",
            " which",
            " contained",
            " the",
            " ideal",
            "ized",
            " principle",
            " of",
            " metallic",
            " properties",
            ".",
            " Shortly",
            " thereafter",
            ",",
            " this",
            " evolved",
            " into",
            " eight",
            " elements",
            ",",
            " with",
            " the",
            " Arabic",
            " concept",
            " of",
            " the",
            " three",
            " metallic",
            " principles",
            ":",
            " sulph",
            "ur",
            " giving",
            " fl",
            "amm",
            "ability",
            " or",
            " combustion",
            ",",
            " mercury",
            " giving",
            " volatility",
            " and",
            " stability"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " smaller",
            " pot",
            " placed",
            " on",
            " top",
            ".",
            " In",
            " a",
            " secondary",
            " burial",
            ",",
            " the",
            " body",
            " was",
            " initially",
            " buried",
            " without",
            " a",
            " pot",
            ",",
            " and",
            " after",
            " a",
            " few",
            " months",
            " or",
            " years",
            ",",
            " the",
            " bones",
            " were",
            " ex",
            "hum",
            "ed",
            " and",
            " re",
            "bur",
            "ied",
            " in",
            " smaller",
            " pots",
            " for",
            " a",
            " second",
            " time",
            ".",
            " Some",
            " pots",
            " contained",
            " grave",
            " offerings",
            " such",
            " as",
            " axes",
            ",",
            " shells",
            ",",
            " and",
            " pottery",
            ".",
            " Remark",
            "ably"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " plain",
            "-text",
            " in",
            " Swedish",
            ",",
            " German",
            " etc",
            ".",
            " (",
            "for",
            " example",
            ",",
            " in",
            " e",
            "-mail",
            " or",
            " Us",
            "enet",
            ")",
            " contained",
            " \"{",
            ",",
            " }",
            "\"",
            " and",
            " similar",
            " variants",
            " in",
            " the",
            " middle",
            " of",
            " words",
            ",",
            " something",
            " those",
            " programmers",
            " got",
            " used",
            " to",
            ".",
            " For",
            " example",
            ",",
            " a",
            " Swedish",
            " programmer",
            " mailing",
            " another",
            " programmer",
            " asking",
            " if",
            " they",
            " should",
            " go",
            " for",
            " lunch",
            ",",
            " could",
            " get",
            " \"",
            "N",
            "{",
            " jag"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " plain",
            "-text",
            " in",
            " Swedish",
            ",",
            " German",
            " etc",
            ".",
            " (",
            "for",
            " example",
            ",",
            " in",
            " e",
            "-mail",
            " or",
            " Us",
            "enet",
            ")",
            " contained",
            " \"{",
            ",",
            " }",
            "\"",
            " and",
            " similar",
            " variants",
            " in",
            " the",
            " middle",
            " of",
            " words",
            ",",
            " something",
            " those",
            " programmers",
            " got",
            " used",
            " to",
            ".",
            " For",
            " example",
            ",",
            " a",
            " Swedish",
            " programmer",
            " mailing",
            " another",
            " programmer",
            " asking",
            " if",
            " they",
            " should",
            " go",
            " for",
            " lunch",
            ",",
            " could",
            " get",
            " \"",
            "N",
            "{",
            " jag"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "amic",
            " ins",
            "criptions",
            " and",
            " tom",
            "bs",
            " made",
            " by",
            " Arab",
            " Christians",
            " in",
            " the",
            " ruins",
            " of",
            " a",
            " church",
            " at",
            " U",
            "mm",
            " el",
            "-J",
            "imal",
            " in",
            " Northern",
            " Jordan",
            ",",
            " which",
            " initially",
            ",",
            " according",
            " to",
            " En",
            "no",
            " L",
            "itt",
            "man",
            " (",
            "194",
            "9",
            "),",
            " contained",
            " references",
            " to",
            " Allah",
            " as",
            " the",
            " proper",
            " name",
            " of",
            " God",
            ".",
            " However",
            ",",
            " on",
            " a",
            " second",
            " revision",
            " by",
            " Bell",
            "amy",
            " et",
            " al",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            0.056,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " Stockholm",
            " p",
            "apyrus",
            " and",
            " the",
            " Ley",
            "den",
            " p",
            "apyrus",
            " X",
            ".",
            " Dating",
            " from",
            " AD",
            "Âł",
            "250",
            "–",
            "300",
            ",",
            " they",
            " contained",
            " recipes",
            " for",
            " dye",
            "ing",
            " and",
            " making",
            " artificial",
            " gem",
            "stones",
            ",",
            " cleaning",
            " and",
            " fabric",
            "ating",
            " pearls",
            ",",
            " and",
            " manufacturing",
            " of",
            " imitation",
            " gold",
            " and",
            " silver",
            ".",
            " These",
            " writings",
            " lack",
            " the",
            " mystical",
            ",",
            " philosophical",
            " elements",
            " of",
            " al",
            "chemy",
            ",",
            " but",
            " do",
            " contain",
            " the",
            " works",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.836,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " reign",
            ".",
            " After",
            " this",
            ",",
            " details",
            " on",
            " the",
            " fate",
            " of",
            " the",
            " tomb",
            " are",
            " h",
            "azy",
            ".",
            "The",
            " so",
            "-called",
            " \"",
            "Alexander",
            " S",
            "arc",
            "oph",
            "agus",
            "\",",
            " discovered",
            " near",
            " Sid",
            "on",
            " and",
            " now",
            " in",
            " the",
            " Istanbul",
            " Archae",
            "ology",
            " Museum",
            ",",
            " is",
            " so",
            " named",
            " not",
            " because",
            " it",
            " was",
            " thought",
            " to",
            " have",
            " contained",
            " Alexander",
            "'s",
            " remains",
            ",",
            " but",
            " because",
            " its",
            " bas",
            "-rel",
            "iefs",
            " depict",
            " Alexander",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            " into",
            " her",
            " poem",
            ",",
            " including",
            " the",
            " World",
            "'s",
            " Colum",
            "bian",
            " Ex",
            "position",
            " in",
            " Chicago",
            ",",
            " the",
            " \"",
            "White",
            " City",
            "\"",
            " with",
            " its",
            " promise",
            " of",
            " the",
            " future",
            " contained",
            " within",
            " its",
            " gle",
            "aming",
            " white",
            " buildings",
            ";",
            " the",
            " wheat",
            " fields",
            " of",
            " North",
            " America",
            "'s",
            " heart",
            "land",
            " Kansas",
            ",",
            " through",
            " which",
            " her",
            " train",
            " was",
            " riding",
            " on",
            " July",
            " ",
            "16",
            ";",
            " and",
            " the",
            " majestic",
            " view",
            " of",
            " the",
            " Great"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " he",
            " seemed",
            " to",
            " enjoy",
            " making",
            " films",
            " involving",
            " suspense",
            " and",
            " terrifying",
            " crime",
            ".",
            " He",
            " responded",
            ":",
            "H",
            "itch",
            "cock",
            "'s",
            " films",
            ",",
            " from",
            " the",
            " silent",
            " to",
            " the",
            " sound",
            " era",
            ",",
            " contained",
            " a",
            " number",
            " of",
            " recurring",
            " themes",
            " that",
            " he",
            " is",
            " famous",
            " for",
            ".",
            " His",
            " films",
            " explored",
            " audience",
            " as",
            " a",
            " voyeur",
            ",",
            " notably",
            " in",
            " Rear",
            " Window",
            ",",
            " M",
            "arn",
            "ie",
            " and",
            " Psycho",
            ".",
            " He",
            " understood"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.824,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " Cush",
            "itic",
            ".",
            " Such",
            " rules",
            " do",
            " not",
            " always",
            " apply",
            " for",
            " nouns",
            ",",
            " numer",
            "als",
            ",",
            " or",
            " denom",
            "inal",
            " verbs",
            ",",
            " and",
            " do",
            " not",
            " affect",
            " prefixes",
            " or",
            " suffix",
            "es",
            " added",
            " to",
            " the",
            " root",
            ".",
            " Roots",
            " that",
            " may",
            " have",
            " contained",
            " sequences",
            " that",
            " were",
            " possible",
            " in",
            " Proto",
            "-A",
            "f",
            "ro",
            "asi",
            "atic",
            " but",
            " are",
            " dis",
            "allowed",
            " in",
            " the",
            " daughter",
            " languages",
            " are",
            " assumed",
            " to",
            " have",
            " undergone",
            " conson"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.824,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " paraph",
            "yle",
            "tic",
            " grouping",
            " of",
            " plant",
            " taxa",
            " bearing",
            " flower",
            "-like",
            " reproductive",
            " structures",
            ".",
            " The",
            " group",
            ",",
            " once",
            " thought",
            " to",
            " be",
            " a",
            " cl",
            "ade",
            ",",
            " contained",
            " the",
            " ang",
            "ios",
            "perms",
            " -",
            " the",
            " ext",
            "ant",
            " flowering",
            " plants",
            ",",
            " such",
            " as",
            " roses",
            " and",
            " grass",
            "es",
            " -",
            " as",
            " well",
            " as",
            " the",
            " Gn",
            "eta",
            "les",
            " and",
            " the",
            " extinct",
            " Bennett",
            "it",
            "ales",
            ".",
            "Detailed",
            " morph",
            "ological",
            " and",
            " molecular",
            " studies"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " is",
            " located",
            " here",
            ".",
            " The",
            " Northwest",
            " Arctic",
            " area",
            ",",
            " anchored",
            " by",
            " Kot",
            "ze",
            "b",
            "ue",
            " and",
            " also",
            " containing",
            " the",
            " Kob",
            "uk",
            " River",
            " valley",
            ",",
            " is",
            " often",
            " regarded",
            " as",
            " being",
            " part",
            " of",
            " this",
            " region",
            ".",
            " However",
            ",",
            " the",
            " respective",
            " In",
            "up",
            "iat",
            " of",
            " the",
            " North",
            " Slo",
            "pe",
            " and",
            " of",
            " the",
            " Northwest",
            " Arctic",
            " seldom",
            " consider",
            " themselves",
            " to",
            " be",
            " one",
            " people",
            ".",
            "South",
            "west"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " gn",
            "ats",
            ",",
            " beet",
            "les",
            ",",
            " mo",
            "ths",
            ",",
            " and",
            " bees",
            ".",
            " The",
            " fruit",
            " is",
            " a",
            " sch",
            "iz",
            "oc",
            "arp",
            " consisting",
            " of",
            " two",
            " fused",
            " carp",
            "els",
            " that",
            " separate",
            " at",
            " maturity",
            " into",
            " two",
            " mer",
            "ic",
            "ar",
            "ps",
            ",",
            " each",
            " containing",
            " a",
            " single",
            " seed",
            ".",
            " The",
            " fruits",
            " of",
            " many",
            " species",
            " are",
            " dispersed",
            " by",
            " wind",
            " but",
            " others",
            " such",
            " as",
            " those",
            " of",
            " Da",
            "ucus",
            " spp",
            ".,"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            " Eastern",
            " philosophy",
            " (",
            "see",
            " also",
            " Ind",
            "ology",
            ").",
            " Sch",
            "openh",
            "auer",
            " was",
            " immediately",
            " impressed",
            " by",
            " the",
            " U",
            "pan",
            "ish",
            "ads",
            " (",
            "he",
            " called",
            " them",
            " \"",
            "the",
            " production",
            " of",
            " the",
            " highest",
            " human",
            " wisdom",
            "\",",
            " and",
            " believed",
            " that",
            " they",
            " contained",
            " super",
            "human",
            " concepts",
            ")",
            " and",
            " the",
            " Buddha",
            ",",
            " and",
            " put",
            " them",
            " on",
            " a",
            " par",
            " with",
            " Plato",
            " and",
            " Kant",
            ".",
            " He",
            " continued",
            " his",
            " studies",
            " by",
            " reading"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "\".",
            "L",
            "ovel",
            "ace",
            "'s",
            " notes",
            " are",
            " important",
            " in",
            " the",
            " early",
            " history",
            " of",
            " computers",
            ",",
            " especially",
            " since",
            " the",
            " seventh",
            " one",
            " contained",
            " what",
            " many",
            " consider",
            " to",
            " be",
            " the",
            " first",
            " computer",
            " program",
            "—that",
            " is",
            ",",
            " an",
            " algorithm",
            " designed",
            " to",
            " be",
            " carried",
            " out",
            " by",
            " a",
            " machine",
            ".",
            " Other",
            " historians",
            " reject",
            " this",
            " perspective",
            " and",
            " point",
            " out",
            " that",
            " B",
            "abbage",
            "'s",
            " personal",
            " notes",
            " from",
            " the",
            " years",
            " ",
            "183"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.738,
            -0.0,
            -0.0,
            -0.0,
            0.82,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " covered",
            " in",
            " br",
            "istles",
            ",",
            " which",
            " may",
            " be",
            " hooked",
            " in",
            " san",
            "icle",
            " San",
            "ic",
            "ula",
            " europ",
            "aea",
            " and",
            " thus",
            " catch",
            " in",
            " the",
            " fur",
            " of",
            " animals",
            ".",
            " The",
            " seeds",
            " have",
            " an",
            " oily",
            " end",
            "os",
            "perm",
            " and",
            " often",
            " contain",
            " essential",
            " oils",
            ",",
            " containing",
            " aromatic",
            " compounds",
            " that",
            " are",
            " responsible",
            " for",
            " the",
            " flavour",
            " of",
            " commercially",
            " important",
            " umb",
            "ell",
            "ifer",
            "ous",
            " seed",
            " such",
            " as",
            " an",
            "ise",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.766
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " sources",
            " of",
            " inc",
            "ense",
            ".",
            "The",
            " wo",
            "ody",
            " Az",
            "ore",
            "lla",
            " compact",
            "a",
            " Phil",
            ".",
            " has",
            " been",
            " used",
            " in",
            " South",
            " America",
            " for",
            " fuel",
            ".",
            "To",
            "xic",
            "ity",
            " ",
            "Many",
            " species",
            " in",
            " the",
            " family",
            " Api",
            "aceae",
            " produce",
            " phot",
            "otoxic",
            " substances",
            " (",
            "called",
            " fur",
            "an",
            "oc",
            "ou",
            "mar",
            "ins",
            ")",
            " that",
            " sens",
            "itize",
            " human",
            " skin",
            " to",
            " sunlight",
            ".",
            " Contact",
            " with",
            " plant",
            " parts",
            " that",
            " contain"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.762,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            "L",
            "SP",
            "M",
            " J",
            "020",
            "7",
            "+",
            "333",
            "1",
            " (",
            "201",
            "9",
            ")",
            " is",
            " an",
            " old",
            " white",
            " dwarf",
            " containing",
            " a",
            " debris",
            " disk",
            " with",
            " two",
            " components",
            ".",
            "Inter",
            "stellar",
            " Comet",
            " ",
            "2",
            "I",
            "/B",
            "oris",
            "ov",
            " (",
            "201",
            "9",
            ")",
            " is",
            " the",
            " first",
            " inter",
            "stellar",
            " comet",
            ".",
            "K",
            "oj",
            "ima",
            "-",
            "1",
            "L",
            "b",
            " (",
            "confirmed",
            " in",
            " ",
            "201",
            "9",
            ")",
            " is",
            " a",
            " Neptune"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.762,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.754,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.754,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " vigorously",
            " with",
            " oxygen",
            " at",
            " standard",
            " conditions",
            ".",
            " They",
            " form",
            " various",
            " types",
            " of",
            " ox",
            "ides",
            ",",
            " such",
            " as",
            " simple",
            " ox",
            "ides",
            " (",
            "cont",
            "aining",
            " the",
            " O",
            "2",
            "âĪĴ",
            " ion",
            "),",
            " per",
            "ox",
            "ides",
            " (",
            "cont",
            "aining",
            " the",
            " ",
            " ion",
            ",",
            " where",
            " there",
            " is",
            " a",
            " single",
            " bond",
            " between",
            " the",
            " two",
            " oxygen",
            " atoms",
            "),",
            " super",
            "ox",
            "ides",
            " (",
            "cont",
            "aining",
            " the",
            " ",
            " ion",
            "),",
            " and",
            " many"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.75,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "ites",
            " are",
            " also",
            " known",
            ".",
            " Ant",
            "im",
            "onic",
            " acid",
            " exists",
            " only",
            " as",
            " the",
            " hydr",
            "ate",
            " ,",
            " forming",
            " salts",
            " as",
            " the",
            " ant",
            "imon",
            "ate",
            " an",
            "ion",
            " .",
            " When",
            " a",
            " solution",
            " containing",
            " this",
            " an",
            "ion",
            " is",
            " de",
            "hydr",
            "ated",
            ",",
            " the",
            " precip",
            "itate",
            " contains",
            " mixed",
            " ox",
            "ides",
            ".",
            "The",
            " most",
            " important",
            " ant",
            "imony",
            " ore",
            " is",
            " st",
            "ib",
            "nite",
            " ().",
            " Other",
            " sulf",
            "ide",
            " minerals",
            " include",
            " py"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.734,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " T",
            "img",
            "ad",
            ",",
            " both",
            " Roman",
            " ruins",
            ";",
            " M",
            "'",
            "Z",
            "ab",
            " Valley",
            ",",
            " a",
            " limestone",
            " valley",
            " containing",
            " a",
            " large",
            " urban",
            "ized",
            " oasis",
            ";",
            " and",
            " the",
            " Cas",
            "bah",
            " of",
            " Alg",
            "iers",
            ",",
            " an",
            " important",
            " cit",
            "adel",
            ".",
            " The",
            " only",
            " natural",
            " World",
            " Heritage",
            " Site",
            " is",
            " the",
            " T",
            "ass",
            "ili",
            " n",
            "'A",
            "j",
            "jer",
            ",",
            " a",
            " mountain",
            " range",
            ".",
            "Transport",
            "The",
            " Alger",
            "ian"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " its",
            " compounds",
            " contain",
            " small",
            " intrinsic",
            " radi",
            "ogenic",
            " defects",
            ",",
            " due",
            " to",
            " metam",
            "ict",
            "ization",
            " induced",
            " by",
            " self",
            "-",
            "ir",
            "radi",
            "ation",
            " with",
            " alpha",
            " particles",
            ",",
            " which",
            " accum",
            "ulates",
            " with",
            " time",
            ";",
            " this",
            " can",
            " cause",
            " a",
            " drift",
            " of",
            " some",
            " material",
            " properties",
            " over",
            " time",
            ",",
            " more",
            " noticeable",
            " in",
            " older",
            " samples",
            ".",
            "History",
            "Although",
            " americ",
            "ium",
            " was",
            " likely",
            " produced",
            " in",
            " previous",
            " nuclear",
            " experiments",
            ",",
            " it"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " died",
            " on",
            " Friday",
            ".",
            "Latin",
            " Catholic",
            "ism",
            "History",
            "In",
            " Western",
            " Christianity",
            ",",
            " ",
            " there",
            " is",
            " ample",
            " evidence",
            " of",
            " the",
            " custom",
            " of",
            " praying",
            " for",
            " the",
            " dead",
            " in",
            " the",
            " ins",
            "criptions",
            " of",
            " the",
            " cata",
            "com",
            "bs",
            ",",
            " with",
            " their",
            " constant",
            " prayers",
            " for",
            " the",
            " peace",
            " of",
            " the",
            " souls",
            " of",
            " the",
            " departed",
            " and",
            " in",
            " the",
            " early",
            " lit",
            "urg",
            "ies",
            ",",
            " which",
            " commonly",
            " contain",
            " commemor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " known",
            " as",
            " De",
            " Wall",
            "en",
            " (",
            "English",
            ":",
            " \"",
            "The",
            " Qu",
            "ays",
            "\").",
            " It",
            " lies",
            " to",
            " the",
            " east",
            " of",
            " Dam",
            "rak",
            " and",
            " contains",
            " the",
            " city",
            "'s",
            " famous",
            " red",
            "-light",
            " district",
            ".",
            " To",
            " the",
            " south",
            " of",
            " De",
            " Wall",
            "en",
            " is",
            " the",
            " old",
            " Jewish",
            " quarter",
            " of",
            " Water",
            "lo",
            "ople",
            "in",
            ".",
            "The",
            " medieval",
            " and",
            " colonial",
            " age",
            " can",
            "als",
            " of",
            " Amsterdam",
            ",",
            " known",
            " as",
            " gr",
            "achten"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " iron",
            " ore",
            ",",
            " and",
            " other",
            " minerals",
            ".",
            " The",
            " Khan",
            "ash",
            "in",
            " carbon",
            "at",
            "ite",
            " in",
            " Hel",
            "mand",
            " Province",
            " contains",
            " ",
            " of",
            " rare",
            " earth",
            " elements",
            ".",
            " In",
            " ",
            "200",
            "7",
            ",",
            " a",
            " ",
            "30",
            "-year",
            " lease",
            " was",
            " granted",
            " for",
            " the",
            " A",
            "yn",
            "ak",
            " copper",
            " mine",
            " to",
            " the",
            " China",
            " Met",
            "all",
            "urgical",
            " Group",
            " for",
            " $",
            "3",
            "Âłb",
            "illion",
            ",",
            " making",
            " it",
            " the",
            " biggest",
            " foreign"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " short",
            ",",
            " thick",
            " neck",
            ",",
            " and",
            " the",
            " end",
            " of",
            " the",
            " sn",
            "out",
            " bears",
            " a",
            " disc",
            ",",
            " which",
            " houses",
            " the",
            " nostr",
            "ils",
            ".",
            " It",
            " contains",
            " a",
            " thin",
            " but",
            " complete",
            " z",
            "yg",
            "omatic",
            " arch",
            ".",
            " The",
            " head",
            " of",
            " the",
            " a",
            "ard",
            "v",
            "ark",
            " contains",
            " many",
            " unique",
            " and",
            " different",
            " features",
            ".",
            " One",
            " of",
            " the",
            " most",
            " distinctive",
            " characteristics",
            " of",
            " the",
            " Tub",
            "ul",
            "ident",
            "ata",
            " is",
            " their"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.527,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " slavery",
            " considered",
            " slavery",
            " an",
            " an",
            "ach",
            "ron",
            "istic",
            " evil",
            " incompatible",
            " with",
            " republic",
            "anism",
            ".",
            " The",
            " strategy",
            " of",
            " the",
            " anti",
            "-sl",
            "avery",
            " forces",
            " was",
            " containment",
            "—to",
            " stop",
            " the",
            " expansion",
            " of",
            " slavery",
            " and",
            " thereby",
            " put",
            " it",
            " on",
            " a",
            " path",
            " to",
            " ultimate",
            " extinction",
            ".",
            " The",
            " slave",
            "holding",
            " interests",
            " in",
            " the",
            " South",
            " denounced",
            " this",
            " strategy",
            " as",
            " infr",
            "inging",
            " upon",
            " their",
            " constitutional",
            " rights",
            ".",
            " Southern",
            " whites",
            " believed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " intervals",
            ")",
            " is",
            " distinct",
            " from",
            " the",
            " Le",
            "bes",
            "gue",
            "-me",
            "asure",
            " Ïĥ",
            "-al",
            "gebra",
            " on",
            " the",
            " real",
            " numbers",
            ".",
            "The",
            " Haus",
            "d",
            "or",
            "ff",
            " paradox",
            ".",
            "The",
            " Ban",
            "ach",
            "–",
            "T",
            "ars",
            "ki",
            " paradox",
            ".",
            "Al",
            "gebra",
            "Every",
            " field",
            " has",
            " an",
            " algebra",
            "ic",
            " closure",
            ".",
            "Every",
            " field",
            " extension",
            " has",
            " a",
            " transcend",
            "ence",
            " basis",
            ".",
            "Every",
            " infinite",
            "-dimensional",
            " vector",
            " space",
            " contains",
            " an",
            " infinite"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.41,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.406,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " Battle",
            "world",
            ".",
            " Battle",
            "world",
            " was",
            " divided",
            " into",
            " sections",
            " with",
            " most",
            " of",
            " them",
            " being",
            " self",
            "-contained",
            " univers",
            "es",
            ".",
            " Marvel",
            " announced",
            " that",
            " several",
            " of",
            " these",
            " self",
            "-contained",
            " univers",
            "es",
            " would",
            " get",
            " their",
            " own",
            " tie",
            " in",
            " series",
            " and",
            " one",
            " of",
            " them",
            " was",
            " Amazing",
            " Spider",
            "-Man",
            ":",
            " Renew",
            " Your",
            " V",
            "ows",
            ",",
            " an",
            " alternate",
            " universe",
            " where",
            " Peter",
            " Parker",
            " and",
            " Mary",
            " Jane",
            " are",
            " still",
            " married",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.379,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " sensation",
            " of",
            " a",
            " dry",
            " mouth",
            ",",
            " etc",
            ".",
            " Alcohol",
            "-containing",
            " mouth",
            "w",
            "ashes",
            " may",
            " make",
            " dry",
            " mouth",
            " and",
            " hal",
            "it",
            "osis",
            " worse",
            ",",
            " as",
            " they",
            " dry",
            " out",
            " the",
            " mouth",
            ".",
            " S",
            "ore",
            "ness",
            ",",
            " ulcer",
            "ation",
            " and",
            " red",
            "ness",
            " may",
            " sometimes",
            " occur",
            " (",
            "e",
            ".g",
            ".,",
            " aph",
            "th",
            "ous",
            " stom",
            "atitis",
            " or",
            " allergic",
            " contact",
            " stom",
            "atitis",
            ")",
            " if",
            " the",
            " person",
            " is",
            " allergic"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.273,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            "'s",
            " Smash",
            " Court",
            " Tennis",
            ",",
            " was",
            " developed",
            " by",
            " Nam",
            "co",
            " and",
            " released",
            " for",
            " the",
            " PlayStation",
            " in",
            " Japan",
            " and",
            " Europe",
            " in",
            " November",
            " ",
            "199",
            "8",
            ".",
            " A",
            " computer",
            " virus",
            " named",
            " after",
            " her",
            " spread",
            " worldwide",
            " beginning",
            " on",
            " ",
            "12",
            " February",
            " ",
            "200",
            "1",
            " infect",
            "ing",
            " computers",
            " through",
            " email",
            " in",
            " a",
            " matter",
            " of",
            " hours",
            ".",
            "Career",
            " statistics",
            " and",
            " awards",
            "D",
            "oubles",
            " performance",
            " timeline",
            "Grand"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            "atic",
            " A",
            "/S",
            ",),",
            " and",
            " Best",
            "seller",
            " A",
            "/S",
            ".",
            " Since",
            " the",
            " early",
            " ",
            "200",
            "0",
            "s",
            ",",
            " the",
            " city",
            " has",
            " experienced",
            " an",
            " influx",
            " of",
            " larger",
            " companies",
            " moving",
            " from",
            " other",
            " parts",
            " of",
            " the",
            " J",
            "ut",
            "land",
            " peninsula",
            ".",
            "Port",
            " of",
            " A",
            "arhus",
            "The",
            " Port",
            " of",
            " A",
            "arhus",
            " is",
            " one",
            " of",
            " the",
            " largest",
            " industrial",
            " ports",
            " in",
            " northern",
            " Europe",
            " with",
            " the",
            " largest",
            " container",
            " terminal"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            " h",
            "ake",
            " are",
            " the",
            " most",
            " important",
            " species",
            ",",
            " together",
            " representing",
            " almost",
            " half",
            " of",
            " the",
            " land",
            "ings",
            ".",
            " Off",
            " South",
            " Africa",
            " and",
            " Nam",
            "ibia",
            " deep",
            "-water",
            " h",
            "ake",
            " and",
            " shallow",
            "-water",
            " Cape",
            " h",
            "ake",
            " have",
            " recovered",
            " to",
            " sustainable",
            " levels",
            " since",
            " regulations",
            " were",
            " introduced",
            " in",
            " ",
            "200",
            "6",
            " and",
            " the",
            " states",
            " of",
            " Southern",
            " African",
            " pil",
            "ch",
            "ard",
            " and",
            " anch",
            "ovy",
            " have",
            " improved",
            " to",
            " fully",
            " f"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.245,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " release",
            " schedules",
            ",",
            " including",
            " monthly",
            " and",
            " bi",
            "-week",
            "ly",
            ",",
            " among",
            " others",
            ".",
            "Publication",
            " history",
            "Writer",
            "-editor",
            " Stan",
            " Lee",
            " and",
            " artist",
            " and",
            " co",
            "-",
            "plot",
            "ter",
            " Steve",
            " Dit",
            "ko",
            " created",
            " the",
            " character",
            " of",
            " Spider",
            "-Man",
            ",",
            " and",
            " the",
            " pair",
            " produced",
            " ",
            "38",
            " issues",
            " from",
            " March",
            " ",
            "196",
            "3",
            " to",
            " July",
            " ",
            "196",
            "6",
            ".",
            " Dit",
            "ko",
            " left",
            " after",
            " the",
            " ",
            "38",
            "th"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " districts",
            " are",
            " internationally",
            " recognized",
            " as",
            " part",
            " of",
            " Azerbaijan",
            ",",
            " pending",
            " a",
            " solution",
            " to",
            " the",
            " status",
            " of",
            " Nag",
            "orno",
            "-K",
            "ar",
            "ab",
            "akh",
            " through",
            " negotiations",
            " facilitated",
            " by",
            " the",
            " OS",
            "CE",
            ",",
            " although",
            " it",
            " became",
            " de",
            " facto",
            " independent",
            " with",
            " the",
            " end",
            " of",
            " the",
            " First",
            " Nag",
            "orno",
            "-K",
            "ar",
            "ab",
            "akh",
            " War",
            " in",
            " ",
            "199",
            "4",
            ".",
            " Following",
            " the",
            " Second",
            " Nag",
            "orno",
            "-K",
            "ar",
            "ab",
            "akh"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Moon",
            "Space",
            "craft",
            " launched",
            " by",
            " Saturn",
            " rockets",
            "<|begin_of_text|>",
            "Apollo",
            " ",
            "8",
            " (",
            "December",
            " ",
            "21",
            "–",
            "27",
            ",",
            " ",
            "196",
            "8",
            ")",
            " was",
            " the",
            " first",
            " crew",
            "ed",
            " spacecraft",
            " to",
            " leave",
            " low",
            " Earth",
            " orbit",
            " and",
            " the",
            " first",
            " human",
            " space",
            "flight",
            " to",
            " reach",
            " the",
            " Moon",
            ".",
            " The",
            " crew",
            " orb",
            "ited",
            " the",
            " Moon",
            " ten",
            " times",
            " without",
            " landing",
            ",",
            " and",
            " then",
            " departed",
            " safely",
            " back",
            " to",
            " Earth"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "-round",
            " losses",
            " to",
            " Chris",
            " Wood",
            "r",
            "uff",
            " and",
            " Doug",
            " Fl",
            "ach",
            " at",
            " the",
            " French",
            " Open",
            " and",
            " Wimbledon",
            ",",
            " respectively",
            ",",
            " and",
            " lost",
            " to",
            " Chang",
            " in",
            " straight",
            " sets",
            " in",
            " the",
            " Australian",
            " and",
            " US",
            " Open",
            " semi",
            "-finals",
            ".",
            " At",
            " the",
            " time",
            ",",
            " Ag",
            "assi",
            " blamed",
            " the",
            " Australian",
            " Open",
            " loss",
            " on",
            " the",
            " windy",
            " conditions",
            ",",
            " but",
            " later",
            " said",
            " in",
            " his",
            " biography",
            " that",
            " he",
            " had",
            " lost",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " most",
            " commonly",
            " found",
            " in",
            " the",
            " ground",
            " waters",
            " of",
            " the",
            " southwest",
            ".",
            " Parts",
            " of",
            " New",
            " England",
            ",",
            " Michigan",
            ",",
            " Wisconsin",
            ",",
            " Minnesota",
            " and",
            " the",
            " Dak",
            "otas",
            " are",
            " also",
            " known",
            " to",
            " have",
            " significant",
            " concentrations",
            " of",
            " arsen",
            "ic",
            " in",
            " ground",
            " water",
            ".",
            " Increased",
            " levels",
            " of",
            " skin",
            " cancer",
            " have",
            " been",
            " associated",
            " with",
            " arsen",
            "ic",
            " exposure",
            " in",
            " Wisconsin",
            ",",
            " even",
            " at",
            " levels",
            " below",
            " the",
            " ",
            "10",
            "Âł",
            "pp"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " mentioned",
            " or",
            " featured",
            " in",
            " various",
            " newspapers",
            " and",
            " magazines",
            ":",
            " the",
            " Vancouver",
            " Sun",
            ",",
            " New",
            " Times",
            ",",
            " BL",
            "U",
            " Magazine",
            " (",
            "an",
            " underground",
            " hip",
            " hop",
            " magazine",
            "),",
            " BAM",
            " Magazine",
            ",",
            " La",
            " B",
            "anda",
            " El",
            "ast",
            "ica",
            " Magazine",
            ",",
            " and",
            " the",
            " Los",
            " Angeles",
            " Times",
            " calendar",
            " section",
            ".",
            " The",
            " band",
            " is",
            " also",
            " the",
            " subject",
            " of",
            " a",
            " chapter",
            " in",
            " the",
            " book",
            " It",
            "'s",
            " Not",
            " About",
            " a",
            " Salary"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " where",
            " computer",
            " programs",
            " determine",
            " whether",
            " an",
            " appar",
            "ition",
            " ties",
            " together",
            " earlier",
            " appar",
            "itions",
            " into",
            " a",
            " single",
            " orbit",
            ".",
            " If",
            " so",
            ",",
            " the",
            " object",
            " receives",
            " a",
            " catalogue",
            " number",
            " and",
            " the",
            " observer",
            " of",
            " the",
            " first",
            " appar",
            "ition",
            " with",
            " a",
            " calculated",
            " orbit",
            " is",
            " declared",
            " the",
            " discover",
            "er",
            ",",
            " and",
            " granted",
            " the",
            " honor",
            " of",
            " naming",
            " the",
            " object",
            " subject",
            " to",
            " the",
            " approval",
            " of",
            " the",
            " International",
            " Astr",
            "onom"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    " podrob",
    "ellas",
    "rani",
    "lichkeit",
    "atum"
  ],
  "bottom_logits": [
    " hol",
    ".mag",
    "oyal",
    "ÙĪØ°",
    "upd"
  ],
  "act_min": -0.0,
  "act_max": 0.922
}