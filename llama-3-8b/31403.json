{
  "index": 31403,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " label",
            " them",
            " as",
            " super",
            "foods",
            ".",
            " They",
            " provide",
            " many",
            " vitamins",
            " including",
            ":",
            " A",
            ",",
            " B",
            "1",
            ",",
            " B",
            "2",
            ",",
            " B",
            "6",
            ",",
            " ni",
            "acin",
            ",",
            " and",
            " C",
            ",",
            " and",
            " are",
            " rich",
            " in",
            " iod",
            "ine",
            ",",
            " potassium",
            ",",
            " iron",
            ",",
            " magnesium",
            ",",
            " and",
            " calcium",
            ".",
            " In",
            " addition",
            ",",
            " commercially",
            " cultivated",
            " micro",
            "alg",
            "ae",
            ",",
            " including",
            " both",
            " algae",
            " and",
            " cyan",
            "ob",
            "acteria",
            ",",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 11,
          "is_repeated_datapoint": false,
          "tokens": [
            " label",
            " them",
            " as",
            " super",
            "foods",
            ".",
            " They",
            " provide",
            " many",
            " vitamins",
            " including",
            ":",
            " A",
            ",",
            " B",
            "1",
            ",",
            " B",
            "2",
            ",",
            " B",
            "6",
            ",",
            " ni",
            "acin",
            ",",
            " and",
            " C",
            ",",
            " and",
            " are",
            " rich",
            " in",
            " iod",
            "ine",
            ",",
            " potassium",
            ",",
            " iron",
            ",",
            " magnesium",
            ",",
            " and",
            " calcium",
            ".",
            " In",
            " addition",
            ",",
            " commercially",
            " cultivated",
            " micro",
            "alg",
            "ae",
            ",",
            " including",
            " both",
            " algae",
            " and",
            " cyan",
            "ob",
            "acteria",
            ",",
            " are"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            "68",
            "Â°",
            "30",
            "'",
            " east",
            " longitude",
            ",",
            " with",
            " both",
            " the",
            " west",
            " and",
            " east",
            " longitude",
            " limits",
            " reaching",
            " to",
            " the",
            " north",
            " pole",
            ".",
            " The",
            " Atlantic",
            "'s",
            " sub",
            "areas",
            " include",
            ":",
            " B",
            "arent",
            "s",
            " Sea",
            ";",
            " Norwegian",
            " Sea",
            ",",
            " Sp",
            "itz",
            "ber",
            "gen",
            ",",
            " and",
            " Bear",
            " Island",
            ";",
            " Sk",
            "ag",
            "err",
            "ak",
            ",",
            " Kat",
            "teg",
            "at",
            ",",
            " Sound",
            ",",
            " Belt",
            " Sea",
            ",",
            " and",
            " Baltic",
            " Sea",
            ";"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            " for",
            " the",
            " same",
            " concept",
            " include",
            ":",
            " partial",
            " phon",
            "emic",
            " script",
            ",",
            " segment",
            "ally",
            " linear",
            " defective",
            " phon",
            "ographic",
            " script",
            ",",
            " conson",
            "ant",
            "ary",
            ",",
            " conson",
            "ant",
            " writing",
            ",",
            " and",
            " conson",
            "antal",
            " alphabet",
            ".",
            "Imp",
            "ure",
            " ab",
            "j",
            "ads",
            " represent",
            " vowels",
            " with",
            " either",
            " optional",
            " di",
            "ac",
            "rit",
            "ics",
            ",",
            " a",
            " limited",
            " number",
            " of",
            " distinct",
            " vowel",
            " glyphs",
            ",",
            " or",
            " both",
            ".",
            "Et",
            "ymology",
            "The",
            " name"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " a",
            " scientifically",
            " un",
            "validated",
            " cancer",
            " treatment",
            ".",
            "Radio",
            "is",
            "ot",
            "opes",
            " of",
            " ca",
            "esium",
            " require",
            " special",
            " precautions",
            ":",
            " the",
            " improper",
            " handling",
            " of",
            " ca",
            "esium",
            "-",
            "137",
            " gamma",
            " ray",
            " sources",
            " can",
            " lead",
            " to",
            " release",
            " of",
            " this",
            " radio",
            "is",
            "otope",
            " and",
            " radiation",
            " injuries",
            ".",
            " Perhaps",
            " the",
            " best",
            "-known",
            " case",
            " is",
            " the",
            " Go",
            "i",
            "Ã¢",
            "nia",
            " accident",
            " of",
            " ",
            "198",
            "7",
            ",",
            " in",
            " which",
            " an"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "0",
            ",",
            " Apple",
            " announced",
            " its",
            " plan",
            " to",
            " become",
            " carbon",
            " neutral",
            " across",
            " its",
            " entire",
            " business",
            ",",
            " manufacturing",
            " supply",
            " chain",
            ",",
            " and",
            " product",
            " life",
            " cycle",
            " by",
            " ",
            "203",
            "0",
            ".",
            " In",
            " the",
            " next",
            " ",
            "10",
            " years",
            ",",
            " Apple",
            " will",
            " try",
            " to",
            " lower",
            " emissions",
            " with",
            " a",
            " series",
            " of",
            " innovative",
            " actions",
            ",",
            " including",
            ":",
            " low",
            " carbon",
            " product",
            " design",
            ",",
            " expanding",
            " energy",
            " efficiency",
            ",",
            " renewable",
            " energy",
            ",",
            " process"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " reduce",
            " disability",
            ".",
            " Some",
            " drug",
            " treatments",
            " that",
            " have",
            " been",
            " used",
            " to",
            " control",
            " at",
            "ax",
            "ia",
            " include",
            ":",
            " ",
            "5",
            "-h",
            "ydro",
            "xy",
            "try",
            "pt",
            "oph",
            "an",
            " (",
            "5",
            "-HT",
            "P",
            "),",
            " ide",
            "ben",
            "one",
            ",",
            " am",
            "ant",
            "ad",
            "ine",
            ",",
            " phys",
            "ost",
            "ig",
            "mine",
            ",",
            " L",
            "-c",
            "arn",
            "it",
            "ine",
            " or",
            " derivatives",
            ",",
            " trim",
            "eth",
            "op",
            "rim",
            "/s",
            "ulf",
            "am",
            "eth",
            "ox",
            "azole"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "'s",
            " bi",
            "ographer",
            " believed",
            " that",
            " Turing",
            "'s",
            " use",
            " of",
            " a",
            " typ",
            "ewriter",
            "-like",
            " model",
            " derived",
            " from",
            " a",
            " youthful",
            " interest",
            ":",
            " \"",
            "Alan",
            " had",
            " dream",
            "t",
            " of",
            " invent",
            "ing",
            " typ",
            "ew",
            "riters",
            " as",
            " a",
            " boy",
            ";",
            " Mrs",
            ".",
            " Turing",
            " had",
            " a",
            " typ",
            "ewriter",
            ",",
            " and",
            " he",
            " could",
            " well",
            " have",
            " begun",
            " by",
            " asking",
            " himself",
            " what",
            " was",
            " meant",
            " by",
            " calling",
            " a",
            " typ",
            "ewriter",
            " '",
            "me",
            "chan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " open",
            " front",
            " un",
            "rounded",
            " vowel",
            " .",
            "In",
            " the",
            " poly",
            "ton",
            "ic",
            " orth",
            "ography",
            " of",
            " Greek",
            ",",
            " alpha",
            ",",
            " like",
            " other",
            " vowel",
            " letters",
            ",",
            " can",
            " occur",
            " with",
            " several",
            " di",
            "ac",
            "ritic",
            " marks",
            ":",
            " any",
            " of",
            " three",
            " accent",
            " symbols",
            " (),",
            " and",
            " either",
            " of",
            " two",
            " breathing",
            " marks",
            " (),",
            " as",
            " well",
            " as",
            " combinations",
            " of",
            " these",
            ".",
            " It",
            " can",
            " also",
            " combine",
            " with",
            " the",
            " iota",
            " subscript",
            " ()",
            ".",
            "Greek"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " same",
            " intentions",
            " and",
            " themes",
            ".",
            " Some",
            " of",
            " these",
            " friends",
            " include",
            ":",
            " David",
            " Am",
            "ram",
            ",",
            " Bob",
            " Kauf",
            "man",
            ";",
            " Diane",
            " di",
            " Pr",
            "ima",
            ";",
            " Jim",
            " C",
            "ohn",
            ";",
            " poets",
            " associated",
            " with",
            " the",
            " Black",
            " Mountain",
            " College",
            " such",
            " as",
            " Charles",
            " Olson",
            ",",
            " Robert",
            " Cree",
            "ley",
            ",",
            " and",
            " Denise",
            " Le",
            "vert",
            "ov",
            ";",
            " poets",
            " associated",
            " with",
            " the",
            " New",
            " York",
            " School",
            " such",
            " as",
            " Frank",
            " O"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " than",
            " a",
            " year",
            " later",
            " on",
            " July",
            " ",
            "21",
            ",",
            " ",
            "199",
            "9",
            ",",
            " Apple",
            " introduced",
            " the",
            " i",
            "Book",
            ",",
            " a",
            " laptop",
            " for",
            " consumers",
            ".",
            " It",
            " was",
            " the",
            " culmination",
            " of",
            " a",
            " strategy",
            " established",
            " by",
            " Jobs",
            " to",
            " produce",
            " only",
            " four",
            " products",
            ":",
            " refined",
            " versions",
            " of",
            " the",
            " Power",
            " Mac",
            "intosh",
            " G",
            "3",
            " desktop",
            " and",
            " Power",
            "Book",
            " G",
            "3",
            " laptop",
            " for",
            " professionals",
            ",",
            " along",
            " with",
            " the",
            " i"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " execute",
            " massive",
            " product",
            " launches",
            " without",
            " having",
            " to",
            " maintain",
            " large",
            ",",
            " profit",
            "-s",
            "apping",
            " invent",
            "ories",
            ".",
            " In",
            " ",
            "201",
            "1",
            ",",
            " Apple",
            "'s",
            " profit",
            " margins",
            " were",
            " ",
            "40",
            " percent",
            ",",
            " compared",
            " with",
            " between",
            " ",
            "10",
            " and",
            " ",
            "20",
            " percent",
            " for",
            " most",
            " other",
            " hardware",
            " companies",
            ".",
            " Cook",
            "'s",
            " catch",
            "phrase",
            " to",
            " describe",
            " his",
            " focus",
            " on",
            " the",
            " company",
            "'s",
            " operational",
            " arm",
            " is",
            ":",
            " \"",
            "Nobody"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.09,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " an",
            " urban",
            " cool",
            " island",
            " effect",
            " during",
            " the",
            " day",
            ".",
            "Black",
            " carbon",
            "Another",
            " al",
            "bedo",
            "-related",
            " effect",
            " on",
            " the",
            " climate",
            " is",
            " from",
            " black",
            " carbon",
            " particles",
            ".",
            " The",
            " size",
            " of",
            " this",
            " effect",
            " is",
            " difficult",
            " to",
            " quantify",
            ":",
            " the",
            " Int",
            "erg",
            "overnment",
            "al",
            " Panel",
            " on",
            " Climate",
            " Change",
            " estimates",
            " that",
            " the",
            " global",
            " mean",
            " radi",
            "ative",
            " forcing",
            " for",
            " black",
            " carbon",
            " aeros",
            "ols",
            " from",
            " fossil",
            " fuels",
            " is",
            " +"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " early",
            " names",
            " include",
            ":",
            "Sil",
            "vester",
            " and",
            " Alvarez",
            " were",
            " two",
            " families",
            " from",
            " Alto",
            " Vista",
            ",",
            " near",
            " the",
            " coast",
            ".",
            "English",
            " inter",
            "regnum",
            " and",
            " economic",
            " development",
            " ",
            "The",
            " British",
            " Empire",
            " took",
            " control",
            " of",
            " the",
            " island",
            " during",
            " the",
            " Nap",
            "ole",
            "onic",
            " Wars",
            " holding",
            " it",
            " from",
            " ",
            "180",
            "6",
            " to",
            " ",
            "181",
            "6",
            ",",
            " after",
            " which",
            " it",
            " was",
            " returned",
            " to",
            " Dutch",
            " authority",
            " in",
            " accordance",
            " with",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            "lo",
            "oters",
            "\"",
            " and",
            " is",
            " frequently",
            " referred",
            " to",
            " and",
            " quoted",
            " by",
            " other",
            " characters",
            " in",
            " the",
            " story",
            ",",
            " but",
            " he",
            " has",
            " only",
            " one",
            " major",
            " appearance",
            ":",
            " during",
            " the",
            " Washington",
            " meeting",
            " with",
            " Hank",
            " Re",
            "arden",
            ".",
            "Lee",
            " H",
            "uns",
            "acker",
            " is",
            " in",
            " charge",
            " of",
            " a",
            " company",
            " called",
            " Am",
            "alg",
            "am",
            "ated",
            " Service",
            " that",
            " takes",
            " over",
            " the",
            " Tw",
            "entieth",
            " Century",
            " Motor",
            " Company",
            ".",
            " He",
            " files",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            ":",
            " Alexandria",
            " (",
            "Egypt",
            "),",
            " Alexandre",
            "ia",
            " (",
            "G",
            "reece",
            "),",
            " Is",
            "k",
            "ender",
            "un",
            " (",
            "Turkey",
            "),",
            " Is",
            "k",
            "and",
            "ari",
            "ya",
            " (",
            "Iraq",
            ")",
            " and",
            " K",
            "and",
            "ah",
            "ar",
            " (",
            "Af",
            "ghan",
            "istan",
            ").",
            "F",
            "unding",
            " of",
            " temples",
            "In",
            " ",
            "334",
            " BC",
            ",",
            " Alexander",
            " the",
            " Great",
            " donated",
            " funds",
            " for",
            " the",
            " completion",
            " of",
            " the",
            " new",
            " temple",
            " of",
            " Athena",
            " Pol",
            "ias"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " amplified",
            " sound",
            " or",
            " alternate",
            " ways",
            " to",
            " access",
            " information",
            " through",
            " vision",
            " and",
            "/or",
            " vibration",
            ".",
            " These",
            " technologies",
            " can",
            " be",
            " grouped",
            " into",
            " three",
            " general",
            " categories",
            ":",
            " Hearing",
            " Technology",
            ",",
            " alert",
            "ing",
            " devices",
            ",",
            " and",
            " communication",
            " support",
            ".",
            "H",
            "earing",
            " aids",
            " ",
            "A",
            " hearing",
            " aid",
            " or",
            " deaf",
            " aid",
            " is",
            " an",
            " electro",
            "-ac",
            "oustic",
            " device",
            " which",
            " is",
            " designed",
            " to",
            " amplify",
            " sound",
            " for",
            " the",
            " wearer",
            ",",
            " usually",
            " with"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " oldest",
            " club",
            " in",
            " Ankara",
            " and",
            " is",
            " associated",
            " with",
            " Ankara",
            "'s",
            " military",
            " arsenal",
            " manufacturing",
            " company",
            " M",
            "KE",
            ".",
            " They",
            " were",
            " the",
            " Turkish",
            " Cup",
            " winners",
            " in",
            " ",
            "197",
            "2",
            " and",
            " ",
            "198",
            "1",
            ".",
            " GenÃ§",
            "ler",
            "bir",
            "liÄŁi",
            ",",
            " founded",
            " in",
            " ",
            "192",
            "3",
            ",",
            " are",
            " known",
            " as",
            " the",
            " Ankara",
            " Gale",
            " or",
            " the",
            " P",
            "opp",
            "ies",
            " because",
            " of",
            " their",
            " colors",
            ":",
            " red",
            " and",
            " black"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " include",
            ":",
            " Alexandria",
            " (",
            "Egypt",
            "),",
            " Alexandre",
            "ia",
            " (",
            "G",
            "reece",
            "),",
            " Is",
            "k",
            "ender",
            "un",
            " (",
            "Turkey",
            "),",
            " Is",
            "k",
            "and",
            "ari",
            "ya",
            " (",
            "Iraq",
            ")",
            " and",
            " K",
            "and",
            "ah",
            "ar",
            " (",
            "Af",
            "ghan",
            "istan",
            ").",
            "F",
            "unding",
            " of",
            " temples",
            "In",
            " ",
            "334",
            " BC",
            ",",
            " Alexander",
            " the",
            " Great",
            " donated",
            " funds",
            " for",
            " the",
            " completion",
            " of",
            " the",
            " new",
            " temple",
            " of",
            " Athena",
            " Pol",
            "ias"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "rous",
            ".",
            " As",
            " there",
            " was",
            " no",
            " colonial",
            "ism",
            " in",
            " the",
            " modern",
            " era",
            " in",
            " Afghanistan",
            ",",
            " European",
            "-style",
            " architecture",
            " is",
            " rare",
            " but",
            " does",
            " exist",
            ":",
            " the",
            " Victory",
            " Arch",
            " at",
            " P",
            "agh",
            "man",
            " and",
            " the",
            " Dar",
            "ul",
            " A",
            "man",
            " Palace",
            " in",
            " Kabul",
            " were",
            " built",
            " in",
            " this",
            " style",
            " in",
            " the",
            " ",
            "192",
            "0",
            "s",
            ".",
            " Afghan",
            " architecture",
            " also",
            " ranges",
            " deep",
            " into",
            " India",
            " such",
            " as",
            " the",
            " Tomb"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " on",
            " linguistic",
            " features",
            ":",
            " for",
            " example",
            ",",
            " Me",
            "inh",
            "of",
            " split",
            " the",
            " presently",
            "-under",
            "stood",
            " Ch",
            "adic",
            " family",
            " into",
            " \"",
            "Ham",
            "ito",
            "-Ch",
            "adic",
            "\",",
            " and",
            " an",
            " unrelated",
            " non",
            "-H",
            "am",
            "itic",
            " \"",
            "Ch",
            "adic",
            "\"",
            " based",
            " on",
            " which",
            " languages",
            " possessed",
            " gramm",
            "atical",
            " gender",
            ".",
            " On",
            " the",
            " other",
            " hand",
            ",",
            " the",
            " classification",
            " also",
            " relied",
            " on",
            " non",
            "-",
            "ling",
            "u",
            "istic",
            " anthrop",
            "ological",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " which",
            " ",
            "75",
            "%",
            " was",
            " exported",
            " to",
            " oil",
            " refin",
            "eries",
            " in",
            " the",
            " United",
            " States",
            ".",
            "In",
            " Alberta",
            ",",
            " five",
            " bit",
            "umen",
            " up",
            "gr",
            "aders",
            " produce",
            " synthetic",
            " crude",
            " oil",
            " and",
            " a",
            " variety",
            " of",
            " other",
            " products",
            ":",
            " The",
            " S",
            "unc",
            "or",
            " Energy",
            " up",
            "gr",
            "ader",
            " near",
            " Fort",
            " McM",
            "urray",
            ",",
            " Alberta",
            " produces",
            " synthetic",
            " crude",
            " oil",
            " plus",
            " diesel",
            " fuel",
            ";",
            " the",
            " Sync",
            "r",
            "ude",
            " Canada",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " wearable",
            " devices",
            ".",
            "Some",
            " wearable",
            " devices",
            " for",
            " visual",
            " impairment",
            " include",
            ":",
            " Or",
            "Cam",
            " device",
            ",",
            " e",
            "S",
            "ight",
            " and",
            " Brain",
            "port",
            ".",
            "Personal",
            " emergency",
            " response",
            " systems",
            "Personal",
            " emergency",
            " response",
            " systems",
            " (",
            "P",
            "ERS",
            "),",
            " or",
            " Tele",
            "care",
            " (",
            "UK",
            " term",
            "),",
            " are",
            " a",
            " particular",
            " sort",
            " of",
            " assist",
            "ive",
            " technology",
            " that",
            " use",
            " electronic",
            " sensors",
            " connected",
            " to",
            " an",
            " alarm",
            " system",
            " to",
            " help",
            " caregivers",
            " manage"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            "adian",
            " colon",
            "ists",
            " found",
            " the",
            " city",
            " of",
            " Troy",
            ".",
            " However",
            ",",
            " this",
            " story",
            " may",
            " reflect",
            " a",
            " cultural",
            " influence",
            " which",
            " had",
            " the",
            " reverse",
            " direction",
            ":",
            " H",
            "itt",
            "ite",
            " c",
            "une",
            "iform",
            " texts",
            " mention",
            " an",
            " Asia",
            " Minor",
            " god",
            " called",
            " App",
            "ali",
            "unas",
            " or",
            " Ap",
            "al",
            "unas",
            " in",
            " connection",
            " with",
            " the",
            " city",
            " of",
            " Wil",
            "usa",
            " att",
            "ested",
            " in",
            " H",
            "itt",
            "ite",
            " ins",
            "criptions",
            ",",
            " which",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "break",
            "down",
            "\"",
            " caused",
            " by",
            " \"",
            "over",
            "work",
            "\".",
            "Dis",
            "appearance",
            ":",
            " ",
            "192",
            "6",
            " ",
            "In",
            " August",
            " ",
            "192",
            "6",
            ",",
            " Archie",
            " asked",
            " Ag",
            "atha",
            " for",
            " a",
            " divorce",
            ".",
            " He",
            " had",
            " fallen",
            " in",
            " love",
            " with",
            " Nancy",
            " N",
            "ee",
            "le",
            ",",
            " a",
            " friend",
            " of",
            " Major",
            " Bel",
            "cher",
            ".",
            " On",
            " ",
            "3",
            "December",
            " ",
            "192",
            "6",
            ",",
            " the",
            " pair",
            " quar",
            "rel",
            "led",
            " after"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " compounds",
            " exist",
            ":",
            " Al",
            "F",
            ",",
            " Al",
            "Cl",
            ",",
            " Al",
            "Br",
            ",",
            " and",
            " Al",
            "I",
            " exist",
            " in",
            " the",
            " g",
            "ase",
            "ous",
            " phase",
            " when",
            " the",
            " respective",
            " tri",
            "hal",
            "ide",
            " is",
            " heated",
            " with",
            " aluminium",
            ",",
            " and",
            " at",
            " cry",
            "ogenic",
            " temperatures",
            ".",
            " A",
            " stable",
            " derivative",
            " of",
            " aluminium",
            " mono",
            "iod",
            "ide",
            " is",
            " the",
            " cyclic",
            " add",
            "uct",
            " formed",
            " with",
            " tri",
            "ethyl",
            "amine",
            ",",
            " Al",
            "4",
            "I",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            "le",
            "th",
            "'s",
            " finest",
            " work",
            ".",
            " This",
            " prose",
            " meditation",
            " is",
            " built",
            " out",
            " of",
            " the",
            " same",
            " fundamental",
            " material",
            " as",
            " the",
            " series",
            " of",
            " Sac",
            " Prairie",
            " journals",
            ",",
            " but",
            " is",
            " organized",
            " around",
            " three",
            " themes",
            ":",
            " \"",
            "the",
            " persistence",
            " of",
            " memory",
            "...",
            "the",
            " sounds",
            " and",
            " od",
            "ors",
            " of",
            " the",
            " country",
            "...",
            "and",
            " Th",
            "ore",
            "au",
            "'s",
            " observation",
            " that",
            " the",
            " '",
            "mass",
            " of",
            " men",
            " lead",
            " lives",
            " of",
            " quiet"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 30,
          "is_repeated_datapoint": false,
          "tokens": [
            " digging",
            " out",
            " ants",
            " and",
            " ter",
            "mites",
            ",",
            " the",
            " a",
            "ard",
            "v",
            "ark",
            " also",
            " excav",
            "ates",
            " bur",
            "rows",
            " in",
            " which",
            " to",
            " live",
            ",",
            " which",
            " generally",
            " fall",
            " into",
            " one",
            " of",
            " three",
            " categories",
            ":",
            " bur",
            "rows",
            " made",
            " while",
            " for",
            "aging",
            ",",
            " refuge",
            " and",
            " resting",
            " location",
            ",",
            " and",
            " permanent",
            " homes",
            ".",
            " Temporary",
            " sites",
            " are",
            " scattered",
            " around",
            " the",
            " home",
            " range",
            " and",
            " are",
            " used",
            " as",
            " ref",
            "uges",
            ",",
            " while"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " century",
            ",",
            " brought",
            " tourists",
            " to",
            " ever",
            "-high",
            "er",
            " elev",
            "ations",
            ",",
            " with",
            " the",
            " Jung",
            "fra",
            "ub",
            "ahn",
            " terminating",
            " at",
            " the",
            " Jung",
            "fra",
            "uj",
            "och",
            ",",
            " well",
            " above",
            " the",
            " eternal",
            " snow",
            "-line",
            ",",
            " after",
            " going",
            " through",
            " a",
            " tunnel",
            " in",
            " E",
            "iger",
            ".",
            " During",
            " this",
            " period",
            " winter",
            " sports",
            " were",
            " slowly",
            " introduced",
            ":",
            " in",
            " ",
            "188",
            "2",
            " the",
            " first",
            " figure",
            " skating",
            " championship",
            " was",
            " held",
            " in",
            " St"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            " compounds",
            " exist",
            ":",
            " Al",
            "F",
            ",",
            " Al",
            "Cl",
            ",",
            " Al",
            "Br",
            ",",
            " and",
            " Al",
            "I",
            " exist",
            " in",
            " the",
            " g",
            "ase",
            "ous",
            " phase",
            " when",
            " the",
            " respective",
            " tri",
            "hal",
            "ide",
            " is",
            " heated",
            " with",
            " aluminium",
            ",",
            " and",
            " at",
            " cry",
            "ogenic",
            " temperatures",
            ".",
            " A",
            " stable",
            " derivative",
            " of",
            " aluminium",
            " mono",
            "iod",
            "ide",
            " is",
            " the",
            " cyclic",
            " add",
            "uct",
            " formed",
            " with",
            " tri",
            "ethyl",
            "amine",
            ",",
            " Al",
            "4",
            "I",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " established",
            " before",
            " ",
            "100",
            "0",
            "Âł",
            "CE",
            ",",
            " but",
            " contact",
            " with",
            " it",
            " was",
            " lost",
            " in",
            " ",
            "140",
            "9",
            " and",
            " it",
            " was",
            " finally",
            " abandoned",
            " during",
            " the",
            " early",
            " Little",
            " Ice",
            " Age",
            ".",
            " This",
            " setback",
            " was",
            " caused",
            " by",
            " a",
            " range",
            " of",
            " factors",
            ":",
            " an",
            " unsustainable",
            " economy",
            " resulted",
            " in",
            " erosion",
            " and",
            " den",
            "ud",
            "ation",
            ",",
            " while",
            " conflicts",
            " with",
            " the",
            " local",
            " In",
            "uit",
            " resulted",
            " in",
            " the",
            " failure",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " accum",
            "ulations",
            " of",
            " pus",
            " in",
            " a",
            " pre",
            "existing",
            ",",
            " rather",
            " than",
            " a",
            " newly",
            " formed",
            ",",
            " anatom",
            "ical",
            " cavity",
            ".",
            "Other",
            " conditions",
            " that",
            " can",
            " cause",
            " similar",
            " symptoms",
            " include",
            ":",
            " cellul",
            "itis",
            ",",
            " a",
            " seb",
            "aceous",
            " cyst",
            ",",
            " and",
            " nec",
            "rot",
            "ising",
            " fasc",
            "i",
            "itis",
            ".",
            " Cell",
            "ul",
            "itis",
            " typically",
            " also",
            " has",
            " an",
            " ery",
            "th",
            "emat",
            "ous",
            " reaction",
            ",",
            " but",
            " does",
            " not",
            " confer",
            " any",
            " pur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " can",
            "als",
            " with",
            " their",
            " ends",
            " emerging",
            " at",
            " the",
            " I",
            "J",
            " bay",
            ".",
            " Known",
            " as",
            " the",
            " Gr",
            "acht",
            "eng",
            "ord",
            "el",
            ",",
            " three",
            " of",
            " the",
            " can",
            "als",
            " were",
            " mostly",
            " for",
            " residential",
            " development",
            ":",
            " the",
            " Her",
            "eng",
            "r",
            "acht",
            " (",
            "where",
            " \"",
            "H",
            "eren",
            "\"",
            " refers",
            " to",
            " Her",
            "en",
            " Re",
            "ge",
            "er",
            "ders",
            " van",
            " de",
            " stad",
            " Amsterdam",
            ",",
            " ruling",
            " lords",
            " of",
            " Amsterdam",
            ",",
            " whilst"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " well",
            " preserved",
            " in",
            " the",
            " country",
            " despite",
            " Western",
            " influences",
            ",",
            " including",
            " global",
            "ized",
            " consumer",
            " culture",
            ".",
            " Some",
            " of",
            " the",
            " main",
            " elements",
            " of",
            " the",
            " Azerbai",
            "j",
            "ani",
            " culture",
            " are",
            ":",
            " music",
            ",",
            " literature",
            ",",
            " folk",
            " dances",
            " and",
            " art",
            ",",
            " cuisine",
            ",",
            " architecture",
            ",",
            " cinemat",
            "ography",
            " and",
            " Nov",
            "ruz",
            " Bay",
            "ram",
            ".",
            " The",
            " latter",
            " is",
            " derived",
            " from",
            " the",
            " traditional",
            " celebration",
            " of",
            " the",
            " New",
            " Year",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.523,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            "review",
            " of",
            " Sil",
            "via",
            " Ferr",
            "ara",
            ",",
            " The",
            " Greatest",
            " In",
            "vention",
            ":",
            " A",
            " History",
            " of",
            " the",
            " World",
            " in",
            " Nine",
            " M",
            "ysterious",
            " Scripts",
            ",",
            " translated",
            " from",
            " the",
            " Italian",
            " by",
            " Todd",
            " Port",
            "now",
            "itz",
            ",",
            " Farr",
            "ar",
            ",",
            " Stra",
            "us",
            " and",
            " Gir",
            "oux",
            ",",
            " ",
            "202",
            "2",
            ",",
            " ",
            "289",
            " pp",
            ".;",
            " and",
            " Joh",
            "anna",
            " Dr",
            "ucker",
            ",",
            " In",
            "venting",
            " the",
            " Alphabet",
            ":",
            " The",
            " Origins"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.342,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " The",
            " Battle",
            " of",
            " Sailor",
            "'s",
            " Creek",
            ":",
            " Confederate",
            " General",
            " Robert",
            " E",
            ".",
            " Lee",
            "'s",
            " Army",
            " of",
            " Northern",
            " Virginia",
            " fights",
            " and",
            " loses",
            " its",
            " last",
            " major",
            " battle",
            " while",
            " in",
            " retreat",
            " from",
            " Richmond",
            ",",
            " Virginia",
            ",",
            " during",
            " the",
            " App",
            "om",
            "atto",
            "x",
            " Campaign",
            ".",
            "186",
            "6",
            " –",
            " The",
            " Grand",
            " Army",
            " of",
            " the",
            " Republic",
            ",",
            " an",
            " American",
            " patriotic",
            " organization",
            " composed",
            " of",
            " Union",
            " veterans",
            " of",
            " the",
            " American"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " Debug",
            "ging",
            " the",
            " Gender",
            " Gap",
            " List",
            " of",
            " pioneers",
            " in",
            " computer",
            " science",
            " Timeline",
            " of",
            " women",
            " in",
            " science",
            " Women",
            " in",
            " computing",
            " Women",
            " in",
            " STEM",
            " fields",
            "Ex",
            "plan",
            "atory",
            " notes",
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            " .",
            " .",
            " .",
            " .",
            " .",
            " .",
            "  ",
            " .",
            " ",
            " With",
            " notes",
            " upon",
            " the",
            " memoir",
            " by",
            " the",
            " translator",
            ".",
            " Miller"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.455,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Myst",
            "icism",
            " of",
            " Paul",
            " the",
            " Apostle",
            ",",
            " Schwe",
            "itzer",
            " first",
            " distingu",
            "ishes",
            " between",
            " two",
            " categories",
            " of",
            " myst",
            "icism",
            ":",
            " primitive",
            " and",
            " developed",
            ".",
            " Primitive",
            " myst",
            "icism",
            " \"",
            "has",
            " not",
            " yet",
            " risen",
            " to",
            " a",
            " conception",
            " of",
            " the",
            " universal",
            ",",
            " and",
            " is",
            " still",
            " confined",
            " to",
            " naive",
            " views",
            " of",
            " earthly",
            " and",
            " super",
            "-earth",
            "ly",
            ",",
            " temporal",
            " and",
            " eternal",
            "\".",
            " Additionally",
            ",",
            " he",
            " argues",
            " that",
            " this",
            " view"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.453,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " Peter",
            " U",
            "stin",
            "ov",
            ",",
            " Ian",
            " Hol",
            "m",
            ",",
            " Tony",
            " Randall",
            ",",
            " Alfred",
            " Mol",
            "ina",
            ",",
            " Or",
            "son",
            " Wel",
            "les",
            ",",
            " David",
            " Such",
            "et",
            ",",
            " Kenneth",
            " Bran",
            "agh",
            ",",
            " and",
            " John",
            " Malk",
            "ovich",
            ".",
            "Overview",
            "In",
            "flu",
            "ences",
            " ",
            "P",
            "oi",
            "rot",
            "'s",
            " name",
            " was",
            " derived",
            " from",
            " two",
            " other",
            " fictional",
            " detectives",
            " of",
            " the",
            " time",
            ":",
            " Marie",
            " Bel",
            "loc",
            " Low",
            "nd",
            "es",
            "'"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.33
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " used",
            " to",
            " create",
            " animated",
            " films",
            " with",
            " a",
            " range",
            " of",
            " text",
            "ural",
            " effects",
            " difficult",
            " to",
            " achieve",
            " with",
            " traditional",
            " cel",
            " animation",
            ".",
            " Sand",
            " animation",
            ":",
            " sand",
            " is",
            " moved",
            " around",
            " on",
            " a",
            " back",
            "-",
            " or",
            " front",
            "-light",
            "ed",
            " piece",
            " of",
            " glass",
            " to",
            " create",
            " each",
            " frame",
            " for",
            " an",
            " animated",
            " film",
            ".",
            " This",
            " creates",
            " an",
            " interesting",
            " effect",
            " when",
            " animated",
            " because",
            " of",
            " the",
            " light",
            " contrast",
            ".",
            " Flip",
            " book",
            ":"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.4,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.142,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Sullivan",
            "'s",
            " I",
            "olan",
            "the",
            ").",
            " ep",
            "ir",
            "rh",
            "ematic",
            " sy",
            "zy",
            "gies",
            ":",
            " These",
            " are",
            " sym",
            "metrical",
            " scenes",
            " that",
            " mirror",
            " each",
            " other",
            " in",
            " meter",
            " and",
            " number",
            " of",
            " lines",
            ".",
            " They",
            " form",
            " part",
            " of",
            " the",
            " first",
            " par",
            "ab",
            "asis",
            " and",
            " they",
            " often",
            " comprise",
            " the",
            " entire",
            " second",
            " par",
            "ab",
            "asis",
            ".",
            " They",
            " are",
            " characterized",
            " by",
            " the",
            " following",
            " elements",
            ":",
            " st",
            "rophe",
            " or",
            " ode"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.385,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " some",
            " way",
            ".",
            " Some",
            " families",
            " show",
            " more",
            " than",
            " one",
            " word",
            " for",
            " a",
            " numeral",
            ":",
            " Ch",
            "adic",
            ",",
            " Sem",
            "itic",
            ",",
            " and",
            " Ber",
            "ber",
            " each",
            " have",
            " two",
            " words",
            " for",
            " two",
            ",",
            " and",
            " Sem",
            "itic",
            " has",
            " four",
            " words",
            " for",
            " one",
            ".",
            " Andr",
            "zej",
            " Z",
            "ab",
            "ors",
            "ki",
            " further",
            " notes",
            " that",
            " the",
            " numbers",
            " \"",
            "one",
            "\",",
            " \"",
            "two",
            "\",",
            " and",
            " \"",
            "five",
            "\"",
            " are",
            " particularly",
            " susceptible"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.144,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "London",
            ",",
            " Vintage",
            ",",
            " ",
            "199",
            "4",
            ")",
            "  ",
            " ",
            " James",
            ",",
            " James",
            ",",
            " \"",
            "Andy",
            " War",
            "hol",
            ":",
            " The",
            " Producer",
            " as",
            " Author",
            "\",",
            " in",
            " Alleg",
            "ories",
            " of",
            " Cinema",
            ":",
            " American",
            " Film",
            " in",
            " the",
            " ",
            "196",
            "0",
            "s",
            " (",
            "198",
            "9",
            "),",
            " pp",
            ".",
            "Âł",
            "58",
            "–",
            "84",
            ".",
            " Princeton",
            ":",
            " Princeton",
            " University",
            " Press",
            ".",
            " ",
            " Kra",
            "uss",
            ",",
            " Ros",
            "al",
            "ind",
            " E"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " Colorado",
            ",",
            " and",
            " in",
            " ",
            "198",
            "5",
            ",",
            " Mich",
            "Ã¨le",
            " M",
            "out",
            "on",
            " set",
            " a",
            " new",
            " record",
            " of",
            " ",
            "11",
            ":",
            "25",
            ".",
            "39",
            ",",
            " and",
            " being",
            " the",
            " first",
            " woman",
            " to",
            " set",
            " a",
            " P",
            "ikes",
            " Peak",
            " record",
            ".",
            " In",
            " ",
            "198",
            "6",
            ",",
            " Audi",
            " formally",
            " left",
            " international",
            " rally",
            " racing",
            " following",
            " an",
            " accident",
            " in",
            " Portugal",
            " involving",
            " driver",
            " Jo",
            "aqu",
            "im",
            " Santos",
            " in",
            " his",
            " Ford"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.32,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.22,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.268,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " a",
            " smaller",
            " Form",
            "os",
            "an",
            " army",
            " at",
            " Chang",
            "hua",
            ",",
            " crippling",
            " the",
            " short",
            "-lived",
            " Republic",
            " of",
            " Form",
            "osa",
            " and",
            " leading",
            " to",
            " its",
            " surrender",
            " two",
            " months",
            " later",
            ".",
            "189",
            "6",
            " –",
            " Anglo",
            "-Z",
            "anz",
            "ibar",
            " War",
            ":",
            " The",
            " shortest",
            " war",
            " in",
            " world",
            " history",
            " (",
            "09",
            ":",
            "02",
            " to",
            " ",
            "09",
            ":",
            "40",
            "),",
            " between",
            " the",
            " United",
            " Kingdom",
            " and",
            " Z",
            "anz",
            "ibar",
            ".",
            "190",
            "1"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.316,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.312,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            ")]",
            " ()",
            "Marvel",
            " Master",
            "works",
            ":",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "8",
            " [#",
            "68",
            "–",
            "77",
            ";",
            " Marvel",
            " Super",
            " Heroes",
            " #",
            "14",
            "]",
            " ()",
            "Marvel",
            " Master",
            "works",
            ":",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "9",
            " [#",
            "78",
            "–",
            "87",
            "]",
            " ()",
            "Marvel",
            " Master",
            "works",
            ":",
            " The",
            " Amazing",
            " Spider",
            "-Man",
            " Vol",
            ".",
            " ",
            "10",
            " [#",
            "88",
            "–",
            "99",
            "]",
            " ()",
            "Marvel"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " although",
            " Alger",
            "ian",
            " piracy",
            " would",
            " continue",
            " until",
            " the",
            " French",
            " conquest",
            " in",
            " ",
            "183",
            "0",
            ".",
            "French",
            " colonization",
            " (",
            "183",
            "0",
            "–",
            "196",
            "2",
            ")",
            " ",
            "Under",
            " the",
            " pretext",
            " of",
            " a",
            " slight",
            " to",
            " their",
            " consul",
            ",",
            " the",
            " French",
            " invaded",
            " and",
            " captured",
            " Alg",
            "iers",
            " in",
            " ",
            "183",
            "0",
            ".",
            " Histor",
            "ian",
            " Ben",
            " Ki",
            "ern",
            "an",
            " wrote",
            " on",
            " the",
            " French",
            " conquest",
            " of",
            " Algeria",
            ":",
            " \""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.289,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.221,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "201",
            "2",
            " Carl",
            " Magnus",
            " Palm",
            ",",
            " Roger",
            " Back",
            "lund",
            ":",
            " AB",
            "BA",
            " –",
            " Absolute",
            " Image",
            ".",
            " SV",
            "T",
            ",",
            " ",
            "2",
            " January",
            " ",
            "201",
            "2",
            " AB",
            "BA",
            " –",
            " Bang",
            " a",
            " bo",
            "omer",
            "ang",
            ".",
            " ABC",
            " ",
            "1",
            ",",
            " ",
            "30",
            " January",
            " ",
            "201",
            "3",
            " (",
            "ABC",
            " page",
            ")",
            " AB",
            "BA",
            ":",
            " When",
            " All",
            " Is",
            " Said",
            " and",
            " Done",
            ",",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.202,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " handle",
            " a",
            " phon",
            "etic",
            " sequence",
            " C",
            "VC",
            "-C",
            "V",
            " as",
            " CV",
            "-",
            "CC",
            "V",
            " or",
            " CV",
            "-C",
            "-C",
            "V",
            ".",
            " However",
            ",",
            " sometimes",
            " phon",
            "etic",
            " C",
            "VC",
            " syll",
            "ables",
            " are",
            " handled",
            " as",
            " single",
            " units",
            ",",
            " and",
            " the",
            " final",
            " conson",
            "ant",
            " may",
            " be",
            " represented",
            ":",
            "in",
            " much",
            " the",
            " same",
            " way",
            " as",
            " the",
            " second",
            " conson",
            "ant",
            " in",
            " CC",
            "V",
            ",",
            " e",
            ".g",
            ".",
            " in",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.168,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " commanding",
            " ",
            "500",
            " My",
            "rm",
            "id",
            "ons",
            "):",
            " Men",
            "est",
            "hi",
            "us",
            ",",
            " E",
            "ud",
            "orus",
            ",",
            " Pe",
            "is",
            "ander",
            ",",
            " Phoenix",
            " and",
            " Al",
            "c",
            "imed",
            "on",
            ".",
            "Tele",
            "ph",
            "us",
            " ",
            "When",
            " the",
            " Greeks",
            " left",
            " for",
            " the",
            " Trojan",
            " War",
            ",",
            " they",
            " accidentally",
            " stopped",
            " in",
            " M",
            "ys",
            "ia",
            ",",
            " ruled",
            " by",
            " King",
            " Tele",
            "ph",
            "us",
            ".",
            " In",
            " the",
            " resulting",
            " battle",
            ",",
            " Achilles",
            " gave"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " ",
            "54",
            "Âł",
            "km",
            " s",
            " on",
            " a",
            " single",
            " charge",
            ".",
            " The",
            " e",
            "-tr",
            "on",
            " was",
            " also",
            " shown",
            " in",
            " the",
            " ",
            "201",
            "3",
            " blockbuster",
            " film",
            " Iron",
            " Man",
            " ",
            "3",
            " and",
            " was",
            " driven",
            " by",
            " Tony",
            " Stark",
            " (",
            "Iron",
            " Man",
            ").",
            "L",
            "aws",
            "uit",
            " on",
            " the",
            " use",
            " of",
            " the",
            " letter",
            " Q",
            " ",
            "In",
            " early",
            " ",
            "200",
            "5",
            ",",
            " Nissan",
            " North",
            " America",
            " Inc",
            ".",
            " filed",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " resisted",
            " due",
            " to",
            " the",
            " company",
            "'s",
            " opinion",
            " that",
            " it",
            " is",
            " not",
            " relevant",
            " to",
            " road",
            " cars",
            ",",
            " but",
            " hybrid",
            " power",
            " unit",
            " technology",
            " has",
            " been",
            " adopted",
            " into",
            " the",
            " sport",
            ",",
            " sw",
            "aying",
            " the",
            " company",
            "'s",
            " view",
            " and",
            " encouraging",
            " research",
            " into",
            " the",
            " program",
            " by",
            " former",
            " Ferrari",
            " team",
            " principal",
            " Stef",
            "ano",
            " D",
            "omen",
            "ical",
            "i",
            ".",
            "A",
            "udi",
            " announced",
            " in",
            " August",
            " ",
            "202",
            "2",
            " that",
            " it",
            " would"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Southern",
            " region",
            " and",
            " then",
            " the",
            " Confeder",
            "acy",
            ".",
            "Per",
            "ceived",
            " insults",
            " to",
            " Southern",
            " collective",
            " honor",
            " included",
            " the",
            " enormous",
            " popularity",
            " of",
            " Uncle",
            " Tom",
            "'s",
            " Cabin",
            " and",
            " abolition",
            "ist",
            " John",
            " Brown",
            "'s",
            " attempt",
            " to",
            " inc",
            "ite",
            " a",
            " slave",
            " rebellion",
            " in",
            " ",
            "185",
            "9",
            ".",
            "While",
            " the",
            " South",
            " moved",
            " towards",
            " a",
            " Southern",
            " nationalism",
            ",",
            " leaders",
            " in",
            " the",
            " North",
            " were",
            " also",
            " becoming",
            " more",
            " nationally",
            " minded",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " population",
            ",",
            " by",
            " ",
            "200",
            "1",
            ".",
            " The",
            " security",
            " brought",
            " about",
            " by",
            " the",
            " ",
            "200",
            "2",
            " peace",
            " settlement",
            " has",
            " led",
            " to",
            " the",
            " resett",
            "lement",
            " of",
            " ",
            "4",
            " million",
            " displaced",
            " persons",
            ",",
            " thus",
            " resulting",
            " in",
            " large",
            "-scale",
            " increases",
            " in",
            " agriculture",
            " production",
            ".",
            "Ang",
            "ola",
            " produced",
            " over",
            " ",
            " of",
            " diamonds",
            " in",
            " ",
            "200",
            "3",
            ",",
            " and",
            " production",
            " was",
            " expected",
            " to",
            " grow",
            " to",
            " ",
            " per"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ã§",
            "ao",
            ",",
            " B",
            "ona",
            "ire",
            ",",
            " and",
            " Ar",
            "uba",
            ".",
            " C",
            "ura",
            "Ã§",
            "ao",
            " was",
            " captured",
            " and",
            " acquired",
            " by",
            " the",
            " W",
            "IC",
            " in",
            " June",
            " ",
            "163",
            "4",
            ",",
            " primarily",
            " by",
            " their",
            " desire",
            " to",
            " obtain",
            " salt",
            ".",
            " In",
            " Van",
            " Wal",
            "bee",
            "ck",
            "'s",
            " report",
            " of",
            " ",
            "163",
            "4",
            ",",
            " Ar",
            "uba",
            " is",
            " mentioned",
            " only",
            " in",
            " relation",
            " to",
            " C",
            "ura",
            "Ã§",
            "ao",
            ",",
            " where"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "vier",
    "modifiable",
    "ifornia",
    "uitka",
    "æª"
  ],
  "bottom_logits": [
    "AMP",
    "Î½Î¿Ïį",
    "itchens",
    "empl",
    "IA"
  ],
  "act_min": -0.0,
  "act_max": 0.644
}