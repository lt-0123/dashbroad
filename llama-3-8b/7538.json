{
  "index": 7538,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            0.758,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " her",
            " brother",
            "'s",
            " policies",
            " which",
            " directly",
            " threaten",
            " his",
            " business",
            ".",
            " When",
            " the",
            " government",
            " passes",
            " laws",
            " and",
            " decre",
            "es",
            " which",
            " make",
            " it",
            " impossible",
            " for",
            " him",
            " to",
            " continue",
            ",",
            " he",
            " sets",
            " all",
            " his",
            " oil",
            " wells",
            " on",
            " fire",
            ",",
            " leaving",
            " a",
            " single",
            " note",
            ":",
            " \"",
            "I",
            " am",
            " leaving",
            " it",
            " as",
            " I",
            " found",
            " it",
            ".",
            " Take",
            " over",
            ".",
            " It",
            "'s",
            " yours",
            ".\"",
            " One",
            " particular",
            " burning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.758,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " assumes",
            " some",
            " degree",
            " of",
            " violence",
            " will",
            " occur",
            ",",
            " an",
            "ar",
            "cho",
            "-capital",
            "ism",
            " as",
            " formulated",
            " by",
            " Roth",
            "bard",
            " and",
            " others",
            " holds",
            " strongly",
            " to",
            " the",
            " central",
            " libertarian",
            " non",
            "ag",
            "gression",
            " axiom",
            ",",
            " sometimes",
            " non",
            "-ag",
            "gression",
            " principle",
            ".",
            " Roth",
            "bard",
            " wrote",
            ":",
            "R",
            "oth",
            "bard",
            "'s",
            " defense",
            " of",
            " the",
            " self",
            "-",
            "ownership",
            " principle",
            " stems",
            " from",
            " what",
            " he",
            " believed",
            " to",
            " be",
            " his",
            " fals"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.758,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            " exists",
            ",",
            " is",
            " probably",
            " more",
            " complex",
            " and",
            " distant",
            " than",
            " we",
            " can",
            " imagine",
            " on",
            " the",
            " basis",
            " of",
            " our",
            " present",
            " state",
            " of",
            " knowledge",
            "\".",
            "Support",
            "ers",
            " of",
            " the",
            " Alta",
            "ic",
            " hypothesis",
            " formerly",
            " set",
            " the",
            " date",
            " of",
            " the",
            " Proto",
            "-Al",
            "ta",
            "ic",
            " language",
            " at",
            " around",
            " ",
            "400",
            "0",
            " BC",
            ",",
            " but",
            " today",
            " at",
            " around",
            " ",
            "500",
            "0",
            " BC",
            " or",
            " ",
            "600",
            "0",
            " BC",
            ".",
            " This"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.746,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-century",
            " American",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " American",
            " novel",
            "ists",
            "20",
            "th",
            "-century",
            " American",
            " short",
            " story",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " short",
            " story",
            " writers",
            "American",
            " male",
            " novel",
            "ists",
            "American",
            " male",
            " short",
            " story",
            " writers",
            "American",
            " science",
            " fiction",
            " writers",
            "An",
            "alog",
            " Science",
            " Fiction",
            " and",
            " Fact",
            " people",
            "Canadian",
            " M",
            "ennon",
            "ites"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " inside",
            " a",
            " glass",
            " of",
            " milk",
            ",",
            " perhaps",
            " poisoned",
            ",",
            " that",
            " Grant",
            " is",
            " bringing",
            " to",
            " his",
            " wife",
            ";",
            " the",
            " light",
            " ensures",
            " that",
            " the",
            " audience",
            "'s",
            " attention",
            " is",
            " on",
            " the",
            " glass",
            ".",
            " Grant",
            "'s",
            " character",
            " is",
            " actually",
            " a",
            " killer",
            ",",
            " as",
            " per",
            " written",
            " in",
            " the",
            " book",
            ",",
            " Before",
            " the",
            " Fact",
            " by",
            " Francis",
            " I",
            "les",
            ",",
            " but",
            " the",
            " studio",
            " felt",
            " that",
            " Grant",
            "'s",
            " image",
            " would",
            " be"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " of",
            " fact",
            " or",
            " of",
            " law",
            ".",
            " In",
            " reviewing",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " an",
            " appellate",
            " court",
            " ordinarily",
            " gives",
            " defer",
            "ence",
            " to",
            " the",
            " trial",
            " court",
            "'s",
            " findings",
            ".",
            " It",
            " is",
            " the",
            " duty",
            " of",
            " trial",
            " judges",
            " or",
            " j",
            "uries",
            " to",
            " find",
            " facts",
            ",",
            " view",
            " the",
            " evidence",
            " firsthand",
            ",",
            " and",
            " observe",
            " witness",
            " testimony",
            ".",
            " When",
            " reviewing",
            " lower",
            " decisions",
            " on",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " of",
            " fact",
            " or",
            " of",
            " law",
            ".",
            " In",
            " reviewing",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " an",
            " appellate",
            " court",
            " ordinarily",
            " gives",
            " defer",
            "ence",
            " to",
            " the",
            " trial",
            " court",
            "'s",
            " findings",
            ".",
            " It",
            " is",
            " the",
            " duty",
            " of",
            " trial",
            " judges",
            " or",
            " j",
            "uries",
            " to",
            " find",
            " facts",
            ",",
            " view",
            " the",
            " evidence",
            " firsthand",
            ",",
            " and",
            " observe",
            " witness",
            " testimony",
            ".",
            " When",
            " reviewing",
            " lower",
            " decisions",
            " on",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " of",
            " fact",
            " or",
            " of",
            " law",
            ".",
            " In",
            " reviewing",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " an",
            " appellate",
            " court",
            " ordinarily",
            " gives",
            " defer",
            "ence",
            " to",
            " the",
            " trial",
            " court",
            "'s",
            " findings",
            ".",
            " It",
            " is",
            " the",
            " duty",
            " of",
            " trial",
            " judges",
            " or",
            " j",
            "uries",
            " to",
            " find",
            " facts",
            ",",
            " view",
            " the",
            " evidence",
            " firsthand",
            ",",
            " and",
            " observe",
            " witness",
            " testimony",
            ".",
            " When",
            " reviewing",
            " lower",
            " decisions",
            " on",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.233,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " of",
            " fact",
            " or",
            " of",
            " law",
            ".",
            " In",
            " reviewing",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " an",
            " appellate",
            " court",
            " ordinarily",
            " gives",
            " defer",
            "ence",
            " to",
            " the",
            " trial",
            " court",
            "'s",
            " findings",
            ".",
            " It",
            " is",
            " the",
            " duty",
            " of",
            " trial",
            " judges",
            " or",
            " j",
            "uries",
            " to",
            " find",
            " facts",
            ",",
            " view",
            " the",
            " evidence",
            " firsthand",
            ",",
            " and",
            " observe",
            " witness",
            " testimony",
            ".",
            " When",
            " reviewing",
            " lower",
            " decisions",
            " on",
            " an",
            " issue",
            " of",
            " fact",
            ",",
            " courts"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            " to",
            " formally",
            " object",
            " at",
            " the",
            " time",
            ",",
            " to",
            " what",
            " one",
            " views",
            " as",
            " improper",
            " action",
            " in",
            " the",
            " lower",
            " court",
            ",",
            " may",
            " result",
            " in",
            " the",
            " affirm",
            "ance",
            " of",
            " the",
            " lower",
            " court",
            "'s",
            " judgment",
            " on",
            " the",
            " grounds",
            " that",
            " one",
            " did",
            " not",
            " \"",
            "preserve",
            " the",
            " issue",
            " for",
            " appeal",
            "\"",
            " by",
            " object",
            "ing",
            ".",
            "In",
            " cases",
            " where",
            " a",
            " judge",
            " rather",
            " than",
            " a",
            " jury",
            " decided",
            " issues",
            " of",
            " fact",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " Robert",
            " Montgomery",
            ",",
            " and",
            " Gene",
            " Raymond",
            ",",
            " the",
            " stars",
            " of",
            " the",
            " film",
            ",",
            " to",
            " surprise",
            " him",
            ".",
            " In",
            " an",
            " episode",
            " of",
            " The",
            " Dick",
            " Cav",
            "ett",
            " Show",
            ",",
            " originally",
            " broadcast",
            " on",
            " ",
            "8",
            " June",
            " ",
            "197",
            "2",
            ",",
            " ",
            " Dick",
            " Cav",
            "ett",
            " stated",
            " as",
            " fact",
            " that",
            " Hitch",
            "cock",
            " had",
            " once",
            " called",
            " actors",
            " cattle",
            ".",
            " Hitch",
            "cock",
            " responded",
            " by",
            " saying",
            " that",
            ",",
            " at",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " most",
            " are",
            " thought",
            " to",
            " adhere",
            " to",
            " the",
            " Sunni",
            " Han",
            "afi",
            " school",
            ".",
            " According",
            " to",
            " Pew",
            " Research",
            " Center",
            ",",
            " as",
            " much",
            " as",
            " ",
            "90",
            "%",
            " are",
            " of",
            " the",
            " Sunni",
            " denomination",
            ",",
            " ",
            "7",
            "%",
            " Shia",
            " and",
            " ",
            "3",
            "%",
            " non",
            "-den",
            "omin",
            "ational",
            ".",
            " The",
            " CIA",
            " Fact",
            "book",
            " various",
            "ly",
            " estimates",
            " up",
            " to",
            " ",
            "89",
            ".",
            "7",
            "%",
            " Sunni",
            " or",
            " up",
            " to",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " law",
            " or",
            " fact",
            ".",
            " ...",
            " [",
            "Coll",
            "ateral",
            " review",
            "],",
            " on",
            " the",
            " other",
            " hand",
            ",",
            " provide",
            "[s",
            "]",
            " an",
            " independent",
            " and",
            " civil",
            " inquiry",
            " into",
            " the",
            " validity",
            " of",
            " a",
            " conviction",
            " and",
            " sentence",
            ",",
            " and",
            " as",
            " such",
            " are",
            " generally",
            " limited",
            " to",
            " challenges",
            " to",
            " constitutional",
            ",",
            " jurisdiction",
            "al",
            ",",
            " or",
            " other",
            " fundamental",
            " violations",
            " that",
            " occurred",
            " at",
            " trial",
            ".\"",
            " \"",
            "G",
            "raham",
            " v",
            ".",
            " B",
            "orgen"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " on",
            " specific",
            " grounds",
            ".",
            " These",
            " grounds",
            " typically",
            " could",
            " include",
            " errors",
            " of",
            " law",
            ",",
            " fact",
            ",",
            " procedure",
            " or",
            " due",
            " process",
            ".",
            " In",
            " different",
            " jurisdictions",
            ",",
            " appellate",
            " courts",
            " are",
            " also",
            " called",
            " appeals",
            " courts",
            ",",
            " courts",
            " of",
            " appeals",
            ",",
            " superior",
            " courts",
            ",",
            " or",
            " supreme",
            " courts",
            ".",
            "The",
            " specific",
            " procedures",
            " for",
            " appealing",
            ",",
            " including",
            " even",
            " whether",
            " there",
            " is",
            " a",
            " right",
            " of",
            " appeal",
            " from",
            " a",
            " particular",
            " type",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.664,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " application",
            " of",
            " law",
            ",",
            " but",
            " it",
            " may",
            " also",
            " be",
            " possible",
            " to",
            " appeal",
            " on",
            " the",
            " basis",
            " of",
            " court",
            " misconduct",
            ",",
            " or",
            " that",
            " a",
            " finding",
            " of",
            " fact",
            " was",
            " entirely",
            " unreasonable",
            " to",
            " make",
            " on",
            " the",
            " evidence",
            ".",
            "The",
            " appellant",
            " in",
            " the",
            " new",
            " case",
            " can",
            " be",
            " either",
            " the",
            " plaintiff",
            " (",
            "or",
            " claim",
            "ant",
            "),",
            " defendant",
            ",",
            " third",
            "-party",
            " interven",
            "or",
            ",",
            " or",
            " respondent",
            " (",
            "app",
            "elle"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.66,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alabama",
            " ",
            " Alabama",
            " Quick",
            "F",
            "acts",
            " from",
            " the",
            " U",
            ".S",
            ".",
            " Census",
            " Bureau",
            " Alabama",
            " State",
            " Fact",
            " Sheet",
            " ",
            " ",
            "181",
            "9",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "Southern",
            " United",
            " States",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "181",
            "9",
            "States",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            "States",
            " of",
            " the",
            " Gulf",
            " Coast",
            " of",
            " the",
            " United",
            " States",
            "States",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.648,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ant",
            "igua",
            ",",
            " West",
            " Indies",
            ".",
            " Thomas",
            " Hear",
            "ne",
            ".",
            " Southampton",
            ".",
            "External",
            " links",
            "  ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ke",
            "ats",
            " and",
            " attributed",
            " to",
            " extraordinary",
            " leaders",
            " who",
            " were",
            " \"",
            "content",
            " in",
            " the",
            " midst",
            " of",
            " uncertainties",
            " and",
            " doubts",
            ",",
            " and",
            " not",
            " compelled",
            " toward",
            " fact",
            " or",
            " reason",
            "\".",
            "In",
            " the",
            " ",
            "21",
            "st",
            " century",
            ",",
            " President",
            " Barack",
            " Obama",
            " named",
            " Lincoln",
            " his",
            " favorite",
            " president",
            " and",
            " insisted",
            " on",
            " using",
            " the",
            " Lincoln",
            " Bible",
            " for",
            " his",
            " inaugural",
            " ceremonies",
            ".",
            "Lin",
            "coln",
            " has",
            " often",
            " been",
            " portrayed",
            " by",
            " Hollywood",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.644,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "rav",
            "els",
            ":",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " the",
            " L",
            "us",
            "aka",
            " Peace",
            " Process",
            ".",
            " New",
            " York",
            " and",
            " London",
            ",",
            " UK",
            ",",
            " Human",
            " Rights",
            " Watch",
            ".",
            "External",
            " links",
            " ",
            "Ang",
            "ola",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "Ang",
            "ola",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            ".",
            "Ang",
            "ola",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            ".",
            "Key",
            " Development",
            " Fore"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " condemned",
            " rhetoric",
            " on",
            " both",
            " moral",
            " and",
            " political",
            " grounds",
            ".",
            " He",
            " states",
            ",",
            " \"",
            "a",
            " speaker",
            " trained",
            " in",
            " the",
            " new",
            " rhetoric",
            " may",
            " use",
            " his",
            " talents",
            " to",
            " deceive",
            " the",
            " jury",
            " and",
            " bew",
            "ilder",
            " his",
            " opponents",
            " so",
            " thoroughly",
            " that",
            " the",
            " trial",
            " loses",
            " all",
            " sembl",
            "ance",
            " of",
            " fairness",
            "\"",
            " He",
            " is",
            " speaking",
            " to",
            " the",
            " \"",
            "art",
            "\"",
            " of",
            " fl",
            "attery",
            ",",
            " and",
            " evidence",
            " points",
            " towards",
            " the",
            " fact"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Azerbaijan",
            " at",
            " University",
            " of",
            " Colorado",
            " at",
            " Boulder",
            " Country",
            " profile",
            " from",
            " BBC",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Azerbaijan",
            " from",
            " International",
            " Futures",
            " V",
            "isions",
            " of",
            " Azerbaijan",
            " Journal",
            " of",
            " The",
            " European",
            " Azerbaijan",
            " Society",
            "Major",
            " government",
            " resources",
            " President",
            " of",
            " Azerbaijan",
            " website",
            " Azerbaijan",
            " State",
            " Statistical",
            " Committee",
            " United",
            " Nations",
            " Office",
            " in",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "196",
            "0",
            ".",
            "External",
            " links",
            " Govern",
            " d",
            "'",
            "And",
            "orra",
            " Official",
            " governmental",
            " site",
            " ",
            " And",
            "orra",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Port",
            "als",
            " to",
            " the",
            " World",
            " from",
            " the",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " And",
            "orra",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " And",
            "orra",
            " from",
            " the",
            " BBC",
            " News",
            " And",
            "orra",
            " –",
            " Gu"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            "Further",
            " reading",
            "External",
            " links",
            " ",
            " Afghanistan",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " ",
            " Research",
            " Guide",
            " to",
            " Afghanistan",
            " ",
            " ",
            "170",
            "9",
            " establishments",
            " in",
            " Asia",
            "Central",
            " Asian",
            " countries",
            "Countries",
            " in",
            " Asia",
            "Em",
            "irates",
            "Iran",
            "ian",
            " Plate",
            "au",
            "Islamic",
            " states",
            "Land",
            "locked",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " one",
            " can",
            " say",
            " is",
            " that",
            " conditions",
            " were",
            " bad",
            " and",
            " that",
            " Swift",
            "'s",
            " irony",
            " brilliantly",
            " underscore",
            "d",
            " this",
            " fact",
            "\".",
            "\"People",
            " are",
            " the",
            " riches",
            " of",
            " a",
            " nation",
            "\"",
            "At",
            " the",
            " start",
            " of",
            " a",
            " new",
            " industrial",
            " age",
            " in",
            " the",
            " ",
            "18",
            "th",
            " century",
            ",",
            " it",
            " was",
            " believed",
            " that",
            " \"",
            "people",
            " are",
            " the",
            " riches",
            " of",
            " the",
            " nation",
            "\",",
            " and",
            " there",
            " was",
            " a",
            " general",
            " faith",
            " in",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "sp",
            "ont",
            "aneous",
            " prose",
            ".\"",
            " He",
            " believed",
            " literature",
            " should",
            " come",
            " from",
            " the",
            " soul",
            " without",
            " conscious",
            " restrictions",
            ".",
            " Gins",
            "berg",
            " was",
            " much",
            " more",
            " prone",
            " to",
            " revise",
            " than",
            " Ker",
            "ou",
            "ac",
            ".",
            " For",
            " example",
            ",",
            " when",
            " Ker",
            "ou",
            "ac",
            " saw",
            " the",
            " first",
            " draft",
            " of",
            " How",
            "l",
            ",",
            " he",
            " disliked",
            " the",
            " fact",
            " that",
            " Gins",
            "berg",
            " had",
            " made",
            " editorial",
            " changes",
            " in",
            " pencil",
            " (",
            "trans",
            "posing",
            " \"",
            "neg"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " mere",
            " population",
            " itself",
            ",",
            " in",
            " Ireland",
            "'s",
            " case",
            ",",
            " did",
            " not",
            " always",
            " mean",
            " greater",
            " wealth",
            " and",
            " economy",
            ".",
            " The",
            " un",
            "controlled",
            " maxim",
            " fails",
            " to",
            " take",
            " into",
            " account",
            " that",
            " a",
            " person",
            " who",
            " does",
            " not",
            " produce",
            " in",
            " an",
            " economic",
            " or",
            " political",
            " way",
            " makes",
            " a",
            " country",
            " poorer",
            ",",
            " not",
            " richer",
            ".",
            " Swift",
            " also",
            " recogn",
            "ises",
            " the",
            " implications",
            " of",
            " this",
            " fact",
            " in",
            " making",
            " merc",
            "ant",
            "il",
            "ist"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "In",
            " some",
            " cases",
            ",",
            " an",
            " appellate",
            " court",
            " may",
            " review",
            " a",
            " lower",
            " court",
            " decision",
            " \"",
            "de",
            " novo",
            "\"",
            " (",
            "or",
            " completely",
            "),",
            " challenging",
            " even",
            " the",
            " lower",
            " court",
            "'s",
            " findings",
            " of",
            " fact",
            ".",
            " ",
            " This",
            " might",
            " be",
            " the",
            " proper",
            " standard",
            " of",
            " review",
            ",",
            " for",
            " example",
            ",",
            " if",
            " the",
            " lower",
            " court",
            " resolved",
            " the",
            " case",
            " by",
            " granting",
            " a",
            " pre",
            "-tr",
            "ial",
            " motion",
            " to",
            " dismiss",
            " or",
            " motion",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            " on",
            " the",
            " same",
            " day",
            ",",
            " called",
            " the",
            " book",
            " \"",
            "a",
            " gentle",
            " satire",
            " on",
            " a",
            " certain",
            " State",
            " and",
            " on",
            " the",
            " illusions",
            " of",
            " an",
            " age",
            " which",
            " may",
            " already",
            " be",
            " behind",
            " us",
            "\".",
            " Julian",
            " Sym",
            "ons",
            " responded",
            ",",
            " on",
            " ",
            "7",
            " September",
            ",",
            " \"",
            "Should",
            " we",
            " not",
            " expect",
            ",",
            " in",
            " Tribune",
            " at",
            " least",
            ",",
            " acknowledgement",
            " of",
            " the",
            " fact",
            " that",
            " it",
            " is",
            " a",
            " satire",
            " not",
            " at",
            " all"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " North",
            " insisted",
            " on",
            " retaining",
            " duty",
            " on",
            " tea",
            " to",
            " en",
            "sh",
            "rine",
            " Parliament",
            "'s",
            " right",
            " to",
            " tax",
            " the",
            " colonies",
            ";",
            " the",
            " amount",
            " was",
            " minor",
            ",",
            " but",
            " ignored",
            " the",
            " fact",
            " it",
            " was",
            " that",
            " very",
            " principle",
            " Americans",
            " found",
            " objection",
            "able",
            ".",
            "T",
            "ensions",
            " escalated",
            " following",
            " the",
            " destruction",
            " of",
            " a",
            " customs",
            " vessel",
            " in",
            " the",
            " June",
            " ",
            "177",
            "2",
            " Gas",
            "pee",
            " Aff",
            "air",
            ",",
            " then",
            " came",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "1",
            ".",
            "02",
            " male",
            "(s",
            ")/",
            "female",
            " (",
            "201",
            "1",
            " est",
            ".)",
            "Urban",
            "ization",
            "urban",
            " population",
            ":",
            " ",
            "68",
            ".",
            "1",
            "%",
            " of",
            " total",
            " population",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "4",
            ".",
            "04",
            "%",
            " annual",
            " rate",
            " of",
            " change",
            " (",
            "202",
            "0",
            "-",
            "202",
            "5",
            " est",
            ".)",
            "Health",
            "According",
            " to",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "2",
            "%",
            " of",
            " adults",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Pas",
            "ht",
            "uns",
            ",",
            " although",
            " many",
            " of",
            " them",
            " are",
            " also",
            " fluent",
            " in",
            " D",
            "ari",
            " while",
            " some",
            " non",
            "-P",
            "as",
            "ht",
            "uns",
            " are",
            " fluent",
            " in",
            " Pas",
            "ht",
            "o",
            ".",
            " Despite",
            " the",
            " Pas",
            "ht",
            "uns",
            " having",
            " been",
            " dominant",
            " in",
            " Afghan",
            " politics",
            " for",
            " centuries",
            ",",
            " D",
            "ari",
            " remained",
            " the",
            " preferred",
            " language",
            " for",
            " government",
            " and",
            " bureaucracy",
            ".",
            " ",
            "According",
            " to",
            " CIA",
            " World",
            " Fact",
            "book",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "In",
            " some",
            " cases",
            ",",
            " an",
            " appellate",
            " court",
            " may",
            " review",
            " a",
            " lower",
            " court",
            " decision",
            " \"",
            "de",
            " novo",
            "\"",
            " (",
            "or",
            " completely",
            "),",
            " challenging",
            " even",
            " the",
            " lower",
            " court",
            "'s",
            " findings",
            " of",
            " fact",
            ".",
            " ",
            " This",
            " might",
            " be",
            " the",
            " proper",
            " standard",
            " of",
            " review",
            ",",
            " for",
            " example",
            ",",
            " if",
            " the",
            " lower",
            " court",
            " resolved",
            " the",
            " case",
            " by",
            " granting",
            " a",
            " pre",
            "-tr",
            "ial",
            " motion",
            " to",
            " dismiss",
            " or",
            " motion",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Afghanistan",
            ".",
            "Af",
            "ghan",
            " Christians",
            ",",
            " who",
            " number",
            " ",
            "500",
            "–",
            "8",
            ",",
            "000",
            ",",
            " practice",
            " their",
            " faith",
            " secretly",
            " due",
            " to",
            " intense",
            " societal",
            " opposition",
            ",",
            " and",
            " there",
            " are",
            " no",
            " public",
            " churches",
            ".",
            "Urban",
            "ization",
            "As",
            " estimated",
            " by",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "26",
            "%",
            " of",
            " the",
            " population",
            " was",
            " urban",
            "ized",
            " as",
            " of",
            " ",
            "202",
            "0",
            ".",
            " This",
            " is",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " marked",
            " by",
            " the",
            " fact",
            " of",
            " having",
            " sensation",
            " and",
            " being",
            " alive",
            ".\"",
            "In",
            " the",
            " Catholic",
            " Church",
            ",",
            " opinion",
            " was",
            " divided",
            " on",
            " how",
            " serious",
            " abortion",
            " was",
            " in",
            " comparison",
            " with",
            " such",
            " acts",
            " as",
            " contraception",
            ",",
            " oral",
            " sex",
            ",",
            " and",
            " sex",
            " in",
            " marriage",
            " for",
            " pleasure",
            " rather",
            " than",
            " pro",
            "creation",
            ".",
            " The",
            " Catholic",
            " Church",
            " did",
            " not",
            " begin",
            " vigorously",
            " opposing",
            " abortion",
            " until",
            " the",
            " ",
            "19",
            "th",
            " century",
            ".",
            " As"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.606,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " do",
            " N",
            "ascimento",
            " and",
            " former",
            " presidential",
            " advisers",
            " were",
            " also",
            " accused",
            " of",
            " mis",
            "appropri",
            "ating",
            " significant",
            " public",
            " funds",
            " for",
            " personal",
            " benefit",
            ".",
            "The",
            " enormous",
            " differences",
            " between",
            " the",
            " regions",
            " pose",
            " a",
            " serious",
            " structural",
            " problem",
            " for",
            " the",
            " Ang",
            "olan",
            " economy",
            ",",
            " illustrated",
            " by",
            " the",
            " fact",
            " that",
            " about",
            " one",
            " third",
            " of",
            " economic",
            " activities",
            " are",
            " concentrated",
            " in",
            " Lu",
            "anda",
            " and",
            " neighbouring",
            " Beng",
            "o",
            " province",
            ",",
            " while",
            " several"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 5,
          "is_repeated_datapoint": false,
          "tokens": [
            "ant",
            " with",
            " reality",
            ".",
            " In",
            " fact",
            ",",
            " just",
            " hours",
            " after",
            " disgr",
            "aced",
            " Volkswagen",
            " CEO",
            " Martin",
            " Winter",
            "k",
            "orn",
            " admitted",
            " to",
            " cheating",
            " on",
            " emissions",
            " data",
            ",",
            " an",
            " advertisement",
            " during",
            " the",
            " ",
            "201",
            "5",
            " Prim",
            "etime",
            " Emmy",
            " Awards",
            " promoted",
            " Audi",
            "'s",
            " latest",
            " advances",
            " in",
            " low",
            " emissions",
            " technology",
            " with",
            " Ker",
            "mit",
            " the",
            " Frog",
            " stating",
            ",",
            " \"",
            "It",
            "'s",
            " not",
            " that",
            " easy",
            " being",
            " green",
            ".\"",
            "V",
            "ors"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " Paramount",
            " Pictures",
            " where",
            " he",
            " was",
            " responsible",
            " for",
            " films",
            " including",
            " John",
            " Trav",
            "olta",
            "'s",
            " Urban",
            " Cowboy",
            ".",
            " His",
            " boyfriend",
            " of",
            " ",
            "12",
            " years",
            " was",
            " Jed",
            " Johnson",
            ",",
            " whom",
            " he",
            " met",
            " in",
            " ",
            "196",
            "8",
            ",",
            " and",
            " who",
            " later",
            " achieved",
            " fame",
            " as",
            " an",
            " interior",
            " designer",
            ".",
            "The",
            " fact",
            " that",
            " War",
            "hol",
            "'s",
            " homosexuality",
            " influenced",
            " his",
            " work",
            " and",
            " shaped",
            " his",
            " relationship",
            " to",
            " the",
            " art",
            " world",
            " is"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " original",
            " set",
            " has",
            " elements",
            ",",
            " and",
            " a",
            " function",
            " whose",
            " domain",
            " is",
            " strictly",
            " smaller",
            " than",
            " its",
            " range",
            ".",
            " In",
            " fact",
            ",",
            " this",
            " is",
            " the",
            " case",
            " in",
            " all",
            " known",
            " models",
            ".",
            "There",
            " is",
            " a",
            " function",
            " f",
            " from",
            " the",
            " real",
            " numbers",
            " to",
            " the",
            " real",
            " numbers",
            " such",
            " that",
            " f",
            " is",
            " not",
            " continuous",
            " at",
            " a",
            ",",
            " but",
            " f",
            " is",
            " sequentially",
            " continuous",
            " at",
            " a",
            ",",
            " i",
            ".e",
            ".,",
            " for"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "¯",
            "Âł",
            "(x",
            "Âł",
            "âĪĴ",
            "Âł",
            "an",
            ")",
            "Âł",
            "+",
            "Âł",
            "1",
            "has",
            " no",
            " zero",
            " in",
            " F",
            ".",
            " However",
            ",",
            " the",
            " union",
            " of",
            " all",
            " finite",
            " fields",
            " of",
            " a",
            " fixed",
            " characteristic",
            " p",
            " is",
            " an",
            " algebra",
            "ically",
            " closed",
            " field",
            ",",
            " which",
            " is",
            ",",
            " in",
            " fact",
            ",",
            " the",
            " algebra",
            "ic",
            " closure",
            " of",
            " the",
            " field",
            " ",
            " with",
            " p",
            " elements",
            ".",
            "Equivalent",
            " properties",
            "Given",
            " a",
            " field"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "hol",
            "'s",
            " death",
            ",",
            " doctors",
            " expected",
            " War",
            "hol",
            " to",
            " survive",
            " the",
            " surgery",
            ",",
            " though",
            " a",
            " re",
            "-e",
            "valuation",
            " of",
            " the",
            " case",
            " about",
            " thirty",
            " years",
            " after",
            " his",
            " death",
            " showed",
            " many",
            " indications",
            " that",
            " War",
            "hol",
            "'s",
            " surgery",
            " was",
            " in",
            " fact",
            " risk",
            "ier",
            " than",
            " originally",
            " thought",
            ".",
            " It",
            " was",
            " widely",
            " reported",
            " at",
            " the",
            " time",
            " that",
            " War",
            "hol",
            " had",
            " died",
            " of",
            " a",
            " \"",
            "routine",
            "\"",
            " surgery",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.428,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " was",
            " glad",
            " that",
            " Bh",
            "akt",
            "ived",
            "anta",
            " Sw",
            "ami",
            ",",
            " an",
            " authentic",
            " sw",
            "ami",
            " from",
            " India",
            ",",
            " was",
            " now",
            " trying",
            " to",
            " spread",
            " the",
            " chanting",
            " in",
            " America",
            ".",
            " Along",
            " with",
            " other",
            " count",
            "erc",
            "ulture",
            " ide",
            "ologists",
            " like",
            " Timothy",
            " Le",
            "ary",
            ",",
            " Gary",
            " Snyder",
            ",",
            " and",
            " Alan",
            " Watts",
            ",",
            " Gins",
            "berg",
            " hoped",
            " to",
            " incorporate",
            " Bh",
            "akt",
            "ived",
            "anta",
            " Sw",
            "ami",
            " and",
            " his",
            " chanting",
            " into",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.371,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " study",
            " of",
            " ancient",
            " humans",
            ",",
            " as",
            " found",
            " in",
            " fossil",
            " hom",
            "in",
            "id",
            " evidence",
            " such",
            " as",
            " pet",
            "r",
            "ifact",
            "ed",
            " bones",
            " and",
            " foot",
            "prints",
            ".",
            " Genetics",
            " and",
            " morphology",
            " of",
            " specimens",
            " are",
            " crucial",
            "ly",
            " important",
            " to",
            " this",
            " field",
            ".",
            " Mark",
            "ers",
            " on",
            " specimens",
            ",",
            " such",
            " as",
            " enamel",
            " fractures",
            " and",
            " dental",
            " decay",
            " on",
            " teeth",
            ",",
            " can",
            " also",
            " give",
            " insight",
            " into",
            " the",
            " behaviour",
            " and",
            " diet",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.359,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "an",
            ")",
            " and",
            " other",
            " Indian",
            " tribes",
            ";",
            " while",
            " besie",
            "ging",
            " the",
            " Mall",
            "ian",
            " cit",
            "adel",
            ",",
            " Alexander",
            " suffered",
            " a",
            " near",
            "-f",
            "atal",
            " injury",
            " when",
            " an",
            " arrow",
            " penetrated",
            " his",
            " armor",
            " and",
            " entered",
            " his",
            " lung",
            ".",
            "Alexander",
            " sent",
            " much",
            " of",
            " his",
            " army",
            " to",
            " Car",
            "mania",
            " (",
            "modern",
            " southern",
            " Iran",
            ")",
            " with",
            " general",
            " Cr",
            "ater",
            "us",
            ",",
            " and",
            " commissioned",
            " a",
            " fleet",
            " to",
            " explore",
            " the",
            " Persian",
            " Gulf"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " board",
            " thrown",
            " in",
            ".",
            " M",
            "abel",
            " was",
            " a",
            " bright",
            ",",
            " attractive",
            " girl",
            " who",
            " was",
            " ten",
            " years",
            " Bell",
            "'s",
            " junior",
            " but",
            " became",
            " the",
            " object",
            " of",
            " his",
            " affection",
            ".",
            " Having",
            " lost",
            " her",
            " hearing",
            " after",
            " a",
            " near",
            "-f",
            "atal",
            " bout",
            " of",
            " scar",
            "let",
            " fever",
            " close",
            " to",
            " her",
            " fifth",
            " birthday",
            ",",
            " she",
            " had",
            " learned",
            " to",
            " read",
            " lips",
            " but",
            " her",
            " father",
            ",",
            " Gard",
            "iner",
            " Greene",
            " Hubbard",
            ",",
            " Bell"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " .",
            " S",
            "ida",
            "oui",
            ",",
            " Ri",
            "adh",
            " (",
            "200",
            "9",
            ").",
            " \"",
            "Islamic",
            " Politics",
            " and",
            " the",
            " Military",
            "Âł",
            "–",
            " Algeria",
            " ",
            "196",
            "2",
            "–",
            "200",
            "8",
            "\".",
            " Religion",
            " and",
            " Politics",
            "Âł",
            "–",
            " Islam",
            " and",
            " Muslim",
            " Civil",
            "isation",
            ".",
            " F",
            "arn",
            "ham",
            ":",
            " Ash",
            "gate",
            " Publishing",
            ".",
            " .",
            "External",
            " links",
            " People",
            "'s",
            " Democratic",
            " Republic",
            " of",
            " Algeria",
            " Official",
            " government",
            " website",
            " ",
            " Portal",
            " of"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.196,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 25,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " well",
            "-",
            "illustr",
            "ated",
            " by",
            " Eu",
            "clid",
            "'s",
            " Elements",
            ",",
            " where",
            " a",
            " list",
            " of",
            " post",
            "ulates",
            " is",
            " given",
            " (",
            "common",
            "-s",
            "ens",
            "ical",
            " geometric",
            " facts",
            " drawn",
            " from",
            " our",
            " experience",
            "),",
            " followed",
            " by",
            " a",
            " list",
            " of",
            " \"",
            "common",
            " notions",
            "\"",
            " (",
            "very",
            " basic",
            ",",
            " self",
            "-e",
            "vid",
            "ent",
            " assertions",
            ").",
            "Post",
            "ulates",
            " It",
            " is",
            " possible",
            " to",
            " draw",
            " a",
            " straight",
            " line",
            " from",
            " any"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " regard",
            " to",
            " material",
            " facts",
            " which",
            " may",
            " be",
            " dis",
            "positive",
            " of",
            " the",
            " matter",
            " at",
            " bar",
            ".",
            " Aff",
            "id",
            "av",
            "its",
            " from",
            " persons",
            " who",
            " are",
            " dead",
            " or",
            " otherwise",
            " incapac",
            "itated",
            ",",
            " or",
            " who",
            " cannot",
            " be",
            " located",
            " or",
            " made",
            " to",
            " appear",
            ",",
            " may",
            " be",
            " accepted",
            " by",
            " the",
            " court",
            ",",
            " but",
            " usually",
            " only",
            " in",
            " the",
            " presence",
            " of",
            " corrobor",
            "ating",
            " evidence",
            ".",
            " An",
            " affidavit",
            " which",
            " reflected",
            " a",
            " better"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "otope",
            " ",
            "227",
            "Ac",
            " is",
            " a",
            " transient",
            " member",
            " of",
            " the",
            " uranium",
            "-",
            "act",
            "inium",
            " series",
            " decay",
            " chain",
            ",",
            " which",
            " begins",
            " with",
            " the",
            " parent",
            " is",
            "otope",
            " ",
            "235",
            "U",
            " (",
            "or",
            " ",
            "239",
            "Pu",
            ")",
            " and",
            " ends",
            " with",
            " the",
            " stable",
            " lead",
            " is",
            "otope",
            " ",
            "207",
            "P",
            "b",
            ".",
            " The",
            " is",
            "otope",
            " ",
            "228",
            "Ac",
            " is",
            " a",
            " transient",
            " member",
            " of",
            " the",
            " thor",
            "ium",
            " series",
            " decay"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.182,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " War",
            "hol",
            "'s",
            " \"",
            "wh",
            "ims",
            "ical",
            "\"",
            " ink",
            " drawings",
            " of",
            " shoe",
            " advertisements",
            " figured",
            " in",
            " some",
            " of",
            " his",
            " earliest",
            " show",
            "ings",
            " at",
            " the",
            " Bod",
            "ley",
            " Gallery",
            " in",
            " New",
            " York",
            " in",
            " ",
            "195",
            "7",
            ".",
            "War",
            "hol",
            " habit",
            "ually",
            " used",
            " the",
            " exped",
            "ient",
            " of",
            " tracing",
            " photographs",
            " projected",
            " with",
            " an",
            " epid",
            "ias",
            "cope",
            ".",
            " Using",
            " prints",
            " by",
            " Edward",
            " W",
            "allow",
            "itch",
            ",",
            " his",
            " \"",
            "first"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.161,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            "Further",
            " reading",
            "A",
            "gricult",
            "ural",
            " Research",
            ",",
            " Liv",
            "elihood",
            "s",
            ",",
            " and",
            " Poverty",
            ":",
            " Studies",
            " of",
            " Economic",
            " and",
            " Social",
            " Imp",
            "acts",
            " in",
            " Six",
            " Countries",
            " Edited",
            " by",
            " Michelle",
            " Ad",
            "ato",
            " and",
            " Ruth",
            " Mein",
            "zen",
            "-D",
            "ick",
            " (",
            "200",
            "7",
            "),",
            " Johns",
            " Hopkins",
            " University",
            " Press",
            " Food",
            " Policy",
            " Report",
            "Cla",
            "ude",
            " Bour",
            "gu",
            "ignon",
            ",",
            " Reg",
            "enerating",
            " the",
            " Soil",
            ":",
            " From",
            " Agr",
            "onomy"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.025,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " was",
            " in",
            " use",
            " for",
            " more",
            " than",
            " ",
            "400",
            " years",
            " from",
            " the",
            " late",
            " ",
            "700",
            "s",
            " till",
            " around",
            " the",
            " mid",
            "-",
            "120",
            "0",
            "s",
            ".",
            "Arch",
            "ae",
            "ological",
            " evidence",
            " indicates",
            " that",
            " A",
            "arhus",
            " was",
            " a",
            " town",
            " as",
            " early",
            " as",
            " the",
            " last",
            " quarter",
            " of",
            " the",
            " ",
            "8",
            "th",
            "Âł",
            "cent",
            "ury",
            ".",
            " Discover",
            "ies",
            " after",
            " a",
            " ",
            "200",
            "3",
            " archaeological",
            " dig",
            " included",
            " half",
            "-b",
            "ur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.013,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "endra",
            " Singh",
            ",",
            " Indian",
            " environmental",
            "ist",
            " ",
            " ",
            " ",
            "195",
            "9",
            "  ",
            " –",
            " Joyce",
            " Sims",
            ",",
            " American",
            " singer",
            " (",
            "d",
            ".",
            " ",
            "202",
            "2",
            ")",
            "196",
            "0",
            " –",
            " Dale",
            " Ellis",
            ",",
            " American",
            " basketball",
            " player",
            "196",
            "1",
            " –",
            " Mary",
            " Ann",
            " Sie",
            "gh",
            "art",
            ",",
            " English",
            " journalist",
            " and",
            " radio",
            " host",
            "196",
            "2",
            " –",
            " Michelle",
            " Ye",
            "oh",
            ",",
            " Malaysian",
            "-H",
            "ong",
            " Kong",
            " actress",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " potassium",
            " production",
            " called",
            " Al",
            "kar",
            "b",
            " was",
            " a",
            " main",
            " source",
            " for",
            " rub",
            "id",
            "ium",
            ".",
            " Al",
            "kar",
            "b",
            " contained",
            " ",
            "21",
            "%",
            " rub",
            "id",
            "ium",
            " while",
            " the",
            " rest",
            " was",
            " potassium",
            " and",
            " a",
            " small",
            " fraction",
            " of",
            " ca",
            "esium",
            ".",
            " Today",
            " the",
            " largest",
            " producers",
            " of",
            " ca",
            "esium",
            ",",
            " for",
            " example",
            " the",
            " T",
            "anco",
            " Mine",
            " in",
            " Manitoba",
            ",",
            " Canada",
            ",",
            " produce",
            " rub",
            "id",
            "ium"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Press",
            ",",
            " ",
            "191",
            "4",
            ".",
            " .",
            " Online",
            " version",
            " at",
            " Harvard",
            " University",
            " Press",
            ".",
            " N",
            "uma",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " P",
            "seudo",
            "-",
            "Pl",
            "ut",
            "arch",
            ",",
            " De",
            " flu",
            "vi",
            "is",
            ",",
            " in",
            " Pl",
            "ut",
            "arch",
            "'s",
            " morals",
            ",",
            " Volume",
            " V",
            ",",
            " edited",
            " and",
            " translated",
            " by",
            " William",
            " Watson",
            " Good",
            "win",
            ",",
            " Boston",
            ":",
            " Little",
            ",",
            " Brown",
            " &",
            " Co",
            ".,"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "195",
            "6",
            " –",
            " Gordon",
            " Singleton",
            ",",
            " Canadian",
            " Olympic",
            " cyclist",
            "195",
            "7",
            " –",
            " Melanie",
            " Griffith",
            ",",
            " American",
            " actress",
            " and",
            " producer",
            "195",
            "8",
            " –",
            " Amanda",
            " Bear",
            "se",
            ",",
            " American",
            " actress",
            ",",
            " comedian",
            " and",
            " director",
            " ",
            " ",
            "195",
            "8",
            "  ",
            " –",
            " Cal",
            "ie",
            " P",
            "istor",
            "ius",
            ",",
            " South",
            " African",
            " engineer",
            " and",
            " academic",
            "195",
            "9",
            " –",
            " Kurt",
            "is",
            " Blow",
            ",",
            " American",
            " rapper",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "inu",
    " KÃ¼r",
    "hti",
    "elsing",
    "ony"
  ],
  "bottom_logits": [
    " throw",
    " Throw",
    " threw",
    "sonian",
    " -:"
  ],
  "act_min": -0.0,
  "act_max": 0.758
}