{
  "index": 44880,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " role",
            ",",
            " in",
            " the",
            " biology",
            " of",
            " the",
            " three",
            " e",
            "uk",
            "ary",
            "otic",
            " groups",
            " of",
            " organisms",
            ":",
            " fungi",
            ",",
            " plants",
            ",",
            " and",
            " animals",
            ".",
            " Some",
            " specialized",
            " ye",
            "asts",
            ",",
            " e",
            ".g",
            ".,",
            " Candid",
            "a",
            " trop",
            "ic",
            "ale",
            ",",
            " P",
            "ich",
            "ia",
            " sp",
            ".,",
            " Rh",
            "od",
            "otor",
            "ula",
            " sp",
            ".,",
            " can",
            " use",
            " al",
            "kan",
            "es",
            " as",
            " a",
            " source",
            " of",
            " carbon",
            " or",
            " energy",
            ".",
            " The",
            " fungus"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 10,
          "is_repeated_datapoint": false,
          "tokens": [
            " been",
            " influenced",
            " by",
            " contact",
            " with",
            " non",
            "-G",
            "erman",
            "ic",
            " linguistic",
            " groups",
            ",",
            " such",
            " as",
            " the",
            " dialect",
            " of",
            " Car",
            "inth",
            "ia",
            ",",
            " where",
            ",",
            " in",
            " the",
            " past",
            ",",
            " many",
            " speakers",
            " were",
            " bilingual",
            " (",
            "and",
            ",",
            " in",
            " the",
            " southeastern",
            " portions",
            " of",
            " the",
            " state",
            ",",
            " many",
            " still",
            " are",
            " even",
            " today",
            ")",
            " with",
            " Sloven",
            "e",
            ",",
            " and",
            " the",
            " dialect",
            " of",
            " Vienna",
            ",",
            " which",
            " has",
            " been",
            " influenced",
            " by"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "ary",
            " Islands",
            "Mor",
            "occo",
            "South",
            " Africa",
            "North",
            " America",
            "External",
            " links",
            " ",
            " –",
            " a",
            " database",
            " of",
            " all",
            " al",
            "gal",
            " names",
            " including",
            " images",
            ",",
            " n",
            "omencl",
            "ature",
            ",",
            " taxonomy",
            ",",
            " distribution",
            ",",
            " bibliography",
            ",",
            " uses",
            ",",
            " extracts",
            " ",
            " ",
            " ",
            " ",
            "End",
            "os",
            "ymb",
            "iotic",
            " events",
            "Poly",
            "ph",
            "yle",
            "tic",
            " groups",
            "Common",
            " names",
            " of",
            " organisms",
            "<|begin_of_text|>",
            "Analysis",
            " of"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "is",
            " (",
            "sleep",
            "ing",
            " sickness",
            ");",
            " respiratory",
            " disease",
            ":",
            " mening",
            "oc",
            "oc",
            "cal",
            " mening",
            "itis",
            ",",
            " and",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            ",",
            " a",
            " water",
            " contact",
            " disease",
            ",",
            " as",
            " of",
            " ",
            "200",
            "5",
            ".",
            "Eth",
            "nic",
            " groups",
            "R",
            "ough",
            "ly",
            " ",
            "37",
            "%",
            " of",
            " Ang",
            "ol",
            "ans",
            " are",
            " Ov",
            "imb",
            "und",
            "u",
            ",",
            " ",
            "25",
            "%",
            " are",
            " Amb",
            "und",
            "u",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.496,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "is",
            " (",
            "sleep",
            "ing",
            " sickness",
            ");",
            " respiratory",
            " disease",
            ":",
            " mening",
            "oc",
            "oc",
            "cal",
            " mening",
            "itis",
            ",",
            " and",
            " sch",
            "ist",
            "os",
            "om",
            "ias",
            "is",
            ",",
            " a",
            " water",
            " contact",
            " disease",
            ",",
            " as",
            " of",
            " ",
            "200",
            "5",
            ".",
            "Eth",
            "nic",
            " groups",
            "R",
            "ough",
            "ly",
            " ",
            "37",
            "%",
            " of",
            " Ang",
            "ol",
            "ans",
            " are",
            " Ov",
            "imb",
            "und",
            "u",
            ",",
            " ",
            "25",
            "%",
            " are",
            " Amb",
            "und",
            "u",
            ","
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.486,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " large",
            " number",
            " of",
            " basal",
            " Devon",
            "ian",
            " and",
            " Carbon",
            "ifer",
            "ous",
            " amphib",
            "ian",
            "-type",
            " tet",
            "rap",
            "od",
            " groups",
            " that",
            " were",
            " formerly",
            " placed",
            " in",
            " Amph",
            "ibia",
            " in",
            " Lin",
            "na",
            "ean",
            " taxonomy",
            ",",
            " and",
            " included",
            " them",
            " elsewhere",
            " under",
            " clad",
            "istic",
            " taxonomy",
            ".",
            " If",
            " the",
            " common",
            " ancestor",
            " of",
            " amphib",
            "ians",
            " and",
            " am",
            "ni",
            "otes",
            " is",
            " included",
            " in",
            " Amph",
            "ibia",
            ",",
            " it",
            " becomes",
            " a",
            " paraph",
            "yle",
            "tic",
            " group"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.004,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            "pet",
            "ont",
            "idae",
            ")",
            " Middle",
            " Jurassic",
            " –",
            " Early",
            " Ple",
            "ist",
            "ocene",
            "These",
            " three",
            " subclasses",
            " do",
            " not",
            " include",
            " all",
            " extinct",
            " amphib",
            "ians",
            ".",
            " Other",
            " extinct",
            " amphib",
            "ian",
            " groups",
            " include",
            " Emb",
            "ol",
            "omer",
            "i",
            " (",
            "Late",
            " Pale",
            "ozo",
            "ic",
            " large",
            " aquatic",
            " predators",
            "),",
            " Seymour",
            "iam",
            "or",
            "pha",
            " (",
            "sem",
            "ia",
            "qu",
            "atic",
            " to",
            " terrestrial",
            " Perm",
            "ian",
            " forms",
            " related",
            " to",
            " am",
            "ni",
            "otes",
            "),",
            " among"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            "er",
            ",",
            " ",
            "0",
            ".",
            "3",
            "%",
            " some",
            " other",
            " race",
            ",",
            " and",
            " ",
            "7",
            ".",
            "7",
            "%",
            " mult",
            "ir",
            "acial",
            ".",
            " Hispanics",
            " and",
            " Latin",
            " Americans",
            " were",
            " ",
            "7",
            "%",
            " of",
            " the",
            " state",
            " population",
            " in",
            " ",
            "201",
            "5",
            ".",
            " From",
            " ",
            "201",
            "5",
            " to",
            " ",
            "201",
            "9",
            ",",
            " the",
            " largest",
            " Hispanic",
            " and",
            " Latin",
            " American",
            " groups",
            " were",
            " Mexican",
            " Americans",
            ",",
            " Puerto",
            " Ric",
            "ans",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " electoral",
            " code",
            ",",
            " and",
            " the",
            " representation",
            " of",
            " women",
            " in",
            " elected",
            " bodies",
            ".",
            " In",
            " April",
            " ",
            "201",
            "1",
            ",",
            " Bout",
            "ef",
            "li",
            "ka",
            " promised",
            " further",
            " constitutional",
            " and",
            " political",
            " reform",
            ".",
            " However",
            ",",
            " elections",
            " are",
            " routinely",
            " criticised",
            " by",
            " opposition",
            " groups",
            " as",
            " unfair",
            " and",
            " international",
            " human",
            " rights",
            " groups",
            " say",
            " that",
            " media",
            " censorship",
            " and",
            " harassment",
            " of",
            " political",
            " opponents",
            " continue",
            ".",
            "On",
            " ",
            "2",
            " April",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " electoral",
            " code",
            ",",
            " and",
            " the",
            " representation",
            " of",
            " women",
            " in",
            " elected",
            " bodies",
            ".",
            " In",
            " April",
            " ",
            "201",
            "1",
            ",",
            " Bout",
            "ef",
            "li",
            "ka",
            " promised",
            " further",
            " constitutional",
            " and",
            " political",
            " reform",
            ".",
            " However",
            ",",
            " elections",
            " are",
            " routinely",
            " criticised",
            " by",
            " opposition",
            " groups",
            " as",
            " unfair",
            " and",
            " international",
            " human",
            " rights",
            " groups",
            " say",
            " that",
            " media",
            " censorship",
            " and",
            " harassment",
            " of",
            " political",
            " opponents",
            " continue",
            ".",
            "On",
            " ",
            "2",
            " April",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.482,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " electoral",
            " code",
            ",",
            " and",
            " the",
            " representation",
            " of",
            " women",
            " in",
            " elected",
            " bodies",
            ".",
            " In",
            " April",
            " ",
            "201",
            "1",
            ",",
            " Bout",
            "ef",
            "li",
            "ka",
            " promised",
            " further",
            " constitutional",
            " and",
            " political",
            " reform",
            ".",
            " However",
            ",",
            " elections",
            " are",
            " routinely",
            " criticised",
            " by",
            " opposition",
            " groups",
            " as",
            " unfair",
            " and",
            " international",
            " human",
            " rights",
            " groups",
            " say",
            " that",
            " media",
            " censorship",
            " and",
            " harassment",
            " of",
            " political",
            " opponents",
            " continue",
            ".",
            "On",
            " ",
            "2",
            " April",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            "rap",
            "od",
            " verte",
            "brates",
            " that",
            " are",
            " not",
            " am",
            "ni",
            "otes",
            ".",
            " Amph",
            "ibia",
            " in",
            " its",
            " widest",
            " sense",
            " ()",
            " was",
            " divided",
            " into",
            " three",
            " subclasses",
            ",",
            " two",
            " of",
            " which",
            " are",
            " extinct",
            ":",
            "These",
            " three",
            " subclasses",
            " do",
            " not",
            " include",
            " all",
            " extinct",
            " amphib",
            "ians",
            ".",
            " Other",
            " extinct",
            " amphib",
            "ian",
            " groups",
            " include",
            " Emb",
            "ol",
            "omer",
            "i",
            " (",
            "Late",
            " Pale",
            "ozo",
            "ic",
            " large",
            " aquatic",
            " predators",
            "),",
            " Seymour",
            "iam",
            "or"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.357,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.478,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " groups",
            " in",
            " that",
            " region",
            ",",
            " mainly",
            " Circ",
            "ass",
            "ians",
            ",",
            " T",
            "atars",
            ",",
            " A",
            "zer",
            "is",
            ",",
            " Le",
            "z",
            "gis",
            ",",
            " Che",
            "ch",
            "ens",
            " and",
            " several",
            " Turk",
            "ic",
            " groups",
            " left",
            " their",
            " hom",
            "el",
            "ands",
            " and",
            " settled",
            " in",
            " Anat",
            "olia",
            ".",
            " As",
            " the",
            " Ottoman",
            " Empire",
            " further",
            " sh",
            "rank",
            " in",
            " the",
            " Balk",
            "an",
            " regions",
            " and",
            " then",
            " fragmented",
            " during",
            " the",
            " Balk",
            "an",
            " Wars",
            ",",
            " much"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " made",
            " up",
            " of",
            " a",
            " phosph",
            "ol",
            "ip",
            "id",
            " bil",
            "ayer",
            ",",
            " a",
            " mic",
            "elle",
            " of",
            " hydro",
            "ph",
            "obic",
            " fatty",
            " acid",
            " est",
            "ers",
            " with",
            " polar",
            ",",
            " hydro",
            "phil",
            "ic",
            " phosphate",
            " \"",
            "head",
            "\"",
            " groups",
            ".",
            " Mem",
            "br",
            "anes",
            " contain",
            " additional",
            " components",
            ",",
            " some",
            " of",
            " which",
            " can",
            " participate",
            " in",
            " acid",
            "–",
            "base",
            " reactions",
            ".",
            "In",
            " humans",
            " and",
            " many",
            " other",
            " animals",
            ",",
            " hydro",
            "chlor",
            "ic",
            " acid"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            "As",
            "i",
            "atic",
            " recon",
            "structions",
            " \"",
            "Is",
            " Om",
            "otic",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            "?\"",
            " by",
            " R",
            "olf",
            " The",
            "il",
            " (",
            "200",
            "6",
            ")",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            " webpage",
            " of",
            " Roger",
            " Bl",
            "ench",
            " (",
            "with",
            " family",
            " tree",
            ").",
            " ",
            "Language",
            " families",
            "Eth",
            "nic",
            " groups",
            " in",
            " Africa",
            "Eth",
            "nic",
            " groups",
            " in",
            " Asia",
            "Eth",
            "nic",
            " groups",
            " in",
            " Europe",
            "<|begin_of_text|>",
            "And"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            "As",
            "i",
            "atic",
            " recon",
            "structions",
            " \"",
            "Is",
            " Om",
            "otic",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            "?\"",
            " by",
            " R",
            "olf",
            " The",
            "il",
            " (",
            "200",
            "6",
            ")",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            " webpage",
            " of",
            " Roger",
            " Bl",
            "ench",
            " (",
            "with",
            " family",
            " tree",
            ").",
            " ",
            "Language",
            " families",
            "Eth",
            "nic",
            " groups",
            " in",
            " Africa",
            "Eth",
            "nic",
            " groups",
            " in",
            " Asia",
            "Eth",
            "nic",
            " groups",
            " in",
            " Europe",
            "<|begin_of_text|>",
            "And"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.016,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.009,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.477,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 54,
          "is_repeated_datapoint": false,
          "tokens": [
            "As",
            "i",
            "atic",
            " recon",
            "structions",
            " \"",
            "Is",
            " Om",
            "otic",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            "?\"",
            " by",
            " R",
            "olf",
            " The",
            "il",
            " (",
            "200",
            "6",
            ")",
            " Afro",
            "-",
            "As",
            "i",
            "atic",
            " webpage",
            " of",
            " Roger",
            " Bl",
            "ench",
            " (",
            "with",
            " family",
            " tree",
            ").",
            " ",
            "Language",
            " families",
            "Eth",
            "nic",
            " groups",
            " in",
            " Africa",
            "Eth",
            "nic",
            " groups",
            " in",
            " Asia",
            "Eth",
            "nic",
            " groups",
            " in",
            " Europe",
            "<|begin_of_text|>",
            "And"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.332,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.334,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.475,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " surrounding",
            " Air",
            " France",
            " Flight",
            " ",
            "896",
            "9",
            ",",
            " a",
            " hij",
            "acking",
            " perpetrated",
            " by",
            " the",
            " Armed",
            " Islamic",
            " Group",
            ".",
            " The",
            " Armed",
            " Islamic",
            " Group",
            " declared",
            " a",
            " ceasefire",
            " in",
            " October",
            " ",
            "199",
            "7",
            ".",
            "Al",
            "ger",
            "ia",
            " held",
            " elections",
            " in",
            " ",
            "199",
            "9",
            ",",
            " considered",
            " biased",
            " by",
            " international",
            " observers",
            " and",
            " most",
            " opposition",
            " groups",
            " which",
            " were",
            " won",
            " by",
            " President",
            " Abdel",
            "az",
            "iz",
            " Bout",
            "ef",
            "li",
            "ka",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.354,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " creation",
            " of",
            " agricultural",
            " policy",
            ",",
            " including",
            " consumers",
            ",",
            " ag",
            "rib",
            "usiness",
            ",",
            " trade",
            " l",
            "obbies",
            " and",
            " other",
            " groups",
            ".",
            " Ag",
            "rib",
            "usiness",
            " interests",
            " hold",
            " a",
            " large",
            " amount",
            " of",
            " influence",
            " over",
            " policy",
            " making",
            ",",
            " in",
            " the",
            " form",
            " of",
            " lobbying",
            " and",
            " campaign",
            " contributions",
            ".",
            " Political",
            " action",
            " groups",
            ",",
            " including",
            " those",
            " interested",
            " in",
            " environmental",
            " issues",
            " and",
            " labor",
            " unions",
            ",",
            " also",
            " provide",
            " influence",
            ",",
            " as",
            " do",
            " lobbying"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " are",
            " developed",
            " by",
            " representatives",
            " of",
            " other",
            " standards",
            " organizations",
            ",",
            " government",
            " agencies",
            ",",
            " consumer",
            " groups",
            ",",
            " companies",
            ",",
            " and",
            " others",
            ".",
            " These",
            " standards",
            " ensure",
            " that",
            " the",
            " characteristics",
            " and",
            " performance",
            " of",
            " products",
            " are",
            " consistent",
            ",",
            " that",
            " people",
            " use",
            " the",
            " same",
            " definitions",
            " and",
            " terms",
            ",",
            " and",
            " that",
            " products",
            " are",
            " tested",
            " the",
            " same",
            " way",
            ".",
            " ANSI",
            " also",
            " acc",
            "redits",
            " organizations",
            " that",
            " carry",
            " out",
            " product",
            " or",
            " personnel"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "ad",
            "aceae",
            " could",
            " be",
            " grown",
            " in",
            " the",
            " absence",
            " of",
            " phosph",
            "orus",
            " if",
            " that",
            " element",
            " were",
            " substituted",
            " with",
            " arsen",
            "ic",
            ",",
            " exploiting",
            " the",
            " fact",
            " that",
            " the",
            " arsen",
            "ate",
            " and",
            " phosphate",
            " an",
            "ions",
            " are",
            " similar",
            " struct",
            "urally",
            ".",
            " The",
            " study",
            " was",
            " widely",
            " criticised",
            " and",
            " subsequently",
            " ref",
            "uted",
            " by",
            " independent",
            " researcher",
            " groups",
            ".",
            "Ess",
            "ential",
            " trace",
            " element",
            " in",
            " higher",
            " animals",
            " ",
            "Ars",
            "enic",
            " is",
            " understood",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "35",
            ",",
            "000",
            " Chinese",
            " migrant",
            " workers",
            " lived",
            " in",
            " Algeria",
            ".",
            "The",
            " largest",
            " concentration",
            " of",
            " Alger",
            "ian",
            " migrants",
            " outside",
            " Algeria",
            " is",
            " in",
            " France",
            ",",
            " which",
            " has",
            " reportedly",
            " over",
            " ",
            "1",
            ".",
            "7",
            " million",
            " Alger",
            "ians",
            " of",
            " up",
            " to",
            " the",
            " second",
            " generation",
            ".",
            "Eth",
            "nic",
            " groups",
            "Ar",
            "abs",
            " and",
            " indigenous",
            " Ber",
            "bers",
            " as",
            " well",
            " as",
            " Ph",
            "oen",
            "icians",
            ",",
            " Romans",
            ",",
            " Vand",
            "als",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ins",
            "ular",
            " Southeast",
            " Asia",
            ".",
            " East",
            " Asian",
            "-related",
            " ancestry",
            " became",
            " dominant",
            " in",
            " Ins",
            "ular",
            " Southeast",
            " Asia",
            " already",
            " between",
            " ",
            "15",
            ",",
            "000",
            " years",
            " to",
            " ",
            "12",
            ",",
            "000",
            " years",
            " ago",
            ",",
            " and",
            " may",
            " be",
            " associated",
            " with",
            " Aust",
            "ro",
            "asi",
            "atic",
            " groups",
            ",",
            " which",
            " however",
            " got",
            " again",
            " replaced",
            " by",
            " later",
            " Austr",
            "ones",
            "ian",
            " groups",
            " some",
            " ",
            "10",
            ",",
            "000",
            " to",
            " ",
            "7",
            ",",
            "000"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.052,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ins",
            "ular",
            " Southeast",
            " Asia",
            ".",
            " East",
            " Asian",
            "-related",
            " ancestry",
            " became",
            " dominant",
            " in",
            " Ins",
            "ular",
            " Southeast",
            " Asia",
            " already",
            " between",
            " ",
            "15",
            ",",
            "000",
            " years",
            " to",
            " ",
            "12",
            ",",
            "000",
            " years",
            " ago",
            ",",
            " and",
            " may",
            " be",
            " associated",
            " with",
            " Aust",
            "ro",
            "asi",
            "atic",
            " groups",
            ",",
            " which",
            " however",
            " got",
            " again",
            " replaced",
            " by",
            " later",
            " Austr",
            "ones",
            "ian",
            " groups",
            " some",
            " ",
            "10",
            ",",
            "000",
            " to",
            " ",
            "7",
            ",",
            "000"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.324,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " radicals",
            ".",
            " The",
            " fragment",
            " resulting",
            " from",
            " the",
            " loss",
            " of",
            " a",
            " single",
            " methyl",
            " group",
            " (",
            "M",
            "Âł",
            "âĪĴ",
            "Âł",
            "15",
            ")",
            " is",
            " often",
            " absent",
            ",",
            " and",
            " other",
            " fragments",
            " are",
            " often",
            " spaced",
            " by",
            " intervals",
            " of",
            " fourteen",
            " mass",
            " units",
            ",",
            " corresponding",
            " to",
            " sequential",
            " loss",
            " of",
            " CH",
            "2",
            " groups",
            ".",
            "Chem",
            "ical",
            " properties",
            "Al",
            "kan",
            "es",
            " are",
            " only",
            " weak",
            "ly",
            " reactive",
            " with",
            " most",
            " chemical",
            " compounds",
            "."
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.471,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "am",
            "bo",
            ",",
            " the",
            " G",
            "angu",
            "ela",
            " and",
            " the",
            " X",
            "ind",
            "onga",
            ")",
            " as",
            " well",
            " as",
            " about",
            " ",
            "2",
            "%",
            " mul",
            "att",
            "os",
            " (",
            "mixed",
            " European",
            " and",
            " African",
            "),",
            " ",
            "1",
            ".",
            "6",
            "%",
            " Chinese",
            " and",
            " ",
            "1",
            "%",
            " European",
            ".",
            " The",
            " Amb",
            "und",
            "u",
            " and",
            " Ov",
            "imb",
            "und",
            "u",
            " ethnic",
            " groups",
            " combined",
            " form",
            " a",
            " majority",
            " of",
            " the",
            " population",
            ",",
            " at",
            " ",
            "62"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.018,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " cond",
            "ensation",
            " reactions",
            ".",
            " In",
            " bio",
            "chemistry",
            ",",
            " many",
            " enzymes",
            " employ",
            " acid",
            " catal",
            "ysis",
            ".",
            "Bi",
            "ological",
            " occurrence",
            "Many",
            " bi",
            "ologically",
            " important",
            " molecules",
            " are",
            " acids",
            ".",
            " N",
            "ucle",
            "ic",
            " acids",
            ",",
            " which",
            " contain",
            " acidic",
            " phosphate",
            " groups",
            ",",
            " include",
            " DNA",
            " and",
            " RNA",
            ".",
            " N",
            "ucle",
            "ic",
            " acids",
            " contain",
            " the",
            " genetic",
            " code",
            " that",
            " determines",
            " many",
            " of",
            " an",
            " organism",
            "'s",
            " characteristics",
            ",",
            " and",
            " is",
            " passed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " social",
            " ramifications",
            " of",
            " individual",
            " choice",
            ",",
            " an",
            " approach",
            " called",
            " method",
            "ological",
            " individual",
            "ism",
            ".",
            " It",
            " differs",
            " from",
            " other",
            " schools",
            " of",
            " economic",
            " thought",
            ",",
            " which",
            " have",
            " focused",
            " on",
            " aggregate",
            " variables",
            ",",
            " equilibrium",
            " analysis",
            " and",
            " societal",
            " groups",
            " rather",
            " than",
            " individuals",
            ".",
            "In",
            " the",
            " ",
            "20",
            "th",
            " and",
            " ",
            "21",
            "st",
            " centuries",
            ",",
            " economists",
            " with",
            " a",
            " method",
            "ological",
            " lineage",
            " to",
            " the",
            " early",
            " Austrian",
            " School",
            " developed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.299,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.011,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " group",
            " (-",
            "NH",
            "2",
            ")",
            " gains",
            " a",
            " proton",
            " (-",
            "NH",
            ").",
            " The",
            " entire",
            " molecule",
            " has",
            " a",
            " net",
            " neutral",
            " charge",
            " and",
            " is",
            " a",
            " z",
            "witter",
            "ion",
            ",",
            " with",
            " the",
            " exception",
            " of",
            " amino",
            " acids",
            " with",
            " basic",
            " or",
            " acidic",
            " side",
            " chains",
            ".",
            " As",
            "part",
            "ic",
            " acid",
            ",",
            " for",
            " example",
            ",",
            " possesses",
            " one",
            " proton",
            "ated",
            " am",
            "ine",
            " and",
            " two",
            " de",
            "pro",
            "ton",
            "ated",
            " car",
            "box",
            "yl",
            " groups"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " axiom",
            ",",
            " but",
            " without",
            " this",
            " axiom",
            ",",
            " we",
            " can",
            " do",
            " quite",
            " well",
            " developing",
            " (",
            "the",
            " more",
            " general",
            ")",
            " group",
            " theory",
            ",",
            " and",
            " we",
            " can",
            " even",
            " take",
            " its",
            " neg",
            "ation",
            " as",
            " an",
            " axiom",
            " for",
            " the",
            " study",
            " of",
            " non",
            "-comm",
            "ut",
            "ative",
            " groups",
            ".",
            "Thus",
            ",",
            " an",
            " axiom",
            " is",
            " an",
            " elementary",
            " basis",
            " for",
            " a",
            " formal",
            " logic",
            " system",
            " that",
            " together",
            " with",
            " the",
            " rules",
            " of",
            " inference",
            " define"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " According",
            " to",
            " research",
            " data",
            " by",
            " several",
            " institutions",
            " in",
            " ",
            "201",
            "9",
            ",",
            " the",
            " Pas",
            "ht",
            "uns",
            " are",
            " the",
            " largest",
            " ethnic",
            " group",
            ",",
            " comprising",
            " ",
            "42",
            "%,",
            " followed",
            " by",
            " Taj",
            "iks",
            ",",
            " comprising",
            " ",
            "27",
            "%",
            " of",
            " the",
            " country",
            "'s",
            " population",
            ".",
            " The",
            " other",
            " two",
            " major",
            " ethnic",
            " groups",
            " are",
            " the",
            " Haz",
            "aras",
            " and",
            " U",
            "zb",
            "eks",
            ",",
            " each",
            " at",
            " ",
            "9",
            "%.",
            " A",
            " further"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ous",
            " car",
            "box",
            "y",
            "lic",
            " acids",
            ",",
            " a",
            " carbon",
            "-car",
            "bon",
            " double",
            " bond",
            " separates",
            " the",
            " carb",
            "ony",
            "l",
            " and",
            " hydro",
            "x",
            "yl",
            " groups",
            ".",
            " As",
            "cor",
            "bic",
            " acid",
            "N",
            "ucle",
            "ic",
            " acids",
            " De",
            "oxy",
            "rib",
            "on",
            "ucle",
            "ic",
            " acid",
            " (",
            "DNA",
            ")",
            " Rib",
            "on",
            "ucle",
            "ic",
            " acid",
            " (",
            "RNA",
            ")",
            "References",
            " Listing",
            " of",
            " strengths",
            " of",
            " common",
            " acids",
            " and",
            " bases"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            "91",
            " bicycle",
            " per",
            " household",
            ".",
            " Theft",
            " is",
            " widespread",
            "in",
            " ",
            "201",
            "1",
            ",",
            " about",
            " ",
            "83",
            ",",
            "000",
            " bicycles",
            " were",
            " stolen",
            " in",
            " Amsterdam",
            ".",
            " B",
            "icy",
            "cles",
            " are",
            " used",
            " by",
            " all",
            " socio",
            "-economic",
            " groups",
            " because",
            " of",
            " their",
            " convenience",
            ",",
            " Amsterdam",
            "'s",
            " small",
            " size",
            ",",
            " the",
            " ",
            " of",
            " bike",
            " paths",
            ",",
            " the",
            " flat",
            " terrain",
            ",",
            " and",
            " the",
            " inconvenience",
            " of",
            " driving",
            " an",
            " automobile",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ous",
            " car",
            "box",
            "y",
            "lic",
            " acids",
            ",",
            " a",
            " carbon",
            "-car",
            "bon",
            " double",
            " bond",
            " separates",
            " the",
            " carb",
            "ony",
            "l",
            " and",
            " hydro",
            "x",
            "yl",
            " groups",
            ".",
            " As",
            "cor",
            "bic",
            " acid",
            "N",
            "ucle",
            "ic",
            " acids",
            " De",
            "oxy",
            "rib",
            "on",
            "ucle",
            "ic",
            " acid",
            " (",
            "DNA",
            ")",
            " Rib",
            "on",
            "ucle",
            "ic",
            " acid",
            " (",
            "RNA",
            ")",
            "References",
            " Listing",
            " of",
            " strengths",
            " of",
            " common",
            " acids",
            " and",
            " bases"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.438,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            "es",
            " or",
            " alk",
            "yl",
            " groups",
            " can",
            " also",
            " be",
            " prepared",
            " directly",
            " from",
            " alk",
            "yl",
            " hal",
            "ides",
            " in",
            " the",
            " Corey",
            "–",
            "House",
            "–",
            "Pos",
            "ner",
            "–",
            "Wh",
            "ites",
            "ides",
            " reaction",
            ".",
            " The",
            " Barton",
            "–",
            "Mc",
            "Com",
            "bie",
            " de",
            "o",
            "xygen",
            "ation",
            " removes",
            " hydro",
            "x",
            "yl",
            " groups",
            " from",
            " al",
            "coh",
            "ols",
            " e",
            ".g",
            ".",
            "and",
            " the",
            " Clem",
            "m",
            "ensen",
            " reduction",
            " removes",
            " carb",
            "ony",
            "l",
            " groups",
            " from"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.389,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.396,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.41,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "E",
            "urovision",
            " Song",
            " Contest",
            " entr",
            "ants",
            " of",
            " ",
            "197",
            "4",
            "E",
            "urovision",
            " Song",
            " Contest",
            " winners",
            "Mel",
            "od",
            "ifest",
            "ival",
            "en",
            " contestants",
            "Mel",
            "od",
            "ifest",
            "ival",
            "en",
            " winners",
            "Mus",
            "ical",
            " groups",
            " dis",
            "establish",
            "ed",
            " in",
            " ",
            "198",
            "2",
            "Mus",
            "ical",
            " groups",
            " established",
            " in",
            " ",
            "197",
            "2",
            "Mus",
            "ical",
            " groups",
            " from",
            " Stockholm",
            "Mus",
            "ical",
            " groups",
            " re",
            "establish",
            "ed"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.402,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            "ient",
            "ia",
            ".",
            " Furthermore",
            ",",
            " Sal",
            "ient",
            "ia",
            " includes",
            " all",
            " three",
            " recent",
            " orders",
            " plus",
            " the",
            " Tri",
            "assic",
            " proto",
            "-f",
            "rog",
            ",",
            " Tri",
            "ad",
            "ob",
            "atr",
            "ach",
            "us",
            ".",
            "Ev",
            "olution",
            "ary",
            " history",
            " ",
            "The",
            " first",
            " major",
            " groups",
            " of",
            " amphib",
            "ians",
            " developed",
            " in",
            " the",
            " Devon",
            "ian",
            " period",
            ",",
            " around",
            " ",
            "370",
            " million",
            " years",
            " ago",
            ",",
            " from",
            " l",
            "obe",
            "-f",
            "inned",
            " fish",
            " which",
            " were",
            " similar"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.199,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.207,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.352,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.363,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " dog",
            " weights",
            " (",
            "meaning",
            " the",
            " group",
            " is",
            " relatively",
            " homogeneous",
            ")",
            " and",
            " (",
            "b",
            ")",
            " the",
            " mean",
            " of",
            " each",
            " group",
            " is",
            " distinct",
            " (",
            "if",
            " two",
            " groups",
            " have",
            " the",
            " same",
            " mean",
            ",",
            " then",
            " it",
            " isn",
            "'t",
            " reasonable",
            " to",
            " conclude",
            " that",
            " the",
            " groups",
            " are",
            ",",
            " in",
            " fact",
            ",",
            " separate",
            " in",
            " any",
            " meaningful",
            " way",
            ").",
            "In",
            " the",
            " illustrations",
            " to",
            " the",
            " right",
            ",",
            " groups",
            " are",
            " identified",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.375,
            0.026,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " as",
            " rhetorical",
            " phenomena",
            ".",
            " Proceedings",
            " of",
            " HCI",
            " ",
            "200",
            "9",
            ":",
            " Beyond",
            " Gray",
            " D",
            "roids",
            ":",
            " Domestic",
            " Robot",
            " Design",
            " for",
            " the",
            " ",
            "21",
            "st",
            " Century",
            ".",
            " Cambridge",
            ",",
            " UK",
            ".",
            " ",
            "1",
            " September",
            ".",
            "Tel",
            "otte",
            ",",
            " J",
            ".P",
            ".",
            " Rep",
            "lications",
            ":",
            " A",
            " Rob",
            "otic",
            " History",
            " of",
            " the",
            " Science",
            " Fiction",
            " Film",
            ".",
            " University",
            " of",
            " Illinois",
            " Press",
            ",",
            " ",
            "199",
            "5",
            ".",
            "External"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.361,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.204,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " also",
            " called",
            " first",
            " order",
            " hom",
            "ogeneity",
            ".",
            "Additional",
            " properties",
            " The",
            " arithmetic",
            " mean",
            " of",
            " a",
            " sample",
            " is",
            " always",
            " between",
            " the",
            " largest",
            " and",
            " smallest",
            " values",
            " in",
            " that",
            " sample",
            ".",
            "The",
            " arithmetic",
            " mean",
            " of",
            " any",
            " amount",
            " of",
            " equal",
            "-sized",
            " number",
            " groups",
            " together",
            " is",
            " the",
            " arithmetic",
            " mean",
            " of",
            " the",
            " arithmetic",
            " means",
            " of",
            " each",
            " group",
            ".",
            "Contr",
            "ast",
            " with",
            " median",
            "The",
            " arithmetic",
            " mean",
            " may",
            " be"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.332,
            -0.0,
            -0.0,
            -0.0,
            0.201,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            ":",
            " ",
            " Birth",
            " of",
            " the",
            " Newton",
            ":",
            " ",
            " The",
            " Newton",
            " Hall",
            " of",
            " Fame",
            ":",
            " People",
            " behind",
            " the",
            " Newton",
            ":",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Why",
            " did",
            " Apple",
            " kill",
            " the",
            " Newton",
            "?:",
            " ",
            " Pen",
            " Computing",
            "'s",
            " Newton",
            " Notes",
            " column",
            " archive",
            ":",
            " ",
            " A",
            ".I",
            ".",
            " Magazine",
            " article",
            " by",
            " Ya",
            "eger",
            " on",
            " Newton",
            " H",
            "WR",
            " design",
            ",",
            " algorithms",
            ",",
            " &",
            " quality",
            ":",
            " ",
            " Associated",
            " slides",
            ":"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.326,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.31,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            "s",
            "202",
            "0",
            "s",
            "See",
            " also",
            " BA",
            "FTA",
            " Award",
            " for",
            " Best",
            " Production",
            " Design",
            " Critics",
            "'",
            " Choice",
            " Movie",
            " Award",
            " for",
            " Best",
            " Production",
            " Design",
            "Individual",
            "s",
            " with",
            " multiple",
            " wins",
            "11",
            " wins",
            " Ced",
            "ric",
            " Gib",
            "bons",
            "8",
            " wins",
            " Edwin",
            " B",
            ".",
            " Willis",
            "7",
            " wins",
            " Richard",
            " Day",
            "6",
            " wins",
            " Thomas",
            " Little",
            " Walter",
            " M"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            0.318,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ar",
            "idae",
            ";",
            " T",
            "of",
            "ield",
            "i",
            "aceae",
            " to",
            " the",
            " Mel",
            "anth",
            "iales",
            " and",
            " placed",
            " in",
            " the",
            " L",
            "ili",
            "idae",
            ".",
            "Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Group",
            "The",
            " Ang",
            "ios",
            "perm",
            " Phy",
            "log",
            "eny",
            " Group",
            " system",
            " (",
            "AP",
            "G",
            ")",
            " of",
            " ",
            "199",
            "8",
            " and",
            " AP",
            "G",
            " II",
            " (",
            "200",
            "3",
            ")",
            " assigned",
            " the",
            " Al",
            "ism",
            "ata",
            "les",
            " to",
            " the",
            " monoc"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.305,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 32,
          "is_repeated_datapoint": false,
          "tokens": [
            " currently",
            " scattered",
            " over",
            " most",
            " of",
            " Asia",
            " north",
            " of",
            " ",
            "35",
            "Â°",
            " N",
            " and",
            " in",
            " some",
            " eastern",
            " parts",
            " of",
            " Europe",
            ",",
            " extending",
            " in",
            " longitude",
            " from",
            " the",
            " Balk",
            "an",
            " Peninsula",
            " to",
            " Japan",
            ".",
            " The",
            " group",
            " is",
            " named",
            " after",
            " the",
            " Alt",
            "ai",
            " mountain",
            " range",
            " in",
            " the",
            " center",
            " of",
            " Asia",
            ".",
            "The",
            " Alta",
            "ic",
            " family",
            " was",
            " first",
            " proposed",
            " in",
            " the",
            " ",
            "18",
            "th",
            " century",
            ".",
            " It",
            " was"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 7,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " assigned",
            " it",
            " the",
            " symbol",
            " Ab",
            ",",
            " design",
            "ations",
            " that",
            " were",
            " used",
            " for",
            " a",
            " few",
            " years",
            ".",
            " In",
            " ",
            "193",
            "4",
            ",",
            " H",
            ".",
            " G",
            ".",
            " Mac",
            "Ph",
            "erson",
            " of",
            " University",
            " of",
            " California",
            ",",
            " Berkeley",
            " dispro",
            "ved",
            " Allison",
            "'s",
            " method",
            " and",
            " the",
            " validity",
            " of",
            " his",
            " discovery",
            ".",
            " There",
            " was",
            " another",
            " claim",
            " in",
            " ",
            "193",
            "7",
            ",",
            " by",
            " the",
            " chem",
            "ist",
            " Raj",
            "end",
            "ral"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.194,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.236,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            " two",
            " albums",
            " as",
            " K",
            "HM",
            ".",
            " Marc",
            " Live",
            " r",
            "apped",
            " with",
            " Ice",
            "-T",
            "'s",
            " group",
            " SM",
            "G",
            ".",
            " Marc",
            " also",
            " formed",
            " a",
            " group",
            " with",
            " Black",
            " Silver",
            " called",
            " Live",
            " Black",
            ",",
            " but",
            " while",
            " five",
            " of",
            " their",
            " tracks",
            " were",
            " released",
            " on",
            " a",
            " demo",
            " CD",
            " sold",
            " at",
            " concerts",
            ",",
            " Live",
            " Black",
            "'s",
            " first",
            " album",
            " has",
            " yet",
            " to",
            " be",
            " released",
            ".",
            "In",
            " ",
            "200",
            "8",
            ",",
            " Ice"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " Paris",
            " Pal",
            "ais",
            " Bourbon",
            " (",
            "183",
            "3",
            "–",
            "184",
            "7",
            "),",
            " one",
            " of",
            " the",
            " seats",
            " of",
            " the",
            " French",
            " Parliament",
            ".",
            " ",
            " created",
            " a",
            " statue",
            " group",
            " Achilles",
            " and",
            " Pent",
            "hes",
            "ile",
            "a",
            " (",
            "189",
            "5",
            ";",
            " Vienna",
            ").",
            " Ach",
            "ille",
            "us",
            " (",
            "190",
            "8",
            ")",
            " is",
            " a",
            " lith",
            "ography",
            " by",
            " Max",
            " S",
            "lev",
            "og",
            "t",
            ".",
            "Music",
            " ",
            "A",
            "ch",
            "illes",
            " has",
            " been",
            " frequently"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.21,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 2,
          "is_repeated_datapoint": false,
          "tokens": [
            " example",
            " of",
            " group",
            " trends",
            " in",
            " properties",
            " in",
            " the",
            " periodic",
            " table",
            ",",
            " with",
            " elements",
            " exhibiting",
            " well",
            "-character",
            "ised",
            " hom",
            "olog",
            "ous",
            " behaviour",
            ".",
            " This",
            " family",
            " of",
            " elements",
            " is",
            " also",
            " known",
            " as",
            " the",
            " lithium",
            " family",
            " after",
            " its",
            " leading",
            " element",
            ".",
            "The",
            " alk",
            "ali",
            " metals",
            " are",
            " all",
            " shiny",
            ",",
            " soft",
            ",",
            " highly",
            " reactive",
            " metals",
            " at",
            " standard",
            " temperature",
            " and",
            " pressure",
            " and",
            " readily",
            " lose",
            " their",
            " outer",
            "most",
            " electron"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.188,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " their",
            " lie",
            "ge",
            " lord",
            ",",
            " because",
            " they",
            " should",
            " maintain",
            " and",
            " defend",
            " them",
            " (",
            "Ex",
            " parte",
            " Anderson",
            " (",
            "186",
            "1",
            ")",
            " ",
            "3",
            " El",
            " &",
            " El",
            " ",
            "487",
            ";",
            " ",
            "121",
            " ER",
            " ",
            "525",
            ";",
            " China",
            " Navigation",
            " Co",
            " v",
            " Attorney",
            "-General",
            " (",
            "193",
            "2",
            ")",
            " ",
            "48",
            " T",
            "LR",
            " ",
            "375",
            ";",
            " Attorney",
            "-General",
            " v",
            " Nissan",
            " [",
            "196",
            "9",
            "]",
            " ",
            "1",
            " All",
            " ER"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " spread",
            " over",
            " such",
            " a",
            " large",
            " volume",
            " that",
            " reaching",
            " an",
            " asteroid",
            " without",
            " aiming",
            " carefully",
            " would",
            " be",
            " improbable",
            ".",
            " Nonetheless",
            ",",
            " hundreds",
            " of",
            " thousands",
            " of",
            " asteroids",
            " are",
            " currently",
            " known",
            ",",
            " and",
            " the",
            " total",
            " number",
            " ranges",
            " in",
            " the",
            " millions",
            " or",
            " more",
            ",",
            " depending",
            " on",
            " the",
            " lower",
            " size",
            " cutoff",
            ".",
            " Over",
            " ",
            "200",
            " asteroids",
            " are",
            " known",
            " to",
            " be",
            " larger",
            " than",
            " ",
            "100",
            "Âł",
            "km",
            ",",
            " and",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            0.006,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "olecular",
            " attraction",
            " that",
            " he",
            " afterwards",
            " dis",
            "av",
            "owed",
            " as",
            " worthless",
            ",",
            " was",
            " published",
            " in",
            " the",
            " journal",
            " Ann",
            "alen",
            " der",
            " Phys",
            "ik",
            " in",
            " ",
            "190",
            "0",
            ".",
            " His",
            " ",
            "24",
            "-page",
            " doctoral",
            " dissertation",
            " also",
            " addressed",
            " a",
            " topic",
            " in",
            " molecular",
            " physics",
            ".",
            " T",
            "itled",
            " \"",
            "Eine",
            " neue",
            " Best",
            "imm",
            "ung",
            " der",
            " M",
            "ole",
            "kÃ¼",
            "ld",
            "imension",
            "en",
            "\"",
            " (\"",
            "A",
            " New",
            " Determin",
            "ation",
            " of",
            " Molecular"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " also",
            " All",
            "-time",
            " tennis",
            " records",
            " –",
            " men",
            "'s",
            " singles",
            " List",
            " of",
            " Grand",
            " Slam",
            " men",
            "'s",
            " singles",
            " champions",
            " Tennis",
            " male",
            " players",
            " statistics",
            " Tennis",
            " records",
            " of",
            " the",
            " Open",
            " Era",
            " –",
            " men",
            "'s",
            " singles",
            "Notes",
            "References",
            "Further",
            " reading",
            "External",
            " links",
            "  ",
            "  ",
            " Andre",
            " Ag",
            "assi",
            " Ventures",
            " Fare",
            "well",
            " to",
            " Tennis",
            " Speech",
            " at",
            " the",
            " U",
            ".S",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " activities",
            ".",
            "List",
            " of",
            " amateur",
            " pursuits",
            " ",
            " Amateur",
            " astronomy",
            ",",
            " including",
            " a",
            " list",
            " of",
            " notable",
            " amateur",
            " astronomers",
            " Amateur",
            " chemistry",
            ",",
            " including",
            " a",
            " list",
            " of",
            " notable",
            " amateur",
            " chem",
            "ists",
            " Amateur",
            " film",
            " Amateur",
            " ge",
            "ology",
            " or",
            " rock",
            "h",
            "ounding",
            ",",
            " including",
            " a",
            " list",
            " of",
            " notable",
            " amateur",
            " ge",
            "ologists",
            " Amateur",
            " journalism",
            " Amateur",
            " radio",
            " Amateur",
            " sports",
            " Amateur",
            " theatre",
            " Amateur"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ed",
            " detain",
            "ee",
            " strapped",
            " to",
            " a",
            " chair",
            ",",
            " her",
            " legs",
            " open",
            " to",
            " reveal",
            " her",
            " sexual",
            " organs",
            ",",
            " surrounded",
            " by",
            " two",
            " torment",
            "ors",
            " dressed",
            " in",
            " everyday",
            " clothing",
            ".",
            " Andres",
            " S",
            "err",
            "ano",
            "'s",
            " P",
            "iss",
            " Christ",
            " (",
            "198",
            "9",
            ")",
            " is",
            " a",
            " photograph",
            " of",
            " a",
            " cruc",
            "ifix",
            ",",
            " sacred",
            " to",
            " the",
            " Christian",
            " religion",
            " and",
            " representing",
            " Christ",
            "'s",
            " sacrifice",
            " and",
            " final",
            " suffering",
            ",",
            " submerged",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " reached",
            " must",
            " be",
            " wrong",
            " (",
            "int",
            "uit",
            "ively",
            ",",
            " a",
            " cat",
            " cannot",
            " be",
            " a",
            " dog",
            "),",
            " and",
            " that",
            " the",
            " method",
            " by",
            " which",
            " it",
            " was",
            " reached",
            " must",
            " therefore",
            " be",
            " fall",
            "acious",
            ".",
            "Example",
            " ",
            "3",
            "Arguments",
            " of",
            " the",
            " same",
            " form",
            " can",
            " sometimes",
            " seem",
            " superficial",
            "ly",
            " convincing",
            ",",
            " as",
            " in",
            " the",
            " following",
            " example",
            ":",
            "If",
            " Brian",
            " had",
            " been",
            " thrown",
            " off",
            " the",
            " top",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    " kola",
    " tiener",
    ".XR",
    " massaggi",
    "Ø¨ÙĪØ§Ø³Ø·Ø©"
  ],
  "bottom_logits": [
    " etc",
    " another",
    " Classic",
    " classic",
    " amb"
  ],
  "act_min": -0.0,
  "act_max": 0.562
}