{
  "index": 60488,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            " axes",
            ",",
            " with",
            " the",
            " base",
            " volume",
            " on",
            " the",
            " x",
            "-axis",
            " and",
            " the",
            " solution",
            "'s",
            " pH",
            " value",
            " on",
            " the",
            " y",
            "-axis",
            ".",
            " The",
            " pH",
            " of",
            " the",
            " solution",
            " always",
            " goes",
            " up",
            " as",
            " the",
            " base",
            " is",
            " added",
            " to",
            " the",
            " solution",
            ".",
            "Example",
            ":",
            " Dip",
            "rot",
            "ic",
            " acid",
            " ",
            "For",
            " each",
            " dip",
            "rot",
            "ic",
            " acid",
            " tit",
            "ration",
            " curve",
            ",",
            " from",
            " left",
            " to",
            " right",
            ",",
            " there",
            " are",
            " two"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            ":",
            " ",
            " Kin",
            " selection",
            ".",
            " That",
            " animals",
            " and",
            " humans",
            " are",
            " more",
            " altru",
            "istic",
            " towards",
            " close",
            " kin",
            " than",
            " to",
            " distant",
            " kin",
            " and",
            " non",
            "-",
            "kin",
            " has",
            " been",
            " confirmed",
            " in",
            " numerous",
            " studies",
            " across",
            " many",
            " different",
            " cultures",
            ".",
            " Even",
            " subtle",
            " cues",
            " indicating",
            " kin",
            "ship",
            " may",
            " uncon",
            "sciously",
            " increase",
            " altru",
            "istic",
            " behavior",
            ".",
            " One",
            " kin",
            "ship",
            " cue",
            " is",
            " facial",
            " resemblance",
            ".",
            " One",
            " study",
            " found",
            " that",
            " slightly"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "aan",
            "ite",
            " languages",
            " (",
            "including",
            " Ph",
            "oen",
            "ician",
            ")",
            " in",
            " the",
            " Lev",
            "ant",
            " and",
            " Mes",
            "opot",
            "am",
            "ia",
            ".",
            "Another",
            " example",
            ":",
            " Many",
            " collo",
            "qu",
            "ial",
            " varieties",
            " are",
            " known",
            " for",
            " a",
            " type",
            " of",
            " vowel",
            " harmony",
            " in",
            " which",
            " the",
            " presence",
            " of",
            " an",
            " \"",
            "em",
            "ph",
            "atic",
            " conson",
            "ant",
            "\"",
            " triggers",
            " backed",
            " allo",
            "phones",
            " of",
            " nearby",
            " vowels",
            " (",
            "especially",
            " of",
            " the",
            " low",
            " vowels",
            " ,",
            " which"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.602,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "aan",
            "ite",
            " languages",
            " (",
            "including",
            " Ph",
            "oen",
            "ician",
            ")",
            " in",
            " the",
            " Lev",
            "ant",
            " and",
            " Mes",
            "opot",
            "am",
            "ia",
            ".",
            "Another",
            " example",
            ":",
            " Many",
            " collo",
            "qu",
            "ial",
            " varieties",
            " are",
            " known",
            " for",
            " a",
            " type",
            " of",
            " vowel",
            " harmony",
            " in",
            " which",
            " the",
            " presence",
            " of",
            " an",
            " \"",
            "em",
            "ph",
            "atic",
            " conson",
            "ant",
            "\"",
            " triggers",
            " backed",
            " allo",
            "phones",
            " of",
            " nearby",
            " vowels",
            " (",
            "especially",
            " of",
            " the",
            " low",
            " vowels",
            " ,",
            " which"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.283,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " United",
            " Nations",
            ",",
            " and",
            " the",
            " Arab",
            " Mag",
            "h",
            "reb",
            " Union",
            ",",
            " of",
            " which",
            " it",
            " is",
            " a",
            " founding",
            " member",
            ".",
            "Name",
            " ",
            "Other",
            " forms",
            " of",
            " the",
            " name",
            " are",
            ":",
            " ,",
            " ;",
            " Ber",
            "ber",
            " languages",
            ":",
            " ,",
            " ,",
            " ;",
            " .",
            " It",
            " is",
            " officially",
            " the",
            " People",
            "'s",
            " Democratic",
            " Republic",
            " of",
            " Algeria",
            " (;",
            " ,",
            " abbreviated",
            " as",
            " RAD",
            "P",
            ").",
            "Et",
            "ymology",
            "Al",
            "ger",
            "ia",
            "'s"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " (",
            "For",
            " example",
            ":",
            " \"",
            "Do",
            " you",
            " watch",
            " anime",
            "?\"",
            " or",
            " \"",
            "How",
            " much",
            " anime",
            " have",
            " you",
            " collected",
            "?",
            "\")",
            " As",
            " with",
            " a",
            " few",
            " other",
            " Japanese",
            " words",
            ",",
            " such",
            " as",
            " sak",
            "Ã©",
            " and",
            " PokÃ©mon",
            ",",
            " English",
            " texts",
            " sometimes",
            " spell",
            " anime",
            " as",
            " anim",
            "Ã©",
            " (",
            "as",
            " in",
            " French",
            "),",
            " with",
            " an",
            " acute",
            " accent",
            " over",
            " the",
            " final",
            " e",
            ",",
            " to",
            " cue",
            " the",
            " reader",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 23,
          "is_repeated_datapoint": false,
          "tokens": [
            " their",
            " pig",
            "mentation",
            ".",
            " This",
            " is",
            " the",
            " first",
            " use",
            " of",
            " a",
            " biochemical",
            " criterion",
            " in",
            " plant",
            " system",
            "atics",
            ".",
            " Harvey",
            "'s",
            " four",
            " divisions",
            " are",
            ":",
            " red",
            " algae",
            " (",
            "Rh",
            "odos",
            "per",
            "mae",
            "),",
            " brown",
            " algae",
            " (",
            "Mel",
            "anos",
            "per",
            "mae",
            "),",
            " green",
            " algae",
            " (",
            "Ch",
            "lor",
            "os",
            "per",
            "mae",
            "),",
            " and",
            " Di",
            "atom",
            "aceae",
            ".",
            "At",
            " this",
            " time",
            ",",
            " microscopic",
            " algae",
            " were",
            " discovered",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " labor",
            "-induced",
            " abortion",
            " is",
            " legally",
            " risky",
            " in",
            " the",
            " United",
            " States",
            ".",
            "Other",
            " methods",
            "Histor",
            "ically",
            ",",
            " a",
            " number",
            " of",
            " herbs",
            " reputed",
            " to",
            " possess",
            " abort",
            "if",
            "ac",
            "ient",
            " properties",
            " have",
            " been",
            " used",
            " in",
            " folk",
            " medicine",
            ".",
            " Among",
            " these",
            " are",
            ":",
            " t",
            "ans",
            "y",
            ",",
            " penny",
            "roy",
            "al",
            ",",
            " black",
            " coh",
            "osh",
            ",",
            " and",
            " the",
            " now",
            "-ext",
            "inct",
            " sil",
            "ph",
            "ium",
            ".",
            "In",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            "ides",
            ",",
            " for",
            " every",
            " two",
            " atoms",
            " of",
            " iron",
            ",",
            " there",
            " are",
            " two",
            " or",
            " three",
            " atoms",
            " of",
            " oxygen",
            " respectively",
            " (",
            "Fe",
            "2",
            "O",
            "2",
            " and",
            " Fe",
            "2",
            "O",
            "3",
            ").",
            "As",
            " a",
            " final",
            " example",
            ":",
            " nit",
            "rous",
            " oxide",
            " is",
            " ",
            "63",
            ".",
            "3",
            "%",
            " nitrogen",
            " and",
            " ",
            "36",
            ".",
            "7",
            "%",
            " oxygen",
            ",",
            " nit",
            "ric",
            " oxide",
            " is",
            " ",
            "44",
            ".",
            "05",
            "%",
            " nitrogen",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " their",
            " performance",
            " would",
            " decrease",
            ".",
            " For",
            " example",
            ":",
            " Ath",
            "letes",
            " were",
            " shown",
            " to",
            " worry",
            " more",
            " when",
            " focusing",
            " on",
            " results",
            " and",
            " perfection",
            " rather",
            " than",
            " the",
            " effort",
            " and",
            " growth",
            " involved",
            ".",
            "The",
            " Zone",
            " of",
            " Opt",
            "imal",
            " Function",
            "ing",
            " theory",
            " proposes",
            " that",
            " there",
            " is",
            " a",
            " zone",
            " where",
            " positive",
            " and",
            " negative",
            " emotions",
            " are",
            " in",
            " a",
            " balance",
            " which",
            " lead",
            " to",
            " feelings",
            " of",
            " dissoci",
            "ation",
            " and",
            " intense",
            " concentration"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.43,
            -0.0,
            -0.0,
            -0.0,
            0.57,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            "or",
            "od",
            " dimensions",
            " Shape",
            " factor",
            " (",
            "image",
            " analysis",
            " and",
            " microscopy",
            ")",
            " Finite",
            " Element",
            " Analysis",
            "Aspect",
            " ratios",
            " of",
            " simple",
            " shapes",
            "Rect",
            "angles",
            "For",
            " a",
            " rectangle",
            ",",
            " the",
            " aspect",
            " ratio",
            " denotes",
            " the",
            " ratio",
            " of",
            " the",
            " width",
            " to",
            " the",
            " height",
            " of",
            " the",
            " rectangle",
            ".",
            " A",
            " square",
            " has",
            " the",
            " smallest",
            " possible",
            " aspect",
            " ratio",
            " of",
            " ",
            "1",
            ":",
            "1",
            ".",
            "Examples",
            ":",
            " ",
            "4"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " Women",
            "'s",
            " Hand",
            "ball",
            " Super",
            " League",
            " matches",
            " scheduled",
            " in",
            " Ankara",
            ".",
            "P",
            "arks",
            " ",
            "An",
            "kara",
            " has",
            " many",
            " parks",
            " and",
            " open",
            " spaces",
            " mainly",
            " established",
            " in",
            " the",
            " early",
            " years",
            " of",
            " the",
            " Republic",
            " and",
            " well",
            " maintained",
            " and",
            " expanded",
            " thereafter",
            ".",
            " The",
            " most",
            " important",
            " of",
            " these",
            " parks",
            " are",
            ":",
            " GenÃ§",
            "lik",
            " Park",
            "Ä±",
            " (",
            "houses",
            " an",
            " amusement",
            " park",
            " with",
            " a",
            " large",
            " pond",
            " for",
            " row",
            "ing",
            "),"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.562,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " has",
            " only",
            " found",
            " an",
            " appar",
            "ition",
            ",",
            " which",
            " gets",
            " a",
            " provisional",
            " designation",
            ",",
            " made",
            " up",
            " of",
            " the",
            " year",
            " of",
            " discovery",
            ",",
            " a",
            " letter",
            " representing",
            " the",
            " half",
            "-month",
            " of",
            " discovery",
            ",",
            " and",
            " finally",
            " a",
            " letter",
            " and",
            " a",
            " number",
            " indicating",
            " the",
            " discovery",
            "'s",
            " sequential",
            " number",
            " (",
            "example",
            ":",
            " ).",
            " The",
            " last",
            " step",
            " is",
            " sending",
            " the",
            " locations",
            " and",
            " time",
            " of",
            " observations",
            " to",
            " the",
            " Minor",
            " Planet",
            " Center"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.555,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " document",
            " an",
            " algorithm",
            " (",
            "and",
            " a",
            " computer",
            " program",
            " corresponding",
            " to",
            " it",
            ").",
            " Like",
            " the",
            " program",
            " flow",
            " of",
            " a",
            " M",
            "insky",
            " machine",
            ",",
            " a",
            " flow",
            "chart",
            " always",
            " starts",
            " at",
            " the",
            " top",
            " of",
            " a",
            " page",
            " and",
            " proceeds",
            " down",
            ".",
            " Its",
            " primary",
            " symbols",
            " are",
            " only",
            " four",
            ":",
            " the",
            " directed",
            " arrow",
            " showing",
            " program",
            " flow",
            ",",
            " the",
            " rectangle",
            " (",
            "SEQU",
            "ENCE",
            ",",
            " G",
            "OTO",
            "),",
            " the",
            " diamond",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.551,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            ":",
            " on",
            " arrival",
            " to",
            " Bren",
            "ner",
            " Pass",
            ",",
            " he",
            " failed",
            " to",
            " declare",
            " his",
            " film",
            " stock",
            " to",
            " customs",
            " and",
            " it",
            " was",
            " confiscated",
            ";",
            " one",
            " actress",
            " could",
            " not",
            " enter",
            " the",
            " water",
            " for",
            " a",
            " scene",
            " because",
            " she",
            " was",
            " on",
            " her",
            " period",
            ";",
            " budget",
            " over",
            "runs",
            " meant",
            " that",
            " he",
            " had",
            " to",
            " borrow",
            " money",
            " from",
            " the",
            " actors",
            ".",
            " Hitch",
            "cock",
            " also",
            " needed",
            " a",
            " translator",
            " to",
            " give",
            " instructions"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.547,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " day",
            ".",
            " These",
            " included",
            ":",
            " supporting",
            " abortion",
            " rights",
            ",",
            " opposing",
            " the",
            " Vietnam",
            " War",
            " and",
            " the",
            " military",
            " draft",
            " (",
            "but",
            " condemning",
            " many",
            " draft",
            " dod",
            "gers",
            " as",
            " \"",
            "bum",
            "s",
            "\"),",
            " supporting",
            " Israel",
            " in",
            " the",
            " Y",
            "om",
            " K",
            "ipp",
            "ur",
            " War",
            " of",
            " ",
            "197",
            "3",
            " against",
            " a",
            " coalition",
            " of",
            " Arab",
            " nations",
            " as",
            " \"",
            "civil",
            "ized",
            " men",
            " fighting",
            " sav",
            "ages",
            "\",",
            " claiming",
            " European",
            " colon"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.543,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            " ",
            " (",
            "See",
            " also",
            ":",
            "British",
            " War",
            "ships",
            " in",
            " the",
            " Age",
            " of",
            " Sail",
            ")",
            "  ",
            " ",
            " ",
            "We",
            "bsites",
            " without",
            " authors",
            " ",
            " Canada",
            "'s",
            " Digital",
            " Collections",
            " Program",
            " ",
            " History",
            ".org",
            " ",
            " Maryland",
            " State",
            " House",
            " ",
            " The",
            " History",
            " Place",
            " ",
            " Totally",
            "history",
            ".com",
            " ",
            " U",
            ".S",
            ".",
            " Merchant",
            " Marine",
            " ",
            " U",
            ".S",
            ".",
            " National",
            " Archives",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.111,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Commonwealth",
            " authors",
            " selected",
            " to",
            " celebrate",
            " the",
            " Platinum",
            " Jub",
            "ilee",
            " of",
            " Elizabeth",
            " II",
            ".",
            "Plot",
            " summary",
            "Part",
            " ",
            "1",
            ":",
            " Alex",
            "'s",
            " world",
            "Alex",
            " is",
            " a",
            " ",
            "15",
            "-year",
            "-old",
            " gang",
            " leader",
            " living",
            " in",
            " a",
            " near",
            "-f",
            "uture",
            " dyst",
            "opian",
            " city",
            ".",
            " His",
            " friends",
            " (\"",
            "dro",
            "ogs",
            "\"",
            " in",
            " the",
            " novel",
            "'s",
            " Anglo",
            "-Russian",
            " slang",
            ",",
            " \"",
            "N",
            "ads",
            "at",
            "\")",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.539,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " States",
            ".",
            " Local",
            " ordinances",
            ",",
            " referencing",
            " the",
            " U",
            "BC",
            " added",
            " requirements",
            " to",
            " building",
            " with",
            " ad",
            "obe",
            ".",
            " These",
            " included",
            ":",
            " restriction",
            " of",
            " building",
            " height",
            " of",
            " ad",
            "obe",
            " structures",
            " to",
            " ",
            "1",
            "-story",
            ",",
            " requirements",
            " for",
            " ad",
            "obe",
            " mix",
            " (",
            "compress",
            "ive",
            " and",
            " shear",
            " strength",
            ")",
            " and",
            " new",
            " requirements",
            " which",
            " stated",
            " that",
            " every",
            " building",
            " shall",
            " be",
            " designed",
            " to",
            " withstand",
            " seismic",
            " activity",
            ",",
            " specifically",
            " lateral"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " many",
            " dispar",
            "aging",
            " remarks",
            " about",
            " Germany",
            " and",
            " the",
            " Germans",
            ".",
            " A",
            " typical",
            " example",
            " is",
            ":",
            " \"",
            "For",
            " a",
            " German",
            " it",
            " is",
            " even",
            " good",
            " to",
            " have",
            " somewhat",
            " lengthy",
            " words",
            " in",
            " his",
            " mouth",
            ",",
            " for",
            " he",
            " thinks",
            " slowly",
            ",",
            " and",
            " they",
            " give",
            " him",
            " time",
            " to",
            " reflect",
            ".\"",
            "P",
            "un",
            "ishment",
            " ",
            "The",
            " State",
            ",",
            " Sch",
            "openh",
            "auer",
            " claimed",
            ",",
            " pun",
            "ishes",
            " criminals",
            " to",
            " prevent",
            " future"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.535,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.002,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "Due",
            " to",
            " act",
            "inium",
            "'s",
            " intense",
            " radio",
            "activity",
            ",",
            " only",
            " a",
            " limited",
            " number",
            " of",
            " act",
            "inium",
            " compounds",
            " are",
            " known",
            ".",
            " These",
            " include",
            ":",
            " Ac",
            "F",
            "3",
            ",",
            " Ac",
            "Cl",
            "3",
            ",",
            " Ac",
            "Br",
            "3",
            ",",
            " Ac",
            "OF",
            ",",
            " Ac",
            "O",
            "Cl",
            ",",
            " Ac",
            "O",
            "Br",
            ",",
            " Ac",
            "2",
            "S",
            "3",
            ",",
            " Ac",
            "2",
            "O",
            "3",
            ",",
            " Ac",
            "PO",
            "4",
            " and",
            " Ac",
            "(NO"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " however",
            ",",
            " still",
            " has",
            " a",
            " strong",
            " presence",
            " in",
            " older",
            " segments",
            " of",
            " the",
            " population",
            " and",
            " in",
            " areas",
            " with",
            " high",
            " numbers",
            " of",
            " immigrants",
            ".",
            " Some",
            " examples",
            " of",
            " common",
            ",",
            " traditional",
            " and",
            " unique",
            " A",
            "arhus",
            "ian",
            " words",
            " are",
            ":",
            " tr",
            "Ã¦",
            "ls",
            " ('",
            "t",
            "ires",
            "ome",
            "'),",
            " n",
            "oller",
            " ('",
            "s",
            "illy",
            "'",
            " or",
            " '",
            "d",
            "umb",
            "')",
            " and",
            " d",
            "Ã¦",
            "l",
            "me",
            " (",
            "ex",
            "cl"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.038,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.531,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " copies",
            ".)",
            " ",
            " After",
            " Christie",
            "'s",
            " author",
            "ship",
            " of",
            " the",
            " first",
            " four",
            " West",
            "mac",
            "ott",
            " novels",
            " was",
            " revealed",
            " by",
            " a",
            " journalist",
            " in",
            " ",
            "194",
            "9",
            ",",
            " she",
            " wrote",
            " two",
            " more",
            ",",
            " the",
            " last",
            " in",
            " ",
            "195",
            "6",
            ".",
            "The",
            " other",
            " West",
            "mac",
            "ott",
            " titles",
            " are",
            ":",
            " Un",
            "finished",
            " Portrait",
            " (",
            "193",
            "4",
            "),",
            " Abs",
            "ent",
            " in",
            " the",
            " Spring",
            " (",
            "194",
            "4",
            "),",
            " The"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " interview",
            " with",
            " Na",
            "um",
            " Abram",
            "ov",
            " that",
            " \"",
            "they",
            " can",
            " be",
            " counted",
            " on",
            " the",
            " fingers",
            " of",
            " one",
            " hand",
            "\".",
            " In",
            " ",
            "197",
            "2",
            ",",
            " T",
            "ark",
            "ovsky",
            " told",
            " film",
            " historian",
            " Leon",
            "id",
            " K",
            "oz",
            "lov",
            " his",
            " ten",
            " favorite",
            " films",
            ".",
            " The",
            " list",
            " includes",
            ":",
            " Diary",
            " of",
            " a",
            " Country",
            " Priest",
            " and",
            " M",
            "ouch",
            "ette",
            " by",
            " Robert",
            " B",
            "ress",
            "on",
            ";",
            " Winter",
            " Light",
            ",",
            " Wild"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " interview",
            " with",
            " Na",
            "um",
            " Abram",
            "ov",
            " that",
            " \"",
            "they",
            " can",
            " be",
            " counted",
            " on",
            " the",
            " fingers",
            " of",
            " one",
            " hand",
            "\".",
            " In",
            " ",
            "197",
            "2",
            ",",
            " T",
            "ark",
            "ovsky",
            " told",
            " film",
            " historian",
            " Leon",
            "id",
            " K",
            "oz",
            "lov",
            " his",
            " ten",
            " favorite",
            " films",
            ".",
            " The",
            " list",
            " includes",
            ":",
            " Diary",
            " of",
            " a",
            " Country",
            " Priest",
            " and",
            " M",
            "ouch",
            "ette",
            " by",
            " Robert",
            " B",
            "ress",
            "on",
            ";",
            " Winter",
            " Light",
            ",",
            " Wild"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 55,
          "is_repeated_datapoint": false,
          "tokens": [
            " this",
            " progresses",
            " it",
            " will",
            " begin",
            " to",
            " affect",
            " daily",
            " functioning",
            " and",
            " an",
            " individual",
            "'s",
            " general",
            " quality",
            " of",
            " life",
            ".",
            " It",
            " is",
            " reported",
            " by",
            " the",
            " Cleveland",
            " Clinic",
            " that",
            " panic",
            " disorder",
            " affects",
            " ",
            "2",
            " to",
            " ",
            "3",
            " percent",
            " of",
            " adult",
            " Americans",
            " and",
            " can",
            " begin",
            " around",
            " the",
            " time",
            " of",
            " the",
            " teenage",
            " and",
            " early",
            " adult",
            " years",
            ".",
            " Some",
            " symptoms",
            " include",
            ":",
            " difficulty",
            " breathing",
            ",",
            " chest",
            " pain",
            ",",
            " d"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " is",
            " to",
            " overcome",
            " oneself",
            " instead",
            " of",
            " cultivating",
            " violence",
            " or",
            " aggress",
            "iveness",
            ".",
            " Mori",
            "hei",
            " U",
            "esh",
            "iba",
            " used",
            " the",
            " phrase",
            " ",
            " to",
            " refer",
            " to",
            " this",
            " principle",
            ".",
            "A",
            "ik",
            "ido",
            "'s",
            " fundamental",
            " principles",
            " include",
            ":",
            " ",
            " (",
            "enter",
            "ing",
            "),",
            " ,",
            " ",
            " (",
            "bre",
            "athing",
            " control",
            "),",
            " ",
            " (",
            "tri",
            "angular",
            " principle",
            ")",
            " and",
            " ",
            " (",
            "turn",
            "ing",
            ")",
            " movements",
            " that",
            " redirect",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "1",
            ":",
            " Phon",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "195",
            "7",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " II",
            ".",
            " Form"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.516,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.083,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Register",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "3",
            ":",
            " Index",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Rob",
            "be",
            "ets",
            ",",
            " Mart",
            "ine",
            ".",
            " ",
            "200",
            "4",
            ".",
            " \"",
            "Sw",
            "adesh",
            " ",
            "100",
            " on",
            " Japanese",
            ",",
            " Korean",
            " and",
            " Alta",
            "ic",
            ".\"",
            " Tokyo"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.159,
            -0.0,
            -0.0
          ],
          "train_token_ind": 18,
          "is_repeated_datapoint": false,
          "tokens": [
            " Online",
            " version",
            " at",
            " the",
            " Per",
            "se",
            "us",
            " Digital",
            " Library",
            ".",
            " Pl",
            "ut",
            "arch",
            ".",
            " Lives",
            ",",
            " Volume",
            " I",
            ":",
            " These",
            "us",
            " and",
            " Rom",
            "ulus",
            ".",
            " Ly",
            "cur",
            "g",
            "us",
            " and",
            " N",
            "uma",
            ".",
            " Sol",
            "on",
            " and",
            " Public",
            "ola",
            ".",
            " Trans",
            "lated",
            " by",
            " Bern",
            "ad",
            "otte",
            " Perr",
            "in",
            ".",
            " Lo",
            "eb",
            " Classical",
            " Library",
            " No",
            ".",
            " ",
            "46",
            ".",
            " Cambridge",
            ",",
            " Massachusetts",
            ":",
            " Harvard",
            " University"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            "leh",
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "2",
            ":",
            " Morph",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "196",
            "6",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " III"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Volume",
            " II",
            ":",
            " Perspectives",
            " from",
            " East",
            " and",
            " West",
            " in",
            " Late",
            " Ant",
            "iqu",
            "ity",
            ",",
            " Cambridge",
            " University",
            " Press",
            ",",
            " ",
            "201",
            "1",
            ",",
            " p",
            ".",
            "Âł",
            "59",
            "–",
            "69",
            ".",
            " AndrÃ©",
            "-J",
            "ean",
            " Fest",
            "ug",
            "iÃ¨re",
            ",",
            " La",
            " RÃ©",
            "vÃ©",
            "lation",
            " d",
            "'H",
            "erm",
            "Ã¨s",
            " Tr",
            "ism",
            "Ã©g",
            "iste",
            ",",
            " Paris",
            ",",
            " Les",
            " Bel",
            "les",
            " Let",
            "tres",
            ",",
            " ",
            "201",
            "4",
            " ,",
            " ."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.512,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.082,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "en",
            "leh",
            "re",
            ",",
            " '",
            "Introduction",
            " to",
            " Alta",
            "ic",
            " Lingu",
            "istics",
            ",",
            " Volume",
            " ",
            "2",
            ":",
            " Morph",
            "ology",
            "',",
            " edited",
            " and",
            " published",
            " by",
            " Pent",
            "ti",
            " A",
            "alto",
            ".",
            " Helsinki",
            ":",
            " Su",
            "omal",
            "ais",
            "-U",
            "gr",
            "il",
            "ainen",
            " Se",
            "ura",
            ".",
            "Ram",
            "sted",
            "t",
            ",",
            " G",
            ".J",
            ".",
            " ",
            "196",
            "6",
            ".",
            " E",
            "inf",
            "Ã¼hrung",
            " in",
            " die",
            " alta",
            "ische",
            " Spr",
            "ach",
            "w",
            "issenschaft",
            " III"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.119,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.484,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " algorithm",
            " in",
            " pseud",
            "ocode",
            " or",
            " pid",
            "gin",
            " code",
            ":",
            " Input",
            ":",
            " A",
            " list",
            " of",
            " numbers",
            " L",
            ".",
            " Output",
            ":",
            " The",
            " largest",
            " number",
            " in",
            " the",
            " list",
            " L",
            ".",
            " if",
            " L",
            ".size",
            " =",
            " ",
            "0",
            " return",
            " null",
            " largest",
            " âĨĲ",
            " L",
            "[",
            "0",
            "]",
            " for",
            " each",
            " item",
            " in",
            " L",
            ",",
            " do",
            "    ",
            " if",
            " item",
            " >",
            " largest",
            ",",
            " then",
            "        ",
            " largest",
            " âĨĲ"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " asked",
            " for",
            " a",
            " plea",
            ".",
            " The",
            " available",
            " pleas",
            " are",
            ":",
            " guilty",
            ",",
            " not",
            " guilty",
            ",",
            " and",
            " no",
            " plea",
            ".",
            " The",
            " response",
            " of",
            " \"",
            "no",
            " plea",
            "\"",
            " allows",
            " the",
            " defendant",
            " to",
            " get",
            " legal",
            " advice",
            " on",
            " the",
            " plea",
            ",",
            " which",
            " must",
            " be",
            " made",
            " on",
            " the",
            " second",
            " appearance",
            ".",
            "South",
            " Africa",
            " ",
            "In",
            " South",
            " Africa",
            ",",
            " arr",
            "a",
            "ignment",
            " is",
            " defined",
            " as",
            " the",
            " calling",
            " upon",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.22,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.42,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.25,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.369,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.176,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.279,
            -0.0,
            -0.0,
            -0.0,
            0.365,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.175,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " countries",
            " ",
            "16",
            ":",
            "9",
            " =",
            " ",
            "1",
            ".:",
            " wides",
            "creen",
            " TV",
            " and",
            " most",
            " laptops",
            " ",
            "2",
            ":",
            "1",
            " =",
            " ",
            "2",
            ":",
            " dom",
            "ino",
            "es",
            " ",
            "64",
            ":",
            "27",
            " =",
            " ",
            "2",
            ".:",
            " ultra",
            "-w",
            "ides",
            "creen",
            ",",
            " ",
            "21",
            ":",
            "9",
            " ",
            "32",
            ":",
            "9",
            " =",
            " ",
            "3",
            ".:",
            " super",
            " ultra",
            "-w",
            "ides",
            "creen",
            "Ell",
            "ips"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.457,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " often",
            " rely",
            " on",
            " ambiguous",
            " words",
            " for",
            " artistic",
            " effect",
            ",",
            " as",
            " in",
            " the",
            " song",
            " title",
            " \"",
            "Don",
            "'t",
            " It",
            " Make",
            " My",
            " Brown",
            " Eyes",
            " Blue",
            "\"",
            " (",
            "where",
            " \"",
            "blue",
            "\"",
            " can",
            " refer",
            " to",
            " the",
            " color",
            ",",
            " or",
            " to",
            " sadness",
            ").",
            "In",
            " the",
            " narrative",
            ",",
            " ambiguity",
            " can",
            " be",
            " introduced",
            " in",
            " several",
            " ways",
            ":",
            " motive",
            ",",
            " plot",
            ",",
            " character",
            ".",
            " F",
            ".",
            " Scott",
            " Fitzgerald",
            " uses",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            0.443,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.314,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.295,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 1,
          "is_repeated_datapoint": false,
          "tokens": [
            "04",
            ":",
            "26",
            " and",
            " sets",
            " at",
            " ",
            "21",
            ":",
            "58",
            ",",
            " providing",
            " ",
            "17",
            " hours",
            " ",
            "32",
            " minutes",
            " of",
            " daylight",
            ".",
            " On",
            " the",
            " winter",
            " sol",
            "stice",
            ",",
            " it",
            " rises",
            " at",
            " ",
            "08",
            ":",
            "37",
            " and",
            " sets",
            " at",
            " ",
            "15",
            ":",
            "39",
            " with",
            " ",
            "7",
            " hours",
            " and",
            " ",
            "2",
            " minutes",
            " of",
            " daylight",
            ".",
            " The",
            " difference",
            " in",
            " length",
            " of",
            " days",
            " and",
            " nights",
            " between",
            " summer",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.381,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            "):",
            " It",
            " was",
            " only",
            " with",
            " the",
            " development",
            ",",
            " beginning",
            " in",
            " the",
            " ",
            "193",
            "0",
            "s",
            ",",
            " of",
            " electrom",
            "ech",
            "anical",
            " calcul",
            "ators",
            " using",
            " electrical",
            " rel",
            "ays",
            ",",
            " that",
            " machines",
            " were",
            " built",
            " having",
            " the",
            " scope",
            " B",
            "abbage",
            " had",
            " envisioned",
            ".\"",
            "Math",
            "ematics",
            " during",
            " the",
            " ",
            "19",
            "th",
            " century",
            " up",
            " to",
            " the",
            " mid",
            "-",
            "20",
            "th",
            " century",
            " ",
            "Symbols",
            " and",
            " rules",
            ":",
            " In",
            " rapid",
            " succession"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.416,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " such",
            " systems",
            " includes",
            ":",
            " Lincoln",
            " Near",
            "-E",
            "arth",
            " Aster",
            "oid",
            " Research",
            " (",
            "LINE",
            "AR",
            ")",
            " Near",
            "-E",
            "arth",
            " Aster",
            "oid",
            " Tracking",
            " (",
            "NE",
            "AT",
            ")",
            " Space",
            "watch",
            " Lowell",
            " Observatory",
            " Near",
            "-E",
            "arth",
            "-",
            "Object",
            " Search",
            " (",
            "L",
            "ONE",
            "OS",
            ")",
            " Catal",
            "ina",
            " Sky",
            " Survey",
            " (",
            "CSS",
            ")",
            " Pan",
            "-ST",
            "ARR",
            "S",
            " NE",
            "OW",
            "ISE",
            " Aster",
            "oid",
            " Ter",
            "restrial",
            "-",
            "impact"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "8",
            " June",
            " ",
            "201",
            "2",
            " in",
            " Tampa",
            ",",
            " FL",
            ",",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " comprising",
            " ",
            "17",
            " Barr",
            "ac",
            "uda",
            " players",
            " and",
            " ",
            "7",
            " from",
            " the",
            " lower",
            " English",
            " professional",
            " leagues",
            ",",
            " scored",
            " a",
            " goal",
            " against",
            " the",
            " United",
            " States",
            ".",
            " However",
            ",",
            " the",
            " team",
            " lost",
            " ",
            "3",
            ":",
            "1",
            " to",
            " the",
            " US",
            ".",
            "Daniel",
            " Bailey",
            " had",
            " become",
            " the",
            " first",
            " Ant",
            "ig"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.285,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.208,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.367,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.24,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " L",
            ",",
            " S",
            " ",
            " [",
            "Initialize",
            " R",
            ":",
            " make",
            " the",
            " remaining",
            " length",
            " r",
            " equal",
            " to",
            " the",
            " starting",
            "/",
            "initial",
            "/input",
            " length",
            " l",
            "]:",
            " R",
            " âĨĲ",
            " L",
            "E",
            "0",
            ":",
            " [",
            "Ensure",
            " r",
            " âī¥",
            " s",
            ".",
            "]",
            " ",
            " [",
            "Ensure",
            " the",
            " smaller",
            " of",
            " the",
            " two",
            " numbers",
            " is",
            " in",
            " S",
            " and",
            " the",
            " larger",
            " in",
            " R",
            "]:",
            " IF",
            " R",
            " >",
            " S",
            " THEN",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.328,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 28,
          "is_repeated_datapoint": false,
          "tokens": [
            " entered",
            " Columbia",
            " about",
            " three",
            " hours",
            " and",
            " ten",
            " minutes",
            " before",
            " launch",
            " time",
            ".",
            " Along",
            " with",
            " a",
            " technician",
            ",",
            " he",
            " helped",
            " Armstrong",
            " into",
            " the",
            " left",
            "-hand",
            " couch",
            " at",
            " ",
            "06",
            ":",
            "54",
            ".",
            " Five",
            " minutes",
            " later",
            ",",
            " Collins",
            " joined",
            " him",
            ",",
            " taking",
            " up",
            " his",
            " position",
            " on",
            " the",
            " right",
            "-hand",
            " couch",
            ".",
            " Finally",
            ",",
            " Ald",
            "rin",
            " entered",
            ",",
            " taking",
            " the",
            " center",
            " couch",
            ".",
            " H",
            "aise",
            " left"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.326,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.254,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.264,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            "36",
            "Ar",
            "Âł",
            ":",
            "Âł",
            "38",
            "Ar",
            "Âł",
            ":",
            "Âł",
            "40",
            "Ar",
            " in",
            " the",
            " atmos",
            "pheres",
            " of",
            " the",
            " outer",
            " planets",
            " is",
            " ",
            "840",
            "0",
            "Âł",
            ":",
            "Âł",
            "160",
            "0",
            "Âł",
            ":",
            "Âł",
            "1",
            ".",
            " This",
            " contrasts",
            " with",
            " the",
            " low",
            " abundance",
            " of",
            " prim",
            "ordial",
            " ",
            " in",
            " Earth",
            "'s",
            " atmosphere",
            ",",
            " which",
            " is",
            " only",
            " ",
            "31",
            ".",
            "5",
            " ppm",
            "v",
            " (=",
            " ",
            "934",
            "0",
            " ppm"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.303,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " original",
            " Arabic",
            " at",
            " this",
            " time",
            ".",
            " (",
            "However",
            ",",
            " his",
            " other",
            " book",
            " on",
            " algebra",
            " remains",
            ".)",
            "In",
            " the",
            " early",
            " ",
            "12",
            "th",
            " century",
            ",",
            " Latin",
            " translations",
            " of",
            " said",
            " al",
            "-K",
            "h",
            "war",
            "iz",
            "mi",
            " texts",
            " involving",
            " the",
            " Hindu",
            "–",
            "Ar",
            "abic",
            " numeral",
            " system",
            " and",
            " arithmetic",
            " appeared",
            ":",
            " Liber",
            " Al",
            "gh",
            "o",
            "ar",
            "ism",
            "i",
            " de",
            " pract",
            "ica",
            " ar",
            "ism",
            "etr",
            "ice",
            " ("
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.055,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.26,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "chron",
            "ological",
            ")",
            "  ",
            "  ",
            "  ",
            "  ",
            " Miscellaneous",
            "'''",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "   ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            "  ",
            " ",
            "Further",
            " reading",
            "Articles",
            " Hitch",
            "cock",
            "'s",
            " Style",
            " –",
            " B",
            "FI",
            " Screen",
            "online",
            " Alfred",
            " Hitch",
            "cock",
            ":",
            " England",
            "'s",
            " Biggest",
            " and",
            " Best",
            " Director",
            " Goes",
            " to",
            " Hollywood",
            " –",
            " Life",
            ",",
            " ",
            "20",
            " November"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.014,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.037,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.237,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.041,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " north",
            " eastern",
            " India",
            " and",
            " adjacent",
            " region",
            " of",
            " Bangladesh",
            "P",
            "ala",
            "ung",
            "o",
            "-K",
            "h",
            "mu",
            "ic",
            " languages",
            " Kh",
            "mu",
            "ic",
            ":",
            " ",
            "13",
            " languages",
            " of",
            " Laos",
            " and",
            " Thailand",
            "P",
            "ala",
            "ung",
            "o",
            "-P",
            "akan",
            "ic",
            " languages",
            " Pak",
            "anic",
            " or",
            " P",
            "aly",
            "u",
            ":",
            " ",
            "4",
            " or",
            " ",
            "5",
            " languages",
            " of",
            " southern",
            " China",
            " and",
            " Vietnam",
            " P",
            "ala",
            "ung",
            "ic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.232,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.23,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 6,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Slovakia",
            ".",
            " Andy",
            " War",
            "hol",
            ":",
            " A",
            " Documentary",
            " Film",
            " (",
            "200",
            "6",
            ")",
            " is",
            " a",
            " rever",
            "ential",
            ",",
            " four",
            "-hour",
            " movie",
            " by",
            " Ric",
            " Burns",
            " that",
            " won",
            " a",
            " Pe",
            "ab",
            "ody",
            " Award",
            " in",
            " ",
            "200",
            "6",
            ".",
            " Andy",
            " War",
            "hol",
            ":",
            " Double",
            " Denied",
            " (",
            "200",
            "6",
            ")",
            " is",
            " a",
            " ",
            "52",
            "-minute",
            " movie",
            " by",
            " Ian",
            " Y",
            "ent",
            "ob",
            " about",
            " the",
            " difficulties",
            " authentic",
            "ating"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.192,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 26,
          "is_repeated_datapoint": false,
          "tokens": [
            "thic",
            " triumph",
            " as",
            " much",
            " as",
            " he",
            " did",
            " a",
            " defeat",
            ".",
            " From",
            " A",
            "Ã«",
            "t",
            "ius",
            "'",
            " point",
            " of",
            " view",
            ",",
            " the",
            " best",
            " outcome",
            " was",
            " what",
            " occurred",
            ":",
            " The",
            "od",
            "oric",
            " died",
            ",",
            " At",
            "til",
            "a",
            " was",
            " in",
            " retreat",
            " and",
            " dis",
            "array",
            ",",
            " and",
            " the",
            " Romans",
            " had",
            " the",
            " benefit",
            " of",
            " appearing",
            " victorious",
            ".",
            "In",
            "vasion",
            " of",
            " Italy",
            " and",
            " death",
            " ",
            "At",
            "til",
            "a",
            " returned"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.173,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            "500",
            "Âł",
            "million",
            " iP",
            "ads",
            ",",
            " though",
            " sales",
            " peaked",
            " in",
            " ",
            "201",
            "3",
            ".",
            " The",
            " iPad",
            " still",
            " remains",
            " the",
            " most",
            " popular",
            " tablet",
            " computer",
            " by",
            " sales",
            " ,",
            " and",
            " accounted",
            " for",
            " nine",
            " percent",
            " of",
            " the",
            " company",
            "'s",
            " revenue",
            " .",
            "Apple",
            " sells",
            " several",
            " iPad",
            " accessories",
            ",",
            " including",
            " the",
            " Apple",
            " P",
            "encil",
            ",",
            " Smart",
            " Keyboard",
            ",",
            " Smart",
            " Keyboard",
            " Fol",
            "io",
            ",",
            " Magic",
            " Keyboard",
            ",",
            " and",
            " several",
            " adapters"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " inter",
            "mar",
            "riage",
            " and",
            " family",
            " ties",
            "\"",
            " Construction",
            " of",
            " a",
            " monumental",
            " tomb",
            " for",
            " his",
            " father",
            " Philip",
            ",",
            " \"",
            "to",
            " match",
            " the",
            " greatest",
            " of",
            " the",
            " py",
            "ramids",
            " of",
            " Egypt",
            "\"",
            " Con",
            "quest",
            " of",
            " Arabia",
            " Circ",
            "umn",
            "avigation",
            " of",
            " Africa",
            "The",
            " enormous",
            " scale",
            " of",
            " these",
            " plans",
            " has",
            " led",
            " many",
            " scholars",
            " to",
            " doubt",
            " their",
            " historic",
            "ity",
            ".",
            " Ernst",
            " B",
            "adian",
            " argued",
            " that",
            " they",
            " were"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " birthday",
            " with",
            " a",
            " party",
            ".",
            " For",
            " this",
            " occasion",
            ",",
            " AB",
            "BA",
            " recorded",
            " the",
            " track",
            " \"",
            "H",
            "ov",
            "as",
            " V",
            "itt",
            "ne",
            "\"",
            " (",
            "a",
            " pun",
            " on",
            " the",
            " Swedish",
            " name",
            " for",
            " Jehovah",
            "'s",
            " Witness",
            " and",
            " Anderson",
            "'s",
            " birth",
            "place",
            ",",
            " H",
            "ova",
            ")",
            " as",
            " a",
            " tribute",
            " to",
            " him",
            ",",
            " and",
            " released",
            " it",
            " only",
            " on",
            " ",
            "200",
            " red",
            " vinyl",
            " copies",
            ",",
            " to",
            " be",
            " distributed",
            " to"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " publicity",
            " photos",
            " he",
            " was",
            " shown",
            " doing",
            " so",
            ".",
            " He",
            " also",
            " used",
            " this",
            " as",
            " an",
            " excuse",
            " to",
            " never",
            " have",
            " to",
            " change",
            " his",
            " films",
            " from",
            " his",
            " initial",
            " vision",
            ".",
            " If",
            " a",
            " studio",
            " asked",
            " him",
            " to",
            " change",
            " a",
            " film",
            ",",
            " he",
            " would",
            " claim",
            " that",
            " it",
            " was",
            " already",
            " shot",
            " in",
            " a",
            " single",
            " way",
            ",",
            " and",
            " that",
            " there",
            " were",
            " no",
            " alternative",
            " takes",
            " to",
            " consider",
            ".",
            "This",
            " view"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " electoral",
            " code",
            ",",
            " and",
            " the",
            " representation",
            " of",
            " women",
            " in",
            " elected",
            " bodies",
            ".",
            " In",
            " April",
            " ",
            "201",
            "1",
            ",",
            " Bout",
            "ef",
            "li",
            "ka",
            " promised",
            " further",
            " constitutional",
            " and",
            " political",
            " reform",
            ".",
            " However",
            ",",
            " elections",
            " are",
            " routinely",
            " criticised",
            " by",
            " opposition",
            " groups",
            " as",
            " unfair",
            " and",
            " international",
            " human",
            " rights",
            " groups",
            " say",
            " that",
            " media",
            " censorship",
            " and",
            " harassment",
            " of",
            " political",
            " opponents",
            " continue",
            ".",
            "On",
            " ",
            "2",
            " April",
            " ",
            "201"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "5",
            ",",
            " Frid",
            "a",
            " ens",
            "am",
            ",",
            " which",
            " included",
            " the",
            " original",
            " Swedish",
            " rendition",
            " of",
            " \"",
            "F",
            "ern",
            "ando",
            "\",",
            " a",
            " hit",
            " on",
            " the",
            " Swedish",
            " radio",
            " charts",
            " before",
            " the",
            " English",
            " version",
            " was",
            " released",
            " by",
            " AB",
            "BA",
            ".",
            "A",
            "gn",
            "eth",
            "a",
            " F",
            "Ã¤l",
            "ts",
            "k",
            "og",
            " (",
            "born",
            " ",
            "5",
            " April",
            " ",
            "195",
            "0",
            " in",
            " J",
            "Ã¶n",
            "kÃ¶",
            "ping",
            ",",
            " Sweden",
            ")",
            " sang",
            " with"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "agua",
    "uitka",
    "ahoma",
    "agos",
    "ince"
  ],
  "bottom_logits": [
    " Jas",
    " fast",
    "ummings",
    "otton",
    "Î³Î±"
  ],
  "act_min": -0.0,
  "act_max": 0.629
}