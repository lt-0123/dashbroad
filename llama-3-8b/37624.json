{
  "index": 37624,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.84,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            "-century",
            " American",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " American",
            " novel",
            "ists",
            "20",
            "th",
            "-century",
            " American",
            " short",
            " story",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " male",
            " writers",
            "20",
            "th",
            "-century",
            " Canadian",
            " short",
            " story",
            " writers",
            "American",
            " male",
            " novel",
            "ists",
            "American",
            " male",
            " short",
            " story",
            " writers",
            "American",
            " science",
            " fiction",
            " writers",
            "An",
            "alog",
            " Science",
            " Fiction",
            " and",
            " Fact",
            " people",
            "Canadian",
            " M",
            "ennon",
            "ites"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.832,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 47,
          "is_repeated_datapoint": false,
          "tokens": [
            " inside",
            " a",
            " glass",
            " of",
            " milk",
            ",",
            " perhaps",
            " poisoned",
            ",",
            " that",
            " Grant",
            " is",
            " bringing",
            " to",
            " his",
            " wife",
            ";",
            " the",
            " light",
            " ensures",
            " that",
            " the",
            " audience",
            "'s",
            " attention",
            " is",
            " on",
            " the",
            " glass",
            ".",
            " Grant",
            "'s",
            " character",
            " is",
            " actually",
            " a",
            " killer",
            ",",
            " as",
            " per",
            " written",
            " in",
            " the",
            " book",
            ",",
            " Before",
            " the",
            " Fact",
            " by",
            " Francis",
            " I",
            "les",
            ",",
            " but",
            " the",
            " studio",
            " felt",
            " that",
            " Grant",
            "'s",
            " image",
            " would",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 46,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " most",
            " are",
            " thought",
            " to",
            " adhere",
            " to",
            " the",
            " Sunni",
            " Han",
            "afi",
            " school",
            ".",
            " According",
            " to",
            " Pew",
            " Research",
            " Center",
            ",",
            " as",
            " much",
            " as",
            " ",
            "90",
            "%",
            " are",
            " of",
            " the",
            " Sunni",
            " denomination",
            ",",
            " ",
            "7",
            "%",
            " Shia",
            " and",
            " ",
            "3",
            "%",
            " non",
            "-den",
            "omin",
            "ational",
            ".",
            " The",
            " CIA",
            " Fact",
            "book",
            " various",
            "ly",
            " estimates",
            " up",
            " to",
            " ",
            "89",
            ".",
            "7",
            "%",
            " Sunni",
            " or",
            " up",
            " to",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Azerbaijan",
            " at",
            " University",
            " of",
            " Colorado",
            " at",
            " Boulder",
            " Country",
            " profile",
            " from",
            " BBC",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Azerbaijan",
            " from",
            " International",
            " Futures",
            " V",
            "isions",
            " of",
            " Azerbaijan",
            " Journal",
            " of",
            " The",
            " European",
            " Azerbaijan",
            " Society",
            "Major",
            " government",
            " resources",
            " President",
            " of",
            " Azerbaijan",
            " website",
            " Azerbaijan",
            " State",
            " Statistical",
            " Committee",
            " United",
            " Nations",
            " Office",
            " in",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.73,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Azerbaijan",
            " at",
            " University",
            " of",
            " Colorado",
            " at",
            " Boulder",
            " Country",
            " profile",
            " from",
            " BBC",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Azerbaijan",
            " from",
            " International",
            " Futures",
            " V",
            "isions",
            " of",
            " Azerbaijan",
            " Journal",
            " of",
            " The",
            " European",
            " Azerbaijan",
            " Society",
            "Major",
            " government",
            " resources",
            " President",
            " of",
            " Azerbaijan",
            " website",
            " Azerbaijan",
            " State",
            " Statistical",
            " Committee",
            " United",
            " Nations",
            " Office",
            " in",
            " Azerbaijan"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            "References",
            "C",
            "itations",
            "General",
            " and",
            " cited",
            " sources",
            "Further",
            " reading",
            "External",
            " links",
            " ",
            " Afghanistan",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " ",
            " Research",
            " Guide",
            " to",
            " Afghanistan",
            " ",
            " ",
            "170",
            "9",
            " establishments",
            " in",
            " Asia",
            "Central",
            " Asian",
            " countries",
            "Countries",
            " in",
            " Asia",
            "Em",
            "irates",
            "Iran",
            "ian",
            " Plate",
            "au",
            "Islamic",
            " states",
            "Land",
            "locked",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.723,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            " Ant",
            "igua",
            ",",
            " West",
            " Indies",
            ".",
            " Thomas",
            " Hear",
            "ne",
            ".",
            " Southampton",
            ".",
            "External",
            " links",
            "  ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ",",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " Ant",
            "igua",
            " and",
            " Barb",
            "uda",
            " from",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "196",
            "0",
            ".",
            "External",
            " links",
            " Govern",
            " d",
            "'",
            "And",
            "orra",
            " Official",
            " governmental",
            " site",
            " ",
            " And",
            "orra",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Port",
            "als",
            " to",
            " the",
            " World",
            " from",
            " the",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " And",
            "orra",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " And",
            "orra",
            " from",
            " the",
            " BBC",
            " News",
            " And",
            "orra",
            " –",
            " Gu"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.719,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " ",
            "196",
            "0",
            ".",
            "External",
            " links",
            " Govern",
            " d",
            "'",
            "And",
            "orra",
            " Official",
            " governmental",
            " site",
            " ",
            " And",
            "orra",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            " Port",
            "als",
            " to",
            " the",
            " World",
            " from",
            " the",
            " United",
            " States",
            " Library",
            " of",
            " Congress",
            " And",
            "orra",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            " ",
            " And",
            "orra",
            " from",
            " the",
            " BBC",
            " News",
            " And",
            "orra",
            " –",
            " Gu"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.715,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 34,
          "is_repeated_datapoint": false,
          "tokens": [
            "rav",
            "els",
            ":",
            " The",
            " Rise",
            " and",
            " Fall",
            " of",
            " the",
            " L",
            "us",
            "aka",
            " Peace",
            " Process",
            ".",
            " New",
            " York",
            " and",
            " London",
            ",",
            " UK",
            ",",
            " Human",
            " Rights",
            " Watch",
            ".",
            "External",
            " links",
            " ",
            "Ang",
            "ola",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "Ang",
            "ola",
            " from",
            " U",
            "CB",
            " Libraries",
            " Gov",
            "P",
            "ubs",
            ".",
            "Ang",
            "ola",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            ".",
            "Key",
            " Development",
            " Fore"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.048,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.711,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " Alabama",
            " ",
            " Alabama",
            " Quick",
            "F",
            "acts",
            " from",
            " the",
            " U",
            ".S",
            ".",
            " Census",
            " Bureau",
            " Alabama",
            " State",
            " Fact",
            " Sheet",
            " ",
            " ",
            "181",
            "9",
            " establishments",
            " in",
            " the",
            " United",
            " States",
            "Southern",
            " United",
            " States",
            "States",
            " and",
            " territories",
            " established",
            " in",
            " ",
            "181",
            "9",
            "States",
            " of",
            " the",
            " Confederate",
            " States",
            " of",
            " America",
            "States",
            " of",
            " the",
            " Gulf",
            " Coast",
            " of",
            " the",
            " United",
            " States",
            "States",
            " of",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.707,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "ban",
            "ia",
            ".al",
            "pres",
            "ident",
            ".al",
            "k",
            "ry",
            "emin",
            "ist",
            "ria",
            ".al",
            "par",
            "lament",
            ".al",
            " ",
            "Al",
            "ban",
            "ia",
            " at",
            " The",
            " World",
            " Fact",
            "book",
            " by",
            " Central",
            " Intelligence",
            " Agency",
            " (",
            "C",
            "IA",
            ")",
            " ",
            "Countries",
            " and",
            " territories",
            " where",
            " Alban",
            "ian",
            " is",
            " an",
            " official",
            " language",
            "B",
            "alk",
            "an",
            " countries",
            "Countries",
            " in",
            " Europe",
            "Member",
            " states",
            " of",
            " NATO",
            "Member",
            " states"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "1",
            ".",
            "02",
            " male",
            "(s",
            ")/",
            "female",
            " (",
            "201",
            "1",
            " est",
            ".)",
            "Urban",
            "ization",
            "urban",
            " population",
            ":",
            " ",
            "68",
            ".",
            "1",
            "%",
            " of",
            " total",
            " population",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "4",
            ".",
            "04",
            "%",
            " annual",
            " rate",
            " of",
            " change",
            " (",
            "202",
            "0",
            "-",
            "202",
            "5",
            " est",
            ".)",
            "Health",
            "According",
            " to",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "2",
            "%",
            " of",
            " adults",
            " ("
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.695,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Afghanistan",
            ".",
            "Af",
            "ghan",
            " Christians",
            ",",
            " who",
            " number",
            " ",
            "500",
            "–",
            "8",
            ",",
            "000",
            ",",
            " practice",
            " their",
            " faith",
            " secretly",
            " due",
            " to",
            " intense",
            " societal",
            " opposition",
            ",",
            " and",
            " there",
            " are",
            " no",
            " public",
            " churches",
            ".",
            "Urban",
            "ization",
            "As",
            " estimated",
            " by",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " ",
            "26",
            "%",
            " of",
            " the",
            " population",
            " was",
            " urban",
            "ized",
            " as",
            " of",
            " ",
            "202",
            "0",
            ".",
            " This",
            " is",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            "2",
            ".",
            "One",
            " birth",
            " every",
            " ",
            "23",
            " seconds",
            "\t",
            "One",
            " death",
            " every",
            " ",
            "2",
            " minutes",
            "\t",
            "One",
            " net",
            " migrant",
            " every",
            " ",
            "360",
            " minutes",
            "\t",
            "Net",
            " gain",
            " of",
            " one",
            " person",
            " every",
            " ",
            "29",
            " seconds",
            "The",
            " following",
            " demographic",
            " statistics",
            " are",
            " from",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ",",
            " unless",
            " otherwise",
            " indicated",
            ".",
            "Population",
            "34",
            ",",
            "795",
            ",",
            "287",
            " (",
            "202",
            "2",
            " est",
            ".)",
            "30"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " First",
            " Ministry",
            " Portal",
            " of",
            " the",
            " First",
            " Ministry",
            " Algeria",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " Algeria",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            " ",
            " ",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Algeria",
            " from",
            " International",
            " Futures",
            " EU",
            " Ne",
            "ighbour",
            "hood",
            " Info",
            " Centre",
            ":",
            " Algeria",
            " ",
            "North",
            " African",
            " countries",
            "Mag",
            "h",
            "re",
            "bi",
            " countries",
            "S",
            "ah",
            "aran",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.691,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 12,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " First",
            " Ministry",
            " Portal",
            " of",
            " the",
            " First",
            " Ministry",
            " Algeria",
            ".",
            " The",
            " World",
            " Fact",
            "book",
            ".",
            " Central",
            " Intelligence",
            " Agency",
            ".",
            "  ",
            " Algeria",
            " profile",
            " from",
            " the",
            " BBC",
            " News",
            " ",
            " ",
            " Key",
            " Development",
            " Fore",
            "casts",
            " for",
            " Algeria",
            " from",
            " International",
            " Futures",
            " EU",
            " Ne",
            "ighbour",
            "hood",
            " Info",
            " Centre",
            ":",
            " Algeria",
            " ",
            "North",
            " African",
            " countries",
            "Mag",
            "h",
            "re",
            "bi",
            " countries",
            "S",
            "ah",
            "aran",
            " countries"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " various",
            " authorities",
            ",",
            " institutions",
            ",",
            " and",
            " countries",
            ",",
            " see",
            " for",
            " example",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ".",
            " Correspond",
            "ingly",
            ",",
            " the",
            " extent",
            " and",
            " number",
            " of",
            " oceans",
            " and",
            " seas",
            " vary",
            ".",
            "The",
            " Atlantic",
            " Ocean",
            " is",
            " bounded",
            " on",
            " the",
            " west",
            " by",
            " North",
            " and",
            " South",
            " America",
            ".",
            " It",
            " connects",
            " to",
            " the",
            " Arctic",
            " Ocean",
            " through",
            " the",
            " Denmark",
            " Strait",
            ",",
            " Greenland",
            " Sea",
            ",",
            " Norwegian",
            " Sea",
            " and",
            " B",
            "arent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " of",
            " the",
            " Pas",
            "ht",
            "uns",
            ",",
            " although",
            " many",
            " of",
            " them",
            " are",
            " also",
            " fluent",
            " in",
            " D",
            "ari",
            " while",
            " some",
            " non",
            "-P",
            "as",
            "ht",
            "uns",
            " are",
            " fluent",
            " in",
            " Pas",
            "ht",
            "o",
            ".",
            " Despite",
            " the",
            " Pas",
            "ht",
            "uns",
            " having",
            " been",
            " dominant",
            " in",
            " Afghan",
            " politics",
            " for",
            " centuries",
            ",",
            " D",
            "ari",
            " remained",
            " the",
            " preferred",
            " language",
            " for",
            " government",
            " and",
            " bureaucracy",
            ".",
            " ",
            "According",
            " to",
            " CIA",
            " World",
            " Fact",
            "book",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.688,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 14,
          "is_repeated_datapoint": false,
          "tokens": [
            " various",
            " authorities",
            ",",
            " institutions",
            ",",
            " and",
            " countries",
            ",",
            " see",
            " for",
            " example",
            " the",
            " CIA",
            " World",
            " Fact",
            "book",
            ".",
            " Correspond",
            "ingly",
            ",",
            " the",
            " extent",
            " and",
            " number",
            " of",
            " oceans",
            " and",
            " seas",
            " vary",
            ".",
            "The",
            " Atlantic",
            " Ocean",
            " is",
            " bounded",
            " on",
            " the",
            " west",
            " by",
            " North",
            " and",
            " South",
            " America",
            ".",
            " It",
            " connects",
            " to",
            " the",
            " Arctic",
            " Ocean",
            " through",
            " the",
            " Denmark",
            " Strait",
            ",",
            " Greenland",
            " Sea",
            ",",
            " Norwegian",
            " Sea",
            " and",
            " B",
            "arent"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "300",
            ",",
            "000",
            " mark",
            " is",
            " supported",
            " by",
            " Greek",
            " government",
            " as",
            " well",
            ".",
            " The",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimates",
            " the",
            " Greek",
            " minority",
            " to",
            " constitute",
            " ",
            "0",
            ".",
            "9",
            "%",
            " of",
            " the",
            " population",
            ".",
            " The",
            " US",
            " State",
            " Department",
            " estimates",
            " that",
            " Greeks",
            " make",
            " up",
            " ",
            "1",
            ".",
            "17",
            "%,",
            " and",
            " other",
            " minorities",
            " ",
            "0",
            ".",
            "23",
            "%,",
            " of",
            " the",
            " population",
            ".",
            " The",
            " latter",
            " questions",
            " the",
            " validity"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Ber",
            "ber",
            " by",
            " ",
            "27",
            "%.",
            "Rel",
            "igion",
            "Islam",
            " is",
            " the",
            " predominant",
            " religion",
            " in",
            " Algeria",
            ",",
            " with",
            " its",
            " adher",
            "ents",
            ",",
            " mostly",
            " Sun",
            "nis",
            ",",
            " accounting",
            " for",
            " ",
            "99",
            "%",
            " of",
            " the",
            " population",
            " according",
            " to",
            " a",
            " ",
            "202",
            "1",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimate",
            ",",
            " and",
            " ",
            "97",
            ".",
            "9",
            "%",
            " according",
            " to",
            " Pew",
            " Research",
            " in",
            " ",
            "202",
            "0",
            ".",
            " There"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.684,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 42,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " Ber",
            "ber",
            " by",
            " ",
            "27",
            "%.",
            "Rel",
            "igion",
            "Islam",
            " is",
            " the",
            " predominant",
            " religion",
            " in",
            " Algeria",
            ",",
            " with",
            " its",
            " adher",
            "ents",
            ",",
            " mostly",
            " Sun",
            "nis",
            ",",
            " accounting",
            " for",
            " ",
            "99",
            "%",
            " of",
            " the",
            " population",
            " according",
            " to",
            " a",
            " ",
            "202",
            "1",
            " CIA",
            " World",
            " Fact",
            "book",
            " estimate",
            ",",
            " and",
            " ",
            "97",
            ".",
            "9",
            "%",
            " according",
            " to",
            " Pew",
            " Research",
            " in",
            " ",
            "202",
            "0",
            ".",
            " There"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.656,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " crisis",
            ";",
            " Mend",
            "es",
            " France",
            " advocated",
            " reconciliation",
            ".",
            " Cam",
            "us",
            " also",
            " supported",
            " a",
            " like",
            "-minded",
            " Alger",
            "ian",
            " militant",
            ",",
            " Az",
            "iz",
            " K",
            "ess",
            "ous",
            ".",
            " Cam",
            "us",
            " traveled",
            " to",
            " Algeria",
            " to",
            " negotiate",
            " a",
            " tr",
            "uce",
            " between",
            " the",
            " two",
            " b",
            "ellig",
            "er",
            "ents",
            " but",
            " was",
            " met",
            " with",
            " distrust",
            " by",
            " all",
            " parties",
            ".",
            " In",
            " one",
            ",",
            " often",
            " mis",
            "quoted",
            " incident",
            ",",
            " Cam",
            "us",
            " confronted"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            " altitude",
            " gives",
            " way",
            " to",
            " higher",
            "-e",
            "levation",
            " terrain",
            ".",
            " Elev",
            "ations",
            " around",
            " the",
            " world",
            " that",
            " have",
            " cold",
            " climates",
            " similar",
            " to",
            " those",
            " of",
            " the",
            " polar",
            " regions",
            " have",
            " been",
            " called",
            " Alpine",
            ".",
            " A",
            " rise",
            " from",
            " sea",
            " level",
            " into",
            " the",
            " upper",
            " regions",
            " of",
            " the",
            " atmosphere",
            " causes",
            " the",
            " temperature",
            " to",
            " decrease",
            " (",
            "see",
            " ad",
            "i",
            "ab",
            "atic",
            " lapse",
            " rate",
            ").",
            " The",
            " effect",
            " of",
            " mountain",
            " chains",
            " on",
            " prevailing"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " similar",
            " to",
            " the",
            " prevailing",
            " theory",
            " for",
            " the",
            " origin",
            " of",
            " Earth",
            "'s",
            " moon",
            ".",
            "Character",
            "istics",
            "Size",
            " distribution",
            " ",
            "A",
            "ster",
            "oids",
            " vary",
            " greatly",
            " in",
            " size",
            ",",
            " from",
            " almost",
            " ",
            " for",
            " the",
            " largest",
            " down",
            " to",
            " rocks",
            " just",
            " ",
            "1",
            "Âł",
            "meter",
            " across",
            ",",
            " below",
            " which",
            " an",
            " object",
            " is",
            " classified",
            " as",
            " a",
            " meteor",
            "oid",
            ".",
            " The",
            " three",
            " largest",
            " are",
            " very",
            " much",
            " like",
            " miniature"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "191",
            "4",
            " marked",
            " the",
            " beginning",
            " of",
            " Einstein",
            "'s",
            " gradual",
            " estr",
            "angement",
            " from",
            " the",
            " nation",
            " of",
            " his",
            " birth",
            ".",
            " When",
            " the",
            " \"",
            "Manifest",
            "o",
            " of",
            " the",
            " Nin",
            "ety",
            "-",
            "Three",
            "\"",
            " was",
            " published",
            " in",
            " October",
            " ",
            "191",
            "4",
            "—a",
            " document",
            " signed",
            " by",
            " a",
            " host",
            " of",
            " prominent",
            " German",
            " thinkers",
            " that",
            " justified",
            " Germany",
            "'s",
            " b",
            "ellig",
            "erence",
            "—",
            "E",
            "instein",
            " was",
            " one",
            " of",
            " the",
            " few"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 43,
          "is_repeated_datapoint": false,
          "tokens": [
            " About",
            " half",
            " of",
            " current",
            " scholarly",
            " hypotheses",
            " on",
            " O",
            "ng",
            "ota",
            "'s",
            " origins",
            " align",
            " it",
            " with",
            " Afro",
            "asi",
            "atic",
            " in",
            " some",
            " way",
            ".",
            " Robert",
            " Het",
            "z",
            "ron",
            " proposed",
            " that",
            " Be",
            "ja",
            " is",
            " not",
            " part",
            " of",
            " Cush",
            "itic",
            ",",
            " but",
            " a",
            " separate",
            " branch",
            ".",
            " The",
            " prevailing",
            " opinion",
            ",",
            " however",
            ",",
            " is",
            " that",
            " Be",
            "ja",
            " is",
            " a",
            " branch",
            " of",
            " Cush",
            "itic",
            ".",
            " The",
            " extinct",
            " M",
            "ero"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " according",
            " to",
            " Mal",
            "achi",
            " ",
            "2",
            ":",
            "4",
            "-",
            "7",
            ",",
            " and",
            " the",
            " prevailing",
            " tendency",
            " was",
            " to",
            " place",
            " Aaron",
            " on",
            " a",
            " footing",
            " equal",
            " with",
            " Moses",
            ".",
            " \"",
            "At",
            " times",
            " Aaron",
            ",",
            " and",
            " at",
            " other",
            " times",
            " Moses",
            ",",
            " is",
            " mentioned",
            " first",
            " in",
            " Scripture",
            "—",
            "this",
            " is",
            " to",
            " show",
            " that",
            " they",
            " were",
            " of",
            " equal",
            " rank",
            ",\"",
            " says",
            " the",
            " Mek",
            "hil",
            "ta",
            " of",
            " Rabbi",
            " Ish",
            "ma"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 22,
          "is_repeated_datapoint": false,
          "tokens": [
            "K",
            "Ã¶",
            "pp",
            "en",
            ":",
            " C",
            "fb",
            ")",
            " strongly",
            " influenced",
            " by",
            " its",
            " proximity",
            " to",
            " the",
            " North",
            " Sea",
            " to",
            " the",
            " west",
            ",",
            " with",
            " prevailing",
            " w",
            "ester",
            "ly",
            " winds",
            ".",
            "Am",
            "sterdam",
            ",",
            " as",
            " well",
            " as",
            " most",
            " of",
            " the",
            " North",
            " Holland",
            " province",
            ",",
            " lies",
            " in",
            " USDA",
            " Hard",
            "iness",
            " zone",
            " ",
            "8",
            "b",
            ".",
            " Fro",
            "sts",
            " mainly",
            " occur",
            " during",
            " spells",
            " of",
            " e",
            "aster",
            "ly",
            " or",
            " nor"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " come",
            ".",
            "Sl",
            "avery",
            " ",
            "The",
            " historical",
            " narrative",
            " of",
            " slavery",
            " in",
            " Ar",
            "uba",
            " challenges",
            " the",
            " prevailing",
            " belief",
            " that",
            " the",
            " conditions",
            " for",
            " slaves",
            " here",
            " were",
            " considerably",
            " better",
            " than",
            " in",
            " other",
            " Caribbean",
            " regions",
            " and",
            " in",
            " North",
            " and",
            " South",
            " America",
            ".",
            " Records",
            " of",
            " slavery",
            " in",
            " Ar",
            "uba",
            " are",
            " rather",
            " limited",
            ",",
            " with",
            " mentions",
            " primarily",
            " concerning",
            " C",
            "ura",
            "Ã§",
            "ao",
            " in",
            " the",
            " years",
            " ",
            "175",
            "0",
            " and"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            "0",
            " French",
            " Open",
            " in",
            " a",
            " four",
            "-set",
            " quarter",
            "final",
            ".",
            " Arg",
            "uably",
            " their",
            " best",
            " match",
            " took",
            " place",
            " in",
            " the",
            " round",
            " of",
            " ",
            "16",
            " of",
            " the",
            " ",
            "199",
            "4",
            " US",
            " Open",
            ".",
            " While",
            " both",
            " players",
            " presented",
            " high",
            "-quality",
            " shot",
            "-making",
            ",",
            " the",
            " momentum",
            " changed",
            " from",
            " set",
            " to",
            " set",
            " with",
            " Ag",
            "assi",
            " eventually",
            " prevailing",
            " in",
            " a",
            " five",
            "-set",
            " victory",
            ".",
            " It",
            " turned",
            " out",
            " to",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "ree",
            ",",
            " A",
            "ins",
            "lie",
            " T",
            ".,",
            " ed",
            ".",
            " Encyclopedia",
            " of",
            " Asian",
            " history",
            " (",
            "198",
            "8",
            ")",
            " vol",
            ".",
            " ",
            "1",
            " online",
            ";",
            " vol",
            " ",
            "2",
            " online",
            ";",
            " vol",
            " ",
            "3",
            " online",
            ";",
            " vol",
            " ",
            "4",
            " online",
            " High",
            "am",
            ",",
            " Charles",
            ".",
            " Encyclopedia",
            " of",
            " Ancient",
            " Asian",
            " Civil",
            "izations",
            ".",
            " Facts",
            " on",
            " File",
            " library",
            " of",
            " world",
            " history",
            ".",
            " New",
            " York",
            ":",
            " Facts",
            " On"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 4,
          "is_repeated_datapoint": false,
          "tokens": [
            " leaving",
            " the",
            " Americas",
            " because",
            " prevailing",
            " winds",
            " and",
            " currents",
            " made",
            " the",
            " transport",
            " of",
            " heavy",
            " metals",
            " slow",
            " and",
            " predictable",
            ".",
            "In",
            " the",
            " colonies",
            " of",
            " the",
            " Americas",
            ",",
            " dep",
            "red",
            "ation",
            ",",
            " small",
            "p",
            "ox",
            " and",
            " others",
            " diseases",
            ",",
            " and",
            " slavery",
            " quickly",
            " reduced",
            " the",
            " indigenous",
            " population",
            " of",
            " the",
            " Americas",
            " to",
            " the",
            " extent",
            " that",
            " the",
            " Atlantic",
            " slave",
            " trade",
            " had",
            " to",
            " be",
            " introduced",
            " to",
            " replace",
            " them",
            "a",
            " trade"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " this",
            " religion",
            " has",
            " become",
            " the",
            " basis",
            " of",
            " the",
            " prevailing",
            " religion",
            " of",
            " Europe",
            ";",
            " for",
            " it",
            " is",
            " a",
            " religion",
            " without",
            " any",
            " metaph",
            "ysical",
            " tendency",
            ".",
            " While",
            " all",
            " other",
            " religions",
            " endeavor",
            " to",
            " explain",
            " to",
            " the",
            " people",
            " by",
            " symbols",
            " the",
            " metaph",
            "ysical",
            " significance",
            " of",
            " life",
            ",",
            " the",
            " religion",
            " of",
            " the",
            " Jews",
            " is",
            " entirely",
            " im",
            "manent",
            " and",
            " furn",
            "ishes",
            " nothing",
            " but",
            " a",
            " mere",
            " war",
            "-c",
            "ry",
            " in"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " fact",
            ",",
            " her",
            " brother",
            "'s",
            " policies",
            " which",
            " directly",
            " threaten",
            " his",
            " business",
            ".",
            " When",
            " the",
            " government",
            " passes",
            " laws",
            " and",
            " decre",
            "es",
            " which",
            " make",
            " it",
            " impossible",
            " for",
            " him",
            " to",
            " continue",
            ",",
            " he",
            " sets",
            " all",
            " his",
            " oil",
            " wells",
            " on",
            " fire",
            ",",
            " leaving",
            " a",
            " single",
            " note",
            ":",
            " \"",
            "I",
            " am",
            " leaving",
            " it",
            " as",
            " I",
            " found",
            " it",
            ".",
            " Take",
            " over",
            ".",
            " It",
            "'s",
            " yours",
            ".\"",
            " One",
            " particular",
            " burning"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.52,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            "\"",
            " and",
            " \"",
            "des",
            "erts",
            " that",
            " had",
            " become",
            " water",
            "ed",
            " by",
            " rivers",
            "\",",
            " giving",
            " as",
            " examples",
            " the",
            " growth",
            " of",
            " the",
            " Nile",
            " delta",
            " since",
            " the",
            " time",
            " of",
            " Homer",
            ",",
            " and",
            " \"",
            "the",
            " uphe",
            "aving",
            " of",
            " one",
            " of",
            " the",
            " Ae",
            "olian",
            " islands",
            ",",
            " previous",
            " to",
            " a",
            " volcanic",
            " eruption",
            ".\"",
            "'",
            "A",
            "rist",
            "otle",
            " also",
            " made",
            " many",
            " observations",
            " about",
            " the",
            " hydro",
            "logic",
            " cycle",
            " and",
            " meteor",
            "ology"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.508,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 29,
          "is_repeated_datapoint": false,
          "tokens": [
            "Pres",
            "idents",
            " of",
            " the",
            " United",
            " States",
            "Republican",
            " Party",
            " (",
            "United",
            " States",
            ")",
            " presidential",
            " nominees",
            "Republican",
            " Party",
            " presidents",
            " of",
            " the",
            " United",
            " States",
            "Union",
            " (",
            "American",
            " Civil",
            " War",
            ")",
            " political",
            " leaders",
            "Wh",
            "ig",
            " Party",
            " members",
            " of",
            " the",
            " United",
            " States",
            " House",
            " of",
            " Representatives",
            " from",
            " Illinois",
            "19",
            "th",
            "-century",
            " assass",
            "inated",
            " national",
            " presidents",
            "Ass",
            "ass",
            "inated",
            " former",
            " sub",
            "national",
            " legislators"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.504,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " but",
            " there",
            " is",
            " little",
            " danger",
            " in",
            " eating",
            " fish",
            " because",
            " this",
            " arsen",
            "ic",
            " compound",
            " is",
            " nearly",
            " non",
            "-to",
            "xic",
            ".",
            "Environmental",
            " issues",
            "Ex",
            "posure",
            " ",
            "N",
            "aturally",
            " occurring",
            " sources",
            " of",
            " human",
            " exposure",
            " include",
            " volcanic",
            " ash",
            ",",
            " weather",
            "ing",
            " of",
            " minerals",
            " and",
            " ores",
            ",",
            " and",
            " mineral",
            "ized",
            " groundwater",
            ".",
            " Ar",
            "sen",
            "ic",
            " is",
            " also",
            " found",
            " in",
            " food",
            ",",
            " water",
            ",",
            " soil",
            ",",
            " and"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.48,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " \"",
            "ax",
            "iom",
            "\"",
            " and",
            " a",
            " \"",
            "post",
            "ulate",
            "\"",
            " disappears",
            ".",
            " The",
            " post",
            "ulates",
            " of",
            " Eu",
            "clid",
            " are",
            " profit",
            "ably",
            " motivated",
            " by",
            " saying",
            " that",
            " they",
            " lead",
            " to",
            " a",
            " great",
            " wealth",
            " of",
            " geometric",
            " facts",
            ".",
            " The",
            " truth",
            " of",
            " these",
            " complicated",
            " facts",
            " rests",
            " on",
            " the",
            " acceptance",
            " of",
            " the",
            " basic",
            " hypotheses",
            ".",
            " However",
            ",",
            " by",
            " throwing",
            " out",
            " Eu",
            "clid",
            "'s",
            " fifth",
            " post",
            "ulate",
            ",",
            " one"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.467,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " are",
            " liter",
            "ate",
            " as",
            " of",
            " ",
            "201",
            "5",
            ".",
            "Languages",
            "Port",
            "ug",
            "uese",
            " is",
            " the",
            " official",
            " language",
            " of",
            " Angola",
            ",",
            " but",
            " B",
            "antu",
            " and",
            " other",
            " African",
            " languages",
            " are",
            " also",
            " widely",
            " spoken",
            ".",
            " In",
            " fact",
            ",",
            " K",
            "ik",
            "ongo",
            ",",
            " Kim",
            "b",
            "und",
            "u",
            ",",
            " Umb",
            "und",
            "u",
            ",",
            " T",
            "uch",
            "ok",
            "we",
            ",",
            " G",
            "angu",
            "ela",
            ",",
            " and",
            " Uk",
            "any",
            "ama",
            " have"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            0.463,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 3,
          "is_repeated_datapoint": false,
          "tokens": [
            " grasp",
            " of",
            " the",
            " facts",
            " close",
            " in",
            " time",
            " to",
            " the",
            " actual",
            " events",
            " may",
            " be",
            " used",
            " to",
            " refresh",
            " a",
            " witness",
            "'s",
            " rec",
            "ollection",
            ".",
            " Materials",
            " used",
            " to",
            " refresh",
            " rec",
            "ollection",
            " are",
            " ad",
            "missible",
            " as",
            " evidence",
            ".",
            " If",
            " the",
            " aff",
            "iant",
            " is",
            " a",
            " party",
            " in",
            " the",
            " case",
            ",",
            " the",
            " aff",
            "iant",
            "'s",
            " opponent",
            " may",
            " be",
            " successful",
            " in",
            " having",
            " the",
            " affidavit",
            " admitted",
            " as",
            " evidence",
            ",",
            " as",
            " statements"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.398,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " that",
            " the",
            " process",
            " described",
            " by",
            " No",
            "z",
            "ick",
            ",",
            " with",
            " the",
            " dominant",
            " protection",
            " agency",
            " outlaw",
            "ing",
            " its",
            " competitors",
            ",",
            " in",
            " fact",
            " violates",
            " its",
            " own",
            " clients",
            "'",
            " rights",
            ",",
            " John",
            " Jefferson",
            " actually",
            " advocates",
            " No",
            "z",
            "ick",
            "'s",
            " argument",
            " and",
            " states",
            " that",
            " such",
            " events",
            " would",
            " best",
            " operate",
            " in",
            " laisse",
            "z",
            "-f",
            "aire",
            ".",
            " Robert",
            " El",
            "lick",
            "son",
            " presented",
            " a",
            " Hay",
            "ek",
            "ian",
            " case",
            " against",
            " an"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.398,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            "202",
            "0",
            ",",
            " scientists",
            " reported",
            " that",
            " bit",
            "umen",
            " currently",
            " is",
            " a",
            " significant",
            " and",
            " largely",
            " overlooked",
            " source",
            " of",
            " air",
            " pollution",
            " in",
            " urban",
            " areas",
            ",",
            " especially",
            " during",
            " hot",
            " and",
            " sunny",
            " periods",
            ".",
            "A",
            " bit",
            "umen",
            "-like",
            " substance",
            " found",
            " in",
            " the",
            " Himal",
            "ayas",
            " and",
            " known",
            " as",
            " sh",
            "il",
            "aj",
            "it",
            " is",
            " sometimes",
            " used",
            " as",
            " an",
            " Ay",
            "urved",
            "a",
            " medicine",
            ",",
            " but",
            " is",
            " not",
            " in",
            " fact",
            " a"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.27,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 36,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "26",
            " PBS",
            ",",
            " Montgomery",
            " W",
            "CI",
            "Q",
            " ",
            "7",
            " PBS",
            ",",
            " Mount",
            " Che",
            "aha",
            " The",
            " CW",
            " WT",
            "TO",
            " ",
            "21",
            ",",
            " Hom",
            "ewood",
            "/B",
            "irmingham",
            " W",
            "TV",
            "Y",
            " ",
            "4",
            ".",
            "3",
            ",",
            " Do",
            "than",
            " WH",
            "DF",
            " ",
            "15",
            ",",
            " Florence",
            "/H",
            "unts",
            "ville",
            " WF",
            "NA",
            " ",
            "55",
            ",",
            " Gulf",
            " Sh",
            "ores",
            "/M",
            "obile",
            "/P",
            "ens"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 58,
          "is_repeated_datapoint": false,
          "tokens": [
            ")",
            "180",
            "6",
            " –",
            " On",
            "og",
            "awa",
            " Kis",
            "ab",
            "ur",
            "Åį",
            ",",
            " Japanese",
            " sum",
            "o",
            " wrestler",
            ",",
            " the",
            " ",
            "5",
            "th",
            " Yok",
            "oz",
            "una",
            " (",
            "b",
            ".",
            " ",
            "175",
            "8",
            ")",
            "184",
            "1",
            " –",
            " Peter",
            " Andreas",
            " He",
            "ib",
            "erg",
            ",",
            " Danish",
            " phil",
            "ologist",
            " and",
            " author",
            " (",
            "b",
            ".",
            " ",
            "175",
            "8",
            ")",
            "184",
            "7",
            " –",
            " Charles",
            ",",
            " Austrian",
            " commander",
            " and",
            " duke",
            " of",
            " Tes"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 8,
          "is_repeated_datapoint": false,
          "tokens": [
            " –",
            " Emmanuel",
            " Vit",
            "ale",
            ",",
            " Mal",
            "tes",
            "e",
            " commander",
            " and",
            " politician",
            " (",
            "d",
            ".",
            " ",
            "180",
            "2",
            ")",
            "177",
            "0",
            " –",
            " David",
            " Thompson",
            ",",
            " English",
            "-",
            "Canadian",
            " cart",
            "ographer",
            " and",
            " explorer",
            " (",
            "d",
            ".",
            " ",
            "185",
            "7",
            ")",
            "177",
            "7",
            " –",
            " Carl",
            " Friedrich",
            " Gauss",
            ",",
            " German",
            " mathematic",
            "ian",
            " and",
            " physicist",
            " (",
            "d",
            ".",
            " ",
            "185",
            "5",
            ")",
            "179",
            "9",
            " –",
            " Joseph",
            " Dart",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.256,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " how",
            " Stalin",
            " brought",
            " back",
            " the",
            " Russian",
            " Orthodox",
            " Church",
            " during",
            " the",
            " Second",
            " World",
            " War",
            ".",
            " The",
            " sheep",
            "Âł",
            "–",
            " They",
            " are",
            " not",
            " given",
            " individual",
            " names",
            " or",
            " personalities",
            ".",
            " They",
            " show",
            " limited",
            " understanding",
            " of",
            " Animal",
            "ism",
            " and",
            " the",
            " political",
            " atmosphere",
            " of",
            " the",
            " farm",
            ",",
            " yet",
            " nonetheless",
            ",",
            " they",
            " are",
            " the",
            " voice",
            " of",
            " blind",
            " conformity",
            " as",
            " they",
            " ble",
            "at",
            " their",
            " support",
            " of",
            " Napoleon",
            "'s",
            " ideals",
            " with"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.243,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 49,
          "is_repeated_datapoint": false,
          "tokens": [
            "t",
            " to",
            " write",
            " the",
            " dialogue",
            ",",
            " but",
            " Raymond",
            " Chandler",
            " took",
            " over",
            ",",
            " then",
            " left",
            " over",
            " disagreements",
            " with",
            " the",
            " director",
            ".",
            " In",
            " the",
            " film",
            ",",
            " two",
            " men",
            " casually",
            " meet",
            ",",
            " one",
            " of",
            " whom",
            " spec",
            "ulates",
            " on",
            " a",
            " fool",
            "proof",
            " method",
            " to",
            " murder",
            ";",
            " he",
            " suggests",
            " that",
            " two",
            " people",
            ",",
            " each",
            " wishing",
            " to",
            " do",
            " away",
            " with",
            " someone",
            ",",
            " should",
            " each",
            " perform",
            " the",
            " other",
            "'s",
            " murder"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " had",
            " obviously",
            " understood",
            " wherein",
            " the",
            " sense",
            " of",
            " life",
            " resides",
            ".",
            " [...]",
            " This",
            " trespass",
            "ing",
            " of",
            " the",
            " border",
            " between",
            " nature",
            " and",
            " mankind",
            " is",
            " an",
            " ideal",
            " place",
            " for",
            " the",
            " existence",
            " of",
            " man",
            ".",
            " D",
            "ov",
            "z",
            "hen",
            "ko",
            " understood",
            " this",
            ".\"",
            "He",
            " was",
            " also",
            " not",
            " a",
            " fan",
            " of",
            " block",
            "busters",
            " or",
            " science",
            " fiction",
            ",",
            " largely",
            " dismissing",
            " the",
            " latter",
            " for",
            " its",
            " \"",
            "comic",
            " book",
            "\"",
            " tr"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "id",
            "–",
            "base",
            " equilibrium",
            " plays",
            " a",
            " critical",
            " role",
            " in",
            " regulating",
            " mamm",
            "alian",
            " breathing",
            ".",
            " Oxygen",
            " gas",
            " (",
            "O",
            "2",
            ")",
            " drives",
            " cellular",
            " res",
            "piration",
            ",",
            " the",
            " process",
            " by",
            " which",
            " animals",
            " release",
            " the",
            " chemical",
            " potential",
            " energy",
            " stored",
            " in",
            " food",
            ",",
            " producing",
            " carbon",
            " dioxide",
            " (",
            "CO",
            "2",
            ")",
            " as",
            " a",
            " by",
            "product",
            ".",
            " Oxygen",
            " and",
            " carbon",
            " dioxide",
            " are",
            " exchanged",
            " in",
            " the",
            " lungs",
            ",",
            " and",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "aska",
            " has",
            " an",
            " abundance",
            " of",
            " seafood",
            ",",
            " with",
            " the",
            " primary",
            " fisheries",
            " in",
            " the",
            " B",
            "ering",
            " Sea",
            " and",
            " the",
            " North",
            " Pacific",
            ".",
            " Sea",
            "food",
            " is",
            " one",
            " of",
            " the",
            " few",
            " food",
            " items",
            " that",
            " is",
            " often",
            " cheaper",
            " within",
            " the",
            " state",
            " than",
            " outside",
            " it",
            ".",
            " Many",
            " Al",
            "ask",
            "ans",
            " take",
            " advantage",
            " of",
            " salmon",
            " seasons",
            " to",
            " harvest",
            " portions",
            " of",
            " their",
            " household",
            " diet",
            " while",
            " fishing",
            " for",
            " subs",
            "istence",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Emb",
            "alm",
            "ers",
            " used",
            " salts",
            " from",
            " the",
            " W",
            "adi",
            " Nat",
            "run",
            " for",
            " m",
            "umm",
            "ification",
            ",",
            " which",
            " also",
            " provided",
            " the",
            " gypsum",
            " needed",
            " to",
            " make",
            " plaster",
            ".",
            " Ore",
            "-bearing",
            " rock",
            " formations",
            " were",
            " found",
            " in",
            " distant",
            ",",
            " inh",
            "osp",
            "itable",
            " w",
            "ad",
            "is",
            " in",
            " the",
            " Eastern",
            " Desert",
            " and",
            " the",
            " Sinai",
            ",",
            " requiring",
            " large",
            ",",
            " state",
            "-controlled",
            " exped",
            "itions",
            " to",
            " obtain",
            " natural",
            " resources",
            " found",
            " there"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " originated",
            " in",
            " what",
            " is",
            " today",
            " north",
            "western",
            " Nigeria",
            " and",
            " southern",
            " Niger",
            ".",
            " B",
            "antu",
            " speakers",
            " introduced",
            " the",
            " cultivation",
            " of",
            " bananas",
            " and",
            " tar",
            "o",
            ",",
            " as",
            " well",
            " as",
            " large",
            " cattle",
            " her",
            "ds",
            ",",
            " to",
            " Angola",
            "'s",
            " central",
            " high",
            "lands",
            " and",
            " the",
            " Lu",
            "anda",
            " plain",
            ".",
            "A",
            " number",
            " of",
            " political",
            " entities",
            " were",
            " established",
            ";",
            " the",
            " best",
            "-known",
            " of",
            " these",
            " was",
            " the",
            " Kingdom",
            " of",
            " K",
            "ongo"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "ÑĢÐ¸Ð·",
    "lacak",
    "styleType",
    "ucci",
    "%M"
  ],
  "bottom_logits": [
    " -",
    " ",
    ";",
    " (",
    ","
  ],
  "act_min": -0.0,
  "act_max": 0.84
}