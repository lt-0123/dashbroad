{
  "index": 3673,
  "examples_quantiles": [
    {
      "quantile_name": "Top 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.281,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.703,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 45,
          "is_repeated_datapoint": false,
          "tokens": [
            " Asia",
            "\".",
            "Ge",
            "ographically",
            ",",
            " Asia",
            " is",
            " the",
            " major",
            " eastern",
            " constituent",
            " of",
            " the",
            " continent",
            " of",
            " Euras",
            "ia",
            " with",
            " Europe",
            " being",
            " a",
            " north",
            "western",
            " peninsula",
            " of",
            " the",
            " land",
            "mass",
            ".",
            " Asia",
            ",",
            " Europe",
            " and",
            " Africa",
            " make",
            " up",
            " a",
            " single",
            " continuous",
            " land",
            "mass",
            "—",
            "Af",
            "ro",
            "-E",
            "uras",
            "ia",
            " (",
            "except",
            " for",
            " the",
            " S",
            "uez",
            " Canal",
            ")—",
            "and",
            " share",
            " a",
            " common",
            " continental",
            " shelf",
            ".",
            " Almost"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "chemy",
            ",",
            " Medicine",
            " and",
            " Religion",
            ":",
            " Z",
            "os",
            "imus",
            " of",
            " Pan",
            "opol",
            "is",
            " and",
            " the",
            " Egyptian",
            " Pri",
            "ests",
            " \",",
            " Religion",
            " in",
            " the",
            " Roman",
            " Empire",
            " ",
            "3",
            ".",
            "2",
            " (",
            "201",
            "7",
            "),",
            " p",
            ".",
            "Âł",
            "202",
            "–",
            "220",
            ".",
            " G",
            "eras",
            "imos",
            " Mer",
            "ian",
            "os",
            ",",
            " \"",
            " Al",
            "chemy",
            " \",",
            " In",
            " A",
            ".",
            " K",
            "ald",
            "ell",
            "is",
            " &",
            " N",
            ".",
            " Sin",
            "ios",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.699,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 40,
          "is_repeated_datapoint": false,
          "tokens": [
            "chemy",
            ",",
            " Medicine",
            " and",
            " Religion",
            ":",
            " Z",
            "os",
            "imus",
            " of",
            " Pan",
            "opol",
            "is",
            " and",
            " the",
            " Egyptian",
            " Pri",
            "ests",
            " \",",
            " Religion",
            " in",
            " the",
            " Roman",
            " Empire",
            " ",
            "3",
            ".",
            "2",
            " (",
            "201",
            "7",
            "),",
            " p",
            ".",
            "Âł",
            "202",
            "–",
            "220",
            ".",
            " G",
            "eras",
            "imos",
            " Mer",
            "ian",
            "os",
            ",",
            " \"",
            " Al",
            "chemy",
            " \",",
            " In",
            " A",
            ".",
            " K",
            "ald",
            "ell",
            "is",
            " &",
            " N",
            ".",
            " Sin",
            "ios",
            "s"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.68,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            "riters",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "People",
            " from",
            " K",
            "ost",
            "roma",
            " Obl",
            "ast",
            "G",
            "eras",
            "im",
            "ov",
            " Institute",
            " of",
            " Cin",
            "emat",
            "ography",
            " alumni",
            "Ac",
            "ademic",
            " staff",
            " of",
            " High",
            " Courses",
            " for",
            " Script",
            "writers",
            " and",
            " Film",
            " Directors",
            "People",
            "'s",
            " Artists",
            " of",
            " the",
            " RS",
            "FS",
            "R",
            "Rec",
            "ipients",
            " of",
            " the",
            " Lenin",
            " Prize",
            "C",
            "annes",
            " Film",
            " Festival",
            " Award",
            " for",
            " Best"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 16,
          "is_repeated_datapoint": false,
          "tokens": [
            " Andre",
            "i",
            " T",
            "ark",
            "ovsky",
            " in",
            " ",
            "200",
            "0",
            ".",
            "At",
            " the",
            " entrance",
            " to",
            " the",
            " G",
            "eras",
            "im",
            "ov",
            " Institute",
            " of",
            " Cin",
            "emat",
            "ography",
            " in",
            " Moscow",
            ",",
            " there",
            " is",
            " a",
            " monument",
            " that",
            " includes",
            " statues",
            " of",
            " T",
            "ark",
            "ovsky",
            ",",
            " G",
            "enn",
            "ady",
            " Sh",
            "pal",
            "ik",
            "ov",
            " and",
            " V",
            "asily",
            " Sh",
            "uk",
            "sh",
            "in",
            ".",
            "Re",
            "ception",
            " and",
            " legacy",
            "And",
            "rei",
            " T",
            "ark"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 1",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.279,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.676,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " civilizations",
            ".",
            " Its",
            " ",
            "4",
            ".",
            "7",
            " billion",
            " people",
            " constitute",
            " roughly",
            " ",
            "60",
            "%",
            " of",
            " the",
            " world",
            "'s",
            " population",
            ",",
            " having",
            " more",
            " people",
            " than",
            " all",
            " other",
            " continents",
            " combined",
            ".",
            "Asia",
            " shares",
            " the",
            " land",
            "mass",
            " of",
            " Euras",
            "ia",
            " with",
            " Europe",
            ",",
            " and",
            " of",
            " Afro",
            "-E",
            "uras",
            "ia",
            " with",
            " both",
            " Europe",
            " and",
            " Africa",
            ".",
            " In",
            " general",
            " terms",
            ",",
            " it",
            " is",
            " bounded",
            " on",
            " the",
            " east",
            " by"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.641,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 56,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "90",
            "%",
            " are",
            " frogs",
            ".",
            " The",
            " smallest",
            " amphib",
            "ian",
            " (",
            "and",
            " verte",
            "brate",
            ")",
            " in",
            " the",
            " world",
            " is",
            " a",
            " frog",
            " from",
            " New",
            " Guinea",
            " (",
            "Pa",
            "ed",
            "oph",
            "ry",
            "ne",
            " am",
            "au",
            "ensis",
            ")",
            " with",
            " a",
            " length",
            " of",
            " just",
            " .",
            " The",
            " largest",
            " living",
            " amphib",
            "ian",
            " is",
            " the",
            " ",
            " South",
            " China",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " sl",
            "ig",
            "oi",
            "),",
            " but",
            " this"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " found",
            ",",
            " but",
            " there",
            " are",
            " only",
            " three",
            " living",
            " species",
            ",",
            " the",
            " Chinese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " david",
            "ian",
            "us",
            "),",
            " the",
            " Japanese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " japon",
            "icus",
            ")",
            " and",
            " the",
            " hell",
            "b",
            "ender",
            " (",
            "Crypt",
            "ob",
            "r",
            "anch",
            "us",
            " alleg",
            "ani",
            "ensis",
            ")",
            " from",
            " North",
            " America",
            ".",
            " These",
            " large",
            " amphib",
            "ians",
            " retain",
            " several",
            " lar",
            "val",
            " characteristics"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 48,
          "is_repeated_datapoint": false,
          "tokens": [
            " reluctant",
            " to",
            " build",
            " anything",
            " with",
            " this",
            " un",
            "pro",
            "ven",
            " technology",
            ",",
            " and",
            " has",
            " to",
            " be",
            " c",
            "ajo",
            "led",
            " into",
            " accepting",
            " the",
            " contract",
            ".",
            " When",
            " pressured",
            " by",
            " public",
            " opinion",
            ",",
            " he",
            " discontin",
            "ues",
            " production",
            " of",
            " the",
            " switches",
            ",",
            " forcing",
            " D",
            "agn",
            "y",
            " to",
            " find",
            " an",
            " alternative",
            " source",
            ".",
            "M",
            "idas",
            " Mull",
            "igan",
            " is",
            " a",
            " wealthy",
            " banker",
            " who",
            " myster",
            "iously",
            " disappeared",
            " in",
            " protest",
            " after",
            " he"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.637,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " found",
            ",",
            " but",
            " there",
            " are",
            " only",
            " three",
            " living",
            " species",
            ",",
            " the",
            " Chinese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " david",
            "ian",
            "us",
            "),",
            " the",
            " Japanese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " japon",
            "icus",
            ")",
            " and",
            " the",
            " hell",
            "b",
            "ender",
            " (",
            "Crypt",
            "ob",
            "r",
            "anch",
            "us",
            " alleg",
            "ani",
            "ensis",
            ")",
            " from",
            " North",
            " America",
            ".",
            " These",
            " large",
            " amphib",
            "ians",
            " retain",
            " several",
            " lar",
            "val",
            " characteristics"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 2",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "201",
            "2",
            ".",
            " It",
            " has",
            " an",
            " average",
            " length",
            " of",
            " ",
            " and",
            " is",
            " part",
            " of",
            " a",
            " genus",
            " that",
            " contains",
            " four",
            " of",
            " the",
            " world",
            "'s",
            " ten",
            " smallest",
            " frog",
            " species",
            ".",
            " The",
            " largest",
            " living",
            " amphib",
            "ian",
            " is",
            " the",
            " ",
            " Chinese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " david",
            "ian",
            "us",
            ")",
            " but",
            " this",
            " is",
            " a",
            " great",
            " deal",
            " smaller",
            " than",
            " the",
            " largest",
            " amphib",
            "ian",
            " that",
            " ever"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.633,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 44,
          "is_repeated_datapoint": false,
          "tokens": [
            " ",
            "201",
            "2",
            ".",
            " It",
            " has",
            " an",
            " average",
            " length",
            " of",
            " ",
            " and",
            " is",
            " part",
            " of",
            " a",
            " genus",
            " that",
            " contains",
            " four",
            " of",
            " the",
            " world",
            "'s",
            " ten",
            " smallest",
            " frog",
            " species",
            ".",
            " The",
            " largest",
            " living",
            " amphib",
            "ian",
            " is",
            " the",
            " ",
            " Chinese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " david",
            "ian",
            "us",
            ")",
            " but",
            " this",
            " is",
            " a",
            " great",
            " deal",
            " smaller",
            " than",
            " the",
            " largest",
            " amphib",
            "ian",
            " that",
            " ever"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.629,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.307,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " from",
            " the",
            " Chinese",
            " giant",
            " sal",
            "am",
            "ander",
            " (",
            "And",
            "rias",
            " david",
            "ian",
            "us",
            "),",
            " which",
            " has",
            " been",
            " reported",
            " to",
            " grow",
            " to",
            " a",
            " length",
            " of",
            " ,",
            " to",
            " the",
            " dimin",
            "utive",
            " Thor",
            "ius",
            " penn",
            "at",
            "ulus",
            " from",
            " Mexico",
            " which",
            " seldom",
            " exceeds",
            " ",
            " in",
            " length",
            ".",
            " Sal",
            "am",
            "anders",
            " have",
            " a",
            " mostly",
            " Laur",
            "as",
            "ian",
            " distribution",
            ",",
            " being",
            " present",
            " in",
            " much",
            " of",
            " the",
            " H",
            "olar",
            "ctic"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.625,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 37,
          "is_repeated_datapoint": false,
          "tokens": [
            "raman",
            "m",
            "ara",
            "ÅŁ",
            ",",
            " AydÄ±n",
            ",",
            " Ad",
            "ap",
            "azar",
            "Ä±",
            ",",
            " Deniz",
            "li",
            ",",
            " Mu",
            "ÄŁ",
            "la",
            ",",
            " E",
            "ski",
            "ÅŁehir",
            ",",
            " Trab",
            "zon",
            ",",
            " Or",
            "du",
            ",",
            " Af",
            "yon",
            "kar",
            "ah",
            "is",
            "ar",
            ",",
            " S",
            "ivas",
            ",",
            " Tok",
            "at",
            ",",
            " Z",
            "ong",
            "uld",
            "ak",
            ",",
            " K",
            "Ã¼t",
            "ah",
            "ya",
            ",",
            " Ãĩ",
            "an",
            "akk",
            "ale",
            ",",
            " Os",
            "mani",
            "ye",
            ",",
            " Åŀ",
            "Ä±r"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.621,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.469,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            ",",
            " but",
            " he",
            " had",
            " it",
            " legally",
            " changed",
            " after",
            " a",
            " news",
            " article",
            " called",
            " him",
            " \"",
            "M",
            "idas",
            "\"",
            " in",
            " a",
            " derog",
            "atory",
            " fashion",
            ",",
            " which",
            " Mull",
            "igan",
            " took",
            " as",
            " a",
            " compliment",
            ".",
            "Judge",
            " Narr",
            "ag",
            "ans",
            "ett",
            " is",
            " an",
            " American",
            " jur",
            "ist",
            " who",
            " ruled",
            " in",
            " favor",
            " of",
            " Mid",
            "as",
            " Mull",
            "igan",
            " during",
            " the",
            " case",
            " brought",
            " against",
            " him",
            " by",
            " the",
            " incompetent",
            " loan",
            " applicant",
            ".",
            " When"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 3",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.617,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 13,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " Institute",
            " for",
            " Advanced",
            " Study",
            " Albert",
            " –",
            " The",
            " Digital",
            " Repository",
            " of",
            " the",
            " I",
            "AS",
            ",",
            " which",
            " contains",
            " many",
            " digit",
            "ized",
            " original",
            " documents",
            " and",
            " photographs",
            " ",
            "187",
            "9",
            " births",
            "195",
            "5",
            " deaths",
            "20",
            "th",
            "-century",
            " American",
            " engineers",
            "20",
            "th",
            "-century",
            " American",
            " physicists",
            "20",
            "th",
            "-century",
            " American",
            " writers",
            "American",
            " ag",
            "nost",
            "ics",
            "American",
            " democratic",
            " social",
            "ists"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.178,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.613,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 21,
          "is_repeated_datapoint": false,
          "tokens": [
            "Ä±ÄŁ",
            ",",
            " Mal",
            "at",
            "ya",
            ",",
            " Di",
            "yar",
            "bak",
            "Ä±r",
            ",",
            " Kar",
            "ab",
            "Ã¼k",
            ",",
            " Z",
            "ong",
            "uld",
            "ak",
            " and",
            " S",
            "ivas",
            ".",
            " Comm",
            "uter",
            " rail",
            " also",
            " runs",
            " between",
            " the",
            " stations",
            " of",
            " S",
            "inc",
            "an",
            " and",
            " K",
            "aya",
            "ÅŁ",
            ".",
            " On",
            " ",
            "13",
            " March",
            " ",
            "200",
            "9",
            ",",
            " the",
            " new",
            " YÃ¼ksek",
            " H",
            "Ä±z",
            "lÄ±",
            " T",
            "ren",
            " (",
            "Y",
            "HT",
            ")",
            " high",
            "-speed",
            " rail"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 38,
          "is_repeated_datapoint": false,
          "tokens": [
            " by",
            " Jordan",
            "es",
            " to",
            " half",
            " a",
            " million",
            " strong",
            ".",
            "On",
            " April",
            "Âł",
            "7",
            ",",
            " he",
            " captured",
            " Met",
            "z",
            ".",
            " Other",
            " cities",
            " attacked",
            " can",
            " be",
            " determined",
            " by",
            " the",
            " h",
            "agi",
            "ographic",
            " vitae",
            " written",
            " to",
            " commemorate",
            " their",
            " bishops",
            ":",
            " N",
            "icas",
            "ius",
            " was",
            " slaughtered",
            " before",
            " the",
            " altar",
            " of",
            " his",
            " church",
            " in",
            " Rhe",
            "ims",
            ";",
            " Serv",
            "atus",
            " is",
            " alleged",
            " to",
            " have",
            " saved",
            " Tong",
            "eren",
            " with",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.598,
            -0.0
          ],
          "train_token_ind": 61,
          "is_repeated_datapoint": false,
          "tokens": [
            "9",
            ".",
            " This",
            " was",
            " because",
            " the",
            " US",
            " had",
            " agreed",
            " not",
            " to",
            " make",
            " peace",
            " without",
            " France",
            ",",
            " while",
            " Ar",
            "anj",
            "uez",
            " committed",
            " France",
            " to",
            " keep",
            " fighting",
            " until",
            " Spain",
            " recovered",
            " Gibraltar",
            ",",
            " effectively",
            " making",
            " it",
            " a",
            " condition",
            " of",
            " US",
            " independence",
            " without",
            " the",
            " knowledge",
            " of",
            " Congress",
            ".",
            "To",
            " encourage",
            " French",
            " participation",
            " in",
            " the",
            " struggle",
            " for",
            " independence",
            ",",
            " the",
            " US",
            " representative",
            " in",
            " Paris",
            ",",
            " Sil",
            "as",
            " De"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " regenerated",
            " later",
            ",",
            " but",
            " the",
            " energy",
            " cost",
            " to",
            " the",
            " animal",
            " of",
            " replacing",
            " it",
            " is",
            " significant",
            ".",
            "Some",
            " frogs",
            " and",
            " to",
            "ads",
            " inflate",
            " themselves",
            " to",
            " make",
            " themselves",
            " look",
            " large",
            " and",
            " fierce",
            ",",
            " and",
            " some",
            " sp",
            "ad",
            "ef",
            "oot",
            " to",
            "ads",
            " (",
            "Pel",
            "ob",
            "ates",
            " spp",
            ")",
            " scream",
            " and",
            " leap",
            " towards",
            " the",
            " attacker",
            ".",
            " Giant",
            " sal",
            "am",
            "anders",
            " of",
            " the",
            " genus",
            " And",
            "rias",
            ",",
            " as"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 4",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.594,
            -0.0,
            -0.0
          ],
          "train_token_ind": 60,
          "is_repeated_datapoint": false,
          "tokens": [
            " regenerated",
            " later",
            ",",
            " but",
            " the",
            " energy",
            " cost",
            " to",
            " the",
            " animal",
            " of",
            " replacing",
            " it",
            " is",
            " significant",
            ".",
            "Some",
            " frogs",
            " and",
            " to",
            "ads",
            " inflate",
            " themselves",
            " to",
            " make",
            " themselves",
            " look",
            " large",
            " and",
            " fierce",
            ",",
            " and",
            " some",
            " sp",
            "ad",
            "ef",
            "oot",
            " to",
            "ads",
            " (",
            "Pel",
            "ob",
            "ates",
            " spp",
            ")",
            " scream",
            " and",
            " leap",
            " towards",
            " the",
            " attacker",
            ".",
            " Giant",
            " sal",
            "am",
            "anders",
            " of",
            " the",
            " genus",
            " And",
            "rias",
            ",",
            " as"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "6",
            ",",
            " the",
            " committee",
            " shared",
            " information",
            " and",
            " built",
            " alliances",
            " through",
            " secret",
            " correspondence",
            ",",
            " as",
            " well",
            " as",
            " employing",
            " secret",
            " agents",
            " in",
            " Europe",
            " to",
            " gather",
            " intelligence",
            ",",
            " conduct",
            " undercover",
            " operations",
            ",",
            " analyze",
            " foreign",
            " publications",
            ",",
            " and",
            " initiate",
            " Patriot",
            " propaganda",
            " campaigns",
            ".",
            " P",
            "aine",
            " served",
            " as",
            " secretary",
            ",",
            " while",
            " Benjamin",
            " Franklin",
            " and",
            " Sil",
            "as",
            " De",
            "ane",
            ",",
            " sent",
            " to",
            " France",
            " to",
            " recruit",
            " military",
            " engineers",
            ",",
            " were"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.293,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "ias",
            " tried",
            " to",
            " escape",
            ",",
            " he",
            " tri",
            "pped",
            " over",
            " a",
            " vine",
            " and",
            " was",
            " killed",
            " by",
            " his",
            " purs",
            "uers",
            ",",
            " including",
            " two",
            " of",
            " Alexander",
            "'s",
            " companions",
            ",",
            " Per",
            "dic",
            "cas",
            " and",
            " Leon",
            "n",
            "atus",
            ".",
            " Alexander",
            " was",
            " proclaimed",
            " king",
            " on",
            " the",
            " spot",
            " by",
            " the",
            " nob",
            "les",
            " and",
            " army",
            " at",
            " the",
            " age",
            " of",
            " ",
            "20",
            ".",
            "Cons",
            "olid",
            "ation",
            " of",
            " power",
            "Alexander",
            " began",
            " his"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.59,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " Cumberland",
            ".",
            " Nashville",
            " and",
            " central",
            " Tennessee",
            " thus",
            " fell",
            " to",
            " the",
            " Union",
            ",",
            " leading",
            " to",
            " attr",
            "ition",
            " of",
            " local",
            " food",
            " supplies",
            " and",
            " livestock",
            " and",
            " a",
            " breakdown",
            " in",
            " social",
            " organization",
            ".",
            "Leon",
            "idas",
            " Pol",
            "k",
            "'s",
            " invasion",
            " of",
            " Columbus",
            " ended",
            " Kentucky",
            "'s",
            " policy",
            " of",
            " neutrality",
            " and",
            " turned",
            " it",
            " against",
            " the",
            " Confeder",
            "acy",
            ".",
            " Grant",
            " used",
            " river",
            " transport",
            " and",
            " Andrew",
            " Fo",
            "ote",
            "'s",
            " gun",
            "boats"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            "'s",
            " priority",
            ",",
            " the",
            " examiner",
            " approved",
            " Bell",
            "'s",
            " patent",
            " on",
            " March",
            " ",
            "3",
            ",",
            " ",
            "187",
            "6",
            ".",
            " Gray",
            " had",
            " rein",
            "vented",
            " the",
            " variable",
            " resistance",
            " telephone",
            ",",
            " but",
            " Bell",
            " was",
            " the",
            " first",
            " to",
            " write",
            " down",
            " the",
            " idea",
            " and",
            " the",
            " first",
            " to",
            " test",
            " it",
            " in",
            " a",
            " telephone",
            ".",
            "The",
            " patent",
            " examiner",
            ",",
            " Z",
            "enas",
            " F",
            "isk",
            " Wil",
            "ber",
            ",",
            " later",
            " stated",
            " in",
            " an",
            " affidavit"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 5",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "196",
            "8",
            " War",
            "hol",
            " film",
            " I",
            ",",
            " a",
            " Man",
            ".",
            " Earlier",
            " on",
            " the",
            " day",
            " of",
            " the",
            " attack",
            ",",
            " Sol",
            "anas",
            " had",
            " been",
            " turned",
            " away",
            " from",
            " the",
            " Factory",
            " after",
            " asking",
            " for",
            " the",
            " return",
            " of",
            " a",
            " script",
            " she",
            " had",
            " given",
            " to",
            " War",
            "hol",
            ".",
            " The",
            " script",
            " had",
            " apparently",
            " been",
            " misplaced",
            ".",
            "Am",
            "aya",
            " received",
            " only",
            " minor",
            " injuries",
            " and",
            " was",
            " released",
            " from",
            " the",
            " hospital",
            " later",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.586,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            "196",
            "8",
            " War",
            "hol",
            " film",
            " I",
            ",",
            " a",
            " Man",
            ".",
            " Earlier",
            " on",
            " the",
            " day",
            " of",
            " the",
            " attack",
            ",",
            " Sol",
            "anas",
            " had",
            " been",
            " turned",
            " away",
            " from",
            " the",
            " Factory",
            " after",
            " asking",
            " for",
            " the",
            " return",
            " of",
            " a",
            " script",
            " she",
            " had",
            " given",
            " to",
            " War",
            "hol",
            ".",
            " The",
            " script",
            " had",
            " apparently",
            " been",
            " misplaced",
            ".",
            "Am",
            "aya",
            " received",
            " only",
            " minor",
            " injuries",
            " and",
            " was",
            " released",
            " from",
            " the",
            " hospital",
            " later",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 33,
          "is_repeated_datapoint": false,
          "tokens": [
            " July",
            ".",
            " It",
            " quickly",
            " caught",
            " the",
            " public",
            "'s",
            " fancy",
            ".",
            " An",
            " amended",
            " version",
            " was",
            " published",
            " in",
            " ",
            "190",
            "4",
            ".",
            "The",
            " first",
            " known",
            " melody",
            " written",
            " for",
            " the",
            " song",
            " was",
            " sent",
            " in",
            " by",
            " Sil",
            "as",
            " Pratt",
            " when",
            " the",
            " poem",
            " was",
            " published",
            " in",
            " The",
            " Cong",
            "reg",
            "ational",
            "ist",
            ".",
            " By",
            " ",
            "190",
            "0",
            ",",
            " at",
            " least",
            " ",
            "75",
            " different",
            " melodies",
            " had",
            " been",
            " written",
            ".",
            " A"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.582,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " same",
            " day",
            ".",
            " War",
            "hol",
            " was",
            " seriously",
            " wounded",
            " by",
            " the",
            " attack",
            " and",
            " barely",
            " survived",
            ".",
            " He",
            " had",
            " physical",
            " effects",
            " for",
            " the",
            " rest",
            " of",
            " his",
            " life",
            ",",
            " including",
            " being",
            " required",
            " to",
            " wear",
            " a",
            " surgical",
            " cor",
            "set",
            ".",
            " The",
            " shooting",
            " had",
            " a",
            " profound",
            " effect",
            " on",
            " War",
            "hol",
            "'s",
            " life",
            " and",
            " art",
            ".",
            "Sol",
            "anas",
            " was",
            " arrested",
            " the",
            " day",
            " after",
            " the",
            " assault",
            ",",
            " after",
            " turning",
            " herself"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " shot",
            " War",
            "hol",
            " and",
            " Mario",
            " Am",
            "aya",
            ",",
            " art",
            " critic",
            " and",
            " curator",
            ",",
            " at",
            " War",
            "hol",
            "'s",
            " studio",
            ",",
            " The",
            " Factory",
            ".",
            " Before",
            " the",
            " shooting",
            ",",
            " Sol",
            "anas",
            " had",
            " been",
            " a",
            " marginal",
            " figure",
            " in",
            " the",
            " Factory",
            " scene",
            ".",
            " She",
            " authored",
            " the",
            " SC",
            "UM",
            " Manifest",
            "o",
            ",",
            " a",
            " separat",
            "ist",
            " feminist",
            " tract",
            " that",
            " advocated",
            " the",
            " elimination",
            " of",
            " men",
            ";",
            " and",
            " appeared",
            " in",
            " the",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.578,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 27,
          "is_repeated_datapoint": false,
          "tokens": [
            " shot",
            " War",
            "hol",
            " and",
            " Mario",
            " Am",
            "aya",
            ",",
            " art",
            " critic",
            " and",
            " curator",
            ",",
            " at",
            " War",
            "hol",
            "'s",
            " studio",
            ",",
            " The",
            " Factory",
            ".",
            " Before",
            " the",
            " shooting",
            ",",
            " Sol",
            "anas",
            " had",
            " been",
            " a",
            " marginal",
            " figure",
            " in",
            " the",
            " Factory",
            " scene",
            ".",
            " She",
            " authored",
            " the",
            " SC",
            "UM",
            " Manifest",
            "o",
            ",",
            " a",
            " separat",
            "ist",
            " feminist",
            " tract",
            " that",
            " advocated",
            " the",
            " elimination",
            " of",
            " men",
            ";",
            " and",
            " appeared",
            " in",
            " the",
            " "
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            0.574,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.434,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "AS",
            ",",
            " see",
            " Asphalt",
            " Concrete",
            ".",
            "For",
            " information",
            " on",
            " the",
            " different",
            " types",
            " of",
            " R",
            "AS",
            " and",
            " associated",
            " health",
            " and",
            " safety",
            " concerns",
            ",",
            " see",
            " Asphalt",
            " Sh",
            "ingles",
            ".",
            "For",
            " information",
            " on",
            " in",
            "-place",
            " recycling",
            " methods",
            " used",
            " to",
            " restore",
            " pav",
            "ements",
            " and",
            " road",
            "ways",
            ",",
            " see",
            " Road",
            " Surface",
            ".",
            "E",
            "conomics",
            "Although",
            " bit",
            "umen",
            " typically",
            " makes",
            " up",
            " only",
            " ",
            "4",
            " to",
            " ",
            "5",
            " percent"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 17,
          "is_repeated_datapoint": false,
          "tokens": [
            " movement",
            ".",
            " In",
            " June",
            " ",
            "196",
            "8",
            ",",
            " he",
            " was",
            " almost",
            " killed",
            " by",
            " radical",
            " feminist",
            " Valerie",
            " Sol",
            "anas",
            ",",
            " ",
            " who",
            " shot",
            " him",
            " inside",
            " his",
            " studio",
            ".",
            " After",
            " gall",
            "bl",
            "adder",
            " surgery",
            ",",
            " War",
            "hol",
            " died",
            " of",
            " cardiac",
            " arr",
            "hythm",
            "ia",
            " in",
            " February",
            " ",
            "198",
            "7",
            " at",
            " the",
            " age",
            " of",
            " ",
            "58",
            " in",
            " New",
            " York",
            " City",
            ".",
            "War",
            "hol",
            " has",
            " been",
            " the",
            " subject"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.566
          ],
          "train_token_ind": 62,
          "is_repeated_datapoint": false,
          "tokens": [
            ".",
            " Less",
            " well",
            " known",
            " was",
            " his",
            " support",
            " and",
            " collaboration",
            " with",
            " several",
            " teenagers",
            " during",
            " this",
            " era",
            ",",
            " who",
            " would",
            " achieve",
            " prominence",
            " later",
            " in",
            " life",
            " including",
            " writer",
            " David",
            " Dalton",
            ",",
            " photographer",
            " Stephen",
            " Shore",
            " and",
            " artist",
            " Bib",
            "be",
            " Hansen",
            " (",
            "mother",
            " of",
            " pop",
            " musician",
            " Beck",
            ").",
            "196",
            "8",
            " assassination",
            " attempt",
            " ",
            "On",
            " June",
            " ",
            "3",
            ",",
            " ",
            "196",
            "8",
            ",",
            " radical",
            " feminist",
            " writer",
            " Valerie",
            " Sol",
            "anas"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.559,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " and",
            " the",
            " Cumberland",
            " rivers",
            " were",
            " placed",
            " under",
            " the",
            " command",
            " of",
            " Maj",
            ".",
            " Gen",
            ".",
            " Leon",
            "idas",
            " Pol",
            "k",
            " and",
            " Brig",
            ".",
            " Gen",
            ".",
            " G",
            "ideon",
            " J",
            ".",
            " Pillow",
            ".",
            " The",
            " latter",
            " had",
            " initially",
            " been",
            " in",
            " command",
            " in",
            " Tennessee",
            " as",
            " that",
            " State",
            "'s",
            " top",
            " general",
            ".",
            " Their",
            " imp",
            "ol",
            "itic",
            " occupation",
            " of",
            " Columbus",
            ",",
            " Kentucky",
            ",",
            " on",
            " September",
            " ",
            "3",
            ",",
            " ",
            "186",
            "1"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 6",
      "examples": [
        {
          "tokens_acts_list": [
            0.498,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.422,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.008,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.043,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Das",
            " L",
            "Ã¤ch",
            "eln",
            " der",
            " Gio",
            "conda",
            " (",
            "195",
            "3",
            "),",
            " TV",
            " movie",
            " directed",
            " by",
            " Werner",
            " V",
            "Ã¶l",
            "ger",
            ",",
            " based",
            " on",
            " play",
            " Mort",
            "al",
            " Co",
            "ils",
            ":",
            " Play",
            " Das",
            " L",
            "Ã¤ch",
            "eln",
            " der",
            " Gio",
            "conda",
            " (",
            "195",
            "8",
            "),",
            " TV",
            " movie",
            " directed",
            " by",
            " Michael",
            " K",
            "eh",
            "l",
            "mann",
            ",",
            " based",
            " on",
            " no",
            "ve",
            "lette",
            " \"",
            "The",
            " Gio",
            "conda",
            " Smile",
            "\""
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.473,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 31,
          "is_repeated_datapoint": false,
          "tokens": [
            " also",
            " had",
            " very",
            " contrasting",
            " playing",
            " styles",
            ",",
            " with",
            " Sam",
            "pras",
            " being",
            " considered",
            " the",
            " greatest",
            " server",
            " and",
            " Ag",
            "assi",
            " the",
            " greatest",
            " serve",
            " return",
            "er",
            " at",
            " the",
            " time",
            ".",
            " Ag",
            "assi",
            " and",
            " Sam",
            "pras",
            " met",
            " ",
            "34",
            " times",
            " on",
            " the",
            " tour",
            " level",
            " with",
            " Ag",
            "assi",
            " trailing",
            " ",
            "14",
            "–",
            "20",
            ".",
            "The",
            " ",
            "199",
            "0",
            " US",
            " Open",
            " was",
            " their",
            " first",
            " meeting",
            " in",
            " a",
            " Grand",
            " Slam"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.461,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.465,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 20,
          "is_repeated_datapoint": false,
          "tokens": [
            " day",
            " when",
            " Ag",
            "assi",
            " lost",
            " the",
            " final",
            " to",
            " Sam",
            "pras",
            ".",
            " Ag",
            "assi",
            " admitted",
            " this",
            " loss",
            ",",
            " which",
            " gave",
            " Sam",
            "pras",
            " a",
            " ",
            "9",
            "–",
            "8",
            " lead",
            " in",
            " their",
            " head",
            "-to",
            "-head",
            " meetings",
            ",",
            " took",
            " two",
            " years",
            " for",
            " him",
            " to",
            " get",
            " over",
            " mentally",
            ".",
            "Ag",
            "assi",
            " reached",
            " the",
            " world",
            " No",
            ".",
            " ",
            "1",
            " ranking",
            " for",
            " the",
            " first",
            " time",
            " in",
            " April",
            " ",
            "199",
            "5"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.449,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 15,
          "is_repeated_datapoint": false,
          "tokens": [
            " tar",
            ",",
            " resin",
            " or",
            " bit",
            "umen",
            ".",
            "See",
            " also",
            " Asphalt",
            " plant",
            " Asphalt",
            "ene",
            " ",
            " Bio",
            "as",
            "phalt",
            " Bit",
            "umen",
            "-based",
            " fuel",
            " Bit",
            "umin",
            "ous",
            " rocks",
            " Black",
            "top",
            " Car",
            "ip",
            "hal",
            "te",
            " D",
            "ux",
            "it",
            " Mac",
            "adam",
            " Oil",
            " sands",
            " Pitch",
            " drop",
            " experiment",
            " Pitch",
            " (",
            "res",
            "in",
            ")",
            " Road",
            " surface",
            " Tar",
            " T"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.445,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 24,
          "is_repeated_datapoint": false,
          "tokens": [
            " ,",
            " about",
            " ",
            "40",
            "%",
            " of",
            " the",
            " total",
            ".",
            " Adding",
            " in",
            " the",
            " next",
            " three",
            " most",
            " massive",
            " objects",
            ",",
            " V",
            "esta",
            " (",
            "11",
            "%),",
            " P",
            "allas",
            " (",
            "8",
            ".",
            "5",
            "%),",
            " and",
            " Hy",
            "gie",
            "a",
            " (",
            "3",
            "–",
            "4",
            "%),",
            " brings",
            " this",
            " figure",
            " up",
            " to",
            " a",
            " bit",
            " over",
            " ",
            "60",
            "%,",
            " whereas",
            " the",
            " next",
            " seven",
            " most",
            "-m",
            "ass",
            "ive",
            " asteroids",
            " bring",
            " the",
            " total",
            " up"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 7",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.436,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 51,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " design",
            "ating",
            " Ant",
            "ip",
            "ater",
            ",",
            " recently",
            " removed",
            " as",
            " Maced",
            "onian",
            " v",
            "icer",
            "oy",
            ",",
            " replaced",
            " by",
            " Cr",
            "ater",
            "us",
            ",",
            " as",
            " the",
            " head",
            " of",
            " the",
            " alleged",
            " plot",
            ".",
            " Perhaps",
            " taking",
            " his",
            " summons",
            " to",
            " Babylon",
            " as",
            " a",
            " death",
            " sentence",
            " and",
            " having",
            " seen",
            " the",
            " fate",
            " of",
            " Parm",
            "en",
            "ion",
            " and",
            " Phil",
            "otas",
            ",",
            " Ant",
            "ip",
            "ater",
            " purported",
            "ly",
            " arranged",
            " for",
            " Alexander",
            " to",
            " be"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.408
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            "ort",
            "ense",
            " Powder",
            "maker",
            "A",
            ".H",
            ".J",
            ".",
            " Pr",
            "ins",
            "Har",
            "ald",
            " E",
            ".L",
            ".",
            " Pr",
            "ins",
            "Q",
            " ",
            "B",
            "uell",
            " Qu",
            "ain",
            "James",
            " Q",
            "ues",
            "ada",
            "R",
            "Paul",
            " R",
            "abin",
            "ow",
            "Wil",
            "helm",
            " Rad",
            "loff",
            "La",
            "urence",
            " Ralph",
            "Luc",
            "inda",
            " Ram",
            "berg",
            "Roy",
            " R",
            "app",
            "ap",
            "ort",
            "H",
            "ans",
            " Ras"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.404,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 35,
          "is_repeated_datapoint": false,
          "tokens": [
            " among",
            " the",
            " earliest",
            " recorded",
            " tribes",
            " in",
            " the",
            " area",
            ".",
            " They",
            " lived",
            " in",
            " a",
            " area",
            " that",
            " corresponds",
            " much",
            " of",
            " present",
            "-day",
            " Albania",
            ".",
            " Together",
            " with",
            " the",
            " D",
            "ard",
            "anian",
            " ruler",
            " Cle",
            "itus",
            ",",
            " G",
            "la",
            "uc",
            "ias",
            ",",
            " the",
            " ruler",
            " of",
            " the",
            " T",
            "aul",
            "ant",
            "ian",
            " kingdom",
            ",",
            " fought",
            " against",
            " Alexander",
            " the",
            " Great",
            " at",
            " the",
            " Battle",
            " of",
            " Pel",
            "ium",
            " in",
            " ",
            "335",
            " BC",
            "."
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.381,
            0.074,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.007,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 19,
          "is_repeated_datapoint": false,
          "tokens": [
            " becoming",
            " tennis",
            " players",
            ".",
            " The",
            " Graf",
            "-A",
            "g",
            "assi",
            " family",
            " resides",
            " in",
            " Summer",
            "lin",
            ",",
            " a",
            " community",
            " in",
            " the",
            " Las",
            " Vegas",
            " Valley",
            ".",
            " Graf",
            "'s",
            " mother",
            " and",
            " brother",
            ",",
            " Michael",
            ",",
            " with",
            " his",
            " four",
            " children",
            ",",
            " also",
            " live",
            " there",
            ".",
            "Long",
            "-time",
            " trainer",
            " Gil",
            " Reyes",
            " has",
            " been",
            " called",
            " one",
            " of",
            " Ag",
            "assi",
            "'s",
            " closest",
            " friends",
            ";",
            " some",
            " have",
            " described",
            " him",
            " as",
            " being",
            " a"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.287,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.357,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 59,
          "is_repeated_datapoint": false,
          "tokens": [
            " reign",
            " by",
            " eliminating",
            " potential",
            " rivals",
            " to",
            " the",
            " throne",
            ".",
            " He",
            " had",
            " his",
            " cousin",
            ",",
            " the",
            " former",
            " Amy",
            "nt",
            "as",
            " IV",
            ",",
            " executed",
            ".",
            " He",
            " also",
            " had",
            " two",
            " Maced",
            "onian",
            " princes",
            " from",
            " the",
            " region",
            " of",
            " Lyn",
            "cest",
            "is",
            " killed",
            " for",
            " having",
            " been",
            " involved",
            " in",
            " his",
            " father",
            "'s",
            " assassination",
            ",",
            " but",
            " spared",
            " a",
            " third",
            ",",
            " Alexander",
            " Lyn",
            "cest",
            "es",
            ".",
            " Olymp",
            "ias",
            " had",
            " Cle",
            "op"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Subsample interval 8",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.301,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 50,
          "is_repeated_datapoint": false,
          "tokens": [
            " The",
            " sn",
            "out",
            " resembles",
            " an",
            " elong",
            "ated",
            " pig",
            " sn",
            "out",
            ".",
            " The",
            " mouth",
            " is",
            " small",
            " and",
            " tub",
            "ular",
            ",",
            " typical",
            " of",
            " species",
            " that",
            " feed",
            " on",
            " ants",
            " and",
            " ter",
            "mites",
            ".",
            " The",
            " a",
            "ard",
            "v",
            "ark",
            " has",
            " a",
            " long",
            ",",
            " thin",
            ",",
            " sn",
            "ak",
            "el",
            "ike",
            ",",
            " protr",
            "uding",
            " tongue",
            " (",
            "as",
            " much",
            " as",
            " ",
            " long",
            ")",
            " and",
            " elaborate",
            " structures",
            " supporting",
            " a",
            " keen",
            " sense"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.277,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 53,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " fertil",
            "izers",
            ".",
            " In",
            " agricultural",
            " ent",
            "om",
            "ology",
            ",",
            " the",
            " USDA",
            " began",
            " to",
            " research",
            " biological",
            " control",
            " in",
            " ",
            "188",
            "1",
            ";",
            " it",
            " instituted",
            " its",
            " first",
            " large",
            " program",
            " in",
            " ",
            "190",
            "5",
            ",",
            " searching",
            " Europe",
            " and",
            " Japan",
            " for",
            " natural",
            " enemies",
            " of",
            " the",
            " s",
            "pon",
            "gy",
            " moth",
            " and",
            " brown",
            "-t",
            "ail",
            " moth",
            ",",
            " establishing",
            " paras",
            "it",
            "oids",
            " (",
            "such",
            " as",
            " solitary",
            " was",
            "ps",
            ")"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.272,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 41,
          "is_repeated_datapoint": false,
          "tokens": [
            "200",
            "7",
            ".",
            " The",
            " Ge",
            "odynamics",
            " of",
            " the",
            " Ae",
            "ge",
            "an",
            " and",
            " Anat",
            "olia",
            ".",
            " London",
            ":",
            " Geological",
            " Society",
            ".",
            "External",
            " links",
            " ",
            " ",
            "Anc",
            "ient",
            " Greek",
            " geography",
            "Ge",
            "ography",
            " of",
            " the",
            " Middle",
            " East",
            "Histor",
            "ical",
            " regions",
            " in",
            " Turkey",
            "Pen",
            "ins",
            "ulas",
            " of",
            " Asia",
            "Pen",
            "ins",
            "ulas",
            " of",
            " Turkey",
            "Phys",
            "i",
            "ographic",
            " provinces",
            "Regions",
            " of",
            " Asia"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.266,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 52,
          "is_repeated_datapoint": false,
          "tokens": [
            " people",
            ".",
            " The",
            " origin",
            " of",
            " A",
            "raw",
            "ak",
            " civilization",
            " (",
            "a",
            " name",
            " based",
            " on",
            " a",
            " linguistic",
            " classification",
            ")",
            " is",
            " located",
            " in",
            " the",
            " central",
            " Amazon",
            " region",
            ".",
            " Between",
            " ",
            "150",
            "0",
            " and",
            " ",
            "500",
            " BC",
            ",",
            " the",
            " influence",
            " of",
            " the",
            " A",
            "raw",
            "aks",
            " had",
            " expanded",
            " to",
            " the",
            " Caribbean",
            " basin",
            " and",
            " the",
            " Gu",
            "ian",
            "as",
            ".",
            " Between",
            " ",
            "850",
            " and",
            " ",
            "100",
            "0",
            " AD",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.227,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 9,
          "is_repeated_datapoint": false,
          "tokens": [
            " the",
            " families",
            " of",
            " soldiers",
            " who",
            " died",
            " in",
            " battle",
            ".",
            " As",
            " Young",
            " Heg",
            "el",
            "ians",
            " were",
            " advocating",
            " change",
            " and",
            " progress",
            ",",
            " Sch",
            "openh",
            "auer",
            " claimed",
            " that",
            " misery",
            " is",
            " natural",
            " for",
            " humans",
            " and",
            " that",
            ",",
            " even",
            " if",
            " some",
            " ut",
            "opian",
            " society",
            " were",
            " established",
            ",",
            " people",
            " would",
            " still",
            " fight",
            " each",
            " other",
            " out",
            " of",
            " boredom",
            ",",
            " or",
            " would",
            " star",
            "ve",
            " due",
            " to",
            " over",
            "population",
            ".",
            "In",
            " "
          ],
          "ha_haiku35_resampled": null
        }
      ]
    },
    {
      "quantile_name": "Bottom 1%",
      "examples": [
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.211,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.209,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            0.221,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 39,
          "is_repeated_datapoint": false,
          "tokens": [
            "ogenesis",
            ",",
            " which",
            " the",
            " '",
            "lower",
            "'",
            " As",
            "par",
            "ag",
            "ales",
            " families",
            " retain",
            ".",
            " However",
            ",",
            " the",
            " '",
            "core",
            "'",
            " As",
            "par",
            "ag",
            "ales",
            " (",
            "see",
            " Phy",
            "logen",
            "etics",
            " )",
            " have",
            " reverted",
            " to",
            " successive",
            " micro",
            "spor",
            "ogenesis",
            ".",
            " The",
            " As",
            "par",
            "ag",
            "ales",
            " appear",
            " to",
            " be",
            " unified",
            " by",
            " a",
            " mutation",
            " affecting",
            " their",
            " tel",
            "om",
            "eres",
            " (",
            "a",
            " region",
            " of",
            " repetitive",
            " DNA",
            " at",
            " the"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " also",
            " expressed",
            " by",
            " A",
            ".",
            " K",
            "lingen",
            "he",
            "ben",
            " and",
            " Diet",
            "rich",
            " West",
            "ermann",
            " during",
            " the",
            " ",
            "192",
            "0",
            "s",
            " and",
            " '",
            "30",
            "s",
            ".",
            " However",
            ",",
            " Me",
            "inh",
            "of",
            "'s",
            " \"",
            "Ham",
            "itic",
            "\"",
            " classification",
            " remained",
            " prevalent",
            " throughout",
            " the",
            " early",
            " ",
            "20",
            "th",
            " century",
            " until",
            " it",
            " was",
            " definit",
            "ively",
            " dispro",
            "ven",
            " by",
            " Joseph",
            " Green",
            "berg",
            " in",
            " the",
            " ",
            "194",
            "0",
            "s",
            ","
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " Conversion",
            " of",
            " separate",
            " words",
            " l",
            "Ä«",
            " '",
            "to",
            " me",
            "',",
            " l",
            "aka",
            " '",
            "to",
            " you",
            "',",
            " etc",
            ".",
            " into",
            " indirect",
            "-object",
            " cl",
            "itic",
            " suffix",
            "es",
            ".",
            " Certain",
            " changes",
            " in",
            " the",
            " cardinal",
            " number",
            " system",
            ",",
            " e",
            ".g",
            ".,",
            " ",
            " '",
            "five",
            " days",
            "'",
            " âĨĴ",
            " ,",
            " where",
            " certain",
            " words",
            " have",
            " a",
            " special",
            " plural",
            " with",
            " prefixed",
            " t",
            ".",
            " Loss",
            " of",
            " the",
            " feminine",
            " el",
            "ative",
            " (",
            "com"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            " in",
            " Amsterdam",
            ".",
            " During",
            " the",
            " same",
            " year",
            ",",
            " Amsterdam",
            " was",
            " designated",
            " as",
            " the",
            " World",
            " Book",
            " Capital",
            " for",
            " one",
            " year",
            " by",
            " UNESCO",
            ".",
            "F",
            "amous",
            " festivals",
            " and",
            " events",
            " in",
            " Amsterdam",
            " include",
            ":",
            " Kon",
            "ings",
            "dag",
            " (",
            "which",
            " was",
            " named",
            " K",
            "oning",
            "inned",
            "ag",
            " until",
            " the",
            " crow",
            "ning",
            " of",
            " King",
            " Wil",
            "lem",
            "-A",
            "lex",
            "ander",
            " in",
            " ",
            "201",
            "3",
            ")",
            " (",
            "King",
            "'s",
            " Day",
            " –"
          ],
          "ha_haiku35_resampled": null
        },
        {
          "tokens_acts_list": [
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0,
            -0.0
          ],
          "train_token_ind": 0,
          "is_repeated_datapoint": false,
          "tokens": [
            "avery",
            "Dis",
            "ag",
            "reements",
            " among",
            " states",
            " about",
            " the",
            " future",
            " of",
            " slavery",
            " were",
            " the",
            " main",
            " cause",
            " of",
            " dis",
            "union",
            " and",
            " the",
            " war",
            " that",
            " followed",
            ".",
            " Sl",
            "avery",
            " had",
            " been",
            " controversial",
            " during",
            " the",
            " framing",
            " of",
            " the",
            " Constitution",
            ",",
            " which",
            ",",
            " because",
            " of",
            " compromises",
            ",",
            " ended",
            " up",
            " with",
            " pro",
            "sl",
            "avery",
            " and",
            " ant",
            "isl",
            "avery",
            " features",
            ".",
            " The",
            " issue",
            " of",
            " slavery",
            " had",
            " conf",
            "ounded",
            " the"
          ],
          "ha_haiku35_resampled": null
        }
      ]
    }
  ],
  "top_logits": [
    "mini",
    "ethyst",
    "phalt",
    "sm",
    "ph"
  ],
  "bottom_logits": [
    "WithValue",
    "âĶĶ",
    "@s",
    "Ã¨o",
    " Ortiz"
  ],
  "act_min": -0.0,
  "act_max": 0.703
}
